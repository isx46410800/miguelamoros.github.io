<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Miguel Amorós">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Kubernetes - Miguel's Notes</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Kubernetes";
    var mkdocs_page_input_path = "kubernetes.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Miguel's Notes</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">MkDocs</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../linux/">Linux</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../gitlab/">Gitlab</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bash_scripting/">BashScripting</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../docker/">Docker</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Kubernetes</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#arquitectura">ARQUITECTURA</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#instalacion-minikubekubectl">INSTALACIÓN MINIKUBE/KUBECTL</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods-vs-contenedores">PODS VS CONTENEDORES</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods">PODS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod">CREAR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs-pods">LOGS PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#api-resources">API-RESOURCES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminar-pods">ELIMINAR PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#obtener-yaml-pod">OBTENER YAML POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ip-pod">IP POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#entrar-al-pod">ENTRAR AL POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod-yaml">CREAR POD YAML</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-container-por-pod">2+ CONTAINER POR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#labels">LABELS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas-pods">PROBLEMAS PODs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#replicasets">REPLICASETS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-replicaset">CREAR REPLICASET</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminarmodificar">ELIMINAR/MODIFICAR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs">LOGS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#owner-refernce">OWNER REFERNCE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#adopcion-de-pods-planos">ADOPCIÓN DE PODS PLANOS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas">PROBLEMAS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#deployments">DEPLOYMENTS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-deployment">CREAR DEPLOYMENT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rolling-update">ROLLING UPDATE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#historial-de-deployments">HISTORIAL DE DEPLOYMENTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#roll-backs">ROLL BACKS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#servicios">SERVICIOS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-servicio">CREAR SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#info-servicio">INFO SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#endpoints">ENDPOINTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns">DNS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-cluster-ip">SERVICIO CLUSTER-IP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-node-port">SERVICIO NODE-PORT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-load-balancer">SERVICIO LOAD BALANCER</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#golang">GOLANG</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-api-rest-go">CREAR API REST GO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mensaje-1">MENSAJE 1</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#mensaje-2">MENSAJE 2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dockerfile-golang">DOCKERFILE GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deployment-golang">DEPLOYMENT GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#consumo-del-servicio">CONSUMO DEL SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fronted">FRONTED</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#manifiesto-fronted">MANIFIESTO FRONTED</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#namespaces">NAMESPACES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-namespace">CREAR NAMESPACE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#asignar-namespaces">ASIGNAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#borrar-namespaces">BORRAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-namespaces">DEPLOY NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns-namespaces">DNS NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#contextos-namespaces">CONTEXTOS NAMESPACES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#limitar-ramcpu">LIMITAR RAM/CPU</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#limitsrequest">LIMITS/REQUEST</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#ram">RAM</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpu">CPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#qosquality-of-service">QOS(Quality of Service)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#limitrange">LIMITRANGE</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#valores-por-defecto">VALORES POR DEFECTO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#valores-pod">VALORES POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limites">LIMITES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#resource-quota">RESOURCE QUOTA</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-rq">CREAR RQ</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-rq">DEPLOY RQ</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limitar-no-pods-en-ns">LIMITAR Nº PODS EN NS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#probes">PROBES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#tipos-probes">TIPOS PROBES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-liveness-probe">CREAR LIVENESS PROBE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#liveness-tcp">LIVENESS TCP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#liveness-http">LIVENESS HTTP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#readiness-probe">READINESS PROBE</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#variables-y-configmap">VARIABLES Y CONFIGMAP</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-variables">CREAR VARIABLES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#variables-referenciadas">VARIABLES REFERENCIADAS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configmap">CONFIGMAP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#montando-volumen-configmap">MONTANDO VOLUMEN CONFIGMAP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#volumen-env-configmap">VOLUMEN-ENV CONFIGMAP</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#secrets">SECRETS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear">CREAR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#manifiestos">MANIFIESTOS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#envsubts">ENVSUBTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#volume-secrets">VOLUME SECRETS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#env-secrets">ENV SECRETS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#volumes">VOLUMES</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../jenkins/">Jenkins</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ansible/">Ansible</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../markdown/">Markdown</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bbdd/">Base de datos</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../files/">Archivos Destacados</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Miguel's Notes</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Kubernetes</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/isx46410800/miguelamoros.github.io/edit/master/docs/kubernetes.md"> Edit on isx46410800/python</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="kubernetes">KUBERNETES</h1>
<ul>
<li>
<p><code>K8S</code> Es una herramienta extensible y de código abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuración declarativa como la automatización. Tiene un ecosistema grande y de rápido crecimiento. Los servicios, el soporte y las herramientas están ampliamente disponibles.  </p>
</li>
<li>
<p>Funciones:  </p>
<ul>
<li>Service discovery: mira cuantos nodos hay, los escanea para saber de ellos.  </li>
<li>Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma.  </li>
<li>Optimización de recursos en nodos: mira donde colocar el contenedor al host con menos carga.  </li>
<li>Self-healing: crea automaticamente un contenedor cuando uno muere.  </li>
<li>Configuración de secretos</li>
<li>Escalamiento horizontal</li>
</ul>
</li>
</ul>
<h2 id="arquitectura">ARQUITECTURA</h2>
<p><img alt="" src="../images/kubernetes.png" />  </p>
<ul>
<li>
<p><strong>MASTER/NODE</strong>: Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las máquinas, host, máquinas virutal.<br />
El master es como la aduana y los nodes son  los barcos que se llevan los contenedores de la duana.  </p>
</li>
<li>
<p><strong>API SERVER</strong>: Aplication Program Interface, significa que yo me puedo comunicar con un servicio a través de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programación. Ambos son en JSON, por lo que acaba procesando todo en código JSON.  </p>
</li>
<li>
<p><strong>KUBE-SCHEDULE</strong>: es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y éste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor.  </p>
</li>
<li>
<p><strong>KUBE-CONTROLLER</strong>: dentro tiene el <em>node controler</em> (se encarga de ver nodos, si se cae uno, levanta otra máquina), el <em>replication</em>(encargado de mantener todas las réplicas especificadas), el <em>end point controller</em>(se encarga de la red y pods) y tenemos el <em>service account y tokens controller</em>(para la autenticación).  </p>
</li>
<li>
<p><strong>ETCD</strong>: es la base de datos de kubernetes donde están todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versión nueva y queremos volver atrás, en el <em>etcd</em> está guardado el estado y configuración anterior.  </p>
</li>
<li>
<p><strong>KUBELET</strong>: se encuentra en cada nodo y tienen dos funciones, en enviar y recibir información al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo.  </p>
</li>
<li>
<p><strong>KUBE-PROXY</strong>:  se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods.  </p>
</li>
<li>
<p><strong>CONTAINER-RUNTIME</strong>: el software de contenedores que tiene instalado el nodo: docker,etc.  </p>
</li>
</ul>
<h2 id="instalacion-minikubekubectl">INSTALACIÓN MINIKUBE/KUBECTL</h2>
<ul>
<li>
<p><strong>MINIKUBE</strong>: crea o simula un cluster pequeño que nos permite hacerlo en local.  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Documentación Kubernetes</a>  </p>
</li>
<li>
<p>Ejecutamos esta orden y sino sale vacío , vamos bien:<br />
<code>grep -E --color 'vmx|svm' /proc/cpuinfo</code>  </p>
</li>
<li>
<p>Instalamos <code>kubectl</code>, la intermediario para hablar con kubernetes:  </p>
<ul>
<li>
<p><code>curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"</code>  </p>
</li>
<li>
<p><code>chmod +x ./kubectl</code>  </p>
</li>
<li>
<p><code>sudo mv ./kubectl /usr/bin/kubectl</code>  </p>
</li>
<li>
<p><code>kubectl version --client</code>  </p>
</li>
</ul>
</li>
<li>
<p>Para usar minikube se necesita un <code>Hypervisor</code>(o monitor de máquina virtual (virtual machine monitor)1​ es una plataforma que permite aplicar diversas técnicas de control de virtualización para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora):  </p>
<ul>
<li>KVM</li>
<li>VirtualBox</li>
<li>Docker</li>
</ul>
</li>
<li>
<p>Descargamos <code>minikube</code>:  </p>
<ul>
<li>
<p><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube</code>  </p>
</li>
<li>
<p><code>sudo mv minikube /usr/bin/</code>  </p>
</li>
<li>
<p><code>minikube status</code>  </p>
</li>
</ul>
<p><code>[isx46410800@miguel curso_kubernetes]$ minikube status
🤷  There is no local cluster named "minikube"
👉  To fix this, run: "minikube start"
[isx46410800@miguel curso_kubernetes]$ minikube start
😄  minikube v1.13.1 on Fedora 27
✨  Automatically selected the docker driver
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
💾  Downloading Kubernetes v1.19.2 preload ...
    &gt; preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB
🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
🧯  Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity)
💡  Suggestion: 
    Try at least one of the following to free up space on the device:
    1. Run "docker system prune" to remove unused docker data
    2. Increase the amount of memory allocated to Docker for Desktop via
    Docker icon &gt; Preferences &gt; Resources &gt; Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the docker container runtime
🍿  Related issue: https://github.com/kubernetes/minikube/issues/9024
🐳  Preparing Kubernetes v1.19.2 on Docker 19.03.8 ...
🔎  Verifying Kubernetes components...
🌟  Enabled addons: default-storageclass, storage-provisioner
🏄  Done! kubectl is now configured to use "minikube" by default</code>  </p>
</li>
<li>
<p>Comprobamos de nuevo que sí funciona <code>minikube status</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>

<ul>
<li>
<p><strong>COMANDOS BÁSICOS MINIKUBE</strong>:  </p>
<ul>
<li><code>minikube status</code></li>
<li><code>minikube stop/start/delete</code></li>
</ul>
</li>
<li>
<p>Repositorio <a href="https://github.com/ricardoandre97/k8s-resources">curso Kubernetes</a>  </p>
</li>
</ul>
<h2 id="pods-vs-contenedores">PODS VS CONTENEDORES</h2>
<p><img alt="" src="../images/kubernetes2.png" />  </p>
<ul>
<li>
<p>Los <strong>contenedores</strong> se ejecutan de manera aislada en un namespace:  </p>
<ul>
<li>IPC (Inter Process Communication)</li>
<li>Cgroup</li>
<li>Network</li>
<li>Mount</li>
<li>PID</li>
<li>User</li>
<li>UTS (Unix Timesharing System)</li>
</ul>
</li>
<li>
<p>Los <strong>PODS</strong> sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como:  </p>
<ul>
<li>De red(verse en la misma red)</li>
<li>IPC(verse los procesos)</li>
<li>UTS</li>
</ul>
</li>
</ul>
<blockquote>
<p>Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.</p>
</blockquote>
<h2 id="pods">PODS</h2>
<h3 id="crear-pod">CREAR POD</h3>
<ul>
<li>
<p>Primero tenemos que tener encendido el simulador:<br />
<code>minikube start</code>  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/reference/kubectl/conventions/">Documentación</a>:<br />
<code>versión v1.19 la última</code>  </p>
</li>
<li>
<p>Creamos un pod de prueba <code>kubectl run nombrePod --image:xxx:tag</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ 

pod/pod-test created
</code></pre>

<ul>
<li>Vemos que lo hemos creado y está corriendo:  </li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
pod-test   1/1     Running   0          22s
</code></pre>

<blockquote>
<p>Normalmente hay un contenedor por pod, se suele asimilar a eso.  </p>
</blockquote>
<h3 id="logs-pods">LOGS PODS</h3>
<ul>
<li>
<p>Un pod es la unidad más pequeña para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre.   </p>
</li>
<li>
<p>Creamos uno pod mal aposta para ver el error:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll
pod/pod-test2 created
[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME        READY   STATUS         RESTARTS   AGE
pod-test    1/1     Running        0          5m19s
pod-test2   0/1     ErrImagePull   0          14s
</code></pre>

<ul>
<li>Para ver los ´logs´ usamos <code>kubectl describe pod nombrePod</code>:<br />
<code>kubectl describe pod pod-test</code>  <blockquote>
<p>En el apartado <code>events</code> nos describe los logs paso a paso.  </p>
</blockquote>
</li>
</ul>
<h3 id="api-resources">API-RESOURCES</h3>
<ul>
<li>Para ver todos los recursos que hay y los shortnames de comandos se usa:<br />
<code>kubectl api-resources</code>  </li>
</ul>
<h3 id="eliminar-pods">ELIMINAR PODS</h3>
<ul>
<li>
<p>Para eliminar pods usamos <code>kubectl delete pod podName ...</code>:<br />
<code>kubectl delete pod pod-test2</code>  </p>
</li>
<li>
<p>Todos:<br />
<code>kubectl delete pod --all</code>  </p>
</li>
</ul>
<h3 id="obtener-yaml-pod">OBTENER YAML POD</h3>
<ul>
<li>
<p>Podemos obtener info solo del pod concreto:<br />
<code>kubectl get pod pod-test</code></p>
</li>
<li>
<p>Para más info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request:<br />
<code>kubectl get pod pod-test -o yaml</code>  </p>
</li>
<li>
<p>Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a través de un fichero será mejor que no ir poniendo una orden 50 veces.  </p>
</li>
</ul>
<h3 id="ip-pod">IP POD</h3>
<ul>
<li>Para poder ver la IP del POD podemos usar cualquiera de estos comandos:<br />
<code>kubectl describe pod pod-test</code><br />
<code>kubectl get pod pod-test -o yaml</code>  </li>
</ul>
<blockquote>
<p>En este caso es 172.18.0.3  </p>
</blockquote>
<ul>
<li>
<p>Para verlo ingresamos directamente al navegador la ip.  </p>
</li>
<li>
<p>Si no funciona tenemos que mapear el puerto:<br />
<code>kubectl port-forward pod-test 7000:80</code>  </p>
</li>
</ul>
<p><img alt="" src="../images/kubernetes4.png" />  </p>
<ul>
<li>Comprobamos la respuesta:<br />
<code>curl 172.18.0.3:80</code>  </li>
</ul>
<h3 id="entrar-al-pod">ENTRAR AL POD</h3>
<ul>
<li>
<p>Para ingresar a la consola del POD:<br />
<code>kubectl exec -it pod-test -- sh</code>  </p>
<blockquote>
<p>Cuando solo hay un contenedor, no se especifica el nombre del pod.  </p>
</blockquote>
</li>
<li>
<p>Cuando hay más contenedores <code>c, --container=''</code>:<br />
<code>kubectl exec -it pod-test -c containerName -- sh</code>  </p>
</li>
</ul>
<h3 id="crear-pod-yaml">CREAR POD YAML</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>
<p>Las api versions las podemos ver en:<br />
<code>kubectl api-versions</code>  </p>
</li>
<li>
<p>Los kind los podemos ver en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Para crearlo a través del fichero YAML:<br />
<code>kubectl apply -f pod.yaml</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl apply -f pod.yaml
pod/pod-test2 created
[isx46410800@miguel pods]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test    1/1     Running   0          6h54m
pod-test2   1/1     Running   0          7s
</code></pre>

<ul>
<li>
<p>Para borrarlo:<br />
<code>kubectl delete -f pod.yaml</code>  </p>
</li>
<li>
<p>Para crear dos o más PODS, se pone <code>---</code> de separación:  </p>
</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
spec:
  containers:
    - name: container2
      image: nginx:alpine
</code></pre>

<h3 id="2-container-por-pod">2+ CONTAINER POR POD</h3>
<ul>
<li>Para crear dos o  más containers en un POD se añade en la subsección containers:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
    - name: container2
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
</code></pre>

<blockquote>
<p>Nos dará error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente.  </p>
</blockquote>
<ul>
<li>Vemos los <code>logs</code> en <code>kubectl logs podName -c container</code>:  </li>
</ul>
<pre><code>263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2
Traceback (most recent call last):
...
  File &quot;/usr/local/lib/python3.6/socketserver.py&quot;, line 470, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address in use
</code></pre>

<ul>
<li>Arreglamos el fallo del puerto y comprobamos cada container del POD:  </li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh
/ # cat index.html 
cont1
/ # exit
[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh
/ # cat index.html 
cont2
</code></pre>

<h3 id="labels">LABELS</h3>
<ul>
<li>Los labels son etiquetas que se ponen debajo de los <code>metadata</code>:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
  labels:
    app: front-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
---   
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>Los labels nos sirve para poder filtrar PODs con <code>kubectl get pods -l nombre=valor</code>:</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl get pods -l app=back-end
NAME        READY   STATUS    RESTARTS   AGE
pod-test3   1/1     Running   0          62s
[isx46410800@miguel pods]$ kubectl get pods -l env=dev
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   1/1     Running   0          78s
pod-test3   1/1     Running   0          78s
</code></pre>

<blockquote>
<p>Los LABELS más usado es el de APP. Muy importantes para administrar replicas.  </p>
</blockquote>
<h3 id="problemas-pods">PROBLEMAS PODs</h3>
<ul>
<li>
<p>Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion <code>image</code> y al hacer el apply puede ser que te deje actualizar.  </p>
</li>
</ul>
<h2 id="replicasets">REPLICASETS</h2>
<ul>
<li>
<p>Es un objeto separado del POD a un nivel más alto(el replicaset crea PODs y es su dueño).  </p>
</li>
<li>
<p>Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar.  </p>
</li>
<li>
<p>En la metadata del POD mete el <code>OWNER REFERENCE</code> para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.  </p>
</li>
</ul>
<h3 id="crear-replicaset">CREAR REPLICASET</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los replicasets en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-test
  labels:
    app: rs-test
spec:
  # modify replicas according to your case
  replicas: 5
  selector:
    matchLabels:
      app: pod-label
  # pertenece a los PODs que vas a crear
  template:
    metadata:
      labels:
        app: pod-label
    spec:
      containers:
        - name: container1
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
        - name: container2
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8083']
</code></pre>

<ul>
<li>
<p>Lo creamos:<br />
<code>kubectl apply -f replica-set.yaml</code>  </p>
<blockquote>
<p>Lo que creamos son 5 PODs con label(pod-label, sino está lo crea) y dentro de cada POD creamos dos containers con label(pod-label)  </p>
</blockquote>
</li>
<li>
<p>Comprobamos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          105s
rs-test-9jpjg   2/2     Running   0          105s
rs-test-fbwjb   2/2     Running   0          105s
rs-test-hz2kx   2/2     Running   0          105s
rs-test-s6cxx   2/2     Running   0          105s
[isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          119s
rs-test-9jpjg   2/2     Running   0          119s
rs-test-fbwjb   2/2     Running   0          119s
rs-test-hz2kx   2/2     Running   0          119s
rs-test-s6cxx   2/2     Running   0          119s
</code></pre>

<ul>
<li>Ver los <code>replicasets</code> con <code>kubectl get rs</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get rs
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m53s
[isx46410800@miguel replicaset]$ kubectl get replicaset
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m56s
</code></pre>

<h3 id="eliminarmodificar">ELIMINAR/MODIFICAR</h3>
<ul>
<li>En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx
pod &quot;rs-test-s6cxx&quot; deleted
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          5m43s
rs-test-9jpjg   2/2     Running   0          5m43s
rs-test-b9lf4   2/2     Running   0          43s
rs-test-fbwjb   2/2     Running   0          5m43s
rs-test-hz2kx   2/2     Running   0          5m43s
</code></pre>

<ul>
<li>Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ vim replica-set.yaml 
[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml 
replicaset.apps/rs-test configured
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS        RESTARTS   AGE
rs-test-5tsfh   2/2     Running       0          8m29s
rs-test-9jpjg   2/2     Terminating   0          8m29s
rs-test-b9lf4   2/2     Terminating   0          3m29s
rs-test-fbwjb   2/2     Running       0          8m29s
rs-test-hz2kx   2/2     Terminating   0          8m29s
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          9m26s
rs-test-fbwjb   2/2     Running   0          9m26s
</code></pre>

<h3 id="logs">LOGS</h3>
<ul>
<li>
<p>Por describe:<br />
<code>kubectl get rs rs-test -o yaml</code>  </p>
</li>
<li>
<p>Por manifiesto YAML:<br />
<code>kubectl describe rs rs-test</code>  </p>
</li>
</ul>
<h3 id="owner-refernce">OWNER REFERNCE</h3>
<ul>
<li>Lo vemos en la metadata de un pod creado por ReplicaSet <code>kubectl get pod podName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get pod rs-test-5tsfh -o yaml
name: rs-test-5tsfh
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: rs-test
    uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<ul>
<li>Comprobamos que el <code>UID</code> anterior coincide con el replicaset creado <code>kubectl get rs rsName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get rs rs-test -o yaml
name: rs-test
  namespace: default
  resourceVersion: &quot;22732&quot;
  selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test
  uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<h3 id="adopcion-de-pods-planos">ADOPCIÓN DE PODS PLANOS</h3>
<ul>
<li>Vamos a crear primero dos PODs manualmente:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine
pod/pod-test created
[isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine
pod/pod-test2 created
</code></pre>

<ul>
<li>Les creamos un LABEL a cada uno con <code>kubectl label pods podName label=valor</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label
pod/pod-test labeled
[isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label
pod/pod-test2 labeled
</code></pre>

<blockquote>
<p>Tendran el nuevo label pero no tendrán ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET.  </p>
</blockquote>
<ul>
<li>Ahora mediante replicaset cremos 3 replicas con mismo label:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml
replicaset.apps/rs-test created
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
pod-test        1/1     Running   0          3m49s
pod-test2       1/1     Running   0          3m45s
rs-test-8mk72   2/2     Running   0          10s
</code></pre>

<blockquote>
<p>Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.  </p>
</blockquote>
<h3 id="problemas">PROBLEMAS</h3>
<ul>
<li>
<p>Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene.  </p>
</li>
<li>
<p>NO se auto-actualizan solos.  </p>
</li>
<li>
<p>Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.  </p>
</li>
</ul>
<h2 id="deployments">DEPLOYMENTS</h2>
<ul>
<li>
<p>Es un objeto de nivel mayor que los replicaset. Es el dueño del replicaset que a su vez es de sus PODs.  </p>
</li>
<li>
<p>Al deployment se le da una imagen o una nueva versión: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y así succesivamente.  </p>
</li>
<li>
<p>Esto se logra porque los deployments tienen dos valores: Uno de máximo extra y otra de un máximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod más y solo dejar 1 pod inutilizado.    </p>
</li>
<li>
<p>Los deployments pueden mantener un máximo de 10 replicasets  </p>
</li>
</ul>
<h3 id="crear-deployment">CREAR DEPLOYMENT</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los deployments en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>Lo creamos con <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test created
</code></pre>

<ul>
<li>Vemos el deployment creado <code>kubectl get deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           8s
</code></pre>

<ul>
<li>Vemos los labels del deployment <code>kubectl get deployment --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment --show-labels
NAME              READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
deployment-test   3/3     3            3           21s   app=front
</code></pre>

<ul>
<li>Vemos el estado del deployment <code>kubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>Vemos que se ha creado un replicaset y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       4m28s
[isx46410800@miguel deployments]$ kubectl get replicaset --show-labels
NAME                         DESIRED   CURRENT   READY   AGE    LABELS
deployment-test-659b64d66c   3         3         3       5m8s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos que se ha creado 3 replicas del pod y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running   0          4m32s
deployment-test-659b64d66c-pzdct   1/1     Running   0          4m32s
deployment-test-659b64d66c-thknz   1/1     Running   0          4m32s
[isx46410800@miguel deployments]$ kubectl get pods --show-labels
NAME                               READY   STATUS    RESTARTS   AGE     LABELS
deployment-test-659b64d66c-n5qgr   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-pzdct   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-thknz   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos la jerarquía de lo creado para saber quien es el <code>owner reference</code> de cada cosa con <code>kubectl get rs/pod/deployment NAME -o yaml</code>:  </li>
<li>Deployment no tiene dueño</li>
<li>Replicaset su dueño es deployment</li>
<li>Pod su dueño es replicaset</li>
</ul>
<h3 id="rolling-update">ROLLING UPDATE</h3>
<ul>
<li>Actualizamos por ejemplo la imagen de un container del POD en vez de <code>nginx:alpine</code> ponemos <code>nginx</code> y hacemos de nuevo el <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test configured
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running             0          13m
deployment-test-659b64d66c-pzdct   1/1     Running             0          13m
deployment-test-659b64d66c-thknz   1/1     Running             0          13m
deployment-test-69b674677d-2cq4l   0/1     ContainerCreating   0          5s
[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     1            3           14m
[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       14m
deployment-test-69b674677d   1         1         0       18s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   0/1     Terminating         0          14m
deployment-test-659b64d66c-pzdct   1/1     Running             0          14m
deployment-test-659b64d66c-thknz   1/1     Terminating         0          14m
deployment-test-69b674677d-2cq4l   1/1     Running             0          25s
deployment-test-69b674677d-dwdlr   0/1     ContainerCreating   0          1s
deployment-test-69b674677d-dwspw   1/1     Running             0          6s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-69b674677d-2cq4l   1/1     Running   0          43s
deployment-test-69b674677d-dwdlr   1/1     Running   0          19s
deployment-test-69b674677d-dwspw   1/1     Running   0          24s
</code></pre>

<ul>
<li>Vemos el estado en directo de lo que hace con <code>ubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>También podemos ver el resultado en <code>kubectl describe deployment deploymentName</code>:  </li>
</ul>
<pre><code>Events:
  Type    Reason             Age                    From                   Message
  ----    ------             ----                   ----                   -------
  Normal  ScalingReplicaSet  19m                    deployment-controller  Scaled up replica set deployment-test-659b64d66c to 3
  Normal  ScalingReplicaSet  5m18s                  deployment-controller  Scaled up replica set deployment-test-69b674677d to 1
  Normal  ScalingReplicaSet  4m59s                  deployment-controller  Scaled down replica set deploy
</code></pre>

<ul>
<li>Aquí vemos también la estrategía de los valores que comentamos en la introducción:<br />
<code>RollingUpdateStrategy:  25% max unavailable, 25% max surge</code>  </li>
</ul>
<h3 id="historial-de-deployments">HISTORIAL DE DEPLOYMENTS</h3>
<ul>
<li>Podemos ver las actualizaciones o revisiones en el historial de deployments en <code>kubectl rollout history deployment deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
2         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre>

<ul>
<li>Podemos con esto volver a cualquier versión anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo añadiento en la parte de replicaset del manifiesto YAML <code>revisionHistoryLimit: 5</code>:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  revisionHistoryLimit: 5
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>
<p>Para poner un motivo en el <code>change-cause</code> cuando hacemos una versión de deployments indicamos dos maneras:  </p>
</li>
<li>
<p>Con la linea de desplegar <code>kubectl apply -f deployment.yaml --record</code>:<br />
<code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record
  deployment.apps/deployment-test configured
  [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true</code>  </p>
</li>
<li>
<p>Con una subsección en el manifiesto deployment.yaml <code>annotations-&gt; kubernetes.io/change-cause: "message"</code>:<br />
<code>esto es del deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-test
    annotations:
      kubernetes.io/change-cause: "changes port to 110"
    labels:
      app: front</code><br />
<code>kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true
  4         changes port to 110</code>  </p>
</li>
<li>
<p>Para luego ver una revisión en concreta usamos <code>kubectl rollout history deployment deployment-test --revision=3</code>:  </p>
</li>
</ul>
<pre><code>deployment.apps/deployment-test with revision #3
Pod Template:
  Labels:   app=front
    pod-template-hash=fd8445c88
  Annotations:  kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true
  Containers:
   nginx:
    Image:  nginx:alpine
    Port:   90/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>

<h3 id="roll-backs">ROLL BACKS</h3>
<ul>
<li>Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualización de la imagen:  </li>
</ul>
<pre><code>containers:
      - name: nginx
        image: nginx:fake
        ports:
        - containerPort: 110
</code></pre>

<ul>
<li>Vemos el nuevo historial y su fallo:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
3         kubectl apply --filename=deployment.yaml --record=true
4         changes port to 110
5         new version nginx
#
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS         RESTARTS   AGE
deployment-test-5c6896bcd5-h5qts   0/1     ErrImagePull   0          32s
deployment-test-74fb9c6d9f-7dwnr   1/1     Running        0          6m50s
deployment-test-74fb9c6d9f-f5qs8   1/1     Running        0          6m45s
deployment-test-74fb9c6d9f-lsmzj   1/1     Running        0          6m54s
</code></pre>

<ul>
<li>Volvemos haciendo un <code>rollback</code> a una versión anterior con <code>kubectl rollout undo deployment deployment-test --to-revision=4</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4
deployment.apps/deployment-test rolled back
#
[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test
Name:                   deployment-test
Namespace:              default
CreationTimestamp:      Sun, 11 Oct 2020 19:21:04 +0200
Labels:                 app=front
Annotations:            deployment.kubernetes.io/revision: 6
                        kubernetes.io/change-cause: changes port to 110
Selector:               app=front
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=front
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         110/TCP
    Host Port:    0/TCP
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test 
Normal  ScalingReplicaSet  117s (x12 over 15m)  deployment-controller  (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0
</code></pre>

<h2 id="servicios">SERVICIOS</h2>
<ul>
<li>
<p>Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y éste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique.  </p>
</li>
<li>
<p>Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio también tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo.  </p>
</li>
<li>
<p>Los <code>endpoints</code> se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y así el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.  </p>
</li>
</ul>
<h3 id="crear-servicio">CREAR SERVICIO</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los servicios en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---       
# añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<blockquote>
<p>El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.  </p>
</blockquote>
<h3 id="info-servicio">INFO SERVICIO</h3>
<ul>
<li>Vemos lo creado con <code>kubectl get services/svc</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    41h
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   63s
[isx46410800@miguel services]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           79s
</code></pre>

<ul>
<li>Vemos por el label que le indicamos en el YAML:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=front
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   3m35s
</code></pre>

<blockquote>
<p>El cluster-ip se lo da kubernetes si no se lo asignamos directamente  </p>
</blockquote>
<ul>
<li>Profundizamos el servicio con <code>kubectl describe svc my-service</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl describe svc my-service
Name:              my-service
Namespace:         default
Labels:            app=front
Annotations:       &lt;none&gt;
Selector:          app=front
Type:              ClusterIP
IP:                10.97.182.119
Port:              &lt;unset&gt;  8888/TCP
TargetPort:        80/TCP
Endpoints:         172.18.0.2:80,172.18.0.4:80,172.18.0.5:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.  </p>
</blockquote>
<h3 id="endpoints">ENDPOINTS</h3>
<ul>
<li>
<p>Lista de IPs de los pods que tienen el label de mi servicio creado.  </p>
</li>
<li>
<p>Vemos la lista de endpoints con <code>kubectl get endpoints</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.4:80,172.18.0.5:80   10m
</code></pre>

<ul>
<li>Comprobamos que son las mismas de los PODS:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-cgds6   1/1     Running   0          10m   172.18.0.4   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          10m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          10m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<ul>
<li>Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6
pod &quot;deployment-test-b7c99d94b-cgds6&quot; deleted
[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.5:80,172.18.0.6:80   13m
[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          14m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-kcdnx   1/1     Running   0          39s   172.18.0.6   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          14m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<h3 id="dns">DNS</h3>
<ul>
<li>
<p>Creamos un POD nuevo:<br />
<code>[isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh</code>  </p>
</li>
<li>
<p>Funciona que escucha al servicio:  </p>
</li>
</ul>
<pre><code># curl 10.97.182.119:8888
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<blockquote>
<p>Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web.  </p>
</blockquote>
<ul>
<li>Se crea como un tipo de DNS ya que por el nombre del servicio también se comunica y obtiene respuesta:<br />
<code># curl my-service:8888</code>  </li>
</ul>
<h3 id="servicio-cluster-ip">SERVICIO CLUSTER-IP</h3>
<ul>
<li>
<p>IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior.  </p>
</li>
<li>
<p>Le podemos poner un tipo de servicio a los servicios que creamos:  </p>
</li>
</ul>
<pre><code># añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  type: ClusterIP
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<h3 id="servicio-node-port">SERVICIO NODE-PORT</h3>
<ul>
<li>
<p>IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea también un ClusterIP.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test2
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---
# añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service2
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=backend
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
my-service2   NodePort   10.101.52.193   &lt;none&gt;        8888:30072/TCP   21s
[isx46410800@miguel services]$ kubectl get pods -l app=backend
NAME                                READY   STATUS    RESTARTS   AGE
deployment-test2-77448c6d65-gj6l7   1/1     Running   0          36s
deployment-test2-77448c6d65-n8td7   1/1     Running   0          36s
deployment-test2-77448c6d65-sd6zq   1/1     Running   0          36s
</code></pre>

<ul>
<li>
<p>Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio:<br />
<code>http://192.168.1.104:30072</code>  </p>
</li>
<li>
<p>Si no hace en minikube podemos hacer lo siguiente y lo veremos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ minikube service my-service2
|-----------|-------------|-------------|-------------------------|
| NAMESPACE |    NAME     | TARGET PORT |           URL           |
|-----------|-------------|-------------|-------------------------|
| default   | my-service2 |        8888 | http://172.17.0.2:30072 |
|-----------|-------------|-------------|-------------------------|
</code></pre>

<blockquote>
<p>Esa url nos dará el servicio web a través del node port.  </p>
</blockquote>
<h3 id="servicio-load-balancer">SERVICIO LOAD BALANCER</h3>
<ul>
<li>
<p>Hace referencia a un servicio de balanzador de carga.  </p>
</li>
<li>
<p>Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.  </p>
</li>
</ul>
<h2 id="golang">GOLANG</h2>
<ul>
<li>Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petición al front y este le devuelva el servicio que está en backend.  </li>
</ul>
<h3 id="crear-api-rest-go">CREAR API REST GO</h3>
<ul>
<li>
<p><a href="https://dev.to/moficodes/build-your-first-rest-api-with-go-2gcj">DOCUMENTACIÓN</a>  </p>
</li>
<li>
<p>Creamos un fichero simple de API REST en Goland:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel src]$ cat main.go 
package main
import (
    &quot;log&quot;
    &quot;net/http&quot;
)
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write([]byte(`{&quot;message&quot;: &quot;hello world&quot;}`))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil))
}
</code></pre>

<blockquote>
<p>Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestará a la petición el hello wolld como respuesta.  </p>
</blockquote>
<ul>
<li>Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo:<br />
<code>[isx46410800@miguel k8s-hands-on]$ docker pull golang</code>  </li>
</ul>
<p><code>[isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash</code>  </p>
<ul>
<li>Iniciamos el fichero y comprobamos el resultado:  </li>
</ul>
<pre><code>[isx46410800@miguel src]$ docker exec -it goland /bin/bash
root@miguel:/go# go run main.go 
</code></pre>

<p>![./images/kubernetes4.png]  </p>
<h3 id="cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</h3>
<h3 id="mensaje-1">MENSAJE 1</h3>
<ul>
<li>Añadimos unas variables para cambiar el mensaje de respuesta a la petición de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;time&quot;
)
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    resp := fmt.Sprintf(&quot;La hora es %v y el hostname es %v&quot;, time.Now(), os.Getenv(&quot;HOSTNAME&quot;))
    w.Write([]byte(resp))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<p>![./images/kubernetes5.png]  </p>
<h4 id="mensaje-2">MENSAJE 2</h4>
<ul>
<li>Añadimos unas variables para cambiar el mensaje de respuesta a la petición de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;os&quot;
    &quot;time&quot;
    &quot;encoding/json&quot;
)
type HandsOn struct {
   Time     time.Time   `json:&quot;time&quot;`
   Hostname string      `json:&quot;hostname&quot;`
}
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    if r.URL.Path != &quot;/&quot; {
        http.NotFound(w, r)
        return
    }
    resp := HandsOn{
        Time:       time.Now(),
        Hostname:   os.Getenv(&quot;HOSTNAME&quot;),
    }
    jsonResp, err := json.Marshal(&amp;resp)
    if err != nil {
        w.Write([]byte(&quot;Error&quot;))
        return
    }
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write(jsonResp)
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<blockquote>
<p>Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y también darle un mensaje de error si no acaba en /.  </p>
</blockquote>
<p>![./images/kubernetes6.png]  </p>
<p>![./images/kubernetes7.png]  </p>
<h3 id="dockerfile-golang">DOCKERFILE GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM golang:1.13 as builder
# DIRECTORIO A TRABAJAR
WORKDIR /app
# COPIAMOS FICHERO MAIN
COPY main.go .
RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go
# DESDE IMAGEN ALPINE
FROM alpine:latest
# mailcap adds mime detection and ca-certificates help with TLS (basic stuff)
WORKDIR /app
COPY --from=builder /app/app .
# PARA EJECUTARLO
ENTRYPOINT [&quot;./app&quot;]
</code></pre>

<ul>
<li>
<p>Construimos imagen:<br />
<code>[isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on .</code>  </p>
</li>
<li>
<p>Encendemos:<br />
<code>[isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on</code>  </p>
</li>
</ul>
<p>![./images/kubernetes8.png]  </p>
<blockquote>
<p>Ahora nuestra aplicación de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.  </p>
</blockquote>
<h3 id="deployment-golang">DEPLOYMENT GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<blockquote>
<p>Ponemos lo de <code>imagePullPolicy: IfNotPresent</code> para que primero busque si la imagen está constuida localmente antes de mirar en los repos de internet de dockerhub.  </p>
</blockquote>
<ul>
<li>Comprobaciones:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-5d548949c7-dgw9l   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-fg8wr   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-q9s6g   1/1     Running   0          15m
[isx46410800@miguel backend]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-5d548949c7   3         3         3       15m
[isx46410800@miguel backend]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           15m
[isx46410800@miguel backend]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
backend-k8s-hands-on   ClusterIP   10.101.44.56   &lt;none&gt;        80/TCP    16m
kubernetes             ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   3d19h
</code></pre>

<ul>
<li>Visual cambiando a nodeport, nos contestará unos de los PODs la respuesta a la request del usuario:  </li>
</ul>
<p>![./images/kubernetes9.png]  </p>
<h3 id="consumo-del-servicio">CONSUMO DEL SERVICIO</h3>
<ul>
<li>Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos darán las respuestas los PODs del servicio:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh
/ # apk add -U curl
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:57:49.446174694Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:58:10.218346403Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
{&quot;time&quot;:&quot;2020-10-13T19:58:25.365295183Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-66dgx&quot;}/ # 
</code></pre>

<h3 id="fronted">FRONTED</h3>
<ul>
<li>Creamos ahora un index.html de respuesta en un fronted/src/index.html:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine
vi /usr/share/nginx/html/index.html
&lt;div id=&quot;id01&quot;&gt;&lt;/div&gt;

&lt;script&gt;
var xmlhttp = new XMLHttpRequest();
var url = &quot;http://backend-k8s-hands-on&quot;;

xmlhttp.onreadystatechange = function() {
    if (this.readyState == 4 &amp;&amp; this.status == 200) {
        var resp = JSON.parse(this.responseText);
        document.getElementById(&quot;id01&quot;).innerHTML = &quot;&lt;h2&gt;La hora es &quot; + resp.time + &quot;y el hostname es&quot; + resp.hostname &quot;&lt;/h2&quot;&gt;;
    }
};
xmlhttp.open(&quot;GET&quot;, url, true);
xmlhttp.send();
&lt;/script&gt;
</code></pre>

<ul>
<li>Dockerfile:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM nginx:alpine
# COPIAMOS FICHERO MAIN
COPY ./src/index.html /usr/share/nginx/html/index.html
</code></pre>

<h3 id="manifiesto-fronted">MANIFIESTO FRONTED</h3>
<ul>
<li>Despliegue del fronted:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fronted
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: fronted
    spec:
      containers:
      - name: fronted
        image: isx46410800/k8s-hands-on:fronted
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
spec:
  type: NodePort
  selector:
    app: fronted
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 80
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort    10.111.54.241   &lt;none&gt;        80:30740/TCP   78m
fronted-k8s-hands-on   NodePort    10.105.156.14   &lt;none&gt;        80:30159/TCP   9m22s
kubernetes             ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        3d20h
[isx46410800@miguel k8s-hands-on]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-lzrr4   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-mdjh7   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-qxzdv   1/1     Running   0          51m
fronted-k8s-hands-on-78f59c5f77-dpvck   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-q7h9r   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-r7fnm   1/1     Running   0          9m27s
[isx46410800@miguel k8s-hands-on]$ kubectl cluster-info
Kubernetes master is running at https://172.17.0.2:8443
KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<h2 id="namespaces">NAMESPACES</h2>
<ul>
<li>
<p>Son como ambientes separados dentro del cluster de kubernetes.  </p>
</li>
<li>
<p>Cada uno de estos ambientes tienen su deployment, replicaset, pods...  </p>
</li>
<li>
<p>Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster.  </p>
</li>
<li>
<p>En cada namespace se puede limitar los pods, la memoria, usuarios...</p>
</li>
<li>
<p>Ordenes básicas:<br />
<code>kubectl get namespaces</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   3d23h
kube-node-lease   Active   3d23h
kube-public       Active   3d23h
kube-system       Active   3d23h
</code></pre>

<ul>
<li>
<p>Especifica por namespace:<br />
<code>kubectl get pods --namespace default</code>  </p>
</li>
<li>
<p>El default van todos los recursos, lo creado donde no se asignan ningun namespace.  </p>
</li>
<li>Todos los usuarios pueden ver este namespace kube-public.  </li>
<li>
<p>El kube-system tiene todos los objetos del kubernetes.  </p>
</li>
<li>
<p>Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldrán los asignados. Habrá que poner -n/--namespace namespaceName</p>
</li>
</ul>
<h3 id="crear-namespace">CREAR NAMESPACE</h3>
<ul>
<li>Por comando <code>kubectl create namespace nameNamespace</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns
namespace/test-ns created
</code></pre>

<ul>
<li>Para verlo <code>kubectl get namespaces</code> y <code>kubectl describe namespaces test-ns</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
test-ns           Active   4s
[isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns
Name:         test-ns
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Active
No resource quota.
No LimitRange resource.
</code></pre>

<ul>
<li>Por manifiesto YAML:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
</code></pre>

<ul>
<li>Comprobamos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml 
namespace/development created
[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE     LABELS
default           Active   4d      &lt;none&gt;
development       Active   19s     name=development
kube-node-lease   Active   4d      &lt;none&gt;
kube-public       Active   4d      &lt;none&gt;
kube-system       Active   4d      &lt;none&gt;
test-ns           Active   6m33s   &lt;none&gt;
</code></pre>

<h3 id="asignar-namespaces">ASIGNAR NAMESPACES</h3>
<ul>
<li>Creamos un pod y lo asignamos:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns
pod/podtest2 created
[isx46410800@miguel namespaces]$ kubectl get pods -n test-ns
NAME       READY   STATUS    RESTARTS   AGE
podtest2   1/1     Running   0          22s
</code></pre>

<h3 id="borrar-namespaces">BORRAR NAMESPACES</h3>
<ul>
<li>
<p>Borramos POD asignado a namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns</code>  </p>
</li>
<li>
<p>Borrar manifiesto:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml</code>  </p>
</li>
<li>
<p>Borrar namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns</code>  </p>
</li>
</ul>
<h3 id="deploy-namespaces">DEPLOY NAMESPACES</h3>
<ul>
<li>Creamos dos namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
--- 
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
</code></pre>

<ul>
<li>Lo vemos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE   LABELS
default           Active   4d    &lt;none&gt;
dev               Active   6s    name=dev
kube-node-lease   Active   4d    &lt;none&gt;
kube-public       Active   4d    &lt;none&gt;
kube-system       Active   4d    &lt;none&gt;
prod              Active   6s    name=prod
</code></pre>

<ul>
<li>Creamos un deployment con los namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
--- 
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-dev
  namespace: dev
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---     
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-prod
  namespace: prod
  labels:
    app: back
# aqui viene el replicaset
spec:
  replicas: 5
  selector:
    matchLabels:
      app: back
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: back
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml 
namespace/dev unchanged
namespace/prod unchanged
deployment.apps/deploy-dev created
deployment.apps/deploy-prod created
[isx46410800@miguel namespaces]$ kubectl get deploy -n dev
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
deploy-dev   1/1     1            1           26s
[isx46410800@miguel namespaces]$ kubectl get deploy -n prod
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
deploy-prod   5/5     5            5           29s
[isx46410800@miguel namespaces]$ kubectl get rs -n dev
NAME                   DESIRED   CURRENT   READY   AGE
deploy-dev-b7c99d94b   1         1         1       36s
[isx46410800@miguel namespaces]$ kubectl get rs -n prod
NAME                     DESIRED   CURRENT   READY   AGE
deploy-prod-7bfb7875fd   5         5         5       38s
[isx46410800@miguel namespaces]$ kubectl get pods -n dev
NAME                         READY   STATUS    RESTARTS   AGE
deploy-dev-b7c99d94b-xc696   1/1     Running   0          50s
[isx46410800@miguel namespaces]$ kubectl get pods -n prod
NAME                           READY   STATUS    RESTARTS   AGE
deploy-prod-7bfb7875fd-49kzd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-9m7x8   1/1     Running   0          54s
deploy-prod-7bfb7875fd-nbhfd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-tl5gs   1/1     Running   0          54s
deploy-prod-7bfb7875fd-wxrwc   1/1     Running   0          54s
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
dev               Active   10m
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
prod              Active   10m
</code></pre>

<h3 id="dns-namespaces">DNS NAMESPACES</h3>
<ul>
<li>Creamos un namespace y un deploy asignados:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: ci
  labels:
    name: ci
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on:v2
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml 
namespace/ci created
deployment.apps/backend-k8s-hands-on created
service/backend-k8s-hands-on created
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
ci                Active   15s
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
[isx46410800@miguel namespaces]$ kubectl get deploy -n ci
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           32s
[isx46410800@miguel namespaces]$ kubectl get svc -n ci
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   38s
[isx46410800@miguel namespaces]$ kubectl get rs -n ci
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       46s
[isx46410800@miguel namespaces]$ kubectl get pods -n ci
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          49s
</code></pre>

<ul>
<li>
<p>Ahora creamos un POD con el namespace default:<br />
<code>[isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh</code>  </p>
</li>
<li>
<p>Por defecto, cuando los dns que se crean en un namespace siguen esta regla:<br />
<code>serviceName + namespaceName + service.cluster.local</code>  </p>
</li>
<li>
<p>Así desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior:<br />
<code>/ # curl backend-k8s-hands-on.ci.svc.cluster.local</code><br />
<code>{"time":"2020-10-14T01:09:56.22990857Z","hostname":"backend-k8s-hands-on-7d5b6dc559-7xv59"}/</code>  </p>
</li>
<li>
<p>Si no daría error:  </p>
</li>
</ul>
<pre><code>/ # curl backend-k8s-hands-on
curl: (6) Could not resolve host: backend-k8s-hands-on
</code></pre>

<h3 id="contextos-namespaces">CONTEXTOS NAMESPACES</h3>
<ul>
<li>
<p>Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato.  </p>
</li>
<li>
<p>Para ver en que contexto estamos usamos:<br />
<code>kubectl config current-context</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config current-context
minikube
</code></pre>

<ul>
<li>Vemos el archivo de configuración <code>./kube/config</code> que es de donde lee el current-context:  </li>
</ul>
<pre><code>[root@miguel ~]# cat /home/isx46410800/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Mejor con este comando <code>kubectl config view</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Crear un nuevo contexto <code>ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube
Context &quot;ci-context&quot; created.
</code></pre>

<ul>
<li>Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    namespace: ci
    user: minikube
  name: ci-context
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Para cambiar de contexto usamos <code>kubectl config use-context Namecontext</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config use-context ci-context
Switched to context &quot;ci-context&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          19m
[isx46410800@miguel namespaces]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           19m
[isx46410800@miguel namespaces]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       20m
[isx46410800@miguel namespaces]$ kubectl get services
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   20m
[isx46410800@miguel namespaces]$ kubectl config use-context minikube
Switched to context &quot;minikube&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
No resources found in default namespace.
[isx46410800@miguel namespaces]$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   4d
</code></pre>

<h2 id="limitar-ramcpu">LIMITAR RAM/CPU</h2>
<ul>
<li>
<p>La RAM se puede limitar en B, MB y G.  </p>
</li>
<li>
<p>La CPU: 1 cpu es 1000 milicores/milicpus.  </p>
</li>
</ul>
<h3 id="limitsrequest">LIMITS/REQUEST</h3>
<ul>
<li>
<p>Los <strong>LIMITS</strong> es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podría tener más si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminará o reiniciará el pod y lo pondrá en otro sitio que le garantice esa cantidad de recursos indicada.  </p>
</li>
<li>
<p>Los <strong>REQUESTS</strong> es la cantidad de recursos que el pod siempre va a poder disponer. Estará garantizado la cantidad que se le indique.  </p>
</li>
</ul>
<h4 id="ram">RAM</h4>
<ul>
<li>Creamos un ejemplo de limite de RAM:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: &quot;200Mi&quot;
      requests:
        memory: &quot;100Mi&quot;
    command: [&quot;stress&quot;]
    # se indica que le va a dar 150Megas
    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]
</code></pre>

<blockquote>
<p>Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M.  </p>
</blockquote>
<ul>
<li>Comprobamos que lo ha creado <code>kubectl apply -f limit-request.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   1/1     Running   0          38s
</code></pre>

<ul>
<li>Si ponemos el ejemplo anterior con 250M vemos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml 
pod/memory-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
memory-demo   0/1     ContainerCreating   0          4s
[isx46410800@miguel limits-requests]$ kubectl get pods --watch
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   1          12s
memory-demo   0/1     OOMKilled          2          25s
memory-demo   0/1     CrashLoopBackOff   2          26s
^C[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   2          48s
</code></pre>

<ul>
<li>Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient memory.
</code></pre>

<h4 id="cpu">CPU</h4>
<ul>
<li>Ejemplo de limitar CPU:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
spec:
  containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;0.5&quot;
    args:
    - -cpus
    - &quot;2&quot;
    # se le pide 2 cpus y hay limite de 1
</code></pre>

<blockquote>
<p>Aunque se le pida 2, no se eliminará como la RAM sino que soolo tendrá de máximo el LIMIT indicado(1).  </p>
</blockquote>
<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml 
pod/cpu-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS              RESTARTS   AGE
cpu-demo   0/1     ContainerCreating   0          7s
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
cpu-demo   1/1     Running   0          11s
</code></pre>

<ul>
<li>Si vemos la capacidad total de mi cluster <code>kubectl describe node minikube</code>: </li>
</ul>
<pre><code>Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1150m (28%)  1 (25%)
  memory             70Mi (0%)    170Mi (2%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
</code></pre>

<blockquote>
<p>Siempre hay un poco más para que no sobrepase el limite y me vaya todo lento.  </p>
</blockquote>
<ul>
<li>Nuestra cantidad de CPU <code>kubectl describe node minikube</code>:  </li>
</ul>
<pre><code>kubectl describe node minikube
Capacity:
  cpu:                4
</code></pre>

<ul>
<li>Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
cpu-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.
</code></pre>

<h3 id="qosquality-of-service">QOS(Quality of Service)</h3>
<ul>
<li>
<p>Es una propiedad que se le asigna a los pods.  </p>
</li>
<li>
<p>Podemos ver el estado de QoS con:<br />
<code>kubectl get pod podName -o yaml | grep -i qos</code>  </p>
</li>
<li>
<p>Hay diferentes tipos de clases de estado en el que entra el POD:  </p>
<ul>
<li><strong>BestEffort</strong>: No se definen los limites y request. Los asignará el schedule pero puede ser que este consuma y consuma recursos sin parar.</li>
<li><strong>Guaranteed</strong>: Tiene los mismos limites que de request</li>
<li><strong>Burstable</strong>: cuando pueda aumentar el request. El limite es mayor que el request.  </li>
</ul>
</li>
</ul>
<h2 id="limitrange">LIMITRANGE</h2>
<ul>
<li>
<p>Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces.  </p>
</li>
<li>
<p>Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods</p>
</li>
</ul>
<h3 id="valores-por-defecto">VALORES POR DEFECTO</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
</code></pre>

<blockquote>
<p>El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default.  </p>
</blockquote>
<ul>
<li>Comprobamos con <code>kubectl get limitrange -n namespaceName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml 
namespace/dev created
limitrange/mem-limit-range created
#
[isx46410800@miguel limitRange]$ kubectl get limitrange -n dev
NAME              CREATED AT
mem-limit-range   2020-10-14T18:10:15Z
</code></pre>

<ul>
<li>Comprobamos con <code>kubectl describe limitrange LRName -n NSName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev
Name:       mem-limit-range
Namespace:  dev
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    500m             1              -
Container   memory    -    -    256Mi            512Mi          -
</code></pre>

<h3 id="valores-pod">VALORES POD</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
---
# pod
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  namespace: dev
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3
      Started:      Wed, 14 Oct 2020 20:21:43 +0200
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
[isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
      cpu:        500m
      memory:     256Mi
    Environment:  &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos que se han asignado la cpu de 0.5 y Ram 256M.  </p>
</blockquote>
<h3 id="limites">LIMITES</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
---
# pod
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  namespace: dev
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
    resources:
      limits:
        memory: 500M
        cpu: 0.5
      requests:
        memory: 400M
        cpu: 0.3
</code></pre>

<blockquote>
<p>Si se superan los limites en los PODs te dará error, ya que sobrepasa los limites de memoria y ram  </p>
</blockquote>
<h2 id="resource-quota">RESOURCE QUOTA</h2>
<ul>
<li>
<p>Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro.  </p>
</li>
<li>
<p>Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus.  </p>
</li>
<li>
<p>El limitrange opera por objeto, por pod.  </p>
</li>
</ul>
<h3 id="crear-rq">CREAR RQ</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: uat
  labels:
    name: uat
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
  namespace: uat
spec:
  hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
</code></pre>

<ul>
<li>Comprobamos con <code>kubectl describe resourcequota -n nsName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml 
namespace/uat created
resourcequota/mem-cpu-demo created
[isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo
Name:            mem-cpu-demo
Namespace:       uat
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     2
limits.memory    0     2Gi
requests.cpu     0     1
requests.memory  0     1Gi
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe ns uat 
Name:         uat
Labels:       name=uat
Annotations:  &lt;none&gt;
Status:       Active
Resource Quotas
 Name:            mem-cpu-demo
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       0     2
 limits.memory    0     2Gi
 requests.cpu     0     1
 requests.memory  0     1Gi
No LimitRange resource.
</code></pre>

<h3 id="deploy-rq">DEPLOY RQ</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: uat
  labels:
    name: uat
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
  namespace: uat
spec:
  hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
--- 
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  namespace: uat
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 2
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            memory: 500M
            cpu: 0.5
          limits:
            memory: 500M
            cpu: 0.5
</code></pre>

<ul>
<li>Comprobamos lo creado:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl get pods -n uat
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-5f869977fb-84nqs   1/1     Running   0          2m40s
deployment-test-5f869977fb-vg5cj   1/1     Running   0          2m45s
[isx46410800@miguel resource-quota]$ kubectl get rs -n uat
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-5f869977fb   2         2         2       2m54s
deployment-test-df54c6d6d    0         0         0       5m41s
[isx46410800@miguel resource-quota]$ kubectl get deploy -n uat
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   2/2     2            2           5m47s
[isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat
NAME           AGE     REQUEST                                      LIMIT
mem-cpu-demo   5m57s   requests.cpu: 1/1, requests.memory: 1G/1Gi   limits.cpu: 1/2, limits.memory: 1G/2Gi
</code></pre>

<ul>
<li>Con lo creado ahora podemos ver que hemos llegado a los limites <code>kubectl describe ns nsName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe ns uat
Name:         uat
Labels:       name=uat
Annotations:  &lt;none&gt;
Status:       Active
Resource Quotas
 Name:            mem-cpu-demo
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       1     2
 limits.memory    1G    2Gi
 requests.cpu     1     1
 requests.memory  1G    1Gi
No LimitRange resource.
</code></pre>

<ul>
<li>Si ahora modificamos el fichero y creamos 3 replicas, superará el limite indicado. Por lo que solo creará dos y no tres, ya que el 3 superará los limites asignados en el RESOURCE QUOTA.  </li>
</ul>
<h3 id="limitar-no-pods-en-ns">LIMITAR Nº PODS EN NS</h3>
<ul>
<li>Vemos un ejemplo de como limitar el número de pods que se pueden crear en un namespace a través del ResourceQuota:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: qa
  labels:
    name: qa
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
  namespace: qa
spec:
  hard:
    pods: &quot;3&quot;
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-qa
  namespace: qa
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Comprobamos lo creado:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml 
namespace/qa created
resourcequota/pod-demo created
deployment.apps/deployment-qa created
#
[isx46410800@miguel resource-quota]$ kubectl get pods -n qa
NAME                            READY   STATUS    RESTARTS   AGE
deployment-qa-b7c99d94b-h5bxr   1/1     Running   0          10s
deployment-qa-b7c99d94b-tttpn   1/1     Running   0          10s
deployment-qa-b7c99d94b-xdl45   1/1     Running   0          10s
[isx46410800@miguel resource-quota]$ kubectl get rs -n qa
NAME                      DESIRED   CURRENT   READY   AGE
deployment-qa-b7c99d94b   3         3         3       14s
#
[isx46410800@miguel resource-quota]$ kubectl get ns -n qa
NAME              STATUS   AGE
ci                Active   18h
default           Active   4d19h
kube-node-lease   Active   4d19h
kube-public       Active   4d19h
kube-system       Active   4d19h
qa                Active   18s
#
[isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa
NAME       AGE   REQUEST     LIMIT
pod-demo   99s   pods: 3/3   
</code></pre>

<ul>
<li>Más info <code>kubectl describe resourcequota pod-demo -n qa</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa
Name:       pod-demo
Namespace:  qa
Resource    Used  Hard
--------    ----  ----
pods        3     3
</code></pre>

<ul>
<li>Si ponemos 4 replicas, solo se habrán creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota.  </li>
</ul>
<h2 id="probes">PROBES</h2>
<ul>
<li>
<p>Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container.  </p>
</li>
<li>
<p>Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta.  </p>
</li>
<li>
<p>Puede ser este PROBE por:  </p>
</li>
<li>Comando</li>
<li>TCP</li>
<li>HTTP</li>
</ul>
<h3 id="tipos-probes">TIPOS PROBES</h3>
<ul>
<li>
<p>Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que esté funcionando la aplicación del contenedor.  </p>
</li>
<li>
<p>Readiness:  nos ayuda a garantizar el servicio del pod está listo para el request.  </p>
</li>
<li>
<p>Startup: es una prueba que se sube para ver que esté todo configurado y este listo la aplicación para ejecutarse.  </p>
</li>
</ul>
<h3 id="crear-liveness-probe">CREAR LIVENESS PROBE</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># probe liveness
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
</code></pre>

<blockquote>
<p>Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo.  </p>
</blockquote>
<ul>
<li>
<p>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </p>
</li>
<li>
<p>Pruebas:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel probes]$ kubectl apply -f liveness.yaml 
pod/liveness-exec created
[isx46410800@miguel probes]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
liveness-exec   1/1     Running   0          9s
#
[isx46410800@miguel probes]$ kubectl describe pod liveness-exec
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  95s                default-scheduler  Successfully assigned default/liveness-exec to minikube
  Normal   Pulled     90s                kubelet            Successfully pulled image &quot;k8s.gcr.io/busybox&quot; in 3.165552593s
  Warning  Unhealthy  46s (x3 over 56s)  kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    46s                kubelet            Container liveness failed liveness probe, will be restarted
  Normal   Pulling    15s (x2 over 93s)  kubelet            Pulling image &quot;k8s.gcr.io/busybox&quot;
  Normal   Pulled     15s                kubelet            Successfully pulled image &quot;k8s.gcr.io/busybox&quot; in 751.39074ms
  Normal   Created    14s (x2 over 89s)  kubelet            Created container liveness
  Normal   Started    14s (x2 over 88s)  kubelet            Started container liveness
</code></pre>

<h3 id="liveness-tcp">LIVENESS TCP</h3>
<ul>
<li>Una probe con liveness TCP:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
</code></pre>

<ul>
<li>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </li>
</ul>
<h3 id="liveness-http">LIVENESS HTTP</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 5
      periodSeconds: 3
</code></pre>

<ul>
<li>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </li>
</ul>
<h3 id="readiness-probe">READINESS PROBE</h3>
<ul>
<li>Una probe con readiness TCP:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
</code></pre>

<blockquote>
<p>La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren más peticiones de request y por lo tanto no se le de más carga a este contenedor/pod.  </p>
</blockquote>
<h2 id="variables-y-configmap">VARIABLES Y CONFIGMAP</h2>
<h3 id="crear-variables">CREAR VARIABLES</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: envar-demo
spec:
  containers:
  - name: envar-demo-container
    image: nginx:alpine
    env:
    - name: VAR1
      value: &quot;valor de prueba 1&quot;
    - name: VAR2
      value: &quot;valor de prubea 2&quot;
    - name: VAR3
      value: &quot;valor de prubea 3&quot;
</code></pre>

<ul>
<li>Prueba:  </li>
</ul>
<pre><code>[isx46410800@miguel env_variables]$ kubectl apply -f env.yaml 
pod/envar-demo created
#
[isx46410800@miguel env_variables]$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
envar-demo   1/1     Running   0          12s
#
[isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh
/ # env
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT=tcp://10.96.0.1:443
HOSTNAME=envar-demo
SHLVL=1
HOME=/root
VAR1=valor de prueba 1
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
VAR2=valor de prubea 2
VAR3=valor de prubea 3
PWD=/
#
/ # echo $VAR1
valor de prueba 1
</code></pre>

<h3 id="variables-referenciadas">VARIABLES REFERENCIADAS</h3>
<ul>
<li>Se crearian a partir de conseguir la info del pod a partir del <code>[isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml</code>:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-fieldref
spec:
  containers:
    - name: test-container
      image: ngix:alpine
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
  restartPolicy: Never
</code></pre>

<blockquote>
<p>Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc.  </p>
</blockquote>
<h3 id="configmap">CONFIGMAP</h3>
<ul>
<li>
<p>Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creación.  </p>
</li>
<li>
<p>Se forma con la estructura <code>clave: valor</code>. Desde el POD se indica que llave quiere consumir del configmap.  </p>
</li>
<li>
<p>Se puede crear mediante un file.conf o en un objeto configmap.  </p>
</li>
<li>
<p>Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero.</p>
</li>
<li>
<p>Lo creamos con <code>kubectl create configmap nginx-config --from-file=examples/nginx.conf</code> y lo vemos con <code>kubectl get cm</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf
configmap/nginx-config created
#
[isx46410800@miguel configmap]$ kubectl get cm
NAME           DATA   AGE
nginx-config   1      14s
#
[isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config
Name:         nginx-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Data
====
nginx.conf:
----
server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
Events:  &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos que se ha creado en formato llave(nginx.conf) y valor la configuración.  </p>
</blockquote>
<ul>
<li>Ejemplo con todos los archivos del subdirectorio y vemos que se crean más llaves-valor:  </li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples
configmap/nginx-config2 created
#
[isx46410800@miguel configmap]$ kubectl get cm
NAME            DATA   AGE
nginx-config    1      4m27s
nginx-config2   2      4s
#
[isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2
Name:         nginx-config2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Data
====
index.html:
----
hola nginx
nginx.conf:
----
server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
Events:  &lt;none&gt;
</code></pre>

<h3 id="montando-volumen-configmap">MONTANDO VOLUMEN CONFIGMAP</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  labels:
    app: front
  data:
    test: hola
    nginx: |
      server {
        listen       80;
        server_name  localhost;

        location / {
                root   /usr/share/nginx/html;
                index  index.html index.htm;
         }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
                root   /usr/share/nginx/html;
         }
      }
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  annotations:
    kubernetes.io/change-cause: &quot;new version nginx&quot;
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
        apiVersion: v1
        kind: Pod
        metadata:
          name: dapi-test-pod
        spec:
          containers:
            - name: nginx
              image: nginx:alpine
              volumeMounts:
              - name: nginx-volume
                mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas
          volumes:
            - name: nginx-volume
              configMap:
                name: nginx-config
                items:
                - key: nginx
                  path: default.conf
</code></pre>

<blockquote>
<p>En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo.  </p>
</blockquote>
<h3 id="volumen-env-configmap">VOLUMEN-ENV CONFIGMAP</h3>
<ul>
<li>Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script:  </li>
</ul>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  labels:
    app: front
data:
  nginx: |
    server {
        listen       9090;
        server_name  localhost;
        location / {
            root   /usr/share/nginx/html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   /usr/share/nginx/html;
        }
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vars
  labels:
    app: front
data:
  db_host: dev.host.local
  db_user: dev_user
  script: |
    echo DB host es $DB_HOST y DB user es $DB_USER &gt; /usr/share/nginx/html/test.html
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          env:
            - name: DB_HOST
              valueFrom:
                configMapKeyRef:
                  name: vars
                  key: db_host
            - name: DB_USER
              valueFrom:
                configMapKeyRef:
                  name: vars
                  key: db_user
          volumeMounts:
          - name: nginx-vol
            mountPath: /etc/nginx/conf.d
          - name: script-vol
            mountPath: /opt
      volumes:
        - name: nginx-vol
          configMap:
            name: nginx-config
            items:
            - key: nginx
              path: default.conf
        - name: script-vol
          configMap:
            name: vars
            items:
            - key: script
              path: script.sh
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh
/ # ls /opt
script.sh
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
KUBERNETES_PORT_443_TCP_PORT=443
NJS_VERSION=0.4.4
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/
DB_HOST=dev.host.local
DB_USER=dev_user
/ # echo $DB_HOST
dev.host.local
/ # apk add python
/ # sh /opt/script.sh 
/ # cat /usr/share/nginx/html/test.html
DB host es dev.host.local y DB user es dev_user
</code></pre>

<h2 id="secrets">SECRETS</h2>
<ul>
<li>
<p>Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no debería de verse. Funciona al estilo configmap.  </p>
</li>
<li>
<p>Lo podemos montar como una variable de entorno o como un volumen.  </p>
</li>
</ul>
<h3 id="crear">CREAR</h3>
<ul>
<li>Ejemplo de como crearlo:<br />
<code>kubectl create secret generic mysecret --from-file=secret-files/text.txt</code><br />
<code>kubectl get secrets</code>  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ cat secret-files/text.txt 
secret1=hola
#
[isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt
secret/mysecret created
#
[isx46410800@miguel secrets]$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-xbv2l   kubernetes.io/service-account-token   3      7d
mysecret              Opaque                                1      7s
#
[isx46410800@miguel secrets]$ kubectl describe secrets mysecret
Name:         mysecret
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Type:  Opaque
Data
====
text.txt:  26 bytes
#
secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml
apiVersion: v1
data:
  text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M=
kind: Secret
metadata:
  creationTimestamp: &quot;2020-10-17T00:55:07Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:text.txt: {}
      f:type: {}
    manager: kubectl-create
    operation: Update
    time: &quot;2020-10-17T00:55:07Z&quot;
  name: mysecret
  namespace: default
  resourceVersion: &quot;72991&quot;
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2
type: Opaque
</code></pre>

<blockquote>
<p>Vemos que el contenido de los secretos no se ven, están cifrados en BASE64, que se puede descrifrar poniendo <code>| base65 -decode</code>  </p>
</blockquote>
<h3 id="manifiestos">MANIFIESTOS</h3>
<ul>
<li>Creando SECRETS con manifiesto:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
</code></pre>

<blockquote>
<p>Para descrifrarlo hay que pasarlo de base64.  </p>
</blockquote>
<ul>
<li>Con Datastring para que lo codifique en base64:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: opaque
stringData:
  username: usertest
  password: test
</code></pre>

<h3 id="envsubts">ENVSUBTS</h3>
<ul>
<li>Herramienta para poder reemplazar contenido de variables por el contenido:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret2
type: opaque
data:
  username: $VAR1
  password: $VAR2
</code></pre>

<pre><code>[isx46410800@miguel secrets]$ export VAR1=miguel
[isx46410800@miguel secrets]$ export VAR2=amoros
[isx46410800@miguel secrets]$ envsubst &lt; secret-secure.yaml &gt; tmp.yaml
[isx46410800@miguel secrets]$ cat tmp.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret2
type: opaque
data:
  username: miguel
  password: amoros
[isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml
</code></pre>

<blockquote>
<p>Luego podemos decode con base64 y obtenemos el resultado.  </p>
</blockquote>
<h3 id="volume-secrets">VOLUME SECRETS</h3>
<ul>
<li>Un ejemplo de crear un secreto y montarlo como volumen:  </li>
</ul>
<pre><code># creamos el secreto
apiVersion: v1
kind: Secret
metadata:
  name: secret1
type: opaque
stringData:
  username: admin
  password: &quot;123456&quot;
---
# montamos el secreto
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: nginx:alpine
    volumeMounts:
    - name: test
      #donde montamos el secreto
      mountPath: &quot;/opt&quot;
      readOnly: true
  volumes:
  - name: test
    secret:
      secretName: secret1
</code></pre>

<blockquote>
<p>En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo.  </p>
</blockquote>
<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml 
secret/secret1 created
pod/mypod created
#
[isx46410800@miguel secrets]$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-xbv2l   kubernetes.io/service-account-token   3      7d
secret1               opaque                                2      6s
#
[isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh
/ # ls /opt/
password  username
/ # cat /opt/password 
123456/ # 
/ # cat /opt/username 
admin/ # 
</code></pre>

<h3 id="env-secrets">ENV SECRETS</h3>
<ul>
<li>Un ejemplo de crear un secreto y montarlo como varibale de entorno:  </li>
</ul>
<pre><code># creamos el secreto
apiVersion: v1
kind: Secret
metadata:
  name: secret1
type: opaque
stringData:
  username: admin
  password: &quot;123456&quot;
---
# montamos el secreto
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: nginx:alpine
    env:
      - name: USERTEST
        valueFrom:
          secretKeyRef:
            name: secret1
            key: username
      - name: PASSWORDTEST
        valueFrom:
          secretKeyRef:
            name: secret1
            key: password
    volumeMounts:
    - name: test
      #donde montamos el secreto
      mountPath: &quot;/opt&quot;
      readOnly: true
  volumes:
  - name: test
    secret:
      secretName: secret1
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml 
secret/secret1 created
pod/mypod created
[isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh
/ # ls /opt/
password  username
/ # echo $USERTEST $PASSWORDTEST
admin 123456
</code></pre>

<h2 id="volumes">VOLUMES</h2>
<ul>
<li>
<p>Sirven para persistir data de los container y no se pierdan cuando se borran.  </p>
</li>
<li>
<p>Tipos de volumenes:  </p>
</li>
<li><strong>EMPTYDIR</strong>: es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola.  </li>
<li><strong>HOSTPATH</strong>: nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo.  </li>
<li><strong>CLOUDVOLS</strong>: en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. Así si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube.  </li>
<li><strong>PV y PVC</strong>: es la reclamación de un PV. El PV contiene un mount y un volume de origen. A través del PVC accedemos al PV, reclamando los recursos que necesita, y éste accede al cloud.  </li>
<li><strong>RECLAIM</strong>: un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).  </li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../jenkins/" class="btn btn-neutral float-right" title="Jenkins">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../docker/" class="btn btn-neutral" title="Docker"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../docker/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../jenkins/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
