{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apuntes Miguel For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"MkDocs"},{"location":"#apuntes-miguel","text":"For full documentation visit mkdocs.org .","title":"Apuntes Miguel"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"ansible/","text":"ANSIBLE DOCUMENTACI\u00d3N Apuntes Ansible Libros Ansible Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX. Instalaci\u00f3n yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada. Inventarios Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Comando b\u00e1sico ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta Ayuda Ansible ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user Playbook Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula) M\u00f3dulos Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes Variables Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" Condicionales Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\" Bucles Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario Roles Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks... Ansible Galaxy Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker' Resumen Repaso de ansible. Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se conecta por SSH. Inventory Sirve para listar todos los hosts, todas las ips que tenemos que aprovisionar. Con cat /etc/ansible/hosts vemos un ejemplo de los hosts que tenemos que administrar: ## [webservers] - nombre del grupo ## alpha.example.org ## beta.example.org ## 192.168.1.100 ## 192.168.1.110 ## db[01:03].intranet.mydomain.net ## db02.intranet.mydomain.net Probamos conexi\u00f3n con algun hosts poniendo ansible alpha.example.org -m ping : [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m ping localhost | SUCCESS => { \"changed\": false, \"ping\": \"pong\" } Podemos indicarle otro fichero con otros host poniendo la opci\u00f3n -i file_hosts . M\u00f3dulos documentaci\u00f3n m\u00f3dulos Por defecto si no pongo el modulo -m, coge shell como m\u00f3dulo: [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -a 'echo hola miguel' localhost | CHANGED | rc=0 >> hola miguel # [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m shell -a 'uname -a' localhost | CHANGED | rc=0 >> Linux miguel 5.3.11-100.fc29.x86_64 #1 SMP Tue Nov 12 20:41:25 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Instalamos un paquete en la m\u00e1quina remota como superusuario(-b) y preguntando la contrase\u00f1a de root en esa m\u00e1quina(-K): [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -b -K -m dnf -a 'name=vim state=present' BECOME password: localhost | SUCCESS => { \"changed\": false, \"msg\": \"Nothing to do\", \"rc\": 0, \"results\": [] } Playbook Se escribe un yaml y son objetos que se escriben tareas que han de hacer en nuestras m\u00e1quinas remotas: --- - hosts: localhost tasks: - name: instala vim dnf: name=vim state=present become: true - name: saludar shell: echo hola Resultado: [isx46410800@miguel ansible]$ ansible-playbook playbook01.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala vim] ************************************************************************************************** ok: [localhost] TASK [saludar] ****************************************************************************************************** changed: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 otro ejemplo que maneja servicios: - hosts: localhost become: true tasks: - name: instala vim dnf: name=vim state=present - name: saludar shell: echo hola - name: detener apache service: name=httpd state=stopped Usuarios Podemos poner el usuario con la opci\u00f3n -u . No obstante en el fichero de configuraci\u00f3n /etc/ansible/ansible.cfg podemos poner [defaults]remote_users=miguel y entonces cada orden coger\u00e1 como usuario miguel. Podemos cargar otro fichero de conf poniendo ANSIBLE_CONFIG=ruta_file_cfg. Handlers Le pide a ansible que cuando haga una tarea success lo notifique para poder hacer otras cosas. isx46410800@miguel ansible]$ cat playbook02.yaml --- - hosts: localhost become: true tasks: - name: instala apache dnf: name=httpd state=present update_cache=true notify: - \"Reinicia el servidor web\" handlers: - name: reinicia el server apache service: name=httpd state=restarted [isx46410800@miguel ansible]$ ansible-playbook playbook02.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala apache] *********************************************************************************************** ok: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Nos saldr\u00eda una notificaci\u00f3n de un handler al instalar, no sale porque ya estaba instalado. CURSO COMPLETO Environment Vemos un ejemplo de como es un ambiente con Ansible, conectando una m\u00e1quina central con el lenguaje Ansible hacia otros hosts con sistemas operativos y ordenando que tiene que tener cada cosa y como conectarse: Nos conectamos a una instancia ubuntu aws por ssh: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem ubuntu@18.134.164.27 Con el un usuario ubuntu de aws, creamos llaves para conectarnos a ese user: ubuntu@ip-172-31-23-107:~$ sudo su - root@ip-172-31-23-107:~# useradd ansible_user root@ip-172-31-23-107:~# passwd ansibler_user [isx46410800@miguel .ssh]$ ssh-keygen -rw-------. 1 isx46410800 isx46410800 2602 Apr 2 23:22 ansibleuser -rw-r--r--. 1 isx46410800 isx46410800 572 Apr 2 23:22 ansibleuser.pub root@ip-172-31-23-107:/home/ubuntu/.ssh# vi authorized_keys root@ip-172-31-23-107:/home/ubuntu# chown -R ubuntu .ssh/ Ahora podremos conectarnos con la llave privada al usuario ubuntu sin autenticar al tener copiada la llave publica: [isx46410800@miguel .ssh]$ ssh -i ansibleuser ubuntu@18.134.164.27 Nos conectamos a un docker con fedora: [isx46410800@miguel curso_ansible]$ docker run --name container -h container -p 2222:22 --privileged -d isx46410800/ansible:ssh Creamos un usuario y copiamos tambi\u00e9n las llaves al usuario para conectarnos: [root@container docker]# adduser fedora [root@container docker]# passwd fedora [root@container docker]# cd /home/fedora/ [root@container fedora]# mkdir .ssh [root@container fedora]# chmod 700 .ssh [root@container fedora]# vi .ssh/authorized_keys [root@container fedora]# chmod 600 .ssh/authorized_keys [root@container fedora]# chown -R fedora /home/fedora/.ssh Inventory Creamos un primer inventario para conectarnos a la m\u00e1quina de amazon: # conexi\u00f3n a un host remoto, indicando nombre host, ip, llave y usuario al que conectamos ec2 ansible_host=18.134.164.27 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu Probamos con la orden ansible -i inventario nombre_host -atributo opcion del atributo: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } El fichero de configuraci\u00f3n de ansible est\u00e1 en /etc/ansible/ansible.cfg. # este fichero de configuraci\u00f3n se escriben reglas para grupos de hosts o hosts sueltos donde ir\u00e1n a buscar las cosas por defecto a este archivo(/etc/ansible/ansible.cfg) [defaults] INVENTORY=./inventory01 Vemos que conecta igual poniendo el inventario como que no: [isx46410800@miguel curso_ansible]$ ansible ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } M\u00f3dulos Los m\u00f3dulos son la cantidad de opciones que podemos hacer a la hora de conectarnos con las m\u00e1quinas: ping, package, service... Vemos todas con la orden ansible-doc --list : ansible-doc file Hacer un ping: ansible -i inventory01 ec2 -m ping Crear un directorio/file(absent, directory, file, hard, link, touch): ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0775\", \"owner\": \"ubuntu\", \"path\": \"/home/ubuntu/crear_directorio\", \"size\": 4096, \"state\": \"directory\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio/crear_file.txt state=touch' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"dest\": \"/home/ubuntu/crear_directorio/crear_file.txt\", \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 0, \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ crear_file.txt Copiar un fichero: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m copy -a 'src=./ansible.cfg dest=/home/ubuntu/crear_directorio' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"checksum\": \"bf35d403c825217ade9f009d13cbdd6fc0a3078f\", \"dest\": \"/home/ubuntu/crear_directorio/ansible.cfg\", \"gid\": 1000, \"group\": \"ubuntu\", \"md5sum\": \"15b402b635fbd568d10b82d4b67da871\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 208, \"src\": \"/home/ubuntu/.ansible/tmp/ansible-tmp-1617401616.4248621-11162-62473546166429/source\", \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ ansible.cfg crear_file.txt # otro ejemplo de crear un file con contenido y copiarlo ansible ec2 -m copy -a \"content='TopSecret' dest='/opt/data/secret.txt'\" A\u00f1adir una linea a un fichero: [isx46410800@miguel curso_ansible]$ cat file.txt fichero de ejemplo para modulo de a\u00f1adir lineas [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m lineinfile -a 'path=/home/ubuntu/crear_directorio/file.txt line=\"a\u00f1adimos esto al modulo lineinfile\"' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"backup\": \"\", \"changed\": true, \"msg\": \"line added\" } ubuntu@ip-172-31-23-107:~$ cat crear_directorio/file.txt fichero de ejemplo para modulo de a\u00f1adir lineas a\u00f1adimos esto al modulo lineinfile Descargar contenido de un URL y enviarlo en un fichero a un host remoto: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m uri -a 'url=https://api.github.com/users/isx46410800/repos dest=/home/ubuntu/crear_directorio/repos.json' Instalar/borrar un paquete: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' -b [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' --become -b para ser superusuario -k para preguntar passwd si hemos creado un user vamos a la maquina y a\u00f1adimos en la fichero /etc/sudoers: user ALL=(ALL:ALL) NOPASSWD:ALL Encender un servicio: ansible ec2 -m service -a \"name=nginx state=started\" Crear un usuario: ansible ec2 -m user -a \"name=miguel state=present\" Hacer una orden normal de comando: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Playbooks En vez de hacerlo en una linea con comandos, se crea un playbook en formato YAML para hacer ahi el listado de tareas a realizar a los hosts: - name: primer ejemplo playbook hosts: ec2 tasks: # creamos un file con contenido dentro - name: hello file is copied copy: content: \"Hello World\" dest: /home/ubuntu/crear_directorio/hello.txt # creamos un segundo file con contenido dentro - name: hi file is copied copy: content: \"Hi World\" dest: /home/ubuntu/crear_directorio/hi.txt # comprimimos estos dos ficheros - name: hello and hi files compressed archive: path: - /home/ubuntu/crear_directorio/hello.txt - /home/ubuntu/crear_directorio/hi.txt dest: /home/ubuntu/crear_directorio/hh.zip format: zip Lo lanzamos con la orden: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook01.yaml Ejemplo de deploy de una web al host remoto: [isx46410800@miguel curso_ansible]$ cat playbook02-web-static.yaml - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present # encendemos el servicio apache - name: apache running service: name: apache2 state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: /var/www/html state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: /var/www/html #/usr/share/nginx/html Resultado: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook02-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [installing apache server] *************************************************************************************** ok: [ec2] TASK [apache running] ************************************************************************************************* ok: [ec2] TASK [creating var directory] **************************************************************************************** ok: [ec2] TASK [static website is deployed] ************************************************************************************ changed: [ec2] PLAY RECAP *********************************************************************************************************** ec2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Variables Podemos usar variables en el fichero de playbook para no tener que escribir lo mismo: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Tambi\u00e9n podemos poner en la orden de ansible-playbook la opcion --extra-vars e indicar la variable y contenido y piyar\u00eda esa variable como prioridad en vez de la del playbook: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es apache2\" } # [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml --extra-vars webserver=httpd PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es httpd\" } Tabi\u00e9n se puede crear un fichero de variables y llamar a donde est\u00e1n las variables: [isx46410800@miguel curso_ansible]$ cat vars.yaml webserver: apache2 webserver_dir: /var/www/html # - name: ejemplo deploy web static hosts: ec2 become: yes vars_files: - vars-yaml tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" Ejemplo pipeline con diferentes Branchs de git: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html git_branch: ansible-course-index-v2 tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Podemos crear una variable de registro con el contenido de una tarea: - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present register: apache_install_output - name: print register in terminal debug: var: apache_install_output - name: copy the output copy: content: \"{{ apache_install_output }}\" dest: /home/ubuntu/crear_directorio/register.txt Ansible facts: - name: ansible facts hosts: ec2 become: yes #gather_facts: no tasks: # print ansible_facts - name: print ansible_facts debug: var: ansible_facts se puede poner con una variable de gather_facts: no y no saldrian los facts Otras variables como inventory_hostaname, hostvars, group_names, groups... Pr\u00e1ctica DEV y PROD Nos queremos conectar a 3 instancas AWS, una de dev y dos de prod. Creamos nuevo inventario: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod2 ansible_host=35.178.101.37 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu [prod] prod1 prod2 [dev] dev1 Probamos conexi\u00f3n despues de meterle la llave publica a cada uno: [isx46410800@miguel curso_ansible]$ ansible -i inventory02_prod_dev all -m ping Podemos simplicar variables a\u00f1adiendo un grupo de variables: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 prod2 ansible_host=35.178.101.37 [prod] prod1 prod2 [prod:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu git_branch=ansible-course-index-v2 [dev] dev1 [dev:vars] git_branch=ansible-course Tambien se puede crear un directorio host_vars -> dev1 --> vars.yaml con las 3 variables asignadas separados por dos puntos. tambien se puede crear un directorio group_vars -> prod -> vars.yaml con las variables del grupo prod:vars. Luego se borraria porque ya las tenemos ah\u00ed. Resultados: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook09-proyect_dev-prod.yaml PLAY [ejemplo deploy en dev y prod] ********************************************************************************** TASK [Gathering Facts] *********************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [installing apache server] ************************************************************************************** ok: [prod1] ok: [prod2] changed: [dev1] TASK [apache running] ************************************************************************************************ ok: [prod1] ok: [dev1] ok: [prod2] TASK [creating var directory] **************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [static website is deployed] ************************************************************************************ changed: [prod1] changed: [prod2] changed: [dev1] PLAY RECAP *********************************************************************************************************** dev1 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod1 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Vemos la info de variables m\u00e1gicas: - name: play for discover magic variables hosts: dev,prod tasks: - name: print inventory_hostname debug: var: inventory_hostname - name: print hostvars debug: var: hostvars - name: print group_names debug: var: group_names - name: print groups debug: var: groups [isx46410800@miguel curso_ansible]$ ansible-playbook 10-playbook-magic_variables.yaml > magic.tmp Podemos conseguir la info de un host con la orden ansible-inventory: [isx46410800@miguel curso_ansible]$ ansible-inventory --host dev1 { \"ansible_host\": \"35.177.51.40\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"git_branch\": \"ansible-course\" } Ansible Task Control Los LOOPS sirven para ejecutar unas mismas tareas pero en una iterando el item que queremos pasarle como variable. Simplificamos una tares poniendo loop y los items debajo a iterar. Ejemplos: become: yes tasks: - name: \"package is installed\" package: name: \"{\u200c{ item }}\" state: latest loop: - mysql - mongodb-org # become: yes tasks: # .... - name: service is up service: name: \"{\u200c{ item }}\" state: started loop: - mysql - mongod En nuestro ejemplo para hacer iterar el loop para que primero haga deploy de una web y despues de otra: - name: ejemplo deploy web static hosts: dev,prod become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy webs staticas - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/{{ item }}.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html loop: - index - about Utilizamos el WHEN como opci\u00f3n para decir que se haga tal cosa sea igual a esa variable. En este caso las variables no se ponen entre corchetes[]. become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: index static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html - name: about static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/about.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html when: git_branch == 'ansible-course' Resultados: TASK [index static website is deployed] ****************************************************************************** changed: [prod2] changed: [prod1] changed: [dev1] TASK [about static website is deployed] ****************************************************************************** skipping: [prod1] skipping: [prod2] changed: [dev1] Otros Ejemplos comunes del WHEN: when: ansible_distribution == 'Ubuntu' when: app_replicas == 12 when: app_replicas < 12 when: ansible_distribution != 'Centos' when: git_branch is defined when: git_branch is not defined when: ( git_branch in [\"master\", \"development\"] ) when: ( app_replicas == 12 ) and ( ansible_distribution == 'Ubuntu') when: - app_replicas == 12 - ansible_distribution == 'Ubuntu' when: ( app_replicas == 12 ) or ( ansible_distribution == 'Ubuntu' ) Ejemplo de crear usuarios segun si est\u00e1n en una maquina y segun el papel que tengan: [isx46410800@miguel curso_ansible]$ cat users.yaml assignments_users: - name: miguel role: developer - name: isabel role: developer - name: cristina role: ops - name: play create users per role hosts: dev,prod become: yes vars_files: - ./users.yaml tasks: - name: user exists per its role user: name: \"{{ item.name }}\" state: present loop: \"{{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) Lo que hacemos es que si un usuario tiene el rol de develop vaya a las maquinas devs y si es ops que vaya a las de prod. Podemos comprobar los usuarios en cada host con la orden: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Podemos delegar una tarea a otro host, es decir en vez de la m\u00e1quina indicada, que se envie a otra con delegate_to : - name: delegar una tarea a otro host con delegate_to hosts: dev tasks: - name: file copied copy: content: este mensaje de prueba dest: /tmp/message.txt delegate_to: localhost Otro ejemplo de delegar tareas: - name: play to show how to use delegate_to in assignment hosts: dev1 tasks: - name: repos list is downloaded uri: url: https://api.github.com/users/atoumi/repos dest: /tmp/git-repos.json delegate_to: localhost Podemos importar la informaci\u00f3n de tareas con el m\u00f3dulo import_tasks . Se crea un archivo aparte de las tareas que se quieren importar y se a\u00f1aden al playbook principal: - name: play import_tasks hosts: dev1 tasks: - name: import nginx tasks import_tasks: nginx_install.yaml Los handers son avisadores de que haga una cosa o notifique si una tarea ha cambiado, ejemplo: - name: play illustrates the Slide of handlers hosts: somehost tasks: - name: t1 module-a: attr1: val1 - name: t2 module-b: attr1: val1 notify: t3 # run t3 only if t2 CHANGED - name: t4 module-d: attr1: val1 handlers: - name: t3 module-c: attr1: val1 Jinja2 templates DOCUMENTACI\u00d3N JINJA Ejemplo de filtros que ponemos ponerle en las tareas con jinja: # https://jinja.palletsprojects.com/en/2.11.x/templates/#builtin-filters - name: play with jinja2 filters hosts: container gather_facts: no vars: git_username: atoumi git_password: Gfdfd445e git_repos: [\"eks-course\", \"ansible-course\", \"react-csv\"] course_lectures_nb: [6, 6, 10, 8, 9] tasks: - name: j2 filter - capitalize debug: msg: | original : {{ git_username }} with filter: {{ git_username | capitalize }} # require: pip3 install passlib - name: j2 filter - password_hash('sha512') debug: msg: | original : {{ git_password }} with filter: {{ git_password | password_hash('sha512') }} - name: j2 filter - length - nb of repos debug: msg: | original : {{ git_repos }} with filter: {{ git_repos | length }} - name: j2 filter - sum - total nb of lectures debug: msg: | original : {{ course_lectures_nb }} with filter: {{ course_lectures_nb | sum }} - name: j2 filter - max - max nb of lectures in a section debug: msg: | original : {{ course_lectures_nb }} Lista de filtros: ansible jinja Ejemplo de poner filtros creando usuarios, metiendolo en la maquina que le toque segun rol y poniendo su passwd. luego nos conectamos y funciona: where \"vars/31-users.yaml\" content is : assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # - name: play create users per role hosts: dev,prod become: yes vars_files: - vars/31-users.yaml tasks: - name: user exists per its role user: name: \"{\u200c{ item.name }}\" state: present password: \"{\u200c{item.password | password_hash('sha512') }}\" loop: \"{\u200c{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) # ssh ahmed@<ip-server> # .. then put the password (ahmed123) Con el modulo template podemos copiar un fichero que tiene variables del sistemas hacia destino. Si lo hacemos con el modulo copy, se copia literalmente sin sustituir las variables: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb #playbook - name: play to generate SPECS report hosts: dev1 tasks: - name: report is generated template: src: ./file_vars.conf dest: /tmp/specs.conf # ubuntu@ip-172-31-19-134:~$ cat /tmp/specs.conf Distribution : Ubuntu Distribution Release : focal Distribution Version : 20.04 Nbre CPU core : 1 cores Total Memory : 978 mb Se pueden poner tambi\u00e9n sintaxi jinja en el fichero como por ejemplo condicionales: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb # {% if ansible_facts.memtotal_mb < 4000 %} The memory size is small {% else %} The memory size is OK {% endif %} Tambien se puede hacer jinja con loop: #playbook - name: play to generate SPECS report hosts: dev,prod vars: app_pages: - home - login - logout app_users: - name: ahmed role: developer - name: mouath role: developer - name: ali role: ops - name: omar role: ops tasks: - name: report is generated template: src: loop_jinja.conf dest: /tmp/app-report.conf #loop_jinja.conf ==== Print app_pages ===== {% for page in app_pages %} {{ page }}.html is a web page {% endfor %} ==== Print app_users ==== {% for u in app_users %} {{ u.name | capitalize }} is {{ u.role }} {% endfor %} Ejemplo de un /etc/hosts (etc/hosts that includes all hosts where {\u200c{ inventory_hostname }} magic variable is the domain name of the target host) - name: play common /etc/hosts hosts: all become: yes tasks: - name: copy /etc/hosts template: src: hosts.j2 dest: /etc/hosts # 127.0.0.1 localhost # The following lnes are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts {% for host in groups['all'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ host }} {% endfor %} Resumen sintaxi JINJA: Print variable or Expression Print the variable by using the variable name surrounded by double braces. {\u200c{ my_var }} # .i.e: {\u200c{ ansible_distribution }} Filters Variables can be transformed/modified by filters. Think about filter as a function {\u200c{ my_var | my_filter }} # .i.e : {\u200c{ app_title | capitalize }} Read it like my_filter(my_var) If Block {% if CONDITION1 %} blah blah blah {% elif CONDITION2 %} blahelif blahelif blahelif . {% else %} blahelse so far {% endif %} #.i.e {% if git_branch == 'master' %} RELEASE: {\u200c{ app_version }} {% else %} SNAPSHOT: {\u200c{ app_version }}-RC{\u200c{ build_number }} {% endif %} For Loop {% for ELEMENT in ARRAY %} Process {\u200c{ ELEMENT }} {% endfor %} #. i.e: assume that ( app_pages = [\"login.html\", \"index.html\"] ) {% for page in app_pages %} <a href=\"https://example.com/{\u200c{ page }}\">{\u200c{ page }}</a> {% endfor %} Ansible Vault Ansible sirve para desencriptar las passwords que salen en un fichero. Ordenes: ansible-vault create users_password.yaml ansible-vault encrypt users_password.yaml ansible-vault decrypt users_password.yaml ansible-vault edit users_password.yaml ansible-vault show users_password.yaml Partimos del ejemplo: #`ansible-vault create users_password.yaml` assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # playbook - name: play use encrypted vars hosts: localhost vars_files: - users_password.yaml tasks: - name: encrypted variable is printed debug: var: assignment_users Podemos crear en ansible.cfg la variable vault_password_file = file.key con una passwd de vault. Ansible Role & Galaxy Los roles ansibles son la forma en que hacemos esto. Cuando se crea un rol, se descompone el playbook en partes y esas partes se encuentran en una estructura de directorios. Ejemplo de instalar un role: ansible-galaxy install role_file ansible-galaxy install -r ./requirements.yaml ansible-galaxy init my_role #estructura de directorios ansible-galaxy --help Importar role en un playbook: en ansible.cfg: [defaults] roles_path = ./roles ansible-galaxy install -r ./requirements.yaml tasks: - import_role: name: role_file vars: { var1: val1.. } Instalar Jenkkins Primer paso a\u00f1adimos en el ansible.cfg el roles_path, donde se instalaran los roles que queremos instalar: [defaults] INVENTORY=./inventory02_prod_dev roles_path=./roles ansible-galaxy : Instala roles de Ansible Galaxy, una plataforma para el intercambio de roles (recetas) Ansible. Podemos encontrar info de lo que queremos instalar con Ansible en Ansible Galaxy . En este caso buscamos Jenkins y vemos la opci\u00f3n de como descargarlo o si vamos al repo github, vemos un ejemplo de playbook para instalarlo. Primero podemos crear un fichero de requisitos de los paquetes a instalar de jenkins como roles para luego poder instalarlos remotamente: # - src: # name: # version: # roles para instalar jenkins - src: geerlingguy.java name: geerlingguy.java - src: geerlingguy.jenkins name: geerlingguy.jenkins [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - hosts: dev1 become: true vars: #jenkins_hostname: jenkins.example.com java_packages: - openjdk-8-jdk roles: - role: geerlingguy.java - role: geerlingguy.jenkins Instalar Docker A\u00f1adimos a los requisitos: - src: geerlingguy.docker name: geerlingguy.docker Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook poniendo que podamos acceder como usuario el usuario ansible ssh(ubuntu): - hosts: prod1,prod2,dev1 become: yes tasks: - name: docker is installed import_role: name: geerlingguy.docker vars: docker_users: - \"{{ ansible_ssh_user }}\" Instalar Kubernetes A\u00f1adimos a requisitos: - src: geerlingguy.kubernetes name: geerlingguy.kubernetes Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - name: play kubernets is up hosts: cluster become: yes tasks: - name: docker role import_role: name: geerlingguy.docker - name: kubernetes role import_role: name: geerlingguy.kubernetes Hariamos 3 maquinas, 1 master y 2 nodos, los a\u00f1adiriamos al inventario y le pondriamos un grupo [cluster] y en cada host la variable role_kubernetes=node/master Ansible Collection A veces los roles no son suficientes con descargarlos y necesitamos las colecciones, que son un conjunto de playbooks, roles, modulos y plugins. El fichero galaxy.yaml es el unico fichero requerido, aunque hay tambien directorios de roles, plugins, docs, playbooks... Orden: ansible-galaxy collection install file_collection ansible-galaxy install -r ./requirements.yaml ansible-galaxy collection init my_collection_file #estructura de directorios ansible-galaxy --help Donde guardarlos: [defaults] INVENTORY=./inventory02_prod_dev COLLECTIONS_PATHS=./collections A\u00f1adimos a requisitos: - collections: newswangerd.collection_demo Instalamos: [isx46410800@miguel curso_ansible]$ ansible-galaxy collection install -r requirements.yaml Playbook: - name: play usage collections hosts: dev1 collections: - newswangerd.collection_demo tasks: - name: module usage from collection real_facts: name: Abdennour - name: role usage from collection import_role: name: factoid Capstone Project - Put all Together in a Real Project with Go, React and MongoDB Idea: Frotend en una maquina tendremos REACT app, en backend tendremos la app GO y todo est\u00e1ra conectado en una VM con la ddbb mongoDB. Creaci\u00f3n de instancias AWS Creamos dos instancias en Amazon: app y db [isx46410800@miguel project_real]$ cat inventory_project app ansible_host=52.56.149.20 db ansible_host=18.130.63.197 [todo] app db [todo:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu #ansible_ssh_pass=ubuntu2021 [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_proyect Comprobamos que esta conexi\u00f3n funciona: [isx46410800@miguel project_real]$ ansible app,db -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } [isx46410800@miguel project_real]$ ansible todo -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } Instalaci\u00f3n MONGODB Para instalar Mongodb se necesita estos pasos Playbook de tareas para instalarlo e iniciarlo en la instancia de DB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database hosts: db become: yes tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes Comprobamos: isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml PLAY [play Database] ************************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [db] TASK [gnupg is installed] ******************************************************************************************** ok: [db] TASK [mongodb-key is added] ****************************************************************************************** changed: [db] TASK [mongo-db repo is enabled] ************************************************************************************** changed: [db] TASK [mongodb-org is installed] ************************************************************************************** changed: [db] TASK [mongod is enable] ********************************************************************************************** changed: [db] PLAY RECAP *********************************************************************************************************** db : ok=6 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ubuntu@ip-172-31-22-5:~$ sudo systemctl status mongod \u25cf mongod.service - MongoDB Database Server Loaded: loaded (/lib/systemd/system/mongod.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-04-06 12:02:22 UTC; 53s ago Docs: https://docs.mongodb.org/manual Main PID: 13440 (mongod) Memory: 64.2M CGroup: /system.slice/mongod.service \u2514\u250013440 /usr/bin/mongod --config /etc/mongod.conf Apr 06 12:02:22 ip-172-31-22-5 systemd[1]: Started MongoDB Database Server. Configuraci\u00f3n y creaci\u00f3n de superuser de MONGODB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database install mongodb hosts: db become: yes tags: - db-install tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes - name: play Database config and create superuser hosts: db become: yes tags: - db-config-admin tasks: # drop admin user if exists (mongo admin --eval 'db.dropUser(\"superadmin\")') # create admin user (mongo admin --eval 'db.createUser'({ user: \"superadmin\", pwd:})) - name: create admin user command: \"{{ item }}\" loop: - mongo admin --eval 'db.dropUser(\"{{ db_admin_user }}\")' - | mongo admin --eval 'db.createUser( { user: \"{{ db_admin_user }}\", pwd: \"{{ db_admin_pass }}\", roles: [ { role: \"clusterAdmin\", db: \"admin\" }, { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } )' # enable security (/etc/mongod.conf) ---> Restart Mongodb #> security: #> authorization: \"enabled\" - name: security is enabled blockinfile: path: /etc/mongod.conf block: | security: authorization: \"enabled\" state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted las variables hacen referencias al archivo host_vars/db/secret.yaml Creamos un file de contrase\u00f1a para encriptarlo con vault: [isx46410800@miguel project_real]$ mkdir -p host_vars/db # [isx46410800@miguel project_real]$ cat key.txt miguel14031993 # [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt # [isx46410800@miguel project_real]$ ansible-vault create host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r Comprobamos que est\u00e1 todo en orden: ubuntu@ip-172-31-22-5:~$ systemctl status mongod ubuntu@ip-172-31-22-5:~$ cat /etc/mongod.conf ubuntu@ip-172-31-22-5:~$ mongo admin -u superadmin -p PassMy1243r Configuramos ahora el primer usuario mongo : ... - name: play - rest of configuration hosts: db become: yes tags: - db-config tasks: - name: pip3 installed package: name: python3-pip state: latest - name: pip pymongo installed pip: name: pymongo state: latest - name: todo db user exists mongodb_user: login_user: \"{{ db_admin_user }}\" login_password: \"{{ db_admin_pass }}\" database: admin user: \"{{ db_todo_user }}\" password: \"{{ db_todo_pass }}\" state: present roles: - db: \"{{ db_name_todo }}\" role: readWrite [isx46410800@miguel project_real]$ ansible-vault edit host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r db_name_todo: test db_todo_user: todo db_todo_pass: todo [isx46410800@miguel project_real]$ ansible-inventory --host db { \"ansible_host\": \"35.176.225.221\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"db_admin_pass\": \"PassMy1243r\", \"db_admin_user\": \"superadmin\", \"db_name_todo\": \"test\", \"db_todo_pass\": \"todo\", \"db_todo_user\": \"todo\" } Lanzamos solo la tercera parte del playbook a\u00f1adido con la orden: [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config A\u00f1adimos la linea de las interfaces para que reciba desde cualquier ip: - name: db accepts connection from anywhere lineinfile: path: /etc/mongod.conf line: \" bindIp: 0.0.0.0\" regexp: '^(.*)binIp(.*)$' state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config Backend APP Hacemos playbook de instalar goland en el host app: [isx46410800@miguel project_real]$ cat playbooks/backend.yaml - name: play Backend hosts: app become: yes tags: - be-pre-build tasks: - name: go is installed import_role: name: gantsign.golang vars: golang_version: \"1.14\" golang_packages: - github.com/gorilla/mux - go.gomongodb.org/mongo-driver/mongo golang_users: - \"{{ ansible_ssh_user }}\" - name: play Backend hosts: app become: yes tags: - be-build tasks: - name: workspace build exist file: path: /opt/build_dir state: directory - name: git checkout git: repo: https://gitlab.com/isx46410800/curso_ansible.git dest: /opt/build_dir/curso_ansible - name: go build shell: . /etc/profile;go build -o /tmp/todo args: chdir: /opt/build_dir/curso_ansible/server [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt roles_path=./roles [isx46410800@miguel project_real]$ cat requirements.yaml - src: gantsign.golang name: gantsign.golang [isx46410800@miguel project_real]$ ansible-playbook playbooks/backend.yaml --tags be-pre-build *https://github.com/kubernetes-tn/go-to-do-app * *https://github.com/abdennour/ansible-course *","title":"Ansible"},{"location":"ansible/#ansible","text":"DOCUMENTACI\u00d3N Apuntes Ansible Libros Ansible Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX.","title":"ANSIBLE"},{"location":"ansible/#instalacion","text":"yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada.","title":"Instalaci\u00f3n"},{"location":"ansible/#inventarios","text":"Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Inventarios"},{"location":"ansible/#comando-basico","text":"ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta","title":"Comando b\u00e1sico"},{"location":"ansible/#ayuda-ansible","text":"ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user","title":"Ayuda Ansible"},{"location":"ansible/#playbook","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula)","title":"Playbook"},{"location":"ansible/#modulos","text":"Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes","title":"M\u00f3dulos"},{"location":"ansible/#variables","text":"Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\"","title":"Variables"},{"location":"ansible/#condicionales","text":"Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\"","title":"Condicionales"},{"location":"ansible/#bucles","text":"Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario","title":"Bucles"},{"location":"ansible/#roles","text":"Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks...","title":"Roles"},{"location":"ansible/#ansible-galaxy","text":"Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker'","title":"Ansible Galaxy"},{"location":"ansible/#resumen","text":"Repaso de ansible. Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se conecta por SSH.","title":"Resumen"},{"location":"ansible/#inventory","text":"Sirve para listar todos los hosts, todas las ips que tenemos que aprovisionar. Con cat /etc/ansible/hosts vemos un ejemplo de los hosts que tenemos que administrar: ## [webservers] - nombre del grupo ## alpha.example.org ## beta.example.org ## 192.168.1.100 ## 192.168.1.110 ## db[01:03].intranet.mydomain.net ## db02.intranet.mydomain.net Probamos conexi\u00f3n con algun hosts poniendo ansible alpha.example.org -m ping : [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m ping localhost | SUCCESS => { \"changed\": false, \"ping\": \"pong\" } Podemos indicarle otro fichero con otros host poniendo la opci\u00f3n -i file_hosts .","title":"Inventory"},{"location":"ansible/#modulos_1","text":"documentaci\u00f3n m\u00f3dulos Por defecto si no pongo el modulo -m, coge shell como m\u00f3dulo: [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -a 'echo hola miguel' localhost | CHANGED | rc=0 >> hola miguel # [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m shell -a 'uname -a' localhost | CHANGED | rc=0 >> Linux miguel 5.3.11-100.fc29.x86_64 #1 SMP Tue Nov 12 20:41:25 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Instalamos un paquete en la m\u00e1quina remota como superusuario(-b) y preguntando la contrase\u00f1a de root en esa m\u00e1quina(-K): [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -b -K -m dnf -a 'name=vim state=present' BECOME password: localhost | SUCCESS => { \"changed\": false, \"msg\": \"Nothing to do\", \"rc\": 0, \"results\": [] }","title":"M\u00f3dulos"},{"location":"ansible/#playbook_1","text":"Se escribe un yaml y son objetos que se escriben tareas que han de hacer en nuestras m\u00e1quinas remotas: --- - hosts: localhost tasks: - name: instala vim dnf: name=vim state=present become: true - name: saludar shell: echo hola Resultado: [isx46410800@miguel ansible]$ ansible-playbook playbook01.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala vim] ************************************************************************************************** ok: [localhost] TASK [saludar] ****************************************************************************************************** changed: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 otro ejemplo que maneja servicios: - hosts: localhost become: true tasks: - name: instala vim dnf: name=vim state=present - name: saludar shell: echo hola - name: detener apache service: name=httpd state=stopped","title":"Playbook"},{"location":"ansible/#usuarios","text":"Podemos poner el usuario con la opci\u00f3n -u . No obstante en el fichero de configuraci\u00f3n /etc/ansible/ansible.cfg podemos poner [defaults]remote_users=miguel y entonces cada orden coger\u00e1 como usuario miguel. Podemos cargar otro fichero de conf poniendo ANSIBLE_CONFIG=ruta_file_cfg.","title":"Usuarios"},{"location":"ansible/#handlers","text":"Le pide a ansible que cuando haga una tarea success lo notifique para poder hacer otras cosas. isx46410800@miguel ansible]$ cat playbook02.yaml --- - hosts: localhost become: true tasks: - name: instala apache dnf: name=httpd state=present update_cache=true notify: - \"Reinicia el servidor web\" handlers: - name: reinicia el server apache service: name=httpd state=restarted [isx46410800@miguel ansible]$ ansible-playbook playbook02.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala apache] *********************************************************************************************** ok: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Nos saldr\u00eda una notificaci\u00f3n de un handler al instalar, no sale porque ya estaba instalado.","title":"Handlers"},{"location":"ansible/#curso-completo","text":"","title":"CURSO COMPLETO"},{"location":"ansible/#environment","text":"Vemos un ejemplo de como es un ambiente con Ansible, conectando una m\u00e1quina central con el lenguaje Ansible hacia otros hosts con sistemas operativos y ordenando que tiene que tener cada cosa y como conectarse: Nos conectamos a una instancia ubuntu aws por ssh: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem ubuntu@18.134.164.27 Con el un usuario ubuntu de aws, creamos llaves para conectarnos a ese user: ubuntu@ip-172-31-23-107:~$ sudo su - root@ip-172-31-23-107:~# useradd ansible_user root@ip-172-31-23-107:~# passwd ansibler_user [isx46410800@miguel .ssh]$ ssh-keygen -rw-------. 1 isx46410800 isx46410800 2602 Apr 2 23:22 ansibleuser -rw-r--r--. 1 isx46410800 isx46410800 572 Apr 2 23:22 ansibleuser.pub root@ip-172-31-23-107:/home/ubuntu/.ssh# vi authorized_keys root@ip-172-31-23-107:/home/ubuntu# chown -R ubuntu .ssh/ Ahora podremos conectarnos con la llave privada al usuario ubuntu sin autenticar al tener copiada la llave publica: [isx46410800@miguel .ssh]$ ssh -i ansibleuser ubuntu@18.134.164.27 Nos conectamos a un docker con fedora: [isx46410800@miguel curso_ansible]$ docker run --name container -h container -p 2222:22 --privileged -d isx46410800/ansible:ssh Creamos un usuario y copiamos tambi\u00e9n las llaves al usuario para conectarnos: [root@container docker]# adduser fedora [root@container docker]# passwd fedora [root@container docker]# cd /home/fedora/ [root@container fedora]# mkdir .ssh [root@container fedora]# chmod 700 .ssh [root@container fedora]# vi .ssh/authorized_keys [root@container fedora]# chmod 600 .ssh/authorized_keys [root@container fedora]# chown -R fedora /home/fedora/.ssh","title":"Environment"},{"location":"ansible/#inventory_1","text":"Creamos un primer inventario para conectarnos a la m\u00e1quina de amazon: # conexi\u00f3n a un host remoto, indicando nombre host, ip, llave y usuario al que conectamos ec2 ansible_host=18.134.164.27 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu Probamos con la orden ansible -i inventario nombre_host -atributo opcion del atributo: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } El fichero de configuraci\u00f3n de ansible est\u00e1 en /etc/ansible/ansible.cfg. # este fichero de configuraci\u00f3n se escriben reglas para grupos de hosts o hosts sueltos donde ir\u00e1n a buscar las cosas por defecto a este archivo(/etc/ansible/ansible.cfg) [defaults] INVENTORY=./inventory01 Vemos que conecta igual poniendo el inventario como que no: [isx46410800@miguel curso_ansible]$ ansible ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Inventory"},{"location":"ansible/#modulos_2","text":"Los m\u00f3dulos son la cantidad de opciones que podemos hacer a la hora de conectarnos con las m\u00e1quinas: ping, package, service... Vemos todas con la orden ansible-doc --list : ansible-doc file Hacer un ping: ansible -i inventory01 ec2 -m ping Crear un directorio/file(absent, directory, file, hard, link, touch): ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0775\", \"owner\": \"ubuntu\", \"path\": \"/home/ubuntu/crear_directorio\", \"size\": 4096, \"state\": \"directory\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio/crear_file.txt state=touch' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"dest\": \"/home/ubuntu/crear_directorio/crear_file.txt\", \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 0, \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ crear_file.txt Copiar un fichero: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m copy -a 'src=./ansible.cfg dest=/home/ubuntu/crear_directorio' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"checksum\": \"bf35d403c825217ade9f009d13cbdd6fc0a3078f\", \"dest\": \"/home/ubuntu/crear_directorio/ansible.cfg\", \"gid\": 1000, \"group\": \"ubuntu\", \"md5sum\": \"15b402b635fbd568d10b82d4b67da871\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 208, \"src\": \"/home/ubuntu/.ansible/tmp/ansible-tmp-1617401616.4248621-11162-62473546166429/source\", \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ ansible.cfg crear_file.txt # otro ejemplo de crear un file con contenido y copiarlo ansible ec2 -m copy -a \"content='TopSecret' dest='/opt/data/secret.txt'\" A\u00f1adir una linea a un fichero: [isx46410800@miguel curso_ansible]$ cat file.txt fichero de ejemplo para modulo de a\u00f1adir lineas [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m lineinfile -a 'path=/home/ubuntu/crear_directorio/file.txt line=\"a\u00f1adimos esto al modulo lineinfile\"' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"backup\": \"\", \"changed\": true, \"msg\": \"line added\" } ubuntu@ip-172-31-23-107:~$ cat crear_directorio/file.txt fichero de ejemplo para modulo de a\u00f1adir lineas a\u00f1adimos esto al modulo lineinfile Descargar contenido de un URL y enviarlo en un fichero a un host remoto: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m uri -a 'url=https://api.github.com/users/isx46410800/repos dest=/home/ubuntu/crear_directorio/repos.json' Instalar/borrar un paquete: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' -b [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' --become -b para ser superusuario -k para preguntar passwd si hemos creado un user vamos a la maquina y a\u00f1adimos en la fichero /etc/sudoers: user ALL=(ALL:ALL) NOPASSWD:ALL Encender un servicio: ansible ec2 -m service -a \"name=nginx state=started\" Crear un usuario: ansible ec2 -m user -a \"name=miguel state=present\" Hacer una orden normal de comando: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd'","title":"M\u00f3dulos"},{"location":"ansible/#playbooks","text":"En vez de hacerlo en una linea con comandos, se crea un playbook en formato YAML para hacer ahi el listado de tareas a realizar a los hosts: - name: primer ejemplo playbook hosts: ec2 tasks: # creamos un file con contenido dentro - name: hello file is copied copy: content: \"Hello World\" dest: /home/ubuntu/crear_directorio/hello.txt # creamos un segundo file con contenido dentro - name: hi file is copied copy: content: \"Hi World\" dest: /home/ubuntu/crear_directorio/hi.txt # comprimimos estos dos ficheros - name: hello and hi files compressed archive: path: - /home/ubuntu/crear_directorio/hello.txt - /home/ubuntu/crear_directorio/hi.txt dest: /home/ubuntu/crear_directorio/hh.zip format: zip Lo lanzamos con la orden: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook01.yaml Ejemplo de deploy de una web al host remoto: [isx46410800@miguel curso_ansible]$ cat playbook02-web-static.yaml - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present # encendemos el servicio apache - name: apache running service: name: apache2 state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: /var/www/html state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: /var/www/html #/usr/share/nginx/html Resultado: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook02-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [installing apache server] *************************************************************************************** ok: [ec2] TASK [apache running] ************************************************************************************************* ok: [ec2] TASK [creating var directory] **************************************************************************************** ok: [ec2] TASK [static website is deployed] ************************************************************************************ changed: [ec2] PLAY RECAP *********************************************************************************************************** ec2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0","title":"Playbooks"},{"location":"ansible/#variables_1","text":"Podemos usar variables en el fichero de playbook para no tener que escribir lo mismo: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Tambi\u00e9n podemos poner en la orden de ansible-playbook la opcion --extra-vars e indicar la variable y contenido y piyar\u00eda esa variable como prioridad en vez de la del playbook: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es apache2\" } # [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml --extra-vars webserver=httpd PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es httpd\" } Tabi\u00e9n se puede crear un fichero de variables y llamar a donde est\u00e1n las variables: [isx46410800@miguel curso_ansible]$ cat vars.yaml webserver: apache2 webserver_dir: /var/www/html # - name: ejemplo deploy web static hosts: ec2 become: yes vars_files: - vars-yaml tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" Ejemplo pipeline con diferentes Branchs de git: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html git_branch: ansible-course-index-v2 tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Podemos crear una variable de registro con el contenido de una tarea: - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present register: apache_install_output - name: print register in terminal debug: var: apache_install_output - name: copy the output copy: content: \"{{ apache_install_output }}\" dest: /home/ubuntu/crear_directorio/register.txt Ansible facts: - name: ansible facts hosts: ec2 become: yes #gather_facts: no tasks: # print ansible_facts - name: print ansible_facts debug: var: ansible_facts se puede poner con una variable de gather_facts: no y no saldrian los facts Otras variables como inventory_hostaname, hostvars, group_names, groups...","title":"Variables"},{"location":"ansible/#practica-dev-y-prod","text":"Nos queremos conectar a 3 instancas AWS, una de dev y dos de prod. Creamos nuevo inventario: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod2 ansible_host=35.178.101.37 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu [prod] prod1 prod2 [dev] dev1 Probamos conexi\u00f3n despues de meterle la llave publica a cada uno: [isx46410800@miguel curso_ansible]$ ansible -i inventory02_prod_dev all -m ping Podemos simplicar variables a\u00f1adiendo un grupo de variables: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 prod2 ansible_host=35.178.101.37 [prod] prod1 prod2 [prod:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu git_branch=ansible-course-index-v2 [dev] dev1 [dev:vars] git_branch=ansible-course Tambien se puede crear un directorio host_vars -> dev1 --> vars.yaml con las 3 variables asignadas separados por dos puntos. tambien se puede crear un directorio group_vars -> prod -> vars.yaml con las variables del grupo prod:vars. Luego se borraria porque ya las tenemos ah\u00ed. Resultados: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook09-proyect_dev-prod.yaml PLAY [ejemplo deploy en dev y prod] ********************************************************************************** TASK [Gathering Facts] *********************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [installing apache server] ************************************************************************************** ok: [prod1] ok: [prod2] changed: [dev1] TASK [apache running] ************************************************************************************************ ok: [prod1] ok: [dev1] ok: [prod2] TASK [creating var directory] **************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [static website is deployed] ************************************************************************************ changed: [prod1] changed: [prod2] changed: [dev1] PLAY RECAP *********************************************************************************************************** dev1 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod1 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Vemos la info de variables m\u00e1gicas: - name: play for discover magic variables hosts: dev,prod tasks: - name: print inventory_hostname debug: var: inventory_hostname - name: print hostvars debug: var: hostvars - name: print group_names debug: var: group_names - name: print groups debug: var: groups [isx46410800@miguel curso_ansible]$ ansible-playbook 10-playbook-magic_variables.yaml > magic.tmp Podemos conseguir la info de un host con la orden ansible-inventory: [isx46410800@miguel curso_ansible]$ ansible-inventory --host dev1 { \"ansible_host\": \"35.177.51.40\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"git_branch\": \"ansible-course\" }","title":"Pr\u00e1ctica DEV y PROD"},{"location":"ansible/#ansible-task-control","text":"Los LOOPS sirven para ejecutar unas mismas tareas pero en una iterando el item que queremos pasarle como variable. Simplificamos una tares poniendo loop y los items debajo a iterar. Ejemplos: become: yes tasks: - name: \"package is installed\" package: name: \"{\u200c{ item }}\" state: latest loop: - mysql - mongodb-org # become: yes tasks: # .... - name: service is up service: name: \"{\u200c{ item }}\" state: started loop: - mysql - mongod En nuestro ejemplo para hacer iterar el loop para que primero haga deploy de una web y despues de otra: - name: ejemplo deploy web static hosts: dev,prod become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy webs staticas - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/{{ item }}.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html loop: - index - about Utilizamos el WHEN como opci\u00f3n para decir que se haga tal cosa sea igual a esa variable. En este caso las variables no se ponen entre corchetes[]. become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: index static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html - name: about static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/about.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html when: git_branch == 'ansible-course' Resultados: TASK [index static website is deployed] ****************************************************************************** changed: [prod2] changed: [prod1] changed: [dev1] TASK [about static website is deployed] ****************************************************************************** skipping: [prod1] skipping: [prod2] changed: [dev1] Otros Ejemplos comunes del WHEN: when: ansible_distribution == 'Ubuntu' when: app_replicas == 12 when: app_replicas < 12 when: ansible_distribution != 'Centos' when: git_branch is defined when: git_branch is not defined when: ( git_branch in [\"master\", \"development\"] ) when: ( app_replicas == 12 ) and ( ansible_distribution == 'Ubuntu') when: - app_replicas == 12 - ansible_distribution == 'Ubuntu' when: ( app_replicas == 12 ) or ( ansible_distribution == 'Ubuntu' ) Ejemplo de crear usuarios segun si est\u00e1n en una maquina y segun el papel que tengan: [isx46410800@miguel curso_ansible]$ cat users.yaml assignments_users: - name: miguel role: developer - name: isabel role: developer - name: cristina role: ops - name: play create users per role hosts: dev,prod become: yes vars_files: - ./users.yaml tasks: - name: user exists per its role user: name: \"{{ item.name }}\" state: present loop: \"{{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) Lo que hacemos es que si un usuario tiene el rol de develop vaya a las maquinas devs y si es ops que vaya a las de prod. Podemos comprobar los usuarios en cada host con la orden: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Podemos delegar una tarea a otro host, es decir en vez de la m\u00e1quina indicada, que se envie a otra con delegate_to : - name: delegar una tarea a otro host con delegate_to hosts: dev tasks: - name: file copied copy: content: este mensaje de prueba dest: /tmp/message.txt delegate_to: localhost Otro ejemplo de delegar tareas: - name: play to show how to use delegate_to in assignment hosts: dev1 tasks: - name: repos list is downloaded uri: url: https://api.github.com/users/atoumi/repos dest: /tmp/git-repos.json delegate_to: localhost Podemos importar la informaci\u00f3n de tareas con el m\u00f3dulo import_tasks . Se crea un archivo aparte de las tareas que se quieren importar y se a\u00f1aden al playbook principal: - name: play import_tasks hosts: dev1 tasks: - name: import nginx tasks import_tasks: nginx_install.yaml Los handers son avisadores de que haga una cosa o notifique si una tarea ha cambiado, ejemplo: - name: play illustrates the Slide of handlers hosts: somehost tasks: - name: t1 module-a: attr1: val1 - name: t2 module-b: attr1: val1 notify: t3 # run t3 only if t2 CHANGED - name: t4 module-d: attr1: val1 handlers: - name: t3 module-c: attr1: val1","title":"Ansible Task Control"},{"location":"ansible/#jinja2-templates","text":"DOCUMENTACI\u00d3N JINJA Ejemplo de filtros que ponemos ponerle en las tareas con jinja: # https://jinja.palletsprojects.com/en/2.11.x/templates/#builtin-filters - name: play with jinja2 filters hosts: container gather_facts: no vars: git_username: atoumi git_password: Gfdfd445e git_repos: [\"eks-course\", \"ansible-course\", \"react-csv\"] course_lectures_nb: [6, 6, 10, 8, 9] tasks: - name: j2 filter - capitalize debug: msg: | original : {{ git_username }} with filter: {{ git_username | capitalize }} # require: pip3 install passlib - name: j2 filter - password_hash('sha512') debug: msg: | original : {{ git_password }} with filter: {{ git_password | password_hash('sha512') }} - name: j2 filter - length - nb of repos debug: msg: | original : {{ git_repos }} with filter: {{ git_repos | length }} - name: j2 filter - sum - total nb of lectures debug: msg: | original : {{ course_lectures_nb }} with filter: {{ course_lectures_nb | sum }} - name: j2 filter - max - max nb of lectures in a section debug: msg: | original : {{ course_lectures_nb }} Lista de filtros: ansible jinja Ejemplo de poner filtros creando usuarios, metiendolo en la maquina que le toque segun rol y poniendo su passwd. luego nos conectamos y funciona: where \"vars/31-users.yaml\" content is : assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # - name: play create users per role hosts: dev,prod become: yes vars_files: - vars/31-users.yaml tasks: - name: user exists per its role user: name: \"{\u200c{ item.name }}\" state: present password: \"{\u200c{item.password | password_hash('sha512') }}\" loop: \"{\u200c{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) # ssh ahmed@<ip-server> # .. then put the password (ahmed123) Con el modulo template podemos copiar un fichero que tiene variables del sistemas hacia destino. Si lo hacemos con el modulo copy, se copia literalmente sin sustituir las variables: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb #playbook - name: play to generate SPECS report hosts: dev1 tasks: - name: report is generated template: src: ./file_vars.conf dest: /tmp/specs.conf # ubuntu@ip-172-31-19-134:~$ cat /tmp/specs.conf Distribution : Ubuntu Distribution Release : focal Distribution Version : 20.04 Nbre CPU core : 1 cores Total Memory : 978 mb Se pueden poner tambi\u00e9n sintaxi jinja en el fichero como por ejemplo condicionales: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb # {% if ansible_facts.memtotal_mb < 4000 %} The memory size is small {% else %} The memory size is OK {% endif %} Tambien se puede hacer jinja con loop: #playbook - name: play to generate SPECS report hosts: dev,prod vars: app_pages: - home - login - logout app_users: - name: ahmed role: developer - name: mouath role: developer - name: ali role: ops - name: omar role: ops tasks: - name: report is generated template: src: loop_jinja.conf dest: /tmp/app-report.conf #loop_jinja.conf ==== Print app_pages ===== {% for page in app_pages %} {{ page }}.html is a web page {% endfor %} ==== Print app_users ==== {% for u in app_users %} {{ u.name | capitalize }} is {{ u.role }} {% endfor %} Ejemplo de un /etc/hosts (etc/hosts that includes all hosts where {\u200c{ inventory_hostname }} magic variable is the domain name of the target host) - name: play common /etc/hosts hosts: all become: yes tasks: - name: copy /etc/hosts template: src: hosts.j2 dest: /etc/hosts # 127.0.0.1 localhost # The following lnes are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts {% for host in groups['all'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ host }} {% endfor %} Resumen sintaxi JINJA: Print variable or Expression Print the variable by using the variable name surrounded by double braces. {\u200c{ my_var }} # .i.e: {\u200c{ ansible_distribution }} Filters Variables can be transformed/modified by filters. Think about filter as a function {\u200c{ my_var | my_filter }} # .i.e : {\u200c{ app_title | capitalize }} Read it like my_filter(my_var) If Block {% if CONDITION1 %} blah blah blah {% elif CONDITION2 %} blahelif blahelif blahelif . {% else %} blahelse so far {% endif %} #.i.e {% if git_branch == 'master' %} RELEASE: {\u200c{ app_version }} {% else %} SNAPSHOT: {\u200c{ app_version }}-RC{\u200c{ build_number }} {% endif %} For Loop {% for ELEMENT in ARRAY %} Process {\u200c{ ELEMENT }} {% endfor %} #. i.e: assume that ( app_pages = [\"login.html\", \"index.html\"] ) {% for page in app_pages %} <a href=\"https://example.com/{\u200c{ page }}\">{\u200c{ page }}</a> {% endfor %}","title":"Jinja2 templates"},{"location":"ansible/#ansible-vault","text":"Ansible sirve para desencriptar las passwords que salen en un fichero. Ordenes: ansible-vault create users_password.yaml ansible-vault encrypt users_password.yaml ansible-vault decrypt users_password.yaml ansible-vault edit users_password.yaml ansible-vault show users_password.yaml Partimos del ejemplo: #`ansible-vault create users_password.yaml` assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # playbook - name: play use encrypted vars hosts: localhost vars_files: - users_password.yaml tasks: - name: encrypted variable is printed debug: var: assignment_users Podemos crear en ansible.cfg la variable vault_password_file = file.key con una passwd de vault.","title":"Ansible Vault"},{"location":"ansible/#ansible-role-galaxy","text":"Los roles ansibles son la forma en que hacemos esto. Cuando se crea un rol, se descompone el playbook en partes y esas partes se encuentran en una estructura de directorios. Ejemplo de instalar un role: ansible-galaxy install role_file ansible-galaxy install -r ./requirements.yaml ansible-galaxy init my_role #estructura de directorios ansible-galaxy --help Importar role en un playbook: en ansible.cfg: [defaults] roles_path = ./roles ansible-galaxy install -r ./requirements.yaml tasks: - import_role: name: role_file vars: { var1: val1.. }","title":"Ansible Role &amp; Galaxy"},{"location":"ansible/#instalar-jenkkins","text":"Primer paso a\u00f1adimos en el ansible.cfg el roles_path, donde se instalaran los roles que queremos instalar: [defaults] INVENTORY=./inventory02_prod_dev roles_path=./roles ansible-galaxy : Instala roles de Ansible Galaxy, una plataforma para el intercambio de roles (recetas) Ansible. Podemos encontrar info de lo que queremos instalar con Ansible en Ansible Galaxy . En este caso buscamos Jenkins y vemos la opci\u00f3n de como descargarlo o si vamos al repo github, vemos un ejemplo de playbook para instalarlo. Primero podemos crear un fichero de requisitos de los paquetes a instalar de jenkins como roles para luego poder instalarlos remotamente: # - src: # name: # version: # roles para instalar jenkins - src: geerlingguy.java name: geerlingguy.java - src: geerlingguy.jenkins name: geerlingguy.jenkins [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - hosts: dev1 become: true vars: #jenkins_hostname: jenkins.example.com java_packages: - openjdk-8-jdk roles: - role: geerlingguy.java - role: geerlingguy.jenkins","title":"Instalar Jenkkins"},{"location":"ansible/#instalar-docker","text":"A\u00f1adimos a los requisitos: - src: geerlingguy.docker name: geerlingguy.docker Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook poniendo que podamos acceder como usuario el usuario ansible ssh(ubuntu): - hosts: prod1,prod2,dev1 become: yes tasks: - name: docker is installed import_role: name: geerlingguy.docker vars: docker_users: - \"{{ ansible_ssh_user }}\"","title":"Instalar Docker"},{"location":"ansible/#instalar-kubernetes","text":"A\u00f1adimos a requisitos: - src: geerlingguy.kubernetes name: geerlingguy.kubernetes Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - name: play kubernets is up hosts: cluster become: yes tasks: - name: docker role import_role: name: geerlingguy.docker - name: kubernetes role import_role: name: geerlingguy.kubernetes Hariamos 3 maquinas, 1 master y 2 nodos, los a\u00f1adiriamos al inventario y le pondriamos un grupo [cluster] y en cada host la variable role_kubernetes=node/master","title":"Instalar Kubernetes"},{"location":"ansible/#ansible-collection","text":"A veces los roles no son suficientes con descargarlos y necesitamos las colecciones, que son un conjunto de playbooks, roles, modulos y plugins. El fichero galaxy.yaml es el unico fichero requerido, aunque hay tambien directorios de roles, plugins, docs, playbooks... Orden: ansible-galaxy collection install file_collection ansible-galaxy install -r ./requirements.yaml ansible-galaxy collection init my_collection_file #estructura de directorios ansible-galaxy --help Donde guardarlos: [defaults] INVENTORY=./inventory02_prod_dev COLLECTIONS_PATHS=./collections A\u00f1adimos a requisitos: - collections: newswangerd.collection_demo Instalamos: [isx46410800@miguel curso_ansible]$ ansible-galaxy collection install -r requirements.yaml Playbook: - name: play usage collections hosts: dev1 collections: - newswangerd.collection_demo tasks: - name: module usage from collection real_facts: name: Abdennour - name: role usage from collection import_role: name: factoid","title":"Ansible Collection"},{"location":"ansible/#capstone-project-put-all-together-in-a-real-project-with-go-react-and-mongodb","text":"Idea: Frotend en una maquina tendremos REACT app, en backend tendremos la app GO y todo est\u00e1ra conectado en una VM con la ddbb mongoDB.","title":"Capstone Project - Put all Together in a Real Project with Go, React and MongoDB"},{"location":"ansible/#creacion-de-instancias-aws","text":"Creamos dos instancias en Amazon: app y db [isx46410800@miguel project_real]$ cat inventory_project app ansible_host=52.56.149.20 db ansible_host=18.130.63.197 [todo] app db [todo:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu #ansible_ssh_pass=ubuntu2021 [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_proyect Comprobamos que esta conexi\u00f3n funciona: [isx46410800@miguel project_real]$ ansible app,db -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } [isx46410800@miguel project_real]$ ansible todo -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Creaci\u00f3n de instancias AWS"},{"location":"ansible/#instalacion-mongodb","text":"Para instalar Mongodb se necesita estos pasos Playbook de tareas para instalarlo e iniciarlo en la instancia de DB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database hosts: db become: yes tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes Comprobamos: isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml PLAY [play Database] ************************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [db] TASK [gnupg is installed] ******************************************************************************************** ok: [db] TASK [mongodb-key is added] ****************************************************************************************** changed: [db] TASK [mongo-db repo is enabled] ************************************************************************************** changed: [db] TASK [mongodb-org is installed] ************************************************************************************** changed: [db] TASK [mongod is enable] ********************************************************************************************** changed: [db] PLAY RECAP *********************************************************************************************************** db : ok=6 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ubuntu@ip-172-31-22-5:~$ sudo systemctl status mongod \u25cf mongod.service - MongoDB Database Server Loaded: loaded (/lib/systemd/system/mongod.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-04-06 12:02:22 UTC; 53s ago Docs: https://docs.mongodb.org/manual Main PID: 13440 (mongod) Memory: 64.2M CGroup: /system.slice/mongod.service \u2514\u250013440 /usr/bin/mongod --config /etc/mongod.conf Apr 06 12:02:22 ip-172-31-22-5 systemd[1]: Started MongoDB Database Server. Configuraci\u00f3n y creaci\u00f3n de superuser de MONGODB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database install mongodb hosts: db become: yes tags: - db-install tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes - name: play Database config and create superuser hosts: db become: yes tags: - db-config-admin tasks: # drop admin user if exists (mongo admin --eval 'db.dropUser(\"superadmin\")') # create admin user (mongo admin --eval 'db.createUser'({ user: \"superadmin\", pwd:})) - name: create admin user command: \"{{ item }}\" loop: - mongo admin --eval 'db.dropUser(\"{{ db_admin_user }}\")' - | mongo admin --eval 'db.createUser( { user: \"{{ db_admin_user }}\", pwd: \"{{ db_admin_pass }}\", roles: [ { role: \"clusterAdmin\", db: \"admin\" }, { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } )' # enable security (/etc/mongod.conf) ---> Restart Mongodb #> security: #> authorization: \"enabled\" - name: security is enabled blockinfile: path: /etc/mongod.conf block: | security: authorization: \"enabled\" state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted las variables hacen referencias al archivo host_vars/db/secret.yaml Creamos un file de contrase\u00f1a para encriptarlo con vault: [isx46410800@miguel project_real]$ mkdir -p host_vars/db # [isx46410800@miguel project_real]$ cat key.txt miguel14031993 # [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt # [isx46410800@miguel project_real]$ ansible-vault create host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r Comprobamos que est\u00e1 todo en orden: ubuntu@ip-172-31-22-5:~$ systemctl status mongod ubuntu@ip-172-31-22-5:~$ cat /etc/mongod.conf ubuntu@ip-172-31-22-5:~$ mongo admin -u superadmin -p PassMy1243r Configuramos ahora el primer usuario mongo : ... - name: play - rest of configuration hosts: db become: yes tags: - db-config tasks: - name: pip3 installed package: name: python3-pip state: latest - name: pip pymongo installed pip: name: pymongo state: latest - name: todo db user exists mongodb_user: login_user: \"{{ db_admin_user }}\" login_password: \"{{ db_admin_pass }}\" database: admin user: \"{{ db_todo_user }}\" password: \"{{ db_todo_pass }}\" state: present roles: - db: \"{{ db_name_todo }}\" role: readWrite [isx46410800@miguel project_real]$ ansible-vault edit host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r db_name_todo: test db_todo_user: todo db_todo_pass: todo [isx46410800@miguel project_real]$ ansible-inventory --host db { \"ansible_host\": \"35.176.225.221\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"db_admin_pass\": \"PassMy1243r\", \"db_admin_user\": \"superadmin\", \"db_name_todo\": \"test\", \"db_todo_pass\": \"todo\", \"db_todo_user\": \"todo\" } Lanzamos solo la tercera parte del playbook a\u00f1adido con la orden: [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config A\u00f1adimos la linea de las interfaces para que reciba desde cualquier ip: - name: db accepts connection from anywhere lineinfile: path: /etc/mongod.conf line: \" bindIp: 0.0.0.0\" regexp: '^(.*)binIp(.*)$' state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config","title":"Instalaci\u00f3n MONGODB"},{"location":"ansible/#backend-app","text":"Hacemos playbook de instalar goland en el host app: [isx46410800@miguel project_real]$ cat playbooks/backend.yaml - name: play Backend hosts: app become: yes tags: - be-pre-build tasks: - name: go is installed import_role: name: gantsign.golang vars: golang_version: \"1.14\" golang_packages: - github.com/gorilla/mux - go.gomongodb.org/mongo-driver/mongo golang_users: - \"{{ ansible_ssh_user }}\" - name: play Backend hosts: app become: yes tags: - be-build tasks: - name: workspace build exist file: path: /opt/build_dir state: directory - name: git checkout git: repo: https://gitlab.com/isx46410800/curso_ansible.git dest: /opt/build_dir/curso_ansible - name: go build shell: . /etc/profile;go build -o /tmp/todo args: chdir: /opt/build_dir/curso_ansible/server [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt roles_path=./roles [isx46410800@miguel project_real]$ cat requirements.yaml - src: gantsign.golang name: gantsign.golang [isx46410800@miguel project_real]$ ansible-playbook playbooks/backend.yaml --tags be-pre-build *https://github.com/kubernetes-tn/go-to-do-app * *https://github.com/abdennour/ansible-course *","title":"Backend APP"},{"location":"azure/","text":"AZURE QUE ES Microsoft Azure es un servicio de Inform\u00e1tica en la Nube creado por Microsoft para construir, testear, desplegar y gestionar aplicaciones y servicios a trav\u00e9s de centros de datos gestionados por Microsoft. Microsoft Azure, como otros proveedores de nube, nos permite alquilar recursos como espacio de almacenamiento o ciclos de CPU en equipos f\u00edsicos que no debo administrar. Solo se paga por lo que usa (o al menos se mide). Los servicios inform\u00e1ticos ofrecidos suelen variar en funci\u00f3n de cada proveedor. Normalmente estos servicios incluyen: Potencia de proceso: por ejemplo, aplicaciones web o servidores Linux. Almacenamiento: por ejemplo, archivos y bases de datos. Redes: por ejemplo, conexiones seguras entre el proveedor de nube y la empresa. Potencia de proceso Cuando hacemos virtualmente cualquier acci\u00f3n en internet, como pagar una factura online, leer un peri\u00f3dico y enviar un correo electr\u00f3nico, estamos interactuando con servidores de nube que procesan cada solicitud y devuelven una respuesta. Todo esto requiere de c\u00f3mputo. Maquina virtual Contenedores Serverless o informatica sin servidor Almacenamiento La mayor\u00eda de las aplicaciones leen y escriben datos. Y en este sentido, el tipo de datos y c\u00f3mo se almacenan puede ser diferente seg\u00fan el tipo de aplicaci\u00f3n, la necesidad y velocidad requerida. Los proveedores de nube suelen ofrecer soluciones de almacenamiento para m\u00e1quinas virtuales, aplicaciones web, bases de datos, archivos de datos y anal\u00edtica. Por ejemplo, si quiere almacenar texto o un clip de pel\u00edcula, podr\u00eda usar un archivo en disco. Si tuviera un conjunto de relaciones (por ejemplo, una libreta de direcciones), podr\u00eda decidirse por un enfoque m\u00e1s estructurado, como usar una base de datos. La ventaja de utilizar almacenamiento basado en nube, es que no debemos preocuparnos por el escalado. Si se necesita m\u00e1s espacio, se puede agregar pagando un poco m\u00e1s de precio, e inclusive si las necesidades de almacenamiento bajan, tambi\u00e9n bajar\u00e1 el precio asociado. Redes En todos estos casos, las redes cobran una importancia vital. Los proveedores de nube suelen tener servicios de redes que nos permiten: + Crear y configurar Redes Virtuales. + Crear y conectar de extremo a extremo redes en la nube con una infraestructura local (conocidas como site-to-site, y point-to-site). + Parametrizar reglas de acceso a recursos. + Monitorear tr\u00e1fico de redes. + Aplicar reglas, restricciones y protecciones a las comunicaciones. CREAR CUENTA Web azure A trav\u00e9s de azure.com: es la forma m\u00e1s r\u00e1pida y f\u00e1cil que tienen las organizaciones de todos los tama\u00f1os para empezar a usar Azure. Puede administrar las implementaciones y el uso de Azure, como as\u00ed tambi\u00e9n obtener una factura mensual de Microsoft por los servicios usados. Con la ayuda de un Partner de Microsoft. Es un modelo para obtener facturaci\u00f3n local en tu pa\u00eds. De esta manera, Azure se brindar\u00e1 como servicio administrado a trav\u00e9s de un partner, qui\u00e9n te proporcionar\u00e1 el acceso y la facturaci\u00f3n, junto con un soporte t\u00e9cnico b\u00e1sico. A trav\u00e9s de un representante directo de Microsoft, opci\u00f3n pensada para organizaciones de gran tama\u00f1o o clientes que ya trabajan con la marca. A diferencia de azure.com (que requiere tarjeta de cr\u00e9dito), esto habilitar\u00e1 un tipo de contrato especial con varias ventajas al momento de necesitar varias suscripciones. Los servicios de Azure est\u00e1n disponibles a trav\u00e9s de Centros de Datos gestionados por Microsoft. Los mismos est\u00e1n conformados por edificios. xisten +60 regiones anunciadas en todo el mundo, y muchas que est\u00e1n anunciadas como adicionales futuras. Esto representa una presencia f\u00edsica en 140 pa\u00edses. En el mapa podr\u00e1s ver la ubicaci\u00f3n de los centros de datos, a excepci\u00f3n de 3 correspondientes a gobierno por lo cual su ubicaci\u00f3n es secreta. SLA significa en ingl\u00e9s \u201cservice level agreement\u201d, y en espa\u00f1ol \u201cacuerdo de nivel de servicio\u201d. Es un acuerdo escrito entre un proveedor de servicio y su cliente con objeto de fijar el nivel acordado para la calidad de dicho servicio. Este nivel puede ser un porcentaje que representa la disponibilidad m\u00ednima PRINCIPIO 5-3-2 La inform\u00e1tica en la nube es un metodo de gestion de recursos de IT donde los usuarios acceden a los recursos virtuales de computo, red y almacenamiento que estan disponibles online. Estos recursos se pueden aprovionar de manera instantanea y elastica. Se compone de: 5 Caracteristicas 3 metodos de entrega 2 modelos de implementacion Caracteristicas Autoservicio y bajo demanda: un consumidor puede provisionarse de caracteristicas como tiempo de uso, almacenamiento, memoria... Acceso amplio y ubicuo: los recursos pueden ser accecidos desde cualquier lugar y cualquier dispositivo. Ubicacion transparente y agrupacion de recursos: suelen estar en diferentes localizaciones sobre distintos recursos fisicos o virtuales que son dinamicamente asignados. Elasticidad rapida (estirarse y contraerse): pueden aumentar en epocas de mucha carga asi como reducirlo cuando no se use. Servicio medido (e incluso pago por uso): recursos y capacidades segun lo que necesitas. Metodos de entrega IaaS(Infraestructura como Servicio): Cliente tiene capacidad de utilizar almacenamiento, red, recursos sofware, SO, app. No tiene el control sobre la infraestructura pero sino tiene el control del resto apartir del SO. COntrol limitado sobre red como el firewall Ejemplo serian las maquinas virtuales PaaS(Plataforma como Servicio): Podemos desplegar apps propias o de terceros Control sobre las apps y la configuracion de ellas Ejemplo seria servicios hosting Saas(Software como Servicio): Capacidad de usar aplicaciones en una infraestructura de nube que cumple con las 5 caracteristicas No tenemos control sobre ningun componente, solo lo usamos. Ejemplo seria Office 365, exchange, gmail, yahoo, google apps. Modelos de implementaci\u00f3n Nube Privada: en mi propio centro de datos Nube P\u00fablica: en azure o otros proveedores de servicios Puede haber la mezcla con Nube Hibrida. MAQUINAS VIRTUALES Tipo de recurso escalable por Azure Se tiene control total sobre la configuraci\u00f3n y se puede instalar de todo No es necesario comprar hardware fisico para escalar o ampliar Azure tiene servicios para supervisar, proteger y administrar las actualizaciones y revisiones del sistema operativo Soy responsable de: Mantener el SO y sus actualizaciones Trabar sobre la performance Monitorear el espacio de disco usado Componentes: Disco virtual: el disco es el que tendr\u00e1, por ejemplo, el sistema operativo instalado. Gracias al disco virtual puedo iniciar el equipo y guardar informaci\u00f3n en forma persistente Placa de red virtual: al igual que en un equipo f\u00edsico, es la que me facilitar\u00e1 la conexi\u00f3n con una o m\u00e1s redes. Direcciones IP: gracias a la cual podr\u00e9 conectarme al equipo virtual. Estas direcciones IP pueden ser privadas y p\u00fablicas. Grupos de seguridad de red: que nos ayudar\u00e1n a definir desde qu\u00e9 origenes me puedo conectar, y hacia qu\u00e9 destinos puedo acceder, teniendo en cuenta protocolos, puertos, etc. Los Network Security Groups son una manera \u00e1gil de gestionar los permisos de red, para una o m\u00e1s m\u00e1quinas. Configurar: puedo el nombre de la MV, el SO y el tama\u00f1o. Tiene al menos dos discos, uno para el SO y otro temporal para la memoria virtual. Spot Virtual: herramienta que lo que no se use se vaya ahi para ahorrar. CREAR MV Assignment Tasks A Ingresar al Portal de Azure. Crear un \"Grupo de Recursos\" [Resource Group] con el nombre \"azf-vms-1\" Completed on 4 noviembre, 2020 7:01 pm B Dentro del Resource Group, seleccionar la opci\u00f3n \"Crear recurso\" [Create resources]. Seleccionar el grupo \"C\u00f3mputo\" [Compute] y de la lista \"M\u00e1quina Virtual\" [Virtual Machine]. Completed on 4 noviembre, 2020 7:01 pm C En el asistente de creaci\u00f3n, validar que la suscripci\u00f3n seleccionada sea la correcta (probablemente sea \"FREE TRIAL\") y el Resource Group seleccionado es el correcto: \"azf-vms-1\". Completed on 4 noviembre, 2020 7:01 pm D Ingresar un nombre para la m\u00e1quina virtual, por ejemplo \"azf-vm-windows-2019\". Completed on 4 noviembre, 2020 7:01 pm E Seleccionar una regi\u00f3n de Azure. Por ejemplo \"Este de Estados Unidos\" [East US], una imagen \"WIndows Server 2019 Datacenter\" y un tama\u00f1o de m\u00e1quina virtual (explorar todas las im\u00e1genes y elegir un tama\u00f1o como \"B1ms\" (es un equipo barato para este ejercicio). Completed on 4 noviembre, 2020 7:01 pm F Ingresar un usuario [Username] y una contrase\u00f1a dos veces [Password]. \u00a1No olvidarlas! Completed on 4 noviembre, 2020 7:01 pm G Seleccionar el puerto de entrada [Inbound port] \"RDP (3389)\" para poder ingresar luego al equipo. Completed on 4 noviembre, 2020 7:02 pm H Ir al paso siguiente: \"Discos\" [Disks]. Ingresar un Disco de Datos adicional [Create new disk] del menor tama\u00f1o posible. Completed on 4 noviembre, 2020 7:06 pm I Ir al siguiente paso \"Redes\" [Networking]. Crear una nueva red con el nombre \"azf-vnet-1\" con el espacio de direcciones \"10.0.0.0/16\" y crear una subnet con el nombre \"Sub1\" y el espacio de direcciones \"10.0.0.0/24\". Validar que una vez creada la red, est\u00e9 seleccionada en \"Virtual Network\" y \"Subnet\" en el asistente del equipo virtual. Completed on 4 noviembre, 2020 7:11 pm J Crear una IP p\u00fablica con el nombre \"azf-ip-1\", el \"SKU Basic\" y asignaci\u00f3n \"Static\". Completed on 4 noviembre, 2020 7:11 pm K Seleccionar el grupo de seguridad de red [NIC network security group] en \"Basic\". Completed on 4 noviembre, 2020 7:11 pm L Validar que los puertos habilitados son solo \"RDP (3389)\". Completed on 4 noviembre, 2020 7:11 pm M Ir al siguiente paso \"Administraci\u00f3n\" [Management]. Completed on 4 noviembre, 2020 7:13 pm N Seleccionar en diagn\u00f3stico de booteo [Boot diagnostics] en \"On\". Esto requerir\u00e1 crear una cuenta de almacenamiento. Completed on 4 noviembre, 2020 7:16 pm O En el campo \"Cuenta de Almacenamiento de Diagn\u00f3stico\" [Diagnostics storage account] crear una nueva cuenta de almacenamiento con el nombre \"azfstorageXXXX\" donde XXXX es un n\u00famero aleatorio generado por ti (dado que los nombres de cuentas de almacenamiento deben ser \u00fanicos en todo Azure). El tipo de cuenta debe ser \"Storage (general purpose v1) y \"Locally-redundant storage (LRS). Completed on 4 noviembre, 2020 7:17 pm P Habilitar el apagado autom\u00e1tico [Auto-shutdown] y elegir un horario de apagado para tu zona geogr\u00e1fica. Completed on 4 noviembre, 2020 7:17 pm Q No modificar el resto de las opciones e ir al siguiente paso \"Avanzado\" [Advanced]. Completed on 4 noviembre, 2020 7:18 pm R No modificar ninguna opci\u00f3n e ir al siguiente paso \"Etiquetas\" [Tags]. Completed on 4 noviembre, 2020 7:18 pm S Ir al \u00faltimo paso \"Revisi\u00f3n y Creaci\u00f3n\" [Review + create]. Cuando pase todas las validaciones, revisar el resumen de opciones seleccionadas que coincidan con lo solicitado y crear la m\u00e1quina. Completed on 4 noviembre, 2020 7:19 pm T Cuando finalice la creaci\u00f3n, ir al equipo virtual y seleccionar \"Conectar\" [Connect] y elegir \"RDP\". Se descargar\u00e1 un archivo, y desde el cliente de Escritorio Remoto de tu computadora conectarse. U Ingresar el nombre de usuario y contrase\u00f1a que ingresamos en pasos anteriores, y comprobar que nos podemos conectar al equipo. V Ir al Grupo de Recursos [Resource Group] que hemos creado y comprobar que todos los recursos (m\u00e1quina virtual, discos, placas de red) est\u00e1n creados. HERRAMIENTAS AZURE Azure Portal para interactuar con Azure a trav\u00e9s de una interfaz gr\u00e1fica de usuario (GUI). Azure PowerShell y la interfaz de la l\u00ednea de comandos de Azure (CLI) para las interacciones con Azure de l\u00ednea de comandos y basadas en automatizaci\u00f3n. Azure Cloud Shell para una interfaz de l\u00ednea de comandos basada en web. Azure Mobile App para supervisar y administrar los recursos desde el dispositivo m\u00f3vil. AZURE POWERSHELL Instamos la herramienta AzurePowershell , apartir de la versi\u00f3n 7 es multiplataforma. linux # Register the Microsoft signature key sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # Register the Microsoft RedHat repository curl https://packages.microsoft.com/config/rhel/7/prod.repo | sudo tee /etc/yum.repos.d/microsoft.repo # Update the list of products sudo dnf check-update # Install a system component sudo dnf install compat-openssl10 # Install PowerShell sudo dnf install -y powershell # Start PowerShell pwsh Instalamos el modulo de AZ powershell: pwsh Install-Module -Name Az Nos conectamos con nuestra cuenta yendo al link que nos indica: PS /home/isx46410800/Documents> Connect-AzAccount Orden de listar los Resources Groups Get-AzResourceGroup : PS /home/isx46410800/Documents> Get-AzResourceGroup ResourceGroupName : NetworkWatcherRG Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/NetworkWatcherRG # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 Crear un Resource Group por comando New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" : PS /home/isx46410800/Documents> New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" ResourceGroupName : azf-rgexmaple-rg Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-rgexmaple-rg # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 azf-rgexmaple-rg eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 AZURE CLI Documentacion CLI Instalaci\u00f3n linux: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # sudo sh -c 'echo -e \"[azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/azure-cli.repo' # sudo yum install azure-cli Iniciamos sesion haciendo login: az login Listamos los resource groups con az group list --output table : Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded Creamos un resource group con az group create --location \"eastus\" --name \"azf-cli-rg\" : { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cli-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cli-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded AZURE CLOUD SHELL Es un shell a trav\u00e9s del navegador desde cualquier sistema operativo. Necesitamos crear un storage y file share para utilizarlo. Ahora dentro podemos usar las mismas ordenes que cli por ejemplo y ver/crear resource groups como ejemplo: miguel@Azure:~$ az group create --location \"eastus\" --name \"azf-cloudshell-example-rg\" { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cloudshell-example-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cloudshell-example-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } miguel@Azure:~$ az group list --output table Name Location Status ------------------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded azf-cloudshell-rg eastus Succeeded azf-cloudshell-example-rg eastus Succeeded Podemos verlo en pantalla completa con www.shell.azure.com EJEMPLO DE CREAR MV POR SCRIPT Crear una MV con CLI a trav\u00e9s de cloud shell: # create a resource group az group create --name azf-crear-mv-cli --location eastus # Create a virtual network net and subnet. az network vnet create --resource-group azf-cloudshell-example-rg --name azf-vnet-1 --address-prefix 10.0.0.0/16 --subnet-name sub1 --subnet-prefix 10.0.0.0/24 # Create a public IP address. az network public-ip create --name myPublicCliIP --resource-group azf-cloudshell-example-rg --sku Basic --allocation-method Static # create vm customized az vm create --subscription \"Evaluaci\u00f3n gratuita\" -g azf-cloudshell-example-rg --name vm-cli-example --image win2019datacenter --location eastus --size Standard_B1ms --admin-username vmadmin --admin-password mi--GU--el14 --vnet-name azf-vnet-1 --subnet sub1 # add new optional disk az vm disk attach -g azf-cloudshell-example-rg --vm-name vm-cli-example --name myDataDisk --new --size-gb 4 # input port to connect VM az vm open-port --port 3389 --resource-group azf-cloudshell-example-rg --name vm-cli-example # habilitar el auto shutdown az vm auto-shutdown -g azf-cloudshell-example-rg -n vm-cli-example --time 0200 # borrar recurso az group delete --name azf-cloudshell-example-rg AZURE APP SERVICE Azure App Service es un servicio de alojamiento web totalmente administrado que permite crear aplicaciones web, back-ends m\u00f3viles y API RESTful. Desde sitios web peque\u00f1os hasta aplicaciones web con una escala global, existen opciones de precios y rendimiento que se adaptan a todas las necesidades. Azure App Service permite desarrollar software en el lenguaje preferido, ya sea. NET, .NET Core, Java, Ruby, Node.js, PHP o Python. Las aplicaciones se ejecutan y escalan f\u00e1cilmente en los entornos basados tanto en Windows como en Linux. Azure App Service no solo agrega a la aplicaci\u00f3n la funcionalidad de Microsoft Azure, como la seguridad, el equilibrio de carga, el escalado autom\u00e1tico y la administraci\u00f3n automatizada. Tambi\u00e9n puede sacar partido de las funcionalidades de DevOps, por ejemplo, la implementaci\u00f3n continua desde Azure DevOps, GitHub, Docker Hub y otros or\u00edgenes, la administraci\u00f3n de paquetes, entornos de ensayo, dominio personalizado y certificados TLS/SSL. Con Azure App Service se paga por los recursos de proceso de Azure que se utilizan. Los recursos de proceso que usa se determinan mediante el plan de App Service en el que ejecuta las aplicaciones, y determina el costo del servicio. Crear una Web App Home - Crear recurso - Web App Configuraci\u00f3n inicial: A Crear una nueva Web App (elegir del men\u00fa destacados seg\u00fan indica el video). Completed on 6 noviembre, 2020 11:55 pm B Elegir la suscripci\u00f3n, crear un resource group desde el mismo asistente, y completar un nombre: por ejemplo \"azf-webapp-codexxx\" donde xxx son 3 n\u00fameros aleatorios, tanto para la web app como para el resource group. No olvides este RG que lo utilizaremos m\u00e1s adelante. Completed on 6 noviembre, 2020 11:55 pm C Seleccionar la forma de publicaci\u00f3n [Publish] \"Code\". Elegir el Runtime stack .NET Core 3.1 (LTS) y la plataforma Windows. Completed on 6 noviembre, 2020 11:55 pm D Elegir la regi\u00f3n que Azure propone. Se puede seguir el del ejemplo: Central US. Completed on 6 noviembre, 2020 11:55 pm E Crear un Windows Plan con el nombre: \"azf-webapp-code-plan\" y el tama\u00f1o Standard S1 (no te preocupes por el costo, en breve lo eliminaremos). F En las opciones de monitoreo, NO habilitaremos Application Insights. Completed on 6 noviembre, 2020 11:56 pm G No completaremos nada en los tags. H Revisaremos y crearemos la Web App. I Comprobar que desde la URL en la p\u00e1gina de Overview que la web app funciona correctamente. Veremos una URL donde tenemos nuestra web app Services y Plans El plan es el hardware del que voy a disponer. El service es la app o conjunto de apps dentro del plan. En App Service, cada aplicaci\u00f3n se ejecuta en un Plan de App Service. En forma b\u00e1sica, un App Service Plan define un conjunto de recursos de proceso para que una aplicaci\u00f3n web se ejecute. Estos recursos de proceso son an\u00e1logos a la granja de servidores de un hospedaje web convencional. Cuando se crea un plan de App Service en una regi\u00f3n determinada (por ejemplo, Oeste de Europa), se crea un conjunto de recursos de proceso para ese plan en dicha regi\u00f3n. Todas las aplicaciones que coloque en este plan de App Service se ejecutan en estos recursos de proceso seg\u00fan lo definido por el plan de App Service. Cada plan de App Service define: Regi\u00f3n (oeste de EE. UU., este de EE. UU., etc.). N\u00famero de instancias posibles de VM (si, por detr\u00e1s hay VMs). Tama\u00f1o de las instancias de VM (peque\u00f1o, mediano, grande). Plan de tarifa (Gratis, Compartido, B\u00e1sico, Est\u00e1ndar, Premium, PremiumV2 y Aislado). Si bien inicialmente puede resultar algo confuso, cuando en Azure veamos el t\u00e9rmino \u201cApp Service\u201d nos referimos a las Web Apps, API Apps & Mobile Apps. Cuando veamos el t\u00e9rmino \u201cApp Service Plans\u201d nos referimos expl\u00edcitamente a los planes. La relaci\u00f3n entre un App Service Plan un un App Service es una relaci\u00f3n 1:muchos :-). Una App Service Plan puede contener muchos App Services, mientras que un App Service s\u00f3lo puede estar dentro de un App Service Plan. En todos los casos, cuando contratamos un App Service Plan \u201cBasic\u201d, \u201cStandard\u201d, \u201cPremium\u201d e \u201cIsolado\u201d, s\u00f3lo pagamos por el tama\u00f1o del plan que contratemos (scale up) y la cantidad de instancias con las que escalemos (scale out, predeterminadamente configurada en 1): Aunque tengamos muchos App Services (Web Apps, API Apps & Mobile Apps) dentro del mismo plan, s\u00f3lo pagaremos por el tama\u00f1o y cantidad de instancias del App Service Plan, y no por cada una de las Apps que tenga dentro. Por supuesto, como el App Service Plan determina un hardware asociado, tendremos un l\u00edmite. Si ponemos muchos sitios web dentro que consumen muchos recursos y/o tienen demasiado tr\u00e1fico, nuestro App Service Plan comenzar\u00e1 a arrojar errores por no disponibilidad de servicio. Este punto no es menor, y debe ser planeado en el dise\u00f1o de la soluci\u00f3n completa de Azure. Crear una Web App Container la creaci\u00f3n de una Web App for Containers. Esto significa, que nuestra Web App estar\u00e1 en un contenedor de ejemplo en Microsoft Azure. Recordemos que Azure App Service permite generar diversas aplicaciones: Web App. API App. Mobile App. En escencia, son similares, si bien encontraremos en cada una de ellas particularidades propias. La m\u00e1s simple es una Web App, y es justamente en este ejercicio la que generaremos. A diferencia del anterior donde tambi\u00e9n generamos una Web App, esta vez lo haremos utilizando Docker. Configuraci\u00f3n inicial: A Crear un nuevo recurso de tipo \"Web App\" que esta en la lista de populares. B Crear un grupo de recursos seg\u00fan el gusto que tengas. Si elijes un nuevo grupo de recursos (diferente al ejercicio de creaci\u00f3n de Web App Simple) recuerda BORRAR este nuevo grupo de recursos al finalizar el ejercicio. En caso que sea el mismo, puedes dejarlo dado que m\u00e1s adelante borraremos todo. C Elegiremos el tipo de publicaci\u00f3n \"Docker Container\" y el sistema operativo \"Linux\". D Generaremos un nuevo App Service Plan y un tama\u00f1o de tipo \"S1\". E En la configuarci\u00f3n de \"Docker\" elegiremos \"Single Container\". F Con respecto al origen de la instancia elegiremos \"Quickstart\". La opci\u00f3n \"sample\" ser\u00e1 \"NGINX\". G No debemos habilitar nada de monitoreo, dado que no est\u00e1 soportado con contenedores. H Revisar y crear el recurso. I Comprobar por la URL del Web App que todo est\u00e1 funcionando como esperamos. Crear API App Pasos: A Ingresar al Resource Group que hemos generado en la \"Creaci\u00f3n de una Web App Simple\". Deber\u00eda tener el formato \"azf-webapp-codexxx\" con 3 n\u00fameros elegidos por ti. B Creamos un nuevo recurso y buscamos en la barra de b\u00fasqueda \"API App\". C Ingresaremos un nombre para la app, respetando el formato \"azf-apiapp-codexxx\" ingresando los 3 mismos n\u00fameros aleatorios puestos para el resource group y la web app anterior. D El resource group ya deber\u00eda estar seleccionado (es el que ya existe). E En la opci\u00f3n de \"App Service Plan\" debemos elegir el que ya existe porque lo generamos antes. Su nombre deber\u00eda ser \"azf-webapp-code-plan\". F Le damos un clic en \"Create\". G Una vez que se genere, vamos a comprobar que el sitio funciona bien. H Comprobar que en el listado de Apps en el \"App Service Plan\" ahora tenemos m\u00e1s items. En el Deployment center o en el centro de despliegue podemos configurar archivos o repositorios de donde coger nuestro c\u00f3digo fuente para la aplicaci\u00f3n web. https://docs.microsoft.com/es-mx/learn/modules/create-publish-webapp-app-service-vs-code/3-exercise-create-web-application-vs-code?pivots=pythonflask ALMACENAMIENTO AZURE La plataforma de Azure Storage es la soluci\u00f3n de almacenamiento en la nube de Microsoft para los escenarios modernos de almacenamiento de datos. Los servicios principales de almacenamiento ofrecen un almac\u00e9n de objetos escalable de forma masiva para objetos de datos, un almacenamiento en disco para m\u00e1quinas virtuales (VM) de Azure, un servicio de sistema de archivos para la nube, un almac\u00e9n de mensajes para mensajer\u00eda confiable y un almac\u00e9n NoSQL. Los servicios principales y considerados primitivos de Azure Storage son: Blobs de Azure [Azure Blobs]: con opciones de blobs en bloques, anexos y p\u00e1ginas. Archivos de Azure [Azure Files]: recursos compartidos para uso local y en la nube. Colas de Azure [Azure Queues]: almac\u00e9n de mensajer\u00eda simple. Tablas de Azure [Azure Tables]: almac\u00e9n NoSQL sin esquema para datos estructurados. Adem\u00e1s, consideraremos el servicio de Azure Disks, un servicio totalmente administrado para los discos de nuestras m\u00e1quinas virtuales: Discos de Azure [Azure Disks]: conocidos como \u201cdiscos administrados\u201d. Azure disks Los discos administrados de Azure son vol\u00famenes de almacenamiento de nivel de bloque administrados por Azure y utilizados con Azure Virtual Machines. Los discos administrados son como un disco f\u00edsico en un servidor local, pero virtualizados. Con los discos administrados, todo lo que deber\u00e1s hacer es especificar el tama\u00f1o del disco, el tipo de disco y aprovisionar el disco. Una vez que aprovisiona el disco, Azure se encarga del resto. Los tipos de discos disponibles son ultra, unidades de estado s\u00f3lido (SSD) premium, SSD est\u00e1ndar y unidades de disco duro est\u00e1ndar (HDD). Blob storage Azure Blob Storage es la soluci\u00f3n de almacenamiento de objetos de Microsoft para la nube. Blob Storage est\u00e1 optimizado para el almacenamiento de cantidades masivas de datos no estructurados. En la primera lecci\u00f3n de esta secci\u00f3n, ya hemos aprendido sobre los diversos tipos de datos, entre ellos los no estructurados. Blob Storage est\u00e1 dise\u00f1ado para: Servicio de im\u00e1genes o documentos directamente a un explorador. Almacenamiento de archivos para acceso distribuido. Streaming de audio y v\u00eddeo. Escribir en archivos de registro. Almacenamiento de datos para copia de seguridad y restauraci\u00f3n, recuperaci\u00f3n ante desastres y archivado. Almacenamiento de datos para el an\u00e1lisis en local o en un servicio hospedado de Azure. Azure files Archivos de Azure (Azure Files) ofrece recursos compartidos de archivos en la nube totalmente administrados, a los que se puede acceder mediante el protocolo SMB (Bloque de mensajes del servidor) est\u00e1ndar. Los recursos compartidos de Azure Files se pueden montar simult\u00e1neamente en implementaciones de Windows, Linux y macOS en la nube o locales. Adem\u00e1s, los recursos compartidos de archivos de Azure Files se pueden almacenar en la cach\u00e9 de los servidores de Windows Server con Azure File Sync, lo que permite un acceso r\u00e1pido all\u00ed donde se utilizan los datos. Se crea un file share dentro de mi cuenta de storage azure. Se sube archivos y nos podemos conectar al file share a trav\u00e9s de un script que te da en las opciones de azure. Ahora ya tendemos el recursos de red la nueva unidad compartida y se actualiza todo. Tambien podemos hacer un azure file sync que es como un server en la nube que se actualiza en mi ordenador, azure y nube. Nos tendremos que descargar el file sync de azure despues de crear un server y un group sync y funcionar\u00e1 como lo anterior. Colas de azure Azure Queue Storage es un servicio para almacenar grandes cantidades de mensajes, a los que se puede acceder desde cualquier lugar del mundo a trav\u00e9s de llamadas autenticadas mediante HTTP o HTTPS. Un mensaje de la cola puede llegar a tener hasta 64 KB. Una cola puede contener millones de mensajes, hasta el l\u00edmite de capacidad total de una cuenta de almacenamiento. Las colas se utilizan normalmente para crear un trabajo pendiente del trabajo que se va a procesar de forma asincr\u00f3nica. Azure Storage Explorer Azure Storage Explorer es una herramienta gratuita para administrar f\u00e1cilmente sus recursos de almacenamiento en la nube de Azure en cualquier parte, desde Windows, macOS o Linux. Permite cargar, descargar y administrar blobs, archivos, colas y tablas de Azure, as\u00ed como entidades de Azure Cosmos DB y Azure Data Lake Storage. Permite acceder f\u00e1cilmente a los discos de las m\u00e1quinas virtuales y trabajar con Azure Resource Manager o con cuentas de almacenamiento cl\u00e1sicas. Asimismo, administrar y configurar reglas de uso compartido de recursos entre or\u00edgenes. AZURE FUNCTIONS Azure Functions permite ejecutar peque\u00f1os fragmentos de c\u00f3digo (denominados \u201cfunciones\u201d) sin preocuparse por el resto de la infraestructura de la aplicaci\u00f3n. Es ideal para tareas espec\u00edficas, dado que simplifica la necesidad de generar c\u00f3digo reduci\u00e9ndolo s\u00f3lo a la parte \u201cl\u00f3gica\u201d que necesitamos. Imaginemos que necesitamos generar una tarea donde, cada vez que se escribe un nuevo archivo en un Storage Account de Azure, se dispare una funci\u00f3n que guarde en una base de datos registro de dicho archivo. Si pensamos en hacer esto desde cero, probablemente nos requiera: Generar un proyecto con un IDE y un Framework / Lenguaje espec\u00edfico. Construir un servicio y alojarlo en alg\u00fan lugar. Resolver otros aspectos propios del proyecto, donde alojarlo, c\u00f3mo hacer que se dispare cuando se graba un archivo nuevo en el Storage Account, etc. Los desencadenadores son lo que provocan que una funci\u00f3n se ejecute. Un desencadenador define c\u00f3mo se invoca una funci\u00f3n y cada funci\u00f3n debe tener exactamente un desencadenador. Los desencadenadores tienen datos asociados, que a menudo son la carga de la funci\u00f3n. El enlace a una funci\u00f3n es una manera de conectar otro recurso a la funci\u00f3n mediante declaraci\u00f3n. Los enlaces pueden estar conectados como enlaces de entrada, enlaces de salida o ambos. Los datos de los enlaces se proporcionan a la funci\u00f3n como par\u00e1metros. AZURE BASES DE DATOS Microsoft SQL es un motor de base de datos utilizado en forma global y conocida por casi todos. Vamos a conocer las opciones que tenemos de Microsoft SQL en Azure, ya sea como IaaS, PaaS e inclusive Serverless. SQL Server en Azure Virtual Machines nos permite usar versiones completas de SQL Server en la nube sin tener que administrar todo el hardware local. SQL Server en Azure Virtual Machines tambi\u00e9n simplifica los costos de licencia cuando se paga por uso. La galer\u00eda de im\u00e1genes de m\u00e1quina virtual le permite crear una m\u00e1quina virtual con SQL Server con la versi\u00f3n, la edici\u00f3n y el sistema operativo correctos. Esto hace que las m\u00e1quinas virtuales sean una buena opci\u00f3n para muchas cargas de trabajo de SQL Server diferentes. El recurso M\u00e1quinas virtuales SQL es un servicio de administraci\u00f3n independiente de la m\u00e1quina virtual y, de hecho, aparece como un m\u00f3dulo distinto. Cuando Azure detecta que un motor SQL Server est\u00e1 instalado dentro de una m\u00e1quina virtual, habilita el punto de administraci\u00f3n con opciones espec\u00edficas que el administrador de VMs deber\u00e1 comenzar a gestionar. Azure SQL Database es un motor de base de datos de tipo plataforma como servicio (PaaS) totalmente administrado por Microsoft, que se encarga de la mayor\u00eda de las funciones de administraci\u00f3n de bases de datos. \u00bfQu\u00e9 tipo de funciones de administraci\u00f3n est\u00e1n a cargo de Microsoft? Por ejemplo: actualizar el motor, aplicar revisiones, crear copias de seguridad, supervisar sin intervenci\u00f3n del usuario. Azure SQL Database se ejecuta siempre en la \u00faltima versi\u00f3n estable del motor de base de datos de SQL Server y en un sistema operativo revisado con el 99,99 % de disponibilidad. Las capacidades de PaaS que est\u00e1n integradas en Azure SQL Database permiten centrarse en las actividades de administraci\u00f3n y optimizaci\u00f3n de bases de datos espec\u00edficas del dominio que son cr\u00edticas para el negocio, y no en mantener la infraestructura. Se puede descargar el software de SSMS y conectar desde el pc a azure y gestionar las bbdd. Se pueden crear: Single databases(siempre ha de haber un server database) Elastic Pool databses(recursos compartidos y dentro varias ddbb) Managed instances(se crea en la nube las ddbb) Managed instances: Es un servicio de base de datos en la nube inteligente y escalable que combina la mayor compatibilidad con el motor de base de datos de SQL Server en una plataforma como servicio totalmente administrada por Microsoft. Tiene casi un 100 % de compatibilidad con el motor de base de datos m\u00e1s reciente de SQL Server (Enterprise Edition). Url para migrar bbdd a azure COSMOS DB Azure Cosmos DB es un servicio de base de datos con varios modelos distribuido de forma global de Microsoft. En esta lecci\u00f3n vamos a repasar los conceptos de las bases de datos no-sql vs las relacionales, para poder ir entendiendo un poco m\u00e1s de qu\u00e9 se trata este servicio de base de datos de Microsoft. Azure Cosmos DB es un servicio de base de datos multimodelo distribuido y con escalado horizontal. Al ser multimodelo, admite de forma nativa modelos de datos de documentos, pares clave-valor, grafos y en columnas. Con respecto a la administraci\u00f3n e interfaz de comunicaci\u00f3n, Azure Cosmos DB permite acceder a sus datos con diferentes APIs: como SQL (documentos), MongoDB (documentos), Azure Table Storage (clave-valor), Gremlin (grafos) y Cassandra (en columnas). A nivel de funcionalidad, Azure Cosmos DB indexa datos autom\u00e1ticamente sin que haya que ocuparse de la administraci\u00f3n de esquemas ni de \u00edndices. Creaci\u00f3n En un resource group a\u00f1adimos un cosmosDB. Para crear una cuenta de Azure Cosmos DB gratuita por 30 dias link Una vez creado el cosmosDB vamos a Data Explorer y creamos una db y despues un container dentro de esta db. Por ejemplo creamos un container con nombre personas y el key es /dni. Dentro de personas vamos creando nuevos items: { \"dni\": \"46410800C\", \"nombre\": \"Miguel\", \"apellidos\": \"Amor\u00f3s Moret\" } Tambien podemos hacer query: SELECT c.nombre, c.apellidos FROM c where c.dni = \"46410800C\" BALANCEADORES AZURE En Microsoft Azure tenemos diversas opciones de balanceadores, tambi\u00e9n conocidos como equilibradores de carga. El t\u00e9rmino equilibrio de carga hace referencia a la distribuci\u00f3n de cargas de trabajo entre varios recursos de proceso. El equilibrio de carga busca optimizar el uso de recursos, maximizar el rendimiento, minimizar el tiempo de respuesta y evitar la sobrecarga de un solo recurso. Tambi\u00e9n puede mejorar la disponibilidad al compartir una carga de trabajo entre recursos de proceso redundantes. Global frente a regional + Los servicios de equilibrio de carga globales distribuyen el tr\u00e1fico en servidores de back-end regionales, nubes o servicios locales h\u00edbridos. Estos servicios enrutan el tr\u00e1fico del usuario final al servidor de back-end disponible m\u00e1s cercano. Tambi\u00e9n reaccionan a los cambios en la confiabilidad o el rendimiento del servicio, con el fin de maximizar la disponibilidad y el rendimiento. Puede pensar en ellos como sistemas que equilibran la carga entre los stamp, puntos de conexi\u00f3n o unidades de escalado de la aplicaci\u00f3n hospedados en diferentes regiones o zonas geogr\u00e1ficas. Los servicios de equilibrio de carga regionales distribuyen el tr\u00e1fico de las redes virtuales entre las m\u00e1quinas virtuales (VM) o puntos de conexi\u00f3n de servicio zonales y con redundancia de zona de una regi\u00f3n. Puede pensarlos como sistemas que equilibran la carga entre m\u00e1quinas virtuales, contenedores o cl\u00fasteres dentro de una regi\u00f3n en una red virtual. HTTP(S) frente a no HTTP(S) + Los servicios de equilibrio de cargas HTTP(S) son equilibradores de carga de capa 7 que solo aceptan el tr\u00e1fico HTTP(S). Est\u00e1n dise\u00f1ados para las aplicaciones web u otros puntos de conexi\u00f3n HTTP(S). Incluyen caracter\u00edsticas, como la descarga de SSL, el firewall de aplicaciones web, el equilibrio de carga basado en rutas de acceso y la afinidad de sesi\u00f3n. Los servicios de equilibrio de carga que no son HTTP/S pueden controlar el tr\u00e1fico que no es de HTTP(S) y se recomiendan para las cargas de trabajo que no son web. Servicios de Balanceo en Azure Load Balancer + Proporciona un servicio de equilibrio de carga de capa 4 con latencia baja y rendimiento alto (entrante y saliente) para todos los protocolos UDP y TCP. Se dise\u00f1\u00f3 para administrar millones de solicitudes por segundo, a la vez que garantiza que la soluci\u00f3n tiene una alta disponibilidad. Azure Load Balancer tiene redundancia de zona, lo que garantiza una alta disponibilidad en las instancias de Availability Zones. Traffic Manager + Es un equilibrador de carga de tr\u00e1fico basado en DNS que le permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Dado que Traffic Manager es un servicio de equilibrio de carga basado en DNS, solo equilibra la carga en el nivel del dominio. Por ese motivo, no puede conmutar por error tan r\u00e1pidamente como con Front Door, debido a los desaf\u00edos comunes relacionados con el almacenamiento en cach\u00e9 de DNS y a los sistemas que no respetan los TTL de DNS. Application Gateway + Proporciona un controlador de entrega de aplicaciones (ADC) como servicio, que ofrece diversas funcionalidades de equilibrio de carga de capa 7. \u00daselo para optimizar la productividad de las granjas de servidores web al traspasar la carga de la terminaci\u00f3n SSL con mayor actividad de la CPU a la puerta de enlace. Front Door + Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Ejercicio balanceador Vamos a realizar todas las configuraciones necesarias para poder, luego, trabajar sobre un Azure Load Balancer. Estas configuraciones las consideramos pre-requisitos y contemplan: Ejecuci\u00f3n en un modelo de Infraestructura como C\u00f3digo de la creaci\u00f3n de m\u00e1quinas virtuales y otros componentes requeridos para el ejercicio. Son un total de 3 equipos virtuales que utilizan 1 VCore cada uno, todos utilizando un Availability Set (Conjunto de Disponibilidad) con 2 dominios de falla y 5 dominios de actualizaci\u00f3n. Instalaci\u00f3n y configuraci\u00f3n de Internet Information Services (IIS) con una p\u00e1gina web simple. Creaci\u00f3n y asignaci\u00f3n de un Network Security Group (NSG) \u00fanico para los 3 equipos virtuales. Creamos un template y hacemos deploy del siguiente codigo Nos conectamos remotamente al server1 e instalamos el rol IIS de server web. Modificamos el index.html y ponemos que saludamos desde server 1. Ahora en el resource group creamoe un network security group. Dentro creamos un inbound de http por el puerto 80. Ahora de cada network interface las asignamos a este creado y borramos la del template. Ahora si entramos a la Ip de la primera VM por el puerto 80 desde el navegador normal, vemos su index.html. Despues de cada interfaz de red, modificamos su ip configuration y le ponemos ip statica y ip publica desasociada. Despues creamos un recurso nuevo. un load balancer publico, basic y con una ip static. Despues entramos y creamos un backend en nuestra loadblancer con nuestras 3 virtual machines. Creamos un healthprobe para http. A\u00f1adimos un load rule. Ahora si ponemos en el navegador la ip del balanceador, nos ir\u00e1 responiendo dinamicamente, el servidor de respuesta. Azure Trafic Manager Azure Traffic Manager es un balanceador de tr\u00e1fico basado en DNS que permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Trabaja a nivel capa 7 de OSI, aunque s\u00f3lo a nivel DNS. Traffic Manager usa DNS para dirigir las solicitudes del cliente al punto de conexi\u00f3n de servicio m\u00e1s adecuado en funci\u00f3n de un m\u00e9todo de enrutamiento del tr\u00e1fico y el mantenimiento de los puntos de conexi\u00f3n. Un punto de conexi\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Es fundamental entender que Traffic Manager funciona a nivel de DNS. Traffic Manager usa DNS para dirigir a los clientes a puntos de conexi\u00f3n espec\u00edficos del servicio basados en las reglas del m\u00e9todo de enrutamiento de tr\u00e1fico. Los clientes se conectan directamenteal punto de conexi\u00f3n seleccionado. Traffic Manager no es un proxy ni una puerta de enlace. Traffic Manager no ve el tr\u00e1fico que circula entre el cliente y el servicio. EJERCICIO: Vamos a completar los pre-requisitos necesarios para poder llevar adelante el ejercicio de Azure Traffic Manager. Estos pre-requisitos son: 3 App Service Plans con sistema operativo Linux. 3 App Service (Web Apps) con runtime PHP 7.3. 1 VM con Windows creada en Estados Unidos (si no estas en USA) o en Brasil (si est\u00e1s en USA). Publicaci\u00f3n de la soluci\u00f3n PHP (index.php) en cada sitio web (Brasil, Estados Unidos y Asia). La deber\u00e1s descargar desde GitHub. # A Crear un Resource Group para Traffic Manager. Completed on 12 noviembre, 2020 8:04 pm B Crear Sitio Web en Brasil Sur con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). C Crear Sitio Web en Estados Unidos Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). D Crear Sitio Web en Asia Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). E Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Sur de Brasil. F Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Estados Unidos. G Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Asia. H Crear la M\u00e1quina Virtual con Windows en Estados Unidos \u00f3 Brasil, seg\u00fan corresponda a tu ubicaci\u00f3n. Abrimos el ftp de cada uno y copiamos el index.php del github. Ahora creamos un Traffic Manager profile en nuestro grupo de recurso. Creamos 3 endpoints en mi traffic manager y mapeado por geolocalizaci\u00f3n. Segun petici\u00f3n donde estemos nos contesta una u otra Azure Aplication Gateway Azure Application Gateway es un equilibrador de carga de tr\u00e1fico web que permite administrar el tr\u00e1fico a las aplicaciones web. Los equilibradores de carga tradicionales operan en la capa de transporte (OSI capa 4: TCP y UDP) y enrutan el tr\u00e1fico en funci\u00f3n de la direcci\u00f3n IP y puerto de origen a una direcci\u00f3n IP y puerto de destino. Application Gateway puede tomar decisiones de enrutamiento basadas en atributos adicionales de una solicitud HTTP, por ejemplo los encabezados de host o la ruta de acceso del URI. A Paso inicial: confirmar que complet\u00e9 todos los requisitos (sitios web pre-requisitos de la lecci\u00f3n de Traffic Manager). B Crear un Resource Group para Application Gateway. C Crear un Application Gateway (SKU Standard_v2, Zonas 1 2 y 3 del Este de Estados Unidos, y escalado manual con 2 instancias iniciales). D Crear la Virtual Network con las opciones predeterminadas. E Crear un Front-End con una direcci\u00f3n de Front-End p\u00fablica. F Crear un Back-End Pool con los 3 sitios web configurados. G Crear un Routing Rule con Listener Basico. H Crear un HTTP Settings seg\u00fan indicaciones del Video. I Corregir HTTP Settings para evitar error en el estado de salud de los Websites de App Service. J Probar acceso por IP del Front-End (p\u00fablica) para validar que podamos ingresar al sitio web. Hacer F5 para validar que se cambia la p\u00e1gina de inicio seg\u00fan zona. Azure Front Door (AFD) Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Azure Front Door permite definir, administrar y supervisar el enrutamiento global para el tr\u00e1fico web mediante la optimizaci\u00f3n para obtener el mejor rendimiento y la conmutaci\u00f3n por error global r\u00e1pida para alta disponibilidad. Front Door funciona en la capa 7 o la capa HTTP/HTTPS, y usa el protocolo de difusi\u00f3n por proximidad con divisi\u00f3n TCP y la red global de Microsoft para mejorar la conectividad global. Por tanto, seg\u00fan la selecci\u00f3n del m\u00e9todo de enrutamiento en la configuraci\u00f3n, puede asegurarse de que Front Door enruta las solicitudes de cliente al back-end de aplicaci\u00f3n m\u00e1s r\u00e1pido y disponible. Un back-end de aplicaci\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Front Door proporciona una serie de m\u00e9todos de enrutamiento del tr\u00e1fico y opciones de seguimiento de estado del back-end para satisfacer las distintas necesidades de las aplicaciones y los modelos de conmutaci\u00f3n autom\u00e1tica por error. Al igual que Traffic Manager, Front Door es resistente a errores, incluidos los que afectan a una regi\u00f3n completa de Azure. CONTENEDORES los contenedores como unidades de despliegue. Los contenedores ofrecen las ventajas del aislamiento, la portabilidad, la agilidad, la escalabilidad y el control a lo largo de todo el flujo de trabajo del ciclo de vida de la aplicaci\u00f3n. La ventaja m\u00e1s importante es el aislamiento del entorno que se proporciona entre el desarrollo y las operaciones. Azure Container Registry es un servicio privado administrado del Registro de Docker que usa Docker Registry 2.0, que es de c\u00f3digo abierto. Cree y mantenga los registros de Azure Container para almacenar y administrar las im\u00e1genes privadas del contenedor Docker y los artefactos relacionados. EJERCICIO APP SERVICE Azure Container Instances es un servicio de Microsoft Azure que permite en forma r\u00e1pida y sencilla ejecutar un contenedor en Azure, sin tener que administrar ninguna m\u00e1quina virtual y sin necesidad de adoptar un servicio de nivel superior. EJERCICIO Kubernetes es una plataforma de r\u00e1pida evoluci\u00f3n que administra aplicaciones basadas en contenedores y sus componentes de red y almacenamiento asociados. El foco est\u00e1 en las cargas de trabajo de la aplicaci\u00f3n, no en los componentes de infraestructura subyacente. Kubernetes proporciona un enfoque declarativo en las implementaciones, respaldado por un s\u00f3lido conjunto de API para las operaciones de administraci\u00f3n. Azure Kubernetes Service (AKS) proporciona un servicio de Kubernetes administrado que reduce la complejidad de las principales tareas de administraci\u00f3n e implementaci\u00f3n, incluida la coordinaci\u00f3n de actualizaciones. El plano de control de AKS es administrado por la plataforma de Azure, y solo paga por los nodos de AKS que ejecutan sus aplicaciones. AKS se ha dise\u00f1ado sobre el motor de c\u00f3digo abierto de Azure Kubernetes Service (aks-engine). Los nodos del plano de control proporcionan los servicios centrales de Kubernetes y la orquestaci\u00f3n de las cargas de trabajo de las aplicaciones. Los nodos ejecutan las cargas de trabajo de la aplicaci\u00f3n. LOGIC APPS PowerAutomate, antes conocido como Flow, es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. Vamos a conocer algunos detalles del servicio para luego compararlo con Logic App (el servicio en el que realmente queremos hacer doble clic). Es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. El principal usuario objetivo es el de negocio, \u201cCitizen Developer\u201d / Integrador, es decir aquel usuario que crea nuevas aplicaciones comerciales para el consumo de otros utilizando entornos de desarrollo y tiempo de ejecuci\u00f3n autorizados por la TI corporativa. Su foco es brindar una experiencia para integraciones simples con aplicaciones y servicios. Logic Apps es un servicio PaaS (Plataforma como Servicio) para automatizar flujos de trabajo sobre m\u00faltiples aplicaciones SaaS y servicios IaaS, simplificando la complejidad requerida para la integraci\u00f3n empresarial. En si mismo, extiendo las capacidades de Power Automate. El principal usuario objetivo son Desarrolladores y IT Pros, es decir usuarios de sistemas que tienen conocimiento mucho m\u00e1s avanzado que un \u201cCitizen Developer\u201d. Su foco es brindar una experiencia para integraciones complejas y avanzadas, comparativamente con Power Automate. EJERCICIO En este tutorial se muestra c\u00f3mo utilizar Azure Functions con Logic Apps y Cognitive Services en Azure para ejecutar el an\u00e1lisis de opiniones de entradas de Twitter. Una funci\u00f3n desencadenada por HTTP clasifica los tweets en verdes, amarillos o rojos, en funci\u00f3n de la puntuaci\u00f3n de la opini\u00f3n. Se env\u00eda un correo electr\u00f3nico cuando se detecta una opini\u00f3n deficiente. IA, ML Y DL El aprendizaje profundo (deep learning), es un subconjunto del aprendizaje autom\u00e1tico basado en redes neuronales artificiales que permiten a un equipo entrenarse a s\u00ed mismo. En este caso, el proceso de aprendizaje se llama profundo porque la estructura de redes neuronales artificiales se compone de varias capas de entrada, salida y ocultas. Cada capa contiene unidades que transforman los datos de entrada en informaci\u00f3n que la capa siguiente puede usar para realizar una tarea de predicci\u00f3n determinada. Gracias a esta estructura, un equipo puede aprender a trav\u00e9s de su propio procesamiento de datos. El aprendizaje autom\u00e1tico (machine learning) es un subconjunto de la inteligencia artificial que incluye t\u00e9cnicas (como el aprendizaje profundo) que permiten a los equipos mejorar en las tareas con la experiencia. En este caso, el proceso de aprendizaje se basa en los pasos siguientes: Alimente un algoritmo con datos proporcion\u00e1ndole m\u00e1s informaci\u00f3n (por ejemplo, realizando la extracci\u00f3n de caracter\u00edsticas). Utilice estos datos para entrenar un modelo. Pruebe e implemente el modelo. Consuma el modelo implementado para realizar una tarea de predicci\u00f3n automatizada concreta. La inteligencia artificial (IA artificial intelligence) es una t\u00e9cnica que permite a los equipos imitar la inteligencia humana. Incluye el aprendizaje autom\u00e1tico. Es importante conocer la relaci\u00f3n entre aprendizaje autom\u00e1tico, aprendizaje profundo e inteligencia artificial: El aprendizaje autom\u00e1tico es una forma de lograr inteligencia artificial, lo que significa que, mediante el uso de t\u00e9cnicas de aprendizaje autom\u00e1tico y aprendizaje profundo, se pueden crear sistemas inform\u00e1ticos y aplicaciones que puedan realizar tareas asociadas normalmente a la inteligencia humana, como la percepci\u00f3n visual, el reconocimiento de voz, la toma de decisiones y la traducci\u00f3n de un idioma a otro. AZURE COGNITIVE SERVICES Azure Cognitive Services son servicios en la nube con API REST y SDK de biblioteca cliente que ayudan a los desarrolladores a compilar aplicaciones inteligentes cognitivas sin tener inteligencia artificial (IA) directa ni aptitudes o conocimientos sobre ciencia de datos. Azure Cognitive Services permiten a los desarrolladores agregar f\u00e1cilmente caracter\u00edsticas cognitivas en sus aplicaciones. El objetivo de Azure Cognitive Services es ayudar a los desarrolladores a crear aplicaciones que puedan ver, o\u00edr, hablar, comprender e incluso empezar a razonar. Cognitive Services proporciona funciones de aprendizaje autom\u00e1tico para solucionar problemas generales, como el an\u00e1lisis de texto para la opini\u00f3n emocional o el an\u00e1lisis de im\u00e1genes para reconocer objetos o caras. No es necesario tener conocimientos de aprendizaje autom\u00e1tico ni ciencia de datos para usar estos servicios, como tampoco conocer de programaci\u00f3n. Spatial Anchors es un servicio multiplataforma para desarrolladores que le permite crear experiencias de realidad mixta mediante objetos cuya ubicaci\u00f3n persiste en todos los dispositivos a lo largo del tiempo. Estas aplicaciones pueden admitir Microsoft HoloLens, dispositivos iOS compatibles con ARKit y dispositivos Android compatibles con ARCore. Azure Spatial Anchors permite a los desarrolladores trabajar con plataformas de realidad mixta para percibir el espacio, designar puntos precisos de inter\u00e9s y volver a recuperar esos puntos de inter\u00e9s desde los dispositivos compatibles. Estos puntos de inter\u00e9s precisos se conocen como delimitadores espaciales. BIG DATA Big Data (conocido tambi\u00e9n como Macrodatos) es un t\u00e9rmino que describe el gran volumen de datos, tanto estructurados como no estructurados, que inundan los negocios cada d\u00eda. Pero no es la cantidad de datos lo que es importante. Lo que importa con el Big Data es lo que las organizaciones hacen con los datos. Big Data se puede analizar para obtener ideas que conduzcan a mejores decisiones y movimientos de negocios estrat\u00e9gicos. Cuando hablamos de Big Data nos referimos a conjuntos de datos o combinaciones de conjuntos de datos cuyo tama\u00f1o (volumen), complejidad (variabilidad) y velocidad de crecimiento (velocidad) dificultan su captura, gesti\u00f3n, procesamiento o an\u00e1lisis mediante tecnolog\u00edas y herramientas convencionales, tales como bases de datos relacionales y estad\u00edsticas convencionales o paquetes de visualizaci\u00f3n, dentro del tiempo necesario para que sean \u00fatiles. Las Cargas de Trabajo Tradicionales (RDBMS) utilizan procesamiento de transacciones en l\u00ednea (OLTP) y procesamiento anal\u00edtico en l\u00ednea (OLAP). En los sistemas OLTP, los datos suelen ser relacionales con un esquema predefinido y un conjunto de restricciones para mantener la integridad referencial. A menudo, se pueden consolidar datos de varios or\u00edgenes de la organizaci\u00f3n en un almacenamiento de datos, utilizando un proceso ETL para mover y transformar los datos de origen. Una arquitectura de Big Data (Macrodatos) est\u00e1 dise\u00f1ada para controlar la ingesta, el procesamiento y el an\u00e1lisis de datos que son demasiado grandes o complejos para los sistemas de bases de datos tradicionales. Los datos se pueden procesar por lotes o en tiempo real. Las soluciones de macrodatos suelen implicar una gran cantidad de datos no relacionales, tales como datos de clave y valor, documentos JSON o datos de series temporales. Los sistemas RDBMS tradicionales no suelen ser adecuados para almacenar este tipo de datos. El t\u00e9rmino NoSQL hace referencia a la familia de bases de datos que est\u00e1n dise\u00f1adas para contener datos no relacionales. El t\u00e9rmino no es totalmente exacto, porque muchos almacenes de datos no relacionales admiten consultas compatibles con SQL. El t\u00e9rmino NoSQL significa \u201cno solo SQL\u201d. Conceptos Un data warehouse es un repositorio unificado y estructurado para todos los datos que recogen los diversos sistemas de una empresa. El repositorio puede ser f\u00edsico o l\u00f3gico y hace hincapi\u00e9 en la captura de datos de diversas fuentes sobre todo para fines anal\u00edticos y de acceso. Un data lake es un repositorio de almacenamiento que contienen una gran cantidad de datos en bruto, estructurado, semi-estructurados & no estructurados, y que se mantienen all\u00ed hasta que sea necesario. A diferencia de un data warehouse jer\u00e1rquico que almacena datos en ficheros o carpetas, un data lake utiliza una arquitectura plana para almacenar los datos. Existen algunas diferencias clave entre Data Lake y Data Warehouse: Datos: Un data warehouse s\u00f3lo almacena datos que han sido modelados o estructurados, mientras que un Data Lake no hace acepci\u00f3n de datos. Lo almacena todo, estructurado, semiestructurado y no estructurado. Procesamiento: Antes de que una empresa pueda cargar datos en un data warehouse, primero debe darles forma y estructura, es decir, los datos deben ser modelados. Eso se llama schema-on-write. Con un data lake, s\u00f3lo se cargan los datos sin procesar, tal y como est\u00e1n, y cuando est\u00e9 listo para usar los datos, es cuando se le da forma y estructura. Eso se llama schema-on-read. Dos enfoques muy diferentes. Almacenamiento: Una de las principales caracter\u00edsticas de las tecnolog\u00edas de big data, como Hadoop, es que el coste de almacenamiento de datos es relativamente bajo en comparaci\u00f3n con el de un data warehouse. Hay dos razones principales para esto: en primer lugar, Hadoop es software de c\u00f3digo abierto, por lo que la concesi\u00f3n de licencias y el soporte de la comunidad es gratuito. Y segundo, Hadoop est\u00e1 dise\u00f1ado para ser instalado en hardware de bajo coste. Agilidad: Un almac\u00e9n de datos es un repositorio altamente estructurado, por definici\u00f3n. No es t\u00e9cnicamente dif\u00edcil cambiar la estructura, pero puede tomar mucho tiempo dado todos los procesos de negocio que est\u00e1n vinculados a ella. Un data lake, por otro lado, carece de la estructura de un data warehouse, lo que da a los desarrolladores y a los cient\u00edficos de datos la capacidad de configurar y reconfigurar f\u00e1cilmente y en tiempo real sus modelos, consultas y aplicaciones. Seguridad: La tecnolog\u00eda del data warehouse existe desde hace d\u00e9cadas, mientras que la tecnolog\u00eda de big data (la base de un Data Lake) es relativamente nueva. Por lo tanto, la capacidad de asegurar datos en un data warehouse es mucho m\u00e1s madura que asegurar datos en un data lake. Cabe se\u00f1alar, sin embargo, que se est\u00e1 realizando un importante esfuerzo en materia de seguridad en la actualidad en la industria de Big Data. Azure Data Factory es la plataforma que resuelve estos escenarios de datos. Se trata de un servicio de integraci\u00f3n de datos y ETL basado en la nube que le permite crear flujos de trabajo orientados a datos a fin de coordinar el movimiento y la transformaci\u00f3n de datos a escala. Con Azure Data Factory, puede crear y programar flujos de trabajo basados en datos (llamados canalizaciones) que pueden ingerir datos de distintos almacenes de datos. Puede crear procesos ETL complejos que transformen datos visualmente con flujos de datos o mediante servicios de proceso como Azure HDInsight Hadoop, Azure Databricks y Azure SQL Database. Azure Databricks es una plataforma de an\u00e1lisis basada en Apache Spark optimizada para la plataforma de servicios en la nube de Microsoft Azure. Dise\u00f1ada por los fundadores de Apache Spark, Databricks est\u00e1 integrado con Azure para proporcionar una configuraci\u00f3n con un solo clic, flujos de trabajo optimizados y un \u00e1rea de trabajo interactiva que permite la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas empresariales. Y lo mejor: no requiere administraci\u00f3n :-). Apache Spark es un framework de programaci\u00f3n para procesamiento de datos distribuidos dise\u00f1ado para ser r\u00e1pido y de prop\u00f3sito general. Como su propio nombre indica, ha sido desarrollada en el marco del proyecto Apache, lo que garantiza su licencia Open Source. Azure Synapse es un servicio de an\u00e1lisis que engloba el almacenamiento de datos empresariales y el an\u00e1lisis de macrodatos. Le ofrece la libertad de consultar los datos como prefiera, ya sea a petici\u00f3n sin servidor o con recursos aprovisionados, a escala. Azure Synapse re\u00fane estos dos mundos con una experiencia unificada para ingerir, preparar, administrar y servir datos para las necesidades inmediatas de inteligencia empresarial y aprendizaje autom\u00e1tico. MENSAJERIA Azure ofrece varios servicios que le ayudan en la entrega de mensajes de evento en una soluci\u00f3n. Estos servicios son los siguientes: - Event Grid - Event Hubs - Service Bus Evento: es una notificaci\u00f3n ligera de una condici\u00f3n o un cambio de estado. El publicador del evento no tiene ninguna expectativa sobre c\u00f3mo se trata el evento. El consumidor del evento decide qu\u00e9 hacer con la notificaci\u00f3n. Los eventos pueden ser unidades discretas o parte de una serie. Mensaje: son datos sin procesar producidos por un servicio que se consumen o almacenan en otro lugar. El mensaje contiene los datos que desencaden\u00f3 la canalizaci\u00f3n del mensaje. El publicador del mensaje tiene una expectativa sobre la forma en que el consumidor trata el mensaje. Existe un contrato entre ambas partes. Por ejemplo, el publicador env\u00eda un mensaje con los datos sin procesar, espera que el consumidor cree un archivo a partir de esos datos y env\u00eda una respuesta cuando el trabajo finaliza. Microsoft Azure Service Bus es un agente de mensajes de integraci\u00f3n empresarial completamente administrado. Service Bus puede desacoplar aplicaciones y servicios. Service Bus ofrece una plataforma confiable y segura para la transferencia asincr\u00f3nica de datos y estado. Azure Event Hubs es una plataforma de streaming de macrodatos y un servicio de ingesta de eventos. Puede recibir y procesar millones de eventos por segundo. Los datos enviados a un centro de eventos se pueden transformar y almacenar con cualquier proveedor de an\u00e1lisis en tiempo real o adaptadores de procesamiento por lotes y almacenamiento. Azure IoT Hub es la puerta de enlace en la nube que conecta dispositivos IoT para recopilar los datos y dirigir las perspectivas y automatizaci\u00f3n empresariales. Adem\u00e1s, IoT Hub incluye caracter\u00edsticas que enriquecen la relaci\u00f3n entre los dispositivos y los sistemas back-end. Las capacidades de comunicaci\u00f3n bidireccional implican que al tiempo que se reciben datos de los dispositivos, tambi\u00e9n es posible devolver comandos y directivas a los dispositivos. Azure Event Grid permite crear f\u00e1cilmente aplicaciones con arquitecturas basadas en eventos. Event Grid est\u00e1 dise\u00f1ado para eventos, no datos. Cuando un Event Grid es informado de un evento, luego toma acciones determinadas. AZURE ACTIVE DIRECTORY Azure Active Directory (Azure AD) es un servicio de administraci\u00f3n de identidades y acceso basado en la nube de Microsoft que ayuda a los empleados a iniciar sesi\u00f3n y acceder a recursos en: Recursos externos, como Microsoft 365, Azure Portal y miles de otras aplicaciones SaaS. Recursos internos, como las aplicaciones de la red corporativa y la intranet, junto con todas las aplicaciones en la nube desarrolladas por su propia organizaci\u00f3n. Los siguientes perfiles usan Azure AD: Administradores de TI. Si es administrador de TI, puede usar Azure AD para controlar el acceso a sus aplicaciones y a los recursos de est\u00e1s en funci\u00f3n de los requisitos de su empresa. Por ejemplo, puede usar Azure AD para requerir autenticaci\u00f3n multifactor al acceder a recursos importantes de la organizaci\u00f3n. Adem\u00e1s, puede usar Azure AD para automatizar el aprovisionamiento de usuarios entre la instancia existente de Windows Server AD y las aplicaciones en la nube, incluida Microsoft 365. Por \u00faltimo, Azure AD proporciona eficaces herramientas que ayudan a proteger autom\u00e1ticamente las identidades y credenciales de los usuarios y a cumplir los requisitos de gobernanza de acceso. Desarrolladores de aplicaciones. Como desarrollador de aplicaciones, puede usar Azure AD como enfoque basado en est\u00e1ndares para agregar el inicio de sesi\u00f3n \u00fanico (SSO) a cualquier aplicaci\u00f3n, lo que le permite trabajar con las credenciales existentes de un usuario. Azure AD tambi\u00e9n proporciona varias API que pueden ayudarle a crear experiencias de aplicaci\u00f3n personalizadas que usen los datos existentes de la organizaci\u00f3n. Suscriptores de Microsoft 365, Office 365, Azure o Dynamics CRM Online. Los suscriptores usan Azure AD. Cada inquilino de Microsoft 365, Office 365, Azure y Dynamics CRM Online es autom\u00e1ticamente un inquilino de Azure AD. Puede empezar de inmediato a administrar el acceso a las aplicaciones en la nube integradas. Entidades: Cloud Only: Esta opci\u00f3n de identidad es la que, por defecto, se habilita cuando creamos una cuenta en Office 365 o cuando creamos un directorio de Active Directory en Azure nuevo. En este caso, la contrase\u00f1a es gestionada por Azure Active Directory y no tiene relaci\u00f3n absoluta con nuestra infraestructura on-promises. De hecho, tampoco nuestro UPN de usuario tiene relaci\u00f3n con nuestra infraestructura on-promises, y de hecho (finalmente) debemos dar de alta los usuarios uno por uno. Sincronizada: Esta opci\u00f3n es la primera que, de alguna forma, relaciona nuestro servicio de directorio local (Active Directory Domain Services u otro LDAP) con el servicio de directorio en la nube (Azure Active Directory). Federada: La tercera y \u00faltima opci\u00f3n agrega la posibilidad de que el inicio de sesi\u00f3n se efectivice en nuestra infraestructura local. Azure AD Connect: El uso de esta caracter\u00edstica es gratis y est\u00e1 incluido en su suscripci\u00f3n de Azure. Es el componente que nos permite sincronizar identidades, o conectar nuestro servicio de directorio de Azure AD con otros LDAPs propios. Esta herramienta nos permitir\u00e1 sincronizar uno o muchos LDAPs con Azure AD, pero no un LDAP con muchos Azure ADs. Este item es importante de tener en cuenta cuando estemos planificando la arquitectura de soluci\u00f3n. DEVOPS DevOps es \u201cla union de personas, procesos, y productos para permitir la entrega continua de valor a sus usuarios finales\u201d. DevOps es una pr\u00e1ctica de ingenier\u00eda de software que tiene como objetivo unificar el desarrollo de software (Dev) y la operaci\u00f3n del software (Ops). La principal caracter\u00edstica del movimiento DevOps es defender en\u00e9rgicamente la automatizaci\u00f3n y el monitoreo en todos los pasos de la construcci\u00f3n del software, desde la integraci\u00f3n, las pruebas, la liberaci\u00f3n hasta la implementaci\u00f3n y la administraci\u00f3n de la infraestructura. DevOps apunta a ciclos de desarrollo m\u00e1s cortos, mayor frecuencia de implementaci\u00f3n, lanzamientos m\u00e1s confiables, en estrecha alineaci\u00f3n con los objetivos comerciales. Cuando hablamos de DevOps podemos hablar de 3 etapas, que no necesariamente se dan en cascada. Cuando trabajamos DevOps en las organizaciones debemos pensar en: Personas, Procesos y Productos. Cuando hablamos de personas, tenemos que entender que los equipos de trabajo est\u00e1n conformados por Personas: estas tienen emociones, que pueden manejar mejor o peor dependiendo de las situaciones personales y laborales que atraviesen. Entender esto y darle una mirada humana nos facilitar\u00e1 entender que DEBEMOS trabajar con las personas, desarrollar caracter\u00edsticas deseables que lleven a que sean \u201cEasy to work with\u201d, es decir una persona \u201ccon la que es f\u00e1cil y queremos trabajar\". Existen 4 grandes procesos, en alto nivel, que tenemos que tener en cuenta. Estos procesos, por supuesto, pueden darse c\u00edclicamente y no necesariamente en cascada: Planificar: crear un backlog, equipos interdisciplinarios, planificar testing, etc. Desarrollar + Probar: ya conozco que debo hacer, tengo mi backlog al menos en alto nivel, y podemos avanzar en la construcci\u00f3n. Liberar: lo que planifiqu\u00e9 y desarroll\u00e9 + prob\u00e9, lo debo liberar a ambientes. Desarrollo, Testing, Producci\u00f3n, etc. Monitorear + Aprender: un punto clave, es que el trabajo no termin\u00f3 con la liberaci\u00f3n. Reci\u00e9n comienza aqu\u00ed el aprendizaje basado en un monitoreo proactivo para volver a arrancar esta rueda: planificar, desarrollar + probar, y liberar, para volver a aprender. Desarrollo \u00e1gil: pr\u00e1cticas y marcos de trabajo \u00e1giles para el desarrollo. Integraci\u00f3n Continua: en todo momento, integrar nuestro c\u00f3digo con pruebas y chequeos de salud. Entrega / Despliegue Continuo: hacia ambientes, ya sea desarrollo, testing, pre producci\u00f3n o producci\u00f3n, acompa\u00f1ado de una robusta gesti\u00f3n de releases. AZURE DEVOPS Azure DevOps proporciona servicios de desarrollador para ayudar a los equipos a planificar el trabajo, colaborar en el desarrollo de c\u00f3digo y crear e implementar aplicaciones. Los desarrolladores pueden trabajar en la nube con Azure DevOps Services o localmente con Azure DevOps Server. Azure DevOps Server se llamaba anteriormente Visual Studio Team Foundation Server (TFS). Azure Repos proporciona repositorios de Git o Team Foundation Version Control (TFVC) para el control de c\u00f3digo fuente de su c\u00f3digo. Azure Pipelines proporciona servicios de creaci\u00f3n y lanzamiento para admitir la integraci\u00f3n y entrega continuas de sus aplicaciones. Azure Boards ofrece un conjunto de herramientas \u00e1giles para respaldar la planificaci\u00f3n y el seguimiento del trabajo, los defectos de c\u00f3digo y los problemas con los m\u00e9todos Kanban y Scrum. Azure Test Plans proporciona varias herramientas para probar sus aplicaciones, incluidas pruebas manuales / exploratorias y pruebas continuas. Azure Artifacts permite a los equipos compartir paquetes Maven, npm y NuGet de fuentes p\u00fablicas y privadas e integrar el uso compartido de paquetes en sus canalizaciones de CI / CD. Azure DevOps es gratuito hasta 5 usuarios por organizaci\u00f3n. Una organizaci\u00f3n de Azure DevOps permite tener, dentor, una colecci\u00f3n de proyectos con diversos repositorios. La creaci\u00f3n de repositorios tambi\u00e9n es gratuita, e inclusive la utilizaci\u00f3n de Pipelines hosteados por Microsoft para proyectos privados de hasta 1800 minutos por mes tambi\u00e9n es gratuito. En caso de proyectos p\u00fablicos, no hay l\u00edmite de minutos para Pipelines. Precios Vamos a devops starter para crear nuestro pipeline. SEGURIDAD AZURE Azure Security Center es un sistema unificado de administraci\u00f3n de seguridad de la infraestructura que fortalece la posici\u00f3n de seguridad de los centros de datos y proporciona una protecci\u00f3n contra amenazas avanzada de todas las cargas de trabajo h\u00edbridas que se encuentran en la nube, ya sea que est\u00e9n en Azure o no, as\u00ed como tambi\u00e9n en el entorno local. Proteger los recursos es un esfuerzo conjunto entre el proveedor de nube, Azure y usted, el cliente. Cuando migra a la nube, debe asegurarse de que las cargas de trabajo est\u00e9n seguras y, al mismo tiempo, cuando se migra a IaaS (infraestructura como servicio), el cliente tiene una responsabilidad mayor que cuando se encontraba en PaaS (plataforma como servicio) y SaaS (software como servicio). Azure Security Center le brinda las herramientas necesarias para fortalecer la red, proteger los servicios y garantizar que tiene la mejor posici\u00f3n de seguridad. Azure Security Center admite m\u00e1quinas virtuales y servidores en diferentes tipos de entornos h\u00edbridos: Solo Azure Azure y entorno local Azure y otras nubes Azure, otras nubes y entorno local Azure Information Protection (AIP) es una soluci\u00f3n basada en la nube que permite a las organizaciones clasificar y proteger documentos y correos electr\u00f3nicos mediante etiquetas. Se pueden aplicar etiquetas: Autom\u00e1ticamente por los administradores mediante reglas y condiciones. Manualmente por los usuarios. Mediante una combinaci\u00f3n en la que los administradores definen las recomendaciones que se muestran a los usuarios. Las etiquetas pueden clasificar y, opcionalmente, proteger el documento, con lo que puede hacer lo siguiente: Seguir y controlar el modo en que se usa el contenido. Analizar los flujos de datos para obtener una visi\u00f3n general de su negocio : detectar comportamientos de riesgo y tomar medidas correctivas. Hacer un seguimiento del acceso a los documentos e impedir la p\u00e9rdida de datos o su uso indebido. Use Azure Information Protection para aplicar la clasificaci\u00f3n a etiquetas en documentos y correos electr\u00f3nicos. Estas son algunas de las funciones de etiquetar contenido: Clasificaci\u00f3n que se puede detectar independientemente de d\u00f3nde se almacenen los datos o con qui\u00e9n se compartan. Distintivos visuales, como encabezados, pies de p\u00e1gina o marcas de agua. Metadatos que se agregan a los archivos y encabezados de correo electr\u00f3nico en texto no cifrado. Los metadatos de texto no cifrado garantizan que otros servicios puedan identificar la clasificaci\u00f3n y tomar las medidas adecuadas. Microsoft Azure Sentinel es una soluci\u00f3n de administraci\u00f3n de eventos de informaci\u00f3n de seguridad (SIEM) y respuesta automatizada de orquestaci\u00f3n de seguridad (SOAR) que es escalable y nativa de la nube. Azure Sentinel ofrece an\u00e1lisis de seguridad inteligente e inteligencia frente a amenazas en toda la empresa, de forma que proporciona una \u00fanica soluci\u00f3n para la detecci\u00f3n de alertas, la visibilidad de amenazas, la b\u00fasqueda proactiva y la respuesta a amenazas. Azure Sentinel permite obtener una vista general de toda la empresa, lo que suaviza la tensi\u00f3n de ataques cada vez m\u00e1s sofisticados, vol\u00famenes de alertas cada vez mayores y plazos de resoluci\u00f3n largos. Recopile datos a escala de nube de todos los usuarios, dispositivos, aplicaciones y de toda la infraestructura, tanto en el entorno local como en diversas nubes. Detecte amenazas que antes no se detectaban y reduzca los falsos positivos mediante el an\u00e1lisis y la inteligencia de amenazas sin precedentes de Microsoft. Investigue amenazas con inteligencia artificial y busque actividades sospechosas a escala, aprovechando el trabajo de ciberseguridad que ha llevado a cabo Microsoft durante d\u00e9cadas. Responda a los incidentes con rapidez con la orquestaci\u00f3n y la automatizaci\u00f3n de tareas comunes integradas.","title":"Azure"},{"location":"azure/#azure","text":"","title":"AZURE"},{"location":"azure/#que-es","text":"Microsoft Azure es un servicio de Inform\u00e1tica en la Nube creado por Microsoft para construir, testear, desplegar y gestionar aplicaciones y servicios a trav\u00e9s de centros de datos gestionados por Microsoft. Microsoft Azure, como otros proveedores de nube, nos permite alquilar recursos como espacio de almacenamiento o ciclos de CPU en equipos f\u00edsicos que no debo administrar. Solo se paga por lo que usa (o al menos se mide). Los servicios inform\u00e1ticos ofrecidos suelen variar en funci\u00f3n de cada proveedor. Normalmente estos servicios incluyen: Potencia de proceso: por ejemplo, aplicaciones web o servidores Linux. Almacenamiento: por ejemplo, archivos y bases de datos. Redes: por ejemplo, conexiones seguras entre el proveedor de nube y la empresa.","title":"QUE ES"},{"location":"azure/#potencia-de-proceso","text":"Cuando hacemos virtualmente cualquier acci\u00f3n en internet, como pagar una factura online, leer un peri\u00f3dico y enviar un correo electr\u00f3nico, estamos interactuando con servidores de nube que procesan cada solicitud y devuelven una respuesta. Todo esto requiere de c\u00f3mputo. Maquina virtual Contenedores Serverless o informatica sin servidor","title":"Potencia de proceso"},{"location":"azure/#almacenamiento","text":"La mayor\u00eda de las aplicaciones leen y escriben datos. Y en este sentido, el tipo de datos y c\u00f3mo se almacenan puede ser diferente seg\u00fan el tipo de aplicaci\u00f3n, la necesidad y velocidad requerida. Los proveedores de nube suelen ofrecer soluciones de almacenamiento para m\u00e1quinas virtuales, aplicaciones web, bases de datos, archivos de datos y anal\u00edtica. Por ejemplo, si quiere almacenar texto o un clip de pel\u00edcula, podr\u00eda usar un archivo en disco. Si tuviera un conjunto de relaciones (por ejemplo, una libreta de direcciones), podr\u00eda decidirse por un enfoque m\u00e1s estructurado, como usar una base de datos. La ventaja de utilizar almacenamiento basado en nube, es que no debemos preocuparnos por el escalado. Si se necesita m\u00e1s espacio, se puede agregar pagando un poco m\u00e1s de precio, e inclusive si las necesidades de almacenamiento bajan, tambi\u00e9n bajar\u00e1 el precio asociado.","title":"Almacenamiento"},{"location":"azure/#redes","text":"En todos estos casos, las redes cobran una importancia vital. Los proveedores de nube suelen tener servicios de redes que nos permiten: + Crear y configurar Redes Virtuales. + Crear y conectar de extremo a extremo redes en la nube con una infraestructura local (conocidas como site-to-site, y point-to-site). + Parametrizar reglas de acceso a recursos. + Monitorear tr\u00e1fico de redes. + Aplicar reglas, restricciones y protecciones a las comunicaciones.","title":"Redes"},{"location":"azure/#crear-cuenta","text":"Web azure A trav\u00e9s de azure.com: es la forma m\u00e1s r\u00e1pida y f\u00e1cil que tienen las organizaciones de todos los tama\u00f1os para empezar a usar Azure. Puede administrar las implementaciones y el uso de Azure, como as\u00ed tambi\u00e9n obtener una factura mensual de Microsoft por los servicios usados. Con la ayuda de un Partner de Microsoft. Es un modelo para obtener facturaci\u00f3n local en tu pa\u00eds. De esta manera, Azure se brindar\u00e1 como servicio administrado a trav\u00e9s de un partner, qui\u00e9n te proporcionar\u00e1 el acceso y la facturaci\u00f3n, junto con un soporte t\u00e9cnico b\u00e1sico. A trav\u00e9s de un representante directo de Microsoft, opci\u00f3n pensada para organizaciones de gran tama\u00f1o o clientes que ya trabajan con la marca. A diferencia de azure.com (que requiere tarjeta de cr\u00e9dito), esto habilitar\u00e1 un tipo de contrato especial con varias ventajas al momento de necesitar varias suscripciones. Los servicios de Azure est\u00e1n disponibles a trav\u00e9s de Centros de Datos gestionados por Microsoft. Los mismos est\u00e1n conformados por edificios. xisten +60 regiones anunciadas en todo el mundo, y muchas que est\u00e1n anunciadas como adicionales futuras. Esto representa una presencia f\u00edsica en 140 pa\u00edses. En el mapa podr\u00e1s ver la ubicaci\u00f3n de los centros de datos, a excepci\u00f3n de 3 correspondientes a gobierno por lo cual su ubicaci\u00f3n es secreta. SLA significa en ingl\u00e9s \u201cservice level agreement\u201d, y en espa\u00f1ol \u201cacuerdo de nivel de servicio\u201d. Es un acuerdo escrito entre un proveedor de servicio y su cliente con objeto de fijar el nivel acordado para la calidad de dicho servicio. Este nivel puede ser un porcentaje que representa la disponibilidad m\u00ednima","title":"CREAR CUENTA"},{"location":"azure/#principio-5-3-2","text":"La inform\u00e1tica en la nube es un metodo de gestion de recursos de IT donde los usuarios acceden a los recursos virtuales de computo, red y almacenamiento que estan disponibles online. Estos recursos se pueden aprovionar de manera instantanea y elastica. Se compone de: 5 Caracteristicas 3 metodos de entrega 2 modelos de implementacion","title":"PRINCIPIO 5-3-2"},{"location":"azure/#caracteristicas","text":"Autoservicio y bajo demanda: un consumidor puede provisionarse de caracteristicas como tiempo de uso, almacenamiento, memoria... Acceso amplio y ubicuo: los recursos pueden ser accecidos desde cualquier lugar y cualquier dispositivo. Ubicacion transparente y agrupacion de recursos: suelen estar en diferentes localizaciones sobre distintos recursos fisicos o virtuales que son dinamicamente asignados. Elasticidad rapida (estirarse y contraerse): pueden aumentar en epocas de mucha carga asi como reducirlo cuando no se use. Servicio medido (e incluso pago por uso): recursos y capacidades segun lo que necesitas.","title":"Caracteristicas"},{"location":"azure/#metodos-de-entrega","text":"IaaS(Infraestructura como Servicio): Cliente tiene capacidad de utilizar almacenamiento, red, recursos sofware, SO, app. No tiene el control sobre la infraestructura pero sino tiene el control del resto apartir del SO. COntrol limitado sobre red como el firewall Ejemplo serian las maquinas virtuales PaaS(Plataforma como Servicio): Podemos desplegar apps propias o de terceros Control sobre las apps y la configuracion de ellas Ejemplo seria servicios hosting Saas(Software como Servicio): Capacidad de usar aplicaciones en una infraestructura de nube que cumple con las 5 caracteristicas No tenemos control sobre ningun componente, solo lo usamos. Ejemplo seria Office 365, exchange, gmail, yahoo, google apps.","title":"Metodos de entrega"},{"location":"azure/#modelos-de-implementacion","text":"Nube Privada: en mi propio centro de datos Nube P\u00fablica: en azure o otros proveedores de servicios Puede haber la mezcla con Nube Hibrida.","title":"Modelos de implementaci\u00f3n"},{"location":"azure/#maquinas-virtuales","text":"Tipo de recurso escalable por Azure Se tiene control total sobre la configuraci\u00f3n y se puede instalar de todo No es necesario comprar hardware fisico para escalar o ampliar Azure tiene servicios para supervisar, proteger y administrar las actualizaciones y revisiones del sistema operativo Soy responsable de: Mantener el SO y sus actualizaciones Trabar sobre la performance Monitorear el espacio de disco usado Componentes: Disco virtual: el disco es el que tendr\u00e1, por ejemplo, el sistema operativo instalado. Gracias al disco virtual puedo iniciar el equipo y guardar informaci\u00f3n en forma persistente Placa de red virtual: al igual que en un equipo f\u00edsico, es la que me facilitar\u00e1 la conexi\u00f3n con una o m\u00e1s redes. Direcciones IP: gracias a la cual podr\u00e9 conectarme al equipo virtual. Estas direcciones IP pueden ser privadas y p\u00fablicas. Grupos de seguridad de red: que nos ayudar\u00e1n a definir desde qu\u00e9 origenes me puedo conectar, y hacia qu\u00e9 destinos puedo acceder, teniendo en cuenta protocolos, puertos, etc. Los Network Security Groups son una manera \u00e1gil de gestionar los permisos de red, para una o m\u00e1s m\u00e1quinas. Configurar: puedo el nombre de la MV, el SO y el tama\u00f1o. Tiene al menos dos discos, uno para el SO y otro temporal para la memoria virtual. Spot Virtual: herramienta que lo que no se use se vaya ahi para ahorrar.","title":"MAQUINAS VIRTUALES"},{"location":"azure/#crear-mv","text":"Assignment Tasks A Ingresar al Portal de Azure. Crear un \"Grupo de Recursos\" [Resource Group] con el nombre \"azf-vms-1\" Completed on 4 noviembre, 2020 7:01 pm B Dentro del Resource Group, seleccionar la opci\u00f3n \"Crear recurso\" [Create resources]. Seleccionar el grupo \"C\u00f3mputo\" [Compute] y de la lista \"M\u00e1quina Virtual\" [Virtual Machine]. Completed on 4 noviembre, 2020 7:01 pm C En el asistente de creaci\u00f3n, validar que la suscripci\u00f3n seleccionada sea la correcta (probablemente sea \"FREE TRIAL\") y el Resource Group seleccionado es el correcto: \"azf-vms-1\". Completed on 4 noviembre, 2020 7:01 pm D Ingresar un nombre para la m\u00e1quina virtual, por ejemplo \"azf-vm-windows-2019\". Completed on 4 noviembre, 2020 7:01 pm E Seleccionar una regi\u00f3n de Azure. Por ejemplo \"Este de Estados Unidos\" [East US], una imagen \"WIndows Server 2019 Datacenter\" y un tama\u00f1o de m\u00e1quina virtual (explorar todas las im\u00e1genes y elegir un tama\u00f1o como \"B1ms\" (es un equipo barato para este ejercicio). Completed on 4 noviembre, 2020 7:01 pm F Ingresar un usuario [Username] y una contrase\u00f1a dos veces [Password]. \u00a1No olvidarlas! Completed on 4 noviembre, 2020 7:01 pm G Seleccionar el puerto de entrada [Inbound port] \"RDP (3389)\" para poder ingresar luego al equipo. Completed on 4 noviembre, 2020 7:02 pm H Ir al paso siguiente: \"Discos\" [Disks]. Ingresar un Disco de Datos adicional [Create new disk] del menor tama\u00f1o posible. Completed on 4 noviembre, 2020 7:06 pm I Ir al siguiente paso \"Redes\" [Networking]. Crear una nueva red con el nombre \"azf-vnet-1\" con el espacio de direcciones \"10.0.0.0/16\" y crear una subnet con el nombre \"Sub1\" y el espacio de direcciones \"10.0.0.0/24\". Validar que una vez creada la red, est\u00e9 seleccionada en \"Virtual Network\" y \"Subnet\" en el asistente del equipo virtual. Completed on 4 noviembre, 2020 7:11 pm J Crear una IP p\u00fablica con el nombre \"azf-ip-1\", el \"SKU Basic\" y asignaci\u00f3n \"Static\". Completed on 4 noviembre, 2020 7:11 pm K Seleccionar el grupo de seguridad de red [NIC network security group] en \"Basic\". Completed on 4 noviembre, 2020 7:11 pm L Validar que los puertos habilitados son solo \"RDP (3389)\". Completed on 4 noviembre, 2020 7:11 pm M Ir al siguiente paso \"Administraci\u00f3n\" [Management]. Completed on 4 noviembre, 2020 7:13 pm N Seleccionar en diagn\u00f3stico de booteo [Boot diagnostics] en \"On\". Esto requerir\u00e1 crear una cuenta de almacenamiento. Completed on 4 noviembre, 2020 7:16 pm O En el campo \"Cuenta de Almacenamiento de Diagn\u00f3stico\" [Diagnostics storage account] crear una nueva cuenta de almacenamiento con el nombre \"azfstorageXXXX\" donde XXXX es un n\u00famero aleatorio generado por ti (dado que los nombres de cuentas de almacenamiento deben ser \u00fanicos en todo Azure). El tipo de cuenta debe ser \"Storage (general purpose v1) y \"Locally-redundant storage (LRS). Completed on 4 noviembre, 2020 7:17 pm P Habilitar el apagado autom\u00e1tico [Auto-shutdown] y elegir un horario de apagado para tu zona geogr\u00e1fica. Completed on 4 noviembre, 2020 7:17 pm Q No modificar el resto de las opciones e ir al siguiente paso \"Avanzado\" [Advanced]. Completed on 4 noviembre, 2020 7:18 pm R No modificar ninguna opci\u00f3n e ir al siguiente paso \"Etiquetas\" [Tags]. Completed on 4 noviembre, 2020 7:18 pm S Ir al \u00faltimo paso \"Revisi\u00f3n y Creaci\u00f3n\" [Review + create]. Cuando pase todas las validaciones, revisar el resumen de opciones seleccionadas que coincidan con lo solicitado y crear la m\u00e1quina. Completed on 4 noviembre, 2020 7:19 pm T Cuando finalice la creaci\u00f3n, ir al equipo virtual y seleccionar \"Conectar\" [Connect] y elegir \"RDP\". Se descargar\u00e1 un archivo, y desde el cliente de Escritorio Remoto de tu computadora conectarse. U Ingresar el nombre de usuario y contrase\u00f1a que ingresamos en pasos anteriores, y comprobar que nos podemos conectar al equipo. V Ir al Grupo de Recursos [Resource Group] que hemos creado y comprobar que todos los recursos (m\u00e1quina virtual, discos, placas de red) est\u00e1n creados.","title":"CREAR MV"},{"location":"azure/#herramientas-azure","text":"Azure Portal para interactuar con Azure a trav\u00e9s de una interfaz gr\u00e1fica de usuario (GUI). Azure PowerShell y la interfaz de la l\u00ednea de comandos de Azure (CLI) para las interacciones con Azure de l\u00ednea de comandos y basadas en automatizaci\u00f3n. Azure Cloud Shell para una interfaz de l\u00ednea de comandos basada en web. Azure Mobile App para supervisar y administrar los recursos desde el dispositivo m\u00f3vil.","title":"HERRAMIENTAS AZURE"},{"location":"azure/#azure-powershell","text":"Instamos la herramienta AzurePowershell , apartir de la versi\u00f3n 7 es multiplataforma. linux # Register the Microsoft signature key sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # Register the Microsoft RedHat repository curl https://packages.microsoft.com/config/rhel/7/prod.repo | sudo tee /etc/yum.repos.d/microsoft.repo # Update the list of products sudo dnf check-update # Install a system component sudo dnf install compat-openssl10 # Install PowerShell sudo dnf install -y powershell # Start PowerShell pwsh Instalamos el modulo de AZ powershell: pwsh Install-Module -Name Az Nos conectamos con nuestra cuenta yendo al link que nos indica: PS /home/isx46410800/Documents> Connect-AzAccount Orden de listar los Resources Groups Get-AzResourceGroup : PS /home/isx46410800/Documents> Get-AzResourceGroup ResourceGroupName : NetworkWatcherRG Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/NetworkWatcherRG # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 Crear un Resource Group por comando New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" : PS /home/isx46410800/Documents> New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" ResourceGroupName : azf-rgexmaple-rg Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-rgexmaple-rg # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 azf-rgexmaple-rg eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026","title":"AZURE POWERSHELL"},{"location":"azure/#azure-cli","text":"Documentacion CLI Instalaci\u00f3n linux: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # sudo sh -c 'echo -e \"[azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/azure-cli.repo' # sudo yum install azure-cli Iniciamos sesion haciendo login: az login Listamos los resource groups con az group list --output table : Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded Creamos un resource group con az group create --location \"eastus\" --name \"azf-cli-rg\" : { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cli-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cli-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded","title":"AZURE CLI"},{"location":"azure/#azure-cloud-shell","text":"Es un shell a trav\u00e9s del navegador desde cualquier sistema operativo. Necesitamos crear un storage y file share para utilizarlo. Ahora dentro podemos usar las mismas ordenes que cli por ejemplo y ver/crear resource groups como ejemplo: miguel@Azure:~$ az group create --location \"eastus\" --name \"azf-cloudshell-example-rg\" { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cloudshell-example-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cloudshell-example-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } miguel@Azure:~$ az group list --output table Name Location Status ------------------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded azf-cloudshell-rg eastus Succeeded azf-cloudshell-example-rg eastus Succeeded Podemos verlo en pantalla completa con www.shell.azure.com","title":"AZURE CLOUD SHELL"},{"location":"azure/#ejemplo-de-crear-mv-por-script","text":"Crear una MV con CLI a trav\u00e9s de cloud shell: # create a resource group az group create --name azf-crear-mv-cli --location eastus # Create a virtual network net and subnet. az network vnet create --resource-group azf-cloudshell-example-rg --name azf-vnet-1 --address-prefix 10.0.0.0/16 --subnet-name sub1 --subnet-prefix 10.0.0.0/24 # Create a public IP address. az network public-ip create --name myPublicCliIP --resource-group azf-cloudshell-example-rg --sku Basic --allocation-method Static # create vm customized az vm create --subscription \"Evaluaci\u00f3n gratuita\" -g azf-cloudshell-example-rg --name vm-cli-example --image win2019datacenter --location eastus --size Standard_B1ms --admin-username vmadmin --admin-password mi--GU--el14 --vnet-name azf-vnet-1 --subnet sub1 # add new optional disk az vm disk attach -g azf-cloudshell-example-rg --vm-name vm-cli-example --name myDataDisk --new --size-gb 4 # input port to connect VM az vm open-port --port 3389 --resource-group azf-cloudshell-example-rg --name vm-cli-example # habilitar el auto shutdown az vm auto-shutdown -g azf-cloudshell-example-rg -n vm-cli-example --time 0200 # borrar recurso az group delete --name azf-cloudshell-example-rg","title":"EJEMPLO DE CREAR MV POR SCRIPT"},{"location":"azure/#azure-app-service","text":"Azure App Service es un servicio de alojamiento web totalmente administrado que permite crear aplicaciones web, back-ends m\u00f3viles y API RESTful. Desde sitios web peque\u00f1os hasta aplicaciones web con una escala global, existen opciones de precios y rendimiento que se adaptan a todas las necesidades. Azure App Service permite desarrollar software en el lenguaje preferido, ya sea. NET, .NET Core, Java, Ruby, Node.js, PHP o Python. Las aplicaciones se ejecutan y escalan f\u00e1cilmente en los entornos basados tanto en Windows como en Linux. Azure App Service no solo agrega a la aplicaci\u00f3n la funcionalidad de Microsoft Azure, como la seguridad, el equilibrio de carga, el escalado autom\u00e1tico y la administraci\u00f3n automatizada. Tambi\u00e9n puede sacar partido de las funcionalidades de DevOps, por ejemplo, la implementaci\u00f3n continua desde Azure DevOps, GitHub, Docker Hub y otros or\u00edgenes, la administraci\u00f3n de paquetes, entornos de ensayo, dominio personalizado y certificados TLS/SSL. Con Azure App Service se paga por los recursos de proceso de Azure que se utilizan. Los recursos de proceso que usa se determinan mediante el plan de App Service en el que ejecuta las aplicaciones, y determina el costo del servicio.","title":"AZURE APP SERVICE"},{"location":"azure/#crear-una-web-app","text":"Home - Crear recurso - Web App Configuraci\u00f3n inicial: A Crear una nueva Web App (elegir del men\u00fa destacados seg\u00fan indica el video). Completed on 6 noviembre, 2020 11:55 pm B Elegir la suscripci\u00f3n, crear un resource group desde el mismo asistente, y completar un nombre: por ejemplo \"azf-webapp-codexxx\" donde xxx son 3 n\u00fameros aleatorios, tanto para la web app como para el resource group. No olvides este RG que lo utilizaremos m\u00e1s adelante. Completed on 6 noviembre, 2020 11:55 pm C Seleccionar la forma de publicaci\u00f3n [Publish] \"Code\". Elegir el Runtime stack .NET Core 3.1 (LTS) y la plataforma Windows. Completed on 6 noviembre, 2020 11:55 pm D Elegir la regi\u00f3n que Azure propone. Se puede seguir el del ejemplo: Central US. Completed on 6 noviembre, 2020 11:55 pm E Crear un Windows Plan con el nombre: \"azf-webapp-code-plan\" y el tama\u00f1o Standard S1 (no te preocupes por el costo, en breve lo eliminaremos). F En las opciones de monitoreo, NO habilitaremos Application Insights. Completed on 6 noviembre, 2020 11:56 pm G No completaremos nada en los tags. H Revisaremos y crearemos la Web App. I Comprobar que desde la URL en la p\u00e1gina de Overview que la web app funciona correctamente. Veremos una URL donde tenemos nuestra web app","title":"Crear una Web App"},{"location":"azure/#services-y-plans","text":"El plan es el hardware del que voy a disponer. El service es la app o conjunto de apps dentro del plan. En App Service, cada aplicaci\u00f3n se ejecuta en un Plan de App Service. En forma b\u00e1sica, un App Service Plan define un conjunto de recursos de proceso para que una aplicaci\u00f3n web se ejecute. Estos recursos de proceso son an\u00e1logos a la granja de servidores de un hospedaje web convencional. Cuando se crea un plan de App Service en una regi\u00f3n determinada (por ejemplo, Oeste de Europa), se crea un conjunto de recursos de proceso para ese plan en dicha regi\u00f3n. Todas las aplicaciones que coloque en este plan de App Service se ejecutan en estos recursos de proceso seg\u00fan lo definido por el plan de App Service. Cada plan de App Service define: Regi\u00f3n (oeste de EE. UU., este de EE. UU., etc.). N\u00famero de instancias posibles de VM (si, por detr\u00e1s hay VMs). Tama\u00f1o de las instancias de VM (peque\u00f1o, mediano, grande). Plan de tarifa (Gratis, Compartido, B\u00e1sico, Est\u00e1ndar, Premium, PremiumV2 y Aislado). Si bien inicialmente puede resultar algo confuso, cuando en Azure veamos el t\u00e9rmino \u201cApp Service\u201d nos referimos a las Web Apps, API Apps & Mobile Apps. Cuando veamos el t\u00e9rmino \u201cApp Service Plans\u201d nos referimos expl\u00edcitamente a los planes. La relaci\u00f3n entre un App Service Plan un un App Service es una relaci\u00f3n 1:muchos :-). Una App Service Plan puede contener muchos App Services, mientras que un App Service s\u00f3lo puede estar dentro de un App Service Plan. En todos los casos, cuando contratamos un App Service Plan \u201cBasic\u201d, \u201cStandard\u201d, \u201cPremium\u201d e \u201cIsolado\u201d, s\u00f3lo pagamos por el tama\u00f1o del plan que contratemos (scale up) y la cantidad de instancias con las que escalemos (scale out, predeterminadamente configurada en 1): Aunque tengamos muchos App Services (Web Apps, API Apps & Mobile Apps) dentro del mismo plan, s\u00f3lo pagaremos por el tama\u00f1o y cantidad de instancias del App Service Plan, y no por cada una de las Apps que tenga dentro. Por supuesto, como el App Service Plan determina un hardware asociado, tendremos un l\u00edmite. Si ponemos muchos sitios web dentro que consumen muchos recursos y/o tienen demasiado tr\u00e1fico, nuestro App Service Plan comenzar\u00e1 a arrojar errores por no disponibilidad de servicio. Este punto no es menor, y debe ser planeado en el dise\u00f1o de la soluci\u00f3n completa de Azure.","title":"Services y Plans"},{"location":"azure/#crear-una-web-app-container","text":"la creaci\u00f3n de una Web App for Containers. Esto significa, que nuestra Web App estar\u00e1 en un contenedor de ejemplo en Microsoft Azure. Recordemos que Azure App Service permite generar diversas aplicaciones: Web App. API App. Mobile App. En escencia, son similares, si bien encontraremos en cada una de ellas particularidades propias. La m\u00e1s simple es una Web App, y es justamente en este ejercicio la que generaremos. A diferencia del anterior donde tambi\u00e9n generamos una Web App, esta vez lo haremos utilizando Docker. Configuraci\u00f3n inicial: A Crear un nuevo recurso de tipo \"Web App\" que esta en la lista de populares. B Crear un grupo de recursos seg\u00fan el gusto que tengas. Si elijes un nuevo grupo de recursos (diferente al ejercicio de creaci\u00f3n de Web App Simple) recuerda BORRAR este nuevo grupo de recursos al finalizar el ejercicio. En caso que sea el mismo, puedes dejarlo dado que m\u00e1s adelante borraremos todo. C Elegiremos el tipo de publicaci\u00f3n \"Docker Container\" y el sistema operativo \"Linux\". D Generaremos un nuevo App Service Plan y un tama\u00f1o de tipo \"S1\". E En la configuarci\u00f3n de \"Docker\" elegiremos \"Single Container\". F Con respecto al origen de la instancia elegiremos \"Quickstart\". La opci\u00f3n \"sample\" ser\u00e1 \"NGINX\". G No debemos habilitar nada de monitoreo, dado que no est\u00e1 soportado con contenedores. H Revisar y crear el recurso. I Comprobar por la URL del Web App que todo est\u00e1 funcionando como esperamos.","title":"Crear una Web App Container"},{"location":"azure/#crear-api-app","text":"Pasos: A Ingresar al Resource Group que hemos generado en la \"Creaci\u00f3n de una Web App Simple\". Deber\u00eda tener el formato \"azf-webapp-codexxx\" con 3 n\u00fameros elegidos por ti. B Creamos un nuevo recurso y buscamos en la barra de b\u00fasqueda \"API App\". C Ingresaremos un nombre para la app, respetando el formato \"azf-apiapp-codexxx\" ingresando los 3 mismos n\u00fameros aleatorios puestos para el resource group y la web app anterior. D El resource group ya deber\u00eda estar seleccionado (es el que ya existe). E En la opci\u00f3n de \"App Service Plan\" debemos elegir el que ya existe porque lo generamos antes. Su nombre deber\u00eda ser \"azf-webapp-code-plan\". F Le damos un clic en \"Create\". G Una vez que se genere, vamos a comprobar que el sitio funciona bien. H Comprobar que en el listado de Apps en el \"App Service Plan\" ahora tenemos m\u00e1s items. En el Deployment center o en el centro de despliegue podemos configurar archivos o repositorios de donde coger nuestro c\u00f3digo fuente para la aplicaci\u00f3n web. https://docs.microsoft.com/es-mx/learn/modules/create-publish-webapp-app-service-vs-code/3-exercise-create-web-application-vs-code?pivots=pythonflask","title":"Crear API App"},{"location":"azure/#almacenamiento-azure","text":"La plataforma de Azure Storage es la soluci\u00f3n de almacenamiento en la nube de Microsoft para los escenarios modernos de almacenamiento de datos. Los servicios principales de almacenamiento ofrecen un almac\u00e9n de objetos escalable de forma masiva para objetos de datos, un almacenamiento en disco para m\u00e1quinas virtuales (VM) de Azure, un servicio de sistema de archivos para la nube, un almac\u00e9n de mensajes para mensajer\u00eda confiable y un almac\u00e9n NoSQL. Los servicios principales y considerados primitivos de Azure Storage son: Blobs de Azure [Azure Blobs]: con opciones de blobs en bloques, anexos y p\u00e1ginas. Archivos de Azure [Azure Files]: recursos compartidos para uso local y en la nube. Colas de Azure [Azure Queues]: almac\u00e9n de mensajer\u00eda simple. Tablas de Azure [Azure Tables]: almac\u00e9n NoSQL sin esquema para datos estructurados. Adem\u00e1s, consideraremos el servicio de Azure Disks, un servicio totalmente administrado para los discos de nuestras m\u00e1quinas virtuales: Discos de Azure [Azure Disks]: conocidos como \u201cdiscos administrados\u201d.","title":"ALMACENAMIENTO AZURE"},{"location":"azure/#azure-disks","text":"Los discos administrados de Azure son vol\u00famenes de almacenamiento de nivel de bloque administrados por Azure y utilizados con Azure Virtual Machines. Los discos administrados son como un disco f\u00edsico en un servidor local, pero virtualizados. Con los discos administrados, todo lo que deber\u00e1s hacer es especificar el tama\u00f1o del disco, el tipo de disco y aprovisionar el disco. Una vez que aprovisiona el disco, Azure se encarga del resto. Los tipos de discos disponibles son ultra, unidades de estado s\u00f3lido (SSD) premium, SSD est\u00e1ndar y unidades de disco duro est\u00e1ndar (HDD).","title":"Azure disks"},{"location":"azure/#blob-storage","text":"Azure Blob Storage es la soluci\u00f3n de almacenamiento de objetos de Microsoft para la nube. Blob Storage est\u00e1 optimizado para el almacenamiento de cantidades masivas de datos no estructurados. En la primera lecci\u00f3n de esta secci\u00f3n, ya hemos aprendido sobre los diversos tipos de datos, entre ellos los no estructurados. Blob Storage est\u00e1 dise\u00f1ado para: Servicio de im\u00e1genes o documentos directamente a un explorador. Almacenamiento de archivos para acceso distribuido. Streaming de audio y v\u00eddeo. Escribir en archivos de registro. Almacenamiento de datos para copia de seguridad y restauraci\u00f3n, recuperaci\u00f3n ante desastres y archivado. Almacenamiento de datos para el an\u00e1lisis en local o en un servicio hospedado de Azure.","title":"Blob storage"},{"location":"azure/#azure-files","text":"Archivos de Azure (Azure Files) ofrece recursos compartidos de archivos en la nube totalmente administrados, a los que se puede acceder mediante el protocolo SMB (Bloque de mensajes del servidor) est\u00e1ndar. Los recursos compartidos de Azure Files se pueden montar simult\u00e1neamente en implementaciones de Windows, Linux y macOS en la nube o locales. Adem\u00e1s, los recursos compartidos de archivos de Azure Files se pueden almacenar en la cach\u00e9 de los servidores de Windows Server con Azure File Sync, lo que permite un acceso r\u00e1pido all\u00ed donde se utilizan los datos. Se crea un file share dentro de mi cuenta de storage azure. Se sube archivos y nos podemos conectar al file share a trav\u00e9s de un script que te da en las opciones de azure. Ahora ya tendemos el recursos de red la nueva unidad compartida y se actualiza todo. Tambien podemos hacer un azure file sync que es como un server en la nube que se actualiza en mi ordenador, azure y nube. Nos tendremos que descargar el file sync de azure despues de crear un server y un group sync y funcionar\u00e1 como lo anterior.","title":"Azure files"},{"location":"azure/#colas-de-azure","text":"Azure Queue Storage es un servicio para almacenar grandes cantidades de mensajes, a los que se puede acceder desde cualquier lugar del mundo a trav\u00e9s de llamadas autenticadas mediante HTTP o HTTPS. Un mensaje de la cola puede llegar a tener hasta 64 KB. Una cola puede contener millones de mensajes, hasta el l\u00edmite de capacidad total de una cuenta de almacenamiento. Las colas se utilizan normalmente para crear un trabajo pendiente del trabajo que se va a procesar de forma asincr\u00f3nica.","title":"Colas de azure"},{"location":"azure/#azure-storage-explorer","text":"Azure Storage Explorer es una herramienta gratuita para administrar f\u00e1cilmente sus recursos de almacenamiento en la nube de Azure en cualquier parte, desde Windows, macOS o Linux. Permite cargar, descargar y administrar blobs, archivos, colas y tablas de Azure, as\u00ed como entidades de Azure Cosmos DB y Azure Data Lake Storage. Permite acceder f\u00e1cilmente a los discos de las m\u00e1quinas virtuales y trabajar con Azure Resource Manager o con cuentas de almacenamiento cl\u00e1sicas. Asimismo, administrar y configurar reglas de uso compartido de recursos entre or\u00edgenes.","title":"Azure Storage Explorer"},{"location":"azure/#azure-functions","text":"Azure Functions permite ejecutar peque\u00f1os fragmentos de c\u00f3digo (denominados \u201cfunciones\u201d) sin preocuparse por el resto de la infraestructura de la aplicaci\u00f3n. Es ideal para tareas espec\u00edficas, dado que simplifica la necesidad de generar c\u00f3digo reduci\u00e9ndolo s\u00f3lo a la parte \u201cl\u00f3gica\u201d que necesitamos. Imaginemos que necesitamos generar una tarea donde, cada vez que se escribe un nuevo archivo en un Storage Account de Azure, se dispare una funci\u00f3n que guarde en una base de datos registro de dicho archivo. Si pensamos en hacer esto desde cero, probablemente nos requiera: Generar un proyecto con un IDE y un Framework / Lenguaje espec\u00edfico. Construir un servicio y alojarlo en alg\u00fan lugar. Resolver otros aspectos propios del proyecto, donde alojarlo, c\u00f3mo hacer que se dispare cuando se graba un archivo nuevo en el Storage Account, etc. Los desencadenadores son lo que provocan que una funci\u00f3n se ejecute. Un desencadenador define c\u00f3mo se invoca una funci\u00f3n y cada funci\u00f3n debe tener exactamente un desencadenador. Los desencadenadores tienen datos asociados, que a menudo son la carga de la funci\u00f3n. El enlace a una funci\u00f3n es una manera de conectar otro recurso a la funci\u00f3n mediante declaraci\u00f3n. Los enlaces pueden estar conectados como enlaces de entrada, enlaces de salida o ambos. Los datos de los enlaces se proporcionan a la funci\u00f3n como par\u00e1metros.","title":"AZURE FUNCTIONS"},{"location":"azure/#azure-bases-de-datos","text":"Microsoft SQL es un motor de base de datos utilizado en forma global y conocida por casi todos. Vamos a conocer las opciones que tenemos de Microsoft SQL en Azure, ya sea como IaaS, PaaS e inclusive Serverless. SQL Server en Azure Virtual Machines nos permite usar versiones completas de SQL Server en la nube sin tener que administrar todo el hardware local. SQL Server en Azure Virtual Machines tambi\u00e9n simplifica los costos de licencia cuando se paga por uso. La galer\u00eda de im\u00e1genes de m\u00e1quina virtual le permite crear una m\u00e1quina virtual con SQL Server con la versi\u00f3n, la edici\u00f3n y el sistema operativo correctos. Esto hace que las m\u00e1quinas virtuales sean una buena opci\u00f3n para muchas cargas de trabajo de SQL Server diferentes. El recurso M\u00e1quinas virtuales SQL es un servicio de administraci\u00f3n independiente de la m\u00e1quina virtual y, de hecho, aparece como un m\u00f3dulo distinto. Cuando Azure detecta que un motor SQL Server est\u00e1 instalado dentro de una m\u00e1quina virtual, habilita el punto de administraci\u00f3n con opciones espec\u00edficas que el administrador de VMs deber\u00e1 comenzar a gestionar. Azure SQL Database es un motor de base de datos de tipo plataforma como servicio (PaaS) totalmente administrado por Microsoft, que se encarga de la mayor\u00eda de las funciones de administraci\u00f3n de bases de datos. \u00bfQu\u00e9 tipo de funciones de administraci\u00f3n est\u00e1n a cargo de Microsoft? Por ejemplo: actualizar el motor, aplicar revisiones, crear copias de seguridad, supervisar sin intervenci\u00f3n del usuario. Azure SQL Database se ejecuta siempre en la \u00faltima versi\u00f3n estable del motor de base de datos de SQL Server y en un sistema operativo revisado con el 99,99 % de disponibilidad. Las capacidades de PaaS que est\u00e1n integradas en Azure SQL Database permiten centrarse en las actividades de administraci\u00f3n y optimizaci\u00f3n de bases de datos espec\u00edficas del dominio que son cr\u00edticas para el negocio, y no en mantener la infraestructura. Se puede descargar el software de SSMS y conectar desde el pc a azure y gestionar las bbdd. Se pueden crear: Single databases(siempre ha de haber un server database) Elastic Pool databses(recursos compartidos y dentro varias ddbb) Managed instances(se crea en la nube las ddbb) Managed instances: Es un servicio de base de datos en la nube inteligente y escalable que combina la mayor compatibilidad con el motor de base de datos de SQL Server en una plataforma como servicio totalmente administrada por Microsoft. Tiene casi un 100 % de compatibilidad con el motor de base de datos m\u00e1s reciente de SQL Server (Enterprise Edition). Url para migrar bbdd a azure","title":"AZURE BASES DE DATOS"},{"location":"azure/#cosmos-db","text":"Azure Cosmos DB es un servicio de base de datos con varios modelos distribuido de forma global de Microsoft. En esta lecci\u00f3n vamos a repasar los conceptos de las bases de datos no-sql vs las relacionales, para poder ir entendiendo un poco m\u00e1s de qu\u00e9 se trata este servicio de base de datos de Microsoft. Azure Cosmos DB es un servicio de base de datos multimodelo distribuido y con escalado horizontal. Al ser multimodelo, admite de forma nativa modelos de datos de documentos, pares clave-valor, grafos y en columnas. Con respecto a la administraci\u00f3n e interfaz de comunicaci\u00f3n, Azure Cosmos DB permite acceder a sus datos con diferentes APIs: como SQL (documentos), MongoDB (documentos), Azure Table Storage (clave-valor), Gremlin (grafos) y Cassandra (en columnas). A nivel de funcionalidad, Azure Cosmos DB indexa datos autom\u00e1ticamente sin que haya que ocuparse de la administraci\u00f3n de esquemas ni de \u00edndices.","title":"COSMOS DB"},{"location":"azure/#creacion","text":"En un resource group a\u00f1adimos un cosmosDB. Para crear una cuenta de Azure Cosmos DB gratuita por 30 dias link Una vez creado el cosmosDB vamos a Data Explorer y creamos una db y despues un container dentro de esta db. Por ejemplo creamos un container con nombre personas y el key es /dni. Dentro de personas vamos creando nuevos items: { \"dni\": \"46410800C\", \"nombre\": \"Miguel\", \"apellidos\": \"Amor\u00f3s Moret\" } Tambien podemos hacer query: SELECT c.nombre, c.apellidos FROM c where c.dni = \"46410800C\"","title":"Creaci\u00f3n"},{"location":"azure/#balanceadores-azure","text":"En Microsoft Azure tenemos diversas opciones de balanceadores, tambi\u00e9n conocidos como equilibradores de carga. El t\u00e9rmino equilibrio de carga hace referencia a la distribuci\u00f3n de cargas de trabajo entre varios recursos de proceso. El equilibrio de carga busca optimizar el uso de recursos, maximizar el rendimiento, minimizar el tiempo de respuesta y evitar la sobrecarga de un solo recurso. Tambi\u00e9n puede mejorar la disponibilidad al compartir una carga de trabajo entre recursos de proceso redundantes. Global frente a regional + Los servicios de equilibrio de carga globales distribuyen el tr\u00e1fico en servidores de back-end regionales, nubes o servicios locales h\u00edbridos. Estos servicios enrutan el tr\u00e1fico del usuario final al servidor de back-end disponible m\u00e1s cercano. Tambi\u00e9n reaccionan a los cambios en la confiabilidad o el rendimiento del servicio, con el fin de maximizar la disponibilidad y el rendimiento. Puede pensar en ellos como sistemas que equilibran la carga entre los stamp, puntos de conexi\u00f3n o unidades de escalado de la aplicaci\u00f3n hospedados en diferentes regiones o zonas geogr\u00e1ficas. Los servicios de equilibrio de carga regionales distribuyen el tr\u00e1fico de las redes virtuales entre las m\u00e1quinas virtuales (VM) o puntos de conexi\u00f3n de servicio zonales y con redundancia de zona de una regi\u00f3n. Puede pensarlos como sistemas que equilibran la carga entre m\u00e1quinas virtuales, contenedores o cl\u00fasteres dentro de una regi\u00f3n en una red virtual. HTTP(S) frente a no HTTP(S) + Los servicios de equilibrio de cargas HTTP(S) son equilibradores de carga de capa 7 que solo aceptan el tr\u00e1fico HTTP(S). Est\u00e1n dise\u00f1ados para las aplicaciones web u otros puntos de conexi\u00f3n HTTP(S). Incluyen caracter\u00edsticas, como la descarga de SSL, el firewall de aplicaciones web, el equilibrio de carga basado en rutas de acceso y la afinidad de sesi\u00f3n. Los servicios de equilibrio de carga que no son HTTP/S pueden controlar el tr\u00e1fico que no es de HTTP(S) y se recomiendan para las cargas de trabajo que no son web. Servicios de Balanceo en Azure Load Balancer + Proporciona un servicio de equilibrio de carga de capa 4 con latencia baja y rendimiento alto (entrante y saliente) para todos los protocolos UDP y TCP. Se dise\u00f1\u00f3 para administrar millones de solicitudes por segundo, a la vez que garantiza que la soluci\u00f3n tiene una alta disponibilidad. Azure Load Balancer tiene redundancia de zona, lo que garantiza una alta disponibilidad en las instancias de Availability Zones. Traffic Manager + Es un equilibrador de carga de tr\u00e1fico basado en DNS que le permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Dado que Traffic Manager es un servicio de equilibrio de carga basado en DNS, solo equilibra la carga en el nivel del dominio. Por ese motivo, no puede conmutar por error tan r\u00e1pidamente como con Front Door, debido a los desaf\u00edos comunes relacionados con el almacenamiento en cach\u00e9 de DNS y a los sistemas que no respetan los TTL de DNS. Application Gateway + Proporciona un controlador de entrega de aplicaciones (ADC) como servicio, que ofrece diversas funcionalidades de equilibrio de carga de capa 7. \u00daselo para optimizar la productividad de las granjas de servidores web al traspasar la carga de la terminaci\u00f3n SSL con mayor actividad de la CPU a la puerta de enlace. Front Door + Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones.","title":"BALANCEADORES AZURE"},{"location":"azure/#ejercicio-balanceador","text":"Vamos a realizar todas las configuraciones necesarias para poder, luego, trabajar sobre un Azure Load Balancer. Estas configuraciones las consideramos pre-requisitos y contemplan: Ejecuci\u00f3n en un modelo de Infraestructura como C\u00f3digo de la creaci\u00f3n de m\u00e1quinas virtuales y otros componentes requeridos para el ejercicio. Son un total de 3 equipos virtuales que utilizan 1 VCore cada uno, todos utilizando un Availability Set (Conjunto de Disponibilidad) con 2 dominios de falla y 5 dominios de actualizaci\u00f3n. Instalaci\u00f3n y configuraci\u00f3n de Internet Information Services (IIS) con una p\u00e1gina web simple. Creaci\u00f3n y asignaci\u00f3n de un Network Security Group (NSG) \u00fanico para los 3 equipos virtuales. Creamos un template y hacemos deploy del siguiente codigo Nos conectamos remotamente al server1 e instalamos el rol IIS de server web. Modificamos el index.html y ponemos que saludamos desde server 1. Ahora en el resource group creamoe un network security group. Dentro creamos un inbound de http por el puerto 80. Ahora de cada network interface las asignamos a este creado y borramos la del template. Ahora si entramos a la Ip de la primera VM por el puerto 80 desde el navegador normal, vemos su index.html. Despues de cada interfaz de red, modificamos su ip configuration y le ponemos ip statica y ip publica desasociada. Despues creamos un recurso nuevo. un load balancer publico, basic y con una ip static. Despues entramos y creamos un backend en nuestra loadblancer con nuestras 3 virtual machines. Creamos un healthprobe para http. A\u00f1adimos un load rule. Ahora si ponemos en el navegador la ip del balanceador, nos ir\u00e1 responiendo dinamicamente, el servidor de respuesta.","title":"Ejercicio balanceador"},{"location":"azure/#azure-trafic-manager","text":"Azure Traffic Manager es un balanceador de tr\u00e1fico basado en DNS que permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Trabaja a nivel capa 7 de OSI, aunque s\u00f3lo a nivel DNS. Traffic Manager usa DNS para dirigir las solicitudes del cliente al punto de conexi\u00f3n de servicio m\u00e1s adecuado en funci\u00f3n de un m\u00e9todo de enrutamiento del tr\u00e1fico y el mantenimiento de los puntos de conexi\u00f3n. Un punto de conexi\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Es fundamental entender que Traffic Manager funciona a nivel de DNS. Traffic Manager usa DNS para dirigir a los clientes a puntos de conexi\u00f3n espec\u00edficos del servicio basados en las reglas del m\u00e9todo de enrutamiento de tr\u00e1fico. Los clientes se conectan directamenteal punto de conexi\u00f3n seleccionado. Traffic Manager no es un proxy ni una puerta de enlace. Traffic Manager no ve el tr\u00e1fico que circula entre el cliente y el servicio. EJERCICIO: Vamos a completar los pre-requisitos necesarios para poder llevar adelante el ejercicio de Azure Traffic Manager. Estos pre-requisitos son: 3 App Service Plans con sistema operativo Linux. 3 App Service (Web Apps) con runtime PHP 7.3. 1 VM con Windows creada en Estados Unidos (si no estas en USA) o en Brasil (si est\u00e1s en USA). Publicaci\u00f3n de la soluci\u00f3n PHP (index.php) en cada sitio web (Brasil, Estados Unidos y Asia). La deber\u00e1s descargar desde GitHub. # A Crear un Resource Group para Traffic Manager. Completed on 12 noviembre, 2020 8:04 pm B Crear Sitio Web en Brasil Sur con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). C Crear Sitio Web en Estados Unidos Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). D Crear Sitio Web en Asia Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). E Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Sur de Brasil. F Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Estados Unidos. G Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Asia. H Crear la M\u00e1quina Virtual con Windows en Estados Unidos \u00f3 Brasil, seg\u00fan corresponda a tu ubicaci\u00f3n. Abrimos el ftp de cada uno y copiamos el index.php del github. Ahora creamos un Traffic Manager profile en nuestro grupo de recurso. Creamos 3 endpoints en mi traffic manager y mapeado por geolocalizaci\u00f3n. Segun petici\u00f3n donde estemos nos contesta una u otra","title":"Azure Trafic Manager"},{"location":"azure/#azure-aplication-gateway","text":"Azure Application Gateway es un equilibrador de carga de tr\u00e1fico web que permite administrar el tr\u00e1fico a las aplicaciones web. Los equilibradores de carga tradicionales operan en la capa de transporte (OSI capa 4: TCP y UDP) y enrutan el tr\u00e1fico en funci\u00f3n de la direcci\u00f3n IP y puerto de origen a una direcci\u00f3n IP y puerto de destino. Application Gateway puede tomar decisiones de enrutamiento basadas en atributos adicionales de una solicitud HTTP, por ejemplo los encabezados de host o la ruta de acceso del URI. A Paso inicial: confirmar que complet\u00e9 todos los requisitos (sitios web pre-requisitos de la lecci\u00f3n de Traffic Manager). B Crear un Resource Group para Application Gateway. C Crear un Application Gateway (SKU Standard_v2, Zonas 1 2 y 3 del Este de Estados Unidos, y escalado manual con 2 instancias iniciales). D Crear la Virtual Network con las opciones predeterminadas. E Crear un Front-End con una direcci\u00f3n de Front-End p\u00fablica. F Crear un Back-End Pool con los 3 sitios web configurados. G Crear un Routing Rule con Listener Basico. H Crear un HTTP Settings seg\u00fan indicaciones del Video. I Corregir HTTP Settings para evitar error en el estado de salud de los Websites de App Service. J Probar acceso por IP del Front-End (p\u00fablica) para validar que podamos ingresar al sitio web. Hacer F5 para validar que se cambia la p\u00e1gina de inicio seg\u00fan zona.","title":"Azure Aplication Gateway"},{"location":"azure/#azure-front-door-afd","text":"Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Azure Front Door permite definir, administrar y supervisar el enrutamiento global para el tr\u00e1fico web mediante la optimizaci\u00f3n para obtener el mejor rendimiento y la conmutaci\u00f3n por error global r\u00e1pida para alta disponibilidad. Front Door funciona en la capa 7 o la capa HTTP/HTTPS, y usa el protocolo de difusi\u00f3n por proximidad con divisi\u00f3n TCP y la red global de Microsoft para mejorar la conectividad global. Por tanto, seg\u00fan la selecci\u00f3n del m\u00e9todo de enrutamiento en la configuraci\u00f3n, puede asegurarse de que Front Door enruta las solicitudes de cliente al back-end de aplicaci\u00f3n m\u00e1s r\u00e1pido y disponible. Un back-end de aplicaci\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Front Door proporciona una serie de m\u00e9todos de enrutamiento del tr\u00e1fico y opciones de seguimiento de estado del back-end para satisfacer las distintas necesidades de las aplicaciones y los modelos de conmutaci\u00f3n autom\u00e1tica por error. Al igual que Traffic Manager, Front Door es resistente a errores, incluidos los que afectan a una regi\u00f3n completa de Azure.","title":"Azure Front Door (AFD)"},{"location":"azure/#contenedores","text":"los contenedores como unidades de despliegue. Los contenedores ofrecen las ventajas del aislamiento, la portabilidad, la agilidad, la escalabilidad y el control a lo largo de todo el flujo de trabajo del ciclo de vida de la aplicaci\u00f3n. La ventaja m\u00e1s importante es el aislamiento del entorno que se proporciona entre el desarrollo y las operaciones. Azure Container Registry es un servicio privado administrado del Registro de Docker que usa Docker Registry 2.0, que es de c\u00f3digo abierto. Cree y mantenga los registros de Azure Container para almacenar y administrar las im\u00e1genes privadas del contenedor Docker y los artefactos relacionados. EJERCICIO APP SERVICE Azure Container Instances es un servicio de Microsoft Azure que permite en forma r\u00e1pida y sencilla ejecutar un contenedor en Azure, sin tener que administrar ninguna m\u00e1quina virtual y sin necesidad de adoptar un servicio de nivel superior. EJERCICIO Kubernetes es una plataforma de r\u00e1pida evoluci\u00f3n que administra aplicaciones basadas en contenedores y sus componentes de red y almacenamiento asociados. El foco est\u00e1 en las cargas de trabajo de la aplicaci\u00f3n, no en los componentes de infraestructura subyacente. Kubernetes proporciona un enfoque declarativo en las implementaciones, respaldado por un s\u00f3lido conjunto de API para las operaciones de administraci\u00f3n. Azure Kubernetes Service (AKS) proporciona un servicio de Kubernetes administrado que reduce la complejidad de las principales tareas de administraci\u00f3n e implementaci\u00f3n, incluida la coordinaci\u00f3n de actualizaciones. El plano de control de AKS es administrado por la plataforma de Azure, y solo paga por los nodos de AKS que ejecutan sus aplicaciones. AKS se ha dise\u00f1ado sobre el motor de c\u00f3digo abierto de Azure Kubernetes Service (aks-engine). Los nodos del plano de control proporcionan los servicios centrales de Kubernetes y la orquestaci\u00f3n de las cargas de trabajo de las aplicaciones. Los nodos ejecutan las cargas de trabajo de la aplicaci\u00f3n.","title":"CONTENEDORES"},{"location":"azure/#logic-apps","text":"PowerAutomate, antes conocido como Flow, es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. Vamos a conocer algunos detalles del servicio para luego compararlo con Logic App (el servicio en el que realmente queremos hacer doble clic). Es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. El principal usuario objetivo es el de negocio, \u201cCitizen Developer\u201d / Integrador, es decir aquel usuario que crea nuevas aplicaciones comerciales para el consumo de otros utilizando entornos de desarrollo y tiempo de ejecuci\u00f3n autorizados por la TI corporativa. Su foco es brindar una experiencia para integraciones simples con aplicaciones y servicios. Logic Apps es un servicio PaaS (Plataforma como Servicio) para automatizar flujos de trabajo sobre m\u00faltiples aplicaciones SaaS y servicios IaaS, simplificando la complejidad requerida para la integraci\u00f3n empresarial. En si mismo, extiendo las capacidades de Power Automate. El principal usuario objetivo son Desarrolladores y IT Pros, es decir usuarios de sistemas que tienen conocimiento mucho m\u00e1s avanzado que un \u201cCitizen Developer\u201d. Su foco es brindar una experiencia para integraciones complejas y avanzadas, comparativamente con Power Automate. EJERCICIO En este tutorial se muestra c\u00f3mo utilizar Azure Functions con Logic Apps y Cognitive Services en Azure para ejecutar el an\u00e1lisis de opiniones de entradas de Twitter. Una funci\u00f3n desencadenada por HTTP clasifica los tweets en verdes, amarillos o rojos, en funci\u00f3n de la puntuaci\u00f3n de la opini\u00f3n. Se env\u00eda un correo electr\u00f3nico cuando se detecta una opini\u00f3n deficiente.","title":"LOGIC APPS"},{"location":"azure/#ia-ml-y-dl","text":"El aprendizaje profundo (deep learning), es un subconjunto del aprendizaje autom\u00e1tico basado en redes neuronales artificiales que permiten a un equipo entrenarse a s\u00ed mismo. En este caso, el proceso de aprendizaje se llama profundo porque la estructura de redes neuronales artificiales se compone de varias capas de entrada, salida y ocultas. Cada capa contiene unidades que transforman los datos de entrada en informaci\u00f3n que la capa siguiente puede usar para realizar una tarea de predicci\u00f3n determinada. Gracias a esta estructura, un equipo puede aprender a trav\u00e9s de su propio procesamiento de datos. El aprendizaje autom\u00e1tico (machine learning) es un subconjunto de la inteligencia artificial que incluye t\u00e9cnicas (como el aprendizaje profundo) que permiten a los equipos mejorar en las tareas con la experiencia. En este caso, el proceso de aprendizaje se basa en los pasos siguientes: Alimente un algoritmo con datos proporcion\u00e1ndole m\u00e1s informaci\u00f3n (por ejemplo, realizando la extracci\u00f3n de caracter\u00edsticas). Utilice estos datos para entrenar un modelo. Pruebe e implemente el modelo. Consuma el modelo implementado para realizar una tarea de predicci\u00f3n automatizada concreta. La inteligencia artificial (IA artificial intelligence) es una t\u00e9cnica que permite a los equipos imitar la inteligencia humana. Incluye el aprendizaje autom\u00e1tico. Es importante conocer la relaci\u00f3n entre aprendizaje autom\u00e1tico, aprendizaje profundo e inteligencia artificial: El aprendizaje autom\u00e1tico es una forma de lograr inteligencia artificial, lo que significa que, mediante el uso de t\u00e9cnicas de aprendizaje autom\u00e1tico y aprendizaje profundo, se pueden crear sistemas inform\u00e1ticos y aplicaciones que puedan realizar tareas asociadas normalmente a la inteligencia humana, como la percepci\u00f3n visual, el reconocimiento de voz, la toma de decisiones y la traducci\u00f3n de un idioma a otro.","title":"IA, ML Y DL"},{"location":"azure/#azure-cognitive-services","text":"Azure Cognitive Services son servicios en la nube con API REST y SDK de biblioteca cliente que ayudan a los desarrolladores a compilar aplicaciones inteligentes cognitivas sin tener inteligencia artificial (IA) directa ni aptitudes o conocimientos sobre ciencia de datos. Azure Cognitive Services permiten a los desarrolladores agregar f\u00e1cilmente caracter\u00edsticas cognitivas en sus aplicaciones. El objetivo de Azure Cognitive Services es ayudar a los desarrolladores a crear aplicaciones que puedan ver, o\u00edr, hablar, comprender e incluso empezar a razonar. Cognitive Services proporciona funciones de aprendizaje autom\u00e1tico para solucionar problemas generales, como el an\u00e1lisis de texto para la opini\u00f3n emocional o el an\u00e1lisis de im\u00e1genes para reconocer objetos o caras. No es necesario tener conocimientos de aprendizaje autom\u00e1tico ni ciencia de datos para usar estos servicios, como tampoco conocer de programaci\u00f3n. Spatial Anchors es un servicio multiplataforma para desarrolladores que le permite crear experiencias de realidad mixta mediante objetos cuya ubicaci\u00f3n persiste en todos los dispositivos a lo largo del tiempo. Estas aplicaciones pueden admitir Microsoft HoloLens, dispositivos iOS compatibles con ARKit y dispositivos Android compatibles con ARCore. Azure Spatial Anchors permite a los desarrolladores trabajar con plataformas de realidad mixta para percibir el espacio, designar puntos precisos de inter\u00e9s y volver a recuperar esos puntos de inter\u00e9s desde los dispositivos compatibles. Estos puntos de inter\u00e9s precisos se conocen como delimitadores espaciales.","title":"AZURE COGNITIVE SERVICES"},{"location":"azure/#big-data","text":"Big Data (conocido tambi\u00e9n como Macrodatos) es un t\u00e9rmino que describe el gran volumen de datos, tanto estructurados como no estructurados, que inundan los negocios cada d\u00eda. Pero no es la cantidad de datos lo que es importante. Lo que importa con el Big Data es lo que las organizaciones hacen con los datos. Big Data se puede analizar para obtener ideas que conduzcan a mejores decisiones y movimientos de negocios estrat\u00e9gicos. Cuando hablamos de Big Data nos referimos a conjuntos de datos o combinaciones de conjuntos de datos cuyo tama\u00f1o (volumen), complejidad (variabilidad) y velocidad de crecimiento (velocidad) dificultan su captura, gesti\u00f3n, procesamiento o an\u00e1lisis mediante tecnolog\u00edas y herramientas convencionales, tales como bases de datos relacionales y estad\u00edsticas convencionales o paquetes de visualizaci\u00f3n, dentro del tiempo necesario para que sean \u00fatiles. Las Cargas de Trabajo Tradicionales (RDBMS) utilizan procesamiento de transacciones en l\u00ednea (OLTP) y procesamiento anal\u00edtico en l\u00ednea (OLAP). En los sistemas OLTP, los datos suelen ser relacionales con un esquema predefinido y un conjunto de restricciones para mantener la integridad referencial. A menudo, se pueden consolidar datos de varios or\u00edgenes de la organizaci\u00f3n en un almacenamiento de datos, utilizando un proceso ETL para mover y transformar los datos de origen. Una arquitectura de Big Data (Macrodatos) est\u00e1 dise\u00f1ada para controlar la ingesta, el procesamiento y el an\u00e1lisis de datos que son demasiado grandes o complejos para los sistemas de bases de datos tradicionales. Los datos se pueden procesar por lotes o en tiempo real. Las soluciones de macrodatos suelen implicar una gran cantidad de datos no relacionales, tales como datos de clave y valor, documentos JSON o datos de series temporales. Los sistemas RDBMS tradicionales no suelen ser adecuados para almacenar este tipo de datos. El t\u00e9rmino NoSQL hace referencia a la familia de bases de datos que est\u00e1n dise\u00f1adas para contener datos no relacionales. El t\u00e9rmino no es totalmente exacto, porque muchos almacenes de datos no relacionales admiten consultas compatibles con SQL. El t\u00e9rmino NoSQL significa \u201cno solo SQL\u201d.","title":"BIG DATA"},{"location":"azure/#conceptos","text":"Un data warehouse es un repositorio unificado y estructurado para todos los datos que recogen los diversos sistemas de una empresa. El repositorio puede ser f\u00edsico o l\u00f3gico y hace hincapi\u00e9 en la captura de datos de diversas fuentes sobre todo para fines anal\u00edticos y de acceso. Un data lake es un repositorio de almacenamiento que contienen una gran cantidad de datos en bruto, estructurado, semi-estructurados & no estructurados, y que se mantienen all\u00ed hasta que sea necesario. A diferencia de un data warehouse jer\u00e1rquico que almacena datos en ficheros o carpetas, un data lake utiliza una arquitectura plana para almacenar los datos. Existen algunas diferencias clave entre Data Lake y Data Warehouse: Datos: Un data warehouse s\u00f3lo almacena datos que han sido modelados o estructurados, mientras que un Data Lake no hace acepci\u00f3n de datos. Lo almacena todo, estructurado, semiestructurado y no estructurado. Procesamiento: Antes de que una empresa pueda cargar datos en un data warehouse, primero debe darles forma y estructura, es decir, los datos deben ser modelados. Eso se llama schema-on-write. Con un data lake, s\u00f3lo se cargan los datos sin procesar, tal y como est\u00e1n, y cuando est\u00e9 listo para usar los datos, es cuando se le da forma y estructura. Eso se llama schema-on-read. Dos enfoques muy diferentes. Almacenamiento: Una de las principales caracter\u00edsticas de las tecnolog\u00edas de big data, como Hadoop, es que el coste de almacenamiento de datos es relativamente bajo en comparaci\u00f3n con el de un data warehouse. Hay dos razones principales para esto: en primer lugar, Hadoop es software de c\u00f3digo abierto, por lo que la concesi\u00f3n de licencias y el soporte de la comunidad es gratuito. Y segundo, Hadoop est\u00e1 dise\u00f1ado para ser instalado en hardware de bajo coste. Agilidad: Un almac\u00e9n de datos es un repositorio altamente estructurado, por definici\u00f3n. No es t\u00e9cnicamente dif\u00edcil cambiar la estructura, pero puede tomar mucho tiempo dado todos los procesos de negocio que est\u00e1n vinculados a ella. Un data lake, por otro lado, carece de la estructura de un data warehouse, lo que da a los desarrolladores y a los cient\u00edficos de datos la capacidad de configurar y reconfigurar f\u00e1cilmente y en tiempo real sus modelos, consultas y aplicaciones. Seguridad: La tecnolog\u00eda del data warehouse existe desde hace d\u00e9cadas, mientras que la tecnolog\u00eda de big data (la base de un Data Lake) es relativamente nueva. Por lo tanto, la capacidad de asegurar datos en un data warehouse es mucho m\u00e1s madura que asegurar datos en un data lake. Cabe se\u00f1alar, sin embargo, que se est\u00e1 realizando un importante esfuerzo en materia de seguridad en la actualidad en la industria de Big Data. Azure Data Factory es la plataforma que resuelve estos escenarios de datos. Se trata de un servicio de integraci\u00f3n de datos y ETL basado en la nube que le permite crear flujos de trabajo orientados a datos a fin de coordinar el movimiento y la transformaci\u00f3n de datos a escala. Con Azure Data Factory, puede crear y programar flujos de trabajo basados en datos (llamados canalizaciones) que pueden ingerir datos de distintos almacenes de datos. Puede crear procesos ETL complejos que transformen datos visualmente con flujos de datos o mediante servicios de proceso como Azure HDInsight Hadoop, Azure Databricks y Azure SQL Database. Azure Databricks es una plataforma de an\u00e1lisis basada en Apache Spark optimizada para la plataforma de servicios en la nube de Microsoft Azure. Dise\u00f1ada por los fundadores de Apache Spark, Databricks est\u00e1 integrado con Azure para proporcionar una configuraci\u00f3n con un solo clic, flujos de trabajo optimizados y un \u00e1rea de trabajo interactiva que permite la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas empresariales. Y lo mejor: no requiere administraci\u00f3n :-). Apache Spark es un framework de programaci\u00f3n para procesamiento de datos distribuidos dise\u00f1ado para ser r\u00e1pido y de prop\u00f3sito general. Como su propio nombre indica, ha sido desarrollada en el marco del proyecto Apache, lo que garantiza su licencia Open Source. Azure Synapse es un servicio de an\u00e1lisis que engloba el almacenamiento de datos empresariales y el an\u00e1lisis de macrodatos. Le ofrece la libertad de consultar los datos como prefiera, ya sea a petici\u00f3n sin servidor o con recursos aprovisionados, a escala. Azure Synapse re\u00fane estos dos mundos con una experiencia unificada para ingerir, preparar, administrar y servir datos para las necesidades inmediatas de inteligencia empresarial y aprendizaje autom\u00e1tico.","title":"Conceptos"},{"location":"azure/#mensajeria","text":"Azure ofrece varios servicios que le ayudan en la entrega de mensajes de evento en una soluci\u00f3n. Estos servicios son los siguientes: - Event Grid - Event Hubs - Service Bus Evento: es una notificaci\u00f3n ligera de una condici\u00f3n o un cambio de estado. El publicador del evento no tiene ninguna expectativa sobre c\u00f3mo se trata el evento. El consumidor del evento decide qu\u00e9 hacer con la notificaci\u00f3n. Los eventos pueden ser unidades discretas o parte de una serie. Mensaje: son datos sin procesar producidos por un servicio que se consumen o almacenan en otro lugar. El mensaje contiene los datos que desencaden\u00f3 la canalizaci\u00f3n del mensaje. El publicador del mensaje tiene una expectativa sobre la forma en que el consumidor trata el mensaje. Existe un contrato entre ambas partes. Por ejemplo, el publicador env\u00eda un mensaje con los datos sin procesar, espera que el consumidor cree un archivo a partir de esos datos y env\u00eda una respuesta cuando el trabajo finaliza. Microsoft Azure Service Bus es un agente de mensajes de integraci\u00f3n empresarial completamente administrado. Service Bus puede desacoplar aplicaciones y servicios. Service Bus ofrece una plataforma confiable y segura para la transferencia asincr\u00f3nica de datos y estado. Azure Event Hubs es una plataforma de streaming de macrodatos y un servicio de ingesta de eventos. Puede recibir y procesar millones de eventos por segundo. Los datos enviados a un centro de eventos se pueden transformar y almacenar con cualquier proveedor de an\u00e1lisis en tiempo real o adaptadores de procesamiento por lotes y almacenamiento. Azure IoT Hub es la puerta de enlace en la nube que conecta dispositivos IoT para recopilar los datos y dirigir las perspectivas y automatizaci\u00f3n empresariales. Adem\u00e1s, IoT Hub incluye caracter\u00edsticas que enriquecen la relaci\u00f3n entre los dispositivos y los sistemas back-end. Las capacidades de comunicaci\u00f3n bidireccional implican que al tiempo que se reciben datos de los dispositivos, tambi\u00e9n es posible devolver comandos y directivas a los dispositivos. Azure Event Grid permite crear f\u00e1cilmente aplicaciones con arquitecturas basadas en eventos. Event Grid est\u00e1 dise\u00f1ado para eventos, no datos. Cuando un Event Grid es informado de un evento, luego toma acciones determinadas.","title":"MENSAJERIA"},{"location":"azure/#azure-active-directory","text":"Azure Active Directory (Azure AD) es un servicio de administraci\u00f3n de identidades y acceso basado en la nube de Microsoft que ayuda a los empleados a iniciar sesi\u00f3n y acceder a recursos en: Recursos externos, como Microsoft 365, Azure Portal y miles de otras aplicaciones SaaS. Recursos internos, como las aplicaciones de la red corporativa y la intranet, junto con todas las aplicaciones en la nube desarrolladas por su propia organizaci\u00f3n. Los siguientes perfiles usan Azure AD: Administradores de TI. Si es administrador de TI, puede usar Azure AD para controlar el acceso a sus aplicaciones y a los recursos de est\u00e1s en funci\u00f3n de los requisitos de su empresa. Por ejemplo, puede usar Azure AD para requerir autenticaci\u00f3n multifactor al acceder a recursos importantes de la organizaci\u00f3n. Adem\u00e1s, puede usar Azure AD para automatizar el aprovisionamiento de usuarios entre la instancia existente de Windows Server AD y las aplicaciones en la nube, incluida Microsoft 365. Por \u00faltimo, Azure AD proporciona eficaces herramientas que ayudan a proteger autom\u00e1ticamente las identidades y credenciales de los usuarios y a cumplir los requisitos de gobernanza de acceso. Desarrolladores de aplicaciones. Como desarrollador de aplicaciones, puede usar Azure AD como enfoque basado en est\u00e1ndares para agregar el inicio de sesi\u00f3n \u00fanico (SSO) a cualquier aplicaci\u00f3n, lo que le permite trabajar con las credenciales existentes de un usuario. Azure AD tambi\u00e9n proporciona varias API que pueden ayudarle a crear experiencias de aplicaci\u00f3n personalizadas que usen los datos existentes de la organizaci\u00f3n. Suscriptores de Microsoft 365, Office 365, Azure o Dynamics CRM Online. Los suscriptores usan Azure AD. Cada inquilino de Microsoft 365, Office 365, Azure y Dynamics CRM Online es autom\u00e1ticamente un inquilino de Azure AD. Puede empezar de inmediato a administrar el acceso a las aplicaciones en la nube integradas. Entidades: Cloud Only: Esta opci\u00f3n de identidad es la que, por defecto, se habilita cuando creamos una cuenta en Office 365 o cuando creamos un directorio de Active Directory en Azure nuevo. En este caso, la contrase\u00f1a es gestionada por Azure Active Directory y no tiene relaci\u00f3n absoluta con nuestra infraestructura on-promises. De hecho, tampoco nuestro UPN de usuario tiene relaci\u00f3n con nuestra infraestructura on-promises, y de hecho (finalmente) debemos dar de alta los usuarios uno por uno. Sincronizada: Esta opci\u00f3n es la primera que, de alguna forma, relaciona nuestro servicio de directorio local (Active Directory Domain Services u otro LDAP) con el servicio de directorio en la nube (Azure Active Directory). Federada: La tercera y \u00faltima opci\u00f3n agrega la posibilidad de que el inicio de sesi\u00f3n se efectivice en nuestra infraestructura local. Azure AD Connect: El uso de esta caracter\u00edstica es gratis y est\u00e1 incluido en su suscripci\u00f3n de Azure. Es el componente que nos permite sincronizar identidades, o conectar nuestro servicio de directorio de Azure AD con otros LDAPs propios. Esta herramienta nos permitir\u00e1 sincronizar uno o muchos LDAPs con Azure AD, pero no un LDAP con muchos Azure ADs. Este item es importante de tener en cuenta cuando estemos planificando la arquitectura de soluci\u00f3n.","title":"AZURE ACTIVE DIRECTORY"},{"location":"azure/#devops","text":"DevOps es \u201cla union de personas, procesos, y productos para permitir la entrega continua de valor a sus usuarios finales\u201d. DevOps es una pr\u00e1ctica de ingenier\u00eda de software que tiene como objetivo unificar el desarrollo de software (Dev) y la operaci\u00f3n del software (Ops). La principal caracter\u00edstica del movimiento DevOps es defender en\u00e9rgicamente la automatizaci\u00f3n y el monitoreo en todos los pasos de la construcci\u00f3n del software, desde la integraci\u00f3n, las pruebas, la liberaci\u00f3n hasta la implementaci\u00f3n y la administraci\u00f3n de la infraestructura. DevOps apunta a ciclos de desarrollo m\u00e1s cortos, mayor frecuencia de implementaci\u00f3n, lanzamientos m\u00e1s confiables, en estrecha alineaci\u00f3n con los objetivos comerciales. Cuando hablamos de DevOps podemos hablar de 3 etapas, que no necesariamente se dan en cascada. Cuando trabajamos DevOps en las organizaciones debemos pensar en: Personas, Procesos y Productos. Cuando hablamos de personas, tenemos que entender que los equipos de trabajo est\u00e1n conformados por Personas: estas tienen emociones, que pueden manejar mejor o peor dependiendo de las situaciones personales y laborales que atraviesen. Entender esto y darle una mirada humana nos facilitar\u00e1 entender que DEBEMOS trabajar con las personas, desarrollar caracter\u00edsticas deseables que lleven a que sean \u201cEasy to work with\u201d, es decir una persona \u201ccon la que es f\u00e1cil y queremos trabajar\". Existen 4 grandes procesos, en alto nivel, que tenemos que tener en cuenta. Estos procesos, por supuesto, pueden darse c\u00edclicamente y no necesariamente en cascada: Planificar: crear un backlog, equipos interdisciplinarios, planificar testing, etc. Desarrollar + Probar: ya conozco que debo hacer, tengo mi backlog al menos en alto nivel, y podemos avanzar en la construcci\u00f3n. Liberar: lo que planifiqu\u00e9 y desarroll\u00e9 + prob\u00e9, lo debo liberar a ambientes. Desarrollo, Testing, Producci\u00f3n, etc. Monitorear + Aprender: un punto clave, es que el trabajo no termin\u00f3 con la liberaci\u00f3n. Reci\u00e9n comienza aqu\u00ed el aprendizaje basado en un monitoreo proactivo para volver a arrancar esta rueda: planificar, desarrollar + probar, y liberar, para volver a aprender. Desarrollo \u00e1gil: pr\u00e1cticas y marcos de trabajo \u00e1giles para el desarrollo. Integraci\u00f3n Continua: en todo momento, integrar nuestro c\u00f3digo con pruebas y chequeos de salud. Entrega / Despliegue Continuo: hacia ambientes, ya sea desarrollo, testing, pre producci\u00f3n o producci\u00f3n, acompa\u00f1ado de una robusta gesti\u00f3n de releases.","title":"DEVOPS"},{"location":"azure/#azure-devops","text":"Azure DevOps proporciona servicios de desarrollador para ayudar a los equipos a planificar el trabajo, colaborar en el desarrollo de c\u00f3digo y crear e implementar aplicaciones. Los desarrolladores pueden trabajar en la nube con Azure DevOps Services o localmente con Azure DevOps Server. Azure DevOps Server se llamaba anteriormente Visual Studio Team Foundation Server (TFS). Azure Repos proporciona repositorios de Git o Team Foundation Version Control (TFVC) para el control de c\u00f3digo fuente de su c\u00f3digo. Azure Pipelines proporciona servicios de creaci\u00f3n y lanzamiento para admitir la integraci\u00f3n y entrega continuas de sus aplicaciones. Azure Boards ofrece un conjunto de herramientas \u00e1giles para respaldar la planificaci\u00f3n y el seguimiento del trabajo, los defectos de c\u00f3digo y los problemas con los m\u00e9todos Kanban y Scrum. Azure Test Plans proporciona varias herramientas para probar sus aplicaciones, incluidas pruebas manuales / exploratorias y pruebas continuas. Azure Artifacts permite a los equipos compartir paquetes Maven, npm y NuGet de fuentes p\u00fablicas y privadas e integrar el uso compartido de paquetes en sus canalizaciones de CI / CD. Azure DevOps es gratuito hasta 5 usuarios por organizaci\u00f3n. Una organizaci\u00f3n de Azure DevOps permite tener, dentor, una colecci\u00f3n de proyectos con diversos repositorios. La creaci\u00f3n de repositorios tambi\u00e9n es gratuita, e inclusive la utilizaci\u00f3n de Pipelines hosteados por Microsoft para proyectos privados de hasta 1800 minutos por mes tambi\u00e9n es gratuito. En caso de proyectos p\u00fablicos, no hay l\u00edmite de minutos para Pipelines. Precios Vamos a devops starter para crear nuestro pipeline.","title":"AZURE DEVOPS"},{"location":"azure/#seguridad-azure","text":"Azure Security Center es un sistema unificado de administraci\u00f3n de seguridad de la infraestructura que fortalece la posici\u00f3n de seguridad de los centros de datos y proporciona una protecci\u00f3n contra amenazas avanzada de todas las cargas de trabajo h\u00edbridas que se encuentran en la nube, ya sea que est\u00e9n en Azure o no, as\u00ed como tambi\u00e9n en el entorno local. Proteger los recursos es un esfuerzo conjunto entre el proveedor de nube, Azure y usted, el cliente. Cuando migra a la nube, debe asegurarse de que las cargas de trabajo est\u00e9n seguras y, al mismo tiempo, cuando se migra a IaaS (infraestructura como servicio), el cliente tiene una responsabilidad mayor que cuando se encontraba en PaaS (plataforma como servicio) y SaaS (software como servicio). Azure Security Center le brinda las herramientas necesarias para fortalecer la red, proteger los servicios y garantizar que tiene la mejor posici\u00f3n de seguridad. Azure Security Center admite m\u00e1quinas virtuales y servidores en diferentes tipos de entornos h\u00edbridos: Solo Azure Azure y entorno local Azure y otras nubes Azure, otras nubes y entorno local Azure Information Protection (AIP) es una soluci\u00f3n basada en la nube que permite a las organizaciones clasificar y proteger documentos y correos electr\u00f3nicos mediante etiquetas. Se pueden aplicar etiquetas: Autom\u00e1ticamente por los administradores mediante reglas y condiciones. Manualmente por los usuarios. Mediante una combinaci\u00f3n en la que los administradores definen las recomendaciones que se muestran a los usuarios. Las etiquetas pueden clasificar y, opcionalmente, proteger el documento, con lo que puede hacer lo siguiente: Seguir y controlar el modo en que se usa el contenido. Analizar los flujos de datos para obtener una visi\u00f3n general de su negocio : detectar comportamientos de riesgo y tomar medidas correctivas. Hacer un seguimiento del acceso a los documentos e impedir la p\u00e9rdida de datos o su uso indebido. Use Azure Information Protection para aplicar la clasificaci\u00f3n a etiquetas en documentos y correos electr\u00f3nicos. Estas son algunas de las funciones de etiquetar contenido: Clasificaci\u00f3n que se puede detectar independientemente de d\u00f3nde se almacenen los datos o con qui\u00e9n se compartan. Distintivos visuales, como encabezados, pies de p\u00e1gina o marcas de agua. Metadatos que se agregan a los archivos y encabezados de correo electr\u00f3nico en texto no cifrado. Los metadatos de texto no cifrado garantizan que otros servicios puedan identificar la clasificaci\u00f3n y tomar las medidas adecuadas. Microsoft Azure Sentinel es una soluci\u00f3n de administraci\u00f3n de eventos de informaci\u00f3n de seguridad (SIEM) y respuesta automatizada de orquestaci\u00f3n de seguridad (SOAR) que es escalable y nativa de la nube. Azure Sentinel ofrece an\u00e1lisis de seguridad inteligente e inteligencia frente a amenazas en toda la empresa, de forma que proporciona una \u00fanica soluci\u00f3n para la detecci\u00f3n de alertas, la visibilidad de amenazas, la b\u00fasqueda proactiva y la respuesta a amenazas. Azure Sentinel permite obtener una vista general de toda la empresa, lo que suaviza la tensi\u00f3n de ataques cada vez m\u00e1s sofisticados, vol\u00famenes de alertas cada vez mayores y plazos de resoluci\u00f3n largos. Recopile datos a escala de nube de todos los usuarios, dispositivos, aplicaciones y de toda la infraestructura, tanto en el entorno local como en diversas nubes. Detecte amenazas que antes no se detectaban y reduzca los falsos positivos mediante el an\u00e1lisis y la inteligencia de amenazas sin precedentes de Microsoft. Investigue amenazas con inteligencia artificial y busque actividades sospechosas a escala, aprovechando el trabajo de ciberseguridad que ha llevado a cabo Microsoft durante d\u00e9cadas. Responda a los incidentes con rapidez con la orquestaci\u00f3n y la automatizaci\u00f3n de tareas comunes integradas.","title":"SEGURIDAD AZURE"},{"location":"backup/","text":"HACER BACKUPS tar -cvf backup$(date +%Y%m%d).tar /home Herramienta fwbackups: dnf install fwbackups Otras herramientas","title":"Backup"},{"location":"backup/#hacer-backups","text":"tar -cvf backup$(date +%Y%m%d).tar /home Herramienta fwbackups: dnf install fwbackups Otras herramientas","title":"HACER BACKUPS"},{"location":"bash_scripting/","text":"Comandos para Bash Scripting #! /bin/bash validar argumentos # indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0 validar help #help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi comparadores -lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi conficional if edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi bucle for count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0 bucle while cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0 bucle esac for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0 leer por entrada standard count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0 separar opciones con shift opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0 mirar si es directorio, regular file... #files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0 Mayusculas , copiar , buscar , ordenar tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null) Funciones function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"BashScripting"},{"location":"bash_scripting/#comandos-para-bash-scripting","text":"#! /bin/bash","title":"Comandos para Bash Scripting"},{"location":"bash_scripting/#validar-argumentos","text":"# indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0","title":"validar argumentos"},{"location":"bash_scripting/#validar-help","text":"#help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi","title":"validar help"},{"location":"bash_scripting/#comparadores","text":"-lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi","title":"comparadores"},{"location":"bash_scripting/#conficional-if","text":"edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi","title":"conficional if"},{"location":"bash_scripting/#bucle-for","text":"count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0","title":"bucle for"},{"location":"bash_scripting/#bucle-while","text":"cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0","title":"bucle while"},{"location":"bash_scripting/#bucle-esac","text":"for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0","title":"bucle esac"},{"location":"bash_scripting/#leer-por-entrada-standard","text":"count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0","title":"leer por entrada standard"},{"location":"bash_scripting/#separar-opciones-con-shift","text":"opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0","title":"separar opciones con shift"},{"location":"bash_scripting/#mirar-si-es-directorio-regular-file","text":"#files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0","title":"mirar si es directorio, regular file..."},{"location":"bash_scripting/#mayusculas-copiar-buscar-ordenar","text":"tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null)","title":"Mayusculas , copiar , buscar , ordenar"},{"location":"bash_scripting/#funciones","text":"function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"Funciones"},{"location":"bbdd/","text":"BASES DE DATOS POSTGRESQL Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); `` PHPPGADMIN phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin MARIADB Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql MONGODB Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"Base de datos"},{"location":"bbdd/#bases-de-datos","text":"","title":"BASES DE DATOS"},{"location":"bbdd/#postgresql","text":"Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); ``","title":"POSTGRESQL"},{"location":"bbdd/#phppgadmin","text":"phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin","title":"PHPPGADMIN"},{"location":"bbdd/#mariadb","text":"Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql","title":"MARIADB"},{"location":"bbdd/#mongodb","text":"Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"MONGODB"},{"location":"docker/","text":"DOCKER INSTALACI\u00d3N Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world COMANDOS Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container Redes en Docker: docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed Volumes en Docker: docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash Docker Compose: docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap Docker SWARM: docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode ARQUITECTURA Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real. DOCKER IMAGES Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images DOCKERFILE El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito CMD VS ENTRYPOINT CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"] CENTOS-PHP-SSL Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi NGINX-PHP Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh MULTI-STAGE-BUILD Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo. PRUEBA REAL La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php DOCKER CONTAINERS Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen LISTAR/MAPEO PUERTOS docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080. INICIAR/DETENE/PAUSAR Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id VARIABLES DE ENTORNO En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins MYSQL Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333 MONGODB Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones APACHE/NGINX/TOMCAT Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine POSTGRES Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows) JENKINS Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins. LIMITAR RECURSOS Ayuda con: docker --help | grep \"xxxx\" MEMORIA Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb CPU Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2. COPIA DE ARCHIVOS De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/. CONTENEDOR A IMAGEN Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!! SOBREESCRIBIR CMD Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos DESTRUIR CONTAINER Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm... DOCUMENT ROOT El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi. DOCKER VOLUMES Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes VOLUMES HOST Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 VOLUMES ANONYMOYS Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root VOLUMES NAMED VOLUMES Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root PRUEBA REAL Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev DOCKER NETWORK Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping CREAR REDES Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB VER REDES Para ver las redes creadas: docker network inspect netA AGREGAR/CONECTAR REDES Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known ELIMINAR REDES Para eliminar redes: docker network remove netA netB ASIGNAR IPs Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos RED HOST Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel RED NONE Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos EXPONER IPs CONCRETAS Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2 DOCKER COMPOSE Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes INSTALACI\u00d3N Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose EJEMPLO Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down VARIABLES ENTORNO Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env VOL\u00daMENES Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\" REDES Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms BUILD DOCKERFILE Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache CMD CAMBIADO Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net: LIMITAR RECURSOS Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\" POL\u00cdTICA DE REINICIO Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca. NOMBRE PROYECTO Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d DIFERENTE DOCKER-COMPOSE Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d OTROS COMANDOS docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap PROYECTOS DOCKER-COMPOSE MYSQL-WORDPRESS Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: DRUPAL-POSTGRESQL Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados: PRESTASHOP-MYSQL Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: JOOMLA-MYSQL Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: REACT-MONGODB-NODE.JS Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados: GUACAMOLE DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados: ZABBIX Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados: PHPMYADMIN-MYSL Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados: DOCKER SWARM Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio. COMANDOS B\u00c1SICOS docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2 INICIALIZAR Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377 DEPLOY SWARM Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago ESCALAR SERVICIOS Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp MODO GLOBAL Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] NODO DRAIN/PAUSE/ACTIVE DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName LABELS Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] DOCKER REGISTRY Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"Docker"},{"location":"docker/#docker","text":"","title":"DOCKER"},{"location":"docker/#instalacion","text":"Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world","title":"INSTALACI\u00d3N"},{"location":"docker/#comandos","text":"Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container","title":"COMANDOS"},{"location":"docker/#redes-en-docker","text":"docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed","title":"Redes en Docker:"},{"location":"docker/#volumes-en-docker","text":"docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash","title":"Volumes en Docker:"},{"location":"docker/#docker-compose","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"Docker Compose:"},{"location":"docker/#docker-swarm","text":"docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode","title":"Docker SWARM:"},{"location":"docker/#arquitectura","text":"Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real.","title":"ARQUITECTURA"},{"location":"docker/#docker-images","text":"Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images","title":"DOCKER IMAGES"},{"location":"docker/#dockerfile","text":"El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito","title":"DOCKERFILE"},{"location":"docker/#cmd-vs-entrypoint","text":"CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"]","title":"CMD VS ENTRYPOINT"},{"location":"docker/#centos-php-ssl","text":"Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi","title":"CENTOS-PHP-SSL"},{"location":"docker/#nginx-php","text":"Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh","title":"NGINX-PHP"},{"location":"docker/#multi-stage-build","text":"Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo.","title":"MULTI-STAGE-BUILD"},{"location":"docker/#prueba-real","text":"La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php","title":"PRUEBA REAL"},{"location":"docker/#docker-containers","text":"Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen","title":"DOCKER CONTAINERS"},{"location":"docker/#listarmapeo-puertos","text":"docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080.","title":"LISTAR/MAPEO PUERTOS"},{"location":"docker/#iniciardetenepausar","text":"Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id","title":"INICIAR/DETENE/PAUSAR"},{"location":"docker/#variables-de-entorno","text":"En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins","title":"VARIABLES DE ENTORNO"},{"location":"docker/#mysql","text":"Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333","title":"MYSQL"},{"location":"docker/#mongodb","text":"Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones","title":"MONGODB"},{"location":"docker/#apachenginxtomcat","text":"Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine","title":"APACHE/NGINX/TOMCAT"},{"location":"docker/#postgres","text":"Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows)","title":"POSTGRES"},{"location":"docker/#jenkins","text":"Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins.","title":"JENKINS"},{"location":"docker/#limitar-recursos","text":"Ayuda con: docker --help | grep \"xxxx\"","title":"LIMITAR RECURSOS"},{"location":"docker/#memoria","text":"Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb","title":"MEMORIA"},{"location":"docker/#cpu","text":"Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2.","title":"CPU"},{"location":"docker/#copia-de-archivos","text":"De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/.","title":"COPIA DE ARCHIVOS"},{"location":"docker/#contenedor-a-imagen","text":"Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!!","title":"CONTENEDOR A IMAGEN"},{"location":"docker/#sobreescribir-cmd","text":"Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos","title":"SOBREESCRIBIR CMD"},{"location":"docker/#destruir-container","text":"Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm...","title":"DESTRUIR CONTAINER"},{"location":"docker/#document-root","text":"El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi.","title":"DOCUMENT ROOT"},{"location":"docker/#docker-volumes","text":"Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes","title":"DOCKER VOLUMES"},{"location":"docker/#volumes-host","text":"Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7","title":"VOLUMES HOST"},{"location":"docker/#volumes-anonymoys","text":"Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES ANONYMOYS"},{"location":"docker/#volumes-named-volumes","text":"Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES NAMED VOLUMES"},{"location":"docker/#prueba-real_1","text":"Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev","title":"PRUEBA REAL"},{"location":"docker/#docker-network","text":"Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping","title":"DOCKER NETWORK"},{"location":"docker/#crear-redes","text":"Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB","title":"CREAR REDES"},{"location":"docker/#ver-redes","text":"Para ver las redes creadas: docker network inspect netA","title":"VER REDES"},{"location":"docker/#agregarconectar-redes","text":"Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known","title":"AGREGAR/CONECTAR REDES"},{"location":"docker/#eliminar-redes","text":"Para eliminar redes: docker network remove netA netB","title":"ELIMINAR REDES"},{"location":"docker/#asignar-ips","text":"Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos","title":"ASIGNAR IPs"},{"location":"docker/#red-host","text":"Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel","title":"RED HOST"},{"location":"docker/#red-none","text":"Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos","title":"RED NONE"},{"location":"docker/#exponer-ips-concretas","text":"Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2","title":"EXPONER IPs CONCRETAS"},{"location":"docker/#docker-compose_1","text":"Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes","title":"DOCKER COMPOSE"},{"location":"docker/#instalacion_1","text":"Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"INSTALACI\u00d3N"},{"location":"docker/#ejemplo","text":"Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down","title":"EJEMPLO"},{"location":"docker/#variables-entorno","text":"Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env","title":"VARIABLES ENTORNO"},{"location":"docker/#volumenes","text":"Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\"","title":"VOL\u00daMENES"},{"location":"docker/#redes","text":"Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms","title":"REDES"},{"location":"docker/#build-dockerfile","text":"Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache","title":"BUILD DOCKERFILE"},{"location":"docker/#cmd-cambiado","text":"Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net:","title":"CMD CAMBIADO"},{"location":"docker/#limitar-recursos_1","text":"Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\"","title":"LIMITAR RECURSOS"},{"location":"docker/#politica-de-reinicio","text":"Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca.","title":"POL\u00cdTICA DE REINICIO"},{"location":"docker/#nombre-proyecto","text":"Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d","title":"NOMBRE PROYECTO"},{"location":"docker/#diferente-docker-compose","text":"Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d","title":"DIFERENTE DOCKER-COMPOSE"},{"location":"docker/#otros-comandos","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"OTROS COMANDOS"},{"location":"docker/#proyectos-docker-compose","text":"","title":"PROYECTOS DOCKER-COMPOSE"},{"location":"docker/#mysql-wordpress","text":"Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"MYSQL-WORDPRESS"},{"location":"docker/#drupal-postgresql","text":"Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados:","title":"DRUPAL-POSTGRESQL"},{"location":"docker/#prestashop-mysql","text":"Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"PRESTASHOP-MYSQL"},{"location":"docker/#joomla-mysql","text":"Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"JOOMLA-MYSQL"},{"location":"docker/#react-mongodb-nodejs","text":"Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados:","title":"REACT-MONGODB-NODE.JS"},{"location":"docker/#guacamole","text":"DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados:","title":"GUACAMOLE"},{"location":"docker/#zabbix","text":"Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados:","title":"ZABBIX"},{"location":"docker/#phpmyadmin-mysl","text":"Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados:","title":"PHPMYADMIN-MYSL"},{"location":"docker/#docker-swarm_1","text":"Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio.","title":"DOCKER SWARM"},{"location":"docker/#comandos-basicos","text":"docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2","title":"COMANDOS B\u00c1SICOS"},{"location":"docker/#inicializar","text":"Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377","title":"INICIALIZAR"},{"location":"docker/#deploy-swarm","text":"Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago","title":"DEPLOY SWARM"},{"location":"docker/#escalar-servicios","text":"Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp","title":"ESCALAR SERVICIOS"},{"location":"docker/#modo-global","text":"Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"MODO GLOBAL"},{"location":"docker/#nodo-drainpauseactive","text":"DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName","title":"NODO DRAIN/PAUSE/ACTIVE"},{"location":"docker/#labels","text":"Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"LABELS"},{"location":"docker/#docker-registry","text":"Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"DOCKER REGISTRY"},{"location":"files/","text":"Archivos importantes de Linux Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos Destacados"},{"location":"files/#archivos-importantes-de-linux","text":"Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos importantes de Linux"},{"location":"gitlab/","text":"Comandos para GIT git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file Obtener claves para GIT ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io GitKraken Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken Git Tags/Releases Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0 Gitflow Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/ Features A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch Hotfix Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch Releases Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master. Emulador CMDER Descargar","title":"Gitlab"},{"location":"gitlab/#comandos-para-git","text":"git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file","title":"Comandos para GIT"},{"location":"gitlab/#obtener-claves-para-git","text":"ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com","title":"Obtener claves para GIT"},{"location":"gitlab/#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"gitlab/#gitkraken","text":"Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken","title":"GitKraken"},{"location":"gitlab/#git-tagsreleases","text":"Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0","title":"Git Tags/Releases"},{"location":"gitlab/#gitflow","text":"Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/","title":"Gitflow"},{"location":"gitlab/#features","text":"A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch","title":"Features"},{"location":"gitlab/#hotfix","text":"Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch","title":"Hotfix"},{"location":"gitlab/#releases","text":"Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master.","title":"Releases"},{"location":"gitlab/#emulador-cmder","text":"Descargar","title":"Emulador CMDER"},{"location":"jenkins/","text":"Jenkins Instalaci\u00f3n FEDORA Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins UBUNTU/DEBIAN Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins DOCKER En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net: NOTAS A TENER EN CUENTA Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini... PROYECTO CON PARAMETROS Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script. SSH Creacion SSH container Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins Credenciales Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully. Ejercicio mandar un job a maquina remota En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota. BASE DE DATOS JENKINS Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p Creacion bbdd simple MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27); Creaci\u00f3n Buckets en amazon Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key Dump de la bbdd [root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql DUMP AUTOMATIZADO Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea). ANSIBLE Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Playbooks Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG TAGS Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job. DB MYSQL Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada. NGINX SERVER Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro. SECURITY JENKINS Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles: MANAGE USERS Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas: MANAGE ROLES Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron. TRIPS AND TICKS Variables de entorno Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A Cambio URL Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/ CRON Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue. GATILLAR JOBS Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22 MAIL Configurar envio de notificaciones Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent Gmail como server de correo Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo: Email de error Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email. MAVEN Instalacion Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app Configuracion de un job Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar Registrar los resultados A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results: Archivar los jar A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla: GIT SERVER Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido. CAMBIO URL MAVEN/GIT/JENKINS Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job. JOB DSL Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl SEED JOB Ejemplo estructura: job('job_dsl_example') { } DESCRIPCION Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada PAR\u00c1METROS Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas. SCM La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso. TRIGGERS Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron. STEPS Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world MAILER Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones. JOB DE ANSIBLE EN DSL En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/ JOB DE MAVEN EN DSL Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } DSL en GIT Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } PIPELINES Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline JENKINSFILE Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente. MULTIPLE-STEPS pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } } POST-ACTIONS pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } } RETRY pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } } TIMEOUT pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } } VARIABLES ENV pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } } CREDENCIALES pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } } CI/CD BUILD Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } } TEST Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } PUSH A MAQUINA REMOTA AWS Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa CERTIFICADO SSL REGISTRY CON AUTENTICACION Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } } DEPLOY En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } } CI/CD Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"Jenkins"},{"location":"jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"jenkins/#instalacion","text":"","title":"Instalaci\u00f3n"},{"location":"jenkins/#fedora","text":"Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins","title":"FEDORA"},{"location":"jenkins/#ubuntudebian","text":"Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins","title":"UBUNTU/DEBIAN"},{"location":"jenkins/#docker","text":"En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net:","title":"DOCKER"},{"location":"jenkins/#notas-a-tener-en-cuenta","text":"Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini...","title":"NOTAS A TENER EN CUENTA"},{"location":"jenkins/#proyecto-con-parametros","text":"Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script.","title":"PROYECTO CON PARAMETROS"},{"location":"jenkins/#ssh","text":"","title":"SSH"},{"location":"jenkins/#creacion-ssh-container","text":"Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins","title":"Creacion SSH container"},{"location":"jenkins/#credenciales","text":"Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully.","title":"Credenciales"},{"location":"jenkins/#ejercicio-mandar-un-job-a-maquina-remota","text":"En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota.","title":"Ejercicio mandar un job a maquina remota"},{"location":"jenkins/#base-de-datos-jenkins","text":"Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p","title":"BASE DE DATOS JENKINS"},{"location":"jenkins/#creacion-bbdd-simple","text":"MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27);","title":"Creacion bbdd simple"},{"location":"jenkins/#creacion-buckets-en-amazon","text":"Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key","title":"Creaci\u00f3n Buckets en amazon"},{"location":"jenkins/#dump-de-la-bbdd","text":"[root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql","title":"Dump de la bbdd"},{"location":"jenkins/#dump-automatizado","text":"Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea).","title":"DUMP AUTOMATIZADO"},{"location":"jenkins/#ansible","text":"Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"ANSIBLE"},{"location":"jenkins/#playbooks","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG","title":"Playbooks"},{"location":"jenkins/#tags","text":"Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job.","title":"TAGS"},{"location":"jenkins/#db-mysql","text":"Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada.","title":"DB MYSQL"},{"location":"jenkins/#nginx-server","text":"Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro.","title":"NGINX SERVER"},{"location":"jenkins/#security-jenkins","text":"Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles:","title":"SECURITY JENKINS"},{"location":"jenkins/#manage-users","text":"Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas:","title":"MANAGE USERS"},{"location":"jenkins/#manage-roles","text":"Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron.","title":"MANAGE ROLES"},{"location":"jenkins/#trips-and-ticks","text":"","title":"TRIPS AND TICKS"},{"location":"jenkins/#variables-de-entorno","text":"Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A","title":"Variables de entorno"},{"location":"jenkins/#cambio-url","text":"Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/","title":"Cambio URL"},{"location":"jenkins/#cron","text":"Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue.","title":"CRON"},{"location":"jenkins/#gatillar-jobs","text":"Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22","title":"GATILLAR JOBS"},{"location":"jenkins/#mail","text":"","title":"MAIL"},{"location":"jenkins/#configurar-envio-de-notificaciones","text":"Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent","title":"Configurar envio de notificaciones"},{"location":"jenkins/#gmail-como-server-de-correo","text":"Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo:","title":"Gmail como server de correo"},{"location":"jenkins/#email-de-error","text":"Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email.","title":"Email de error"},{"location":"jenkins/#maven","text":"","title":"MAVEN"},{"location":"jenkins/#instalacion_1","text":"Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app","title":"Instalacion"},{"location":"jenkins/#configuracion-de-un-job","text":"Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar","title":"Configuracion de un job"},{"location":"jenkins/#registrar-los-resultados","text":"A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results:","title":"Registrar los resultados"},{"location":"jenkins/#archivar-los-jar","text":"A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla:","title":"Archivar los jar"},{"location":"jenkins/#git-server","text":"Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido.","title":"GIT SERVER"},{"location":"jenkins/#cambio-url-mavengitjenkins","text":"Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job.","title":"CAMBIO URL MAVEN/GIT/JENKINS"},{"location":"jenkins/#job-dsl","text":"Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl","title":"JOB DSL"},{"location":"jenkins/#seed-job","text":"Ejemplo estructura: job('job_dsl_example') { }","title":"SEED JOB"},{"location":"jenkins/#descripcion","text":"Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada","title":"DESCRIPCION"},{"location":"jenkins/#parametros","text":"Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas.","title":"PAR\u00c1METROS"},{"location":"jenkins/#scm","text":"La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso.","title":"SCM"},{"location":"jenkins/#triggers","text":"Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron.","title":"TRIGGERS"},{"location":"jenkins/#steps","text":"Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world","title":"STEPS"},{"location":"jenkins/#mailer","text":"Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones.","title":"MAILER"},{"location":"jenkins/#job-de-ansible-en-dsl","text":"En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/","title":"JOB DE ANSIBLE EN DSL"},{"location":"jenkins/#job-de-maven-en-dsl","text":"Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"JOB DE MAVEN EN DSL"},{"location":"jenkins/#dsl-en-git","text":"Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"DSL en GIT"},{"location":"jenkins/#pipelines","text":"Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline","title":"PIPELINES"},{"location":"jenkins/#jenkinsfile","text":"Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente.","title":"JENKINSFILE"},{"location":"jenkins/#multiple-steps","text":"pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } }","title":"MULTIPLE-STEPS"},{"location":"jenkins/#post-actions","text":"pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } }","title":"POST-ACTIONS"},{"location":"jenkins/#retry","text":"pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } }","title":"RETRY"},{"location":"jenkins/#timeout","text":"pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } }","title":"TIMEOUT"},{"location":"jenkins/#variables-env","text":"pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } }","title":"VARIABLES ENV"},{"location":"jenkins/#credenciales_1","text":"pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } }","title":"CREDENCIALES"},{"location":"jenkins/#cicd","text":"","title":"CI/CD"},{"location":"jenkins/#build","text":"Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"BUILD"},{"location":"jenkins/#test","text":"Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } }","title":"TEST"},{"location":"jenkins/#push-a-maquina-remota-aws","text":"Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa","title":"PUSH A MAQUINA REMOTA AWS"},{"location":"jenkins/#certificado-ssl-registry-con-autenticacion","text":"Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"CERTIFICADO SSL REGISTRY CON AUTENTICACION"},{"location":"jenkins/#deploy","text":"En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } }","title":"DEPLOY"},{"location":"jenkins/#cicd_1","text":"Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"CI/CD"},{"location":"kerberos/","text":"KERBEROS Se conecta el user a keberos y recibe un ticket que demuestra que es quien es. Cuando un user se conecta al server kerbertizado, presenta un ticket y el server lo comprueba sin tener que loguearse todo el rato. Los tickets expiran Demonios: /usr/sbin/krb5kdc y /usr/sbin/kadmind Puertos: 88(keberos), 464(keberos passwd) y 749(keberos admin) COMANDOS klist /etc/krb5.conf /var/kerberos/krb5kdc/kdc.conf /var/kerberos/krb5kdc/kadm5.acl kadmin.local kadmin.local -q \"addprinc -pq kmiguel miguel\" kinit user kdestroy kadmin kadmin -p miguel kdb5_util create -s -P masterkey KSERVER Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-server passwd procps vim nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- cp /opt/docker/krb5.conf /etc/krb5.conf cp /opt/docker/kdc.conf /var/kerberos/krb5kdc/kdc.conf cp /opt/docker/kadm5.acl /var/kerberos/krb5kdc/kadm5.acl #creamos bbdd kdb5_util create -s -P masterkey #creamos unos users principales que desde el client podremos coger ticket (pass kpere para pere) kadmin.local -q \"addprinc -pw kpere pere\" kadmin.local -q \"addprinc -pw kmarta marta\" kadmin.local -q \"addprinc -pw kpau pau\" kadmin.local -q \"addprinc -pw kjordi jordi\" kadmin.local -q \"addprinc -pw kanna anna\" kadmin.local -q \"addprinc -pw kmarta marta/admin\" kadmin.local -q \"addprinc -pw kjulia julia\" kadmin.local -q \"addprinc -pw kadmin admin\" kadmin.local -q \"addprinc -pw kmiguel miguel\" kadmin.local -q \"addprinc -pw kuser01 kuser01\" kadmin.local -q \"addprinc -pw kuser02 kuser02\" kadmin.local -q \"addprinc -pw kuser03 kuser03\" kadmin.local -q \"addprinc -pw kuser04 kuser04\" kadmin.local -q \"addprinc -pw kuser05 kuser05\" kadmin.local -q \"addprinc -pw kuser06 kuser06\" kadmin.local -q \"addprinc -randkey host/sshd.edt.org\" Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis /usr/sbin/krb5kdc && echo \"krb5kdc Ok\" /usr/sbin/kadmind -nofork && echo \"kadmind Ok\" krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG kdc.conf: [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] EDT.ORG = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } kadm5.acl: */admin@EDT.ORG * superuser@EDT.ORG * pau@EDT.ORG * /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org KHOST Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-workstation passwd vim procps nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- #fitxer de conf del client cp /opt/docker/krb5.conf /etc/krb5.conf startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" /bin/bash krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG","title":"KERBEROS"},{"location":"kerberos/#kerberos","text":"Se conecta el user a keberos y recibe un ticket que demuestra que es quien es. Cuando un user se conecta al server kerbertizado, presenta un ticket y el server lo comprueba sin tener que loguearse todo el rato. Los tickets expiran Demonios: /usr/sbin/krb5kdc y /usr/sbin/kadmind Puertos: 88(keberos), 464(keberos passwd) y 749(keberos admin)","title":"KERBEROS"},{"location":"kerberos/#comandos","text":"klist /etc/krb5.conf /var/kerberos/krb5kdc/kdc.conf /var/kerberos/krb5kdc/kadm5.acl kadmin.local kadmin.local -q \"addprinc -pq kmiguel miguel\" kinit user kdestroy kadmin kadmin -p miguel kdb5_util create -s -P masterkey","title":"COMANDOS"},{"location":"kerberos/#kserver","text":"Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-server passwd procps vim nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- cp /opt/docker/krb5.conf /etc/krb5.conf cp /opt/docker/kdc.conf /var/kerberos/krb5kdc/kdc.conf cp /opt/docker/kadm5.acl /var/kerberos/krb5kdc/kadm5.acl #creamos bbdd kdb5_util create -s -P masterkey #creamos unos users principales que desde el client podremos coger ticket (pass kpere para pere) kadmin.local -q \"addprinc -pw kpere pere\" kadmin.local -q \"addprinc -pw kmarta marta\" kadmin.local -q \"addprinc -pw kpau pau\" kadmin.local -q \"addprinc -pw kjordi jordi\" kadmin.local -q \"addprinc -pw kanna anna\" kadmin.local -q \"addprinc -pw kmarta marta/admin\" kadmin.local -q \"addprinc -pw kjulia julia\" kadmin.local -q \"addprinc -pw kadmin admin\" kadmin.local -q \"addprinc -pw kmiguel miguel\" kadmin.local -q \"addprinc -pw kuser01 kuser01\" kadmin.local -q \"addprinc -pw kuser02 kuser02\" kadmin.local -q \"addprinc -pw kuser03 kuser03\" kadmin.local -q \"addprinc -pw kuser04 kuser04\" kadmin.local -q \"addprinc -pw kuser05 kuser05\" kadmin.local -q \"addprinc -pw kuser06 kuser06\" kadmin.local -q \"addprinc -randkey host/sshd.edt.org\" Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis /usr/sbin/krb5kdc && echo \"krb5kdc Ok\" /usr/sbin/kadmind -nofork && echo \"kadmind Ok\" krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG kdc.conf: [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] EDT.ORG = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } kadm5.acl: */admin@EDT.ORG * superuser@EDT.ORG * pau@EDT.ORG * /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org","title":"KSERVER"},{"location":"kerberos/#khost","text":"Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-workstation passwd vim procps nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- #fitxer de conf del client cp /opt/docker/krb5.conf /etc/krb5.conf startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" /bin/bash krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG","title":"KHOST"},{"location":"kubernetes/","text":"KUBERNETES K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal ARQUITECTURA MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc. INSTALACI\u00d3N MINIKUBE/KUBECTL MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes PODS VS CONTENEDORES Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores. PODS CREAR POD Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso. LOGS PODS Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso. API-RESOURCES Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources ELIMINAR PODS Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all OBTENER YAML POD Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces. IP POD Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80 ENTRAR AL POD Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh CREAR POD YAML Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine 2+ CONTAINER POR POD Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2 LABELS Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas. PROBLEMAS PODs Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar. REPLICASETS Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet. CREAR REPLICASET Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s ELIMINAR/MODIFICAR En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s LOGS Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test OWNER REFERNCE Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 ADOPCI\u00d3N DE PODS PLANOS Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena. PROBLEMAS Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones. DEPLOYMENTS Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets CREAR DEPLOYMENT Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset ROLLING UPDATE Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge HISTORIAL DE DEPLOYMENTS Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> ROLL BACKS Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0 SERVICIOS Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio. CREAR SERVICIO Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80. INFO SERVICIO Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint. ENDPOINTS Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none> DNS Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888 SERVICIO CLUSTER-IP IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) SERVICIO NODE-PORT IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port. SERVICIO LOAD BALANCER Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP. GOLANG Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend. CREAR API REST GO DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png] CAMBIOS MENSAJE RESPUESTA MENSAJE 1 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png] MENSAJE 2 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png] DOCKERFILE GOLANG Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos. DEPLOYMENT GOLANG Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png] CONSUMO DEL SERVICIO Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ # FRONTED Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html MANIFIESTO FRONTED Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy NAMESPACES Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName CREAR NAMESPACE Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none> ASIGNAR NAMESPACES Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s BORRAR NAMESPACES Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns DEPLOY NAMESPACES Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m DNS NAMESPACES Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on CONTEXTOS NAMESPACES Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d LIMITAR RAM/CPU La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus. LIMITS/REQUEST Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique. RAM Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory. CPU Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu. QOS(Quality of Service) Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request. LIMITRANGE Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods VALORES POR DEFECTO Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi - VALORES POD Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M. LIMITES Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram RESOURCE QUOTA Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod. CREAR RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource. DEPLOY RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA. LIMITAR N\u00ba PODS EN NS Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota. PROBES Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP TIPOS PROBES Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse. CREAR LIVENESS PROBE Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness LIVENESS TCP Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName LIVENESS HTTP Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName READINESS PROBE Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod. VARIABLES Y CONFIGMAP CREAR VARIABLES Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1 VARIABLES REFERENCIADAS Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc. CONFIGMAP Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> MONTANDO VOLUMEN CONFIGMAP Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo. VOLUMEN-ENV CONFIGMAP Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user SECRETS Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen. CREAR Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode MANIFIESTOS Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test ENVSUBTS Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado. VOLUME SECRETS Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ # ENV SECRETS Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456 VOLUMES Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). EMPTYDIR Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / # HOSTPATH-PV En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none> PVC El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC. PVC-PV Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector. PVC-PODS De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql> CLOUD VOLUMES Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC. RECLAIM POLICY Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). USERS/GROUPS RBAC RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster. ROLES vs CLUSTERROLES En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo. ROLEBINDING vs CLUSTERROLEBINDING Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1. CREAR USERS & GROUPS Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" HABILITAR RBAC Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC SIMPLIFICAMOS CONTEXTO Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel CREAR ROLES Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user. ENLAZAR ROLE & USER Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\" CONFIG MAPS Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again. CREAR CLUSTEROLE Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m CREAR USER ADMIN Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m ROLES A GRUPOS Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m SERVICES ACCOUNT Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr SECRET SA Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc CREAR SA Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s RELACION POD-SA Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ REQUESTS A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones. REQUEST JWT Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante. SA DEPLOYMENT Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa ROLE SA Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1, INGRESS Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada. INGRESS CONTROLLER Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud. CREAR INGRESS CONTROLLER Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s IP INGRESS CONTROLLER Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602 APP INGRESS-CONTROLLER Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv EXPONER EL PUERTO AL EXTERIOR Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 2 APPS EN IC Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: AWS KUBERNETES Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: xxxx Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0 CREAR CLUSTER AWS EKSCTL docs install Creamos cluster master sin nodos: eksctl create cluster --name test-cluster --without-nodegroup --region eu-west-2 --zones eu-west-2a,eu-west-2b Vemos lo creado en el apartado EKS y CloudFormation : Eksctl lee de estos archivos para comunicarse: [isx46410800@miguel ~]$ cat .aws/credentials [default] aws_access_key_id = AKIA5RIFOUI3OMSWWHNM aws_secret_access_key = xxxxx [isx46410800@miguel ~]$ cat .aws/config [default] region = eu-west-2 Al crear el cluster nos crea un directorio ~/.kube/config Si eliminamos este directorio, como si no lo tuvieramos y nos queremos conectar a este cluster usamos la orden: aws eks --region eu-west-2 update-kubeconfig --name test-cluster Ahora si hacemos kubectl get svc y kubectl cluster-info vemos que estamos conectados y referenciados al cluster de AWS: [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7m11s [isx46410800@miguel ~]$ kubectl cluster-info Kubernetes master is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com CoreDNS is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ahora intentamos crear un POD pero vemos que no se acaba de crear porque no tenemos ningun nodo unido a nuestro CLUSTER: [isx46410800@miguel ~]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 0/1 Pending 0 10s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 15s default-scheduler no nodes available to schedule pods Ahora creamos nodos con eksctl con ami version kubernetes auto y asg access para ser escalable: eksctl create nodegroup --cluster test-cluster --region eu-west-2 --name test-workers --node-type t3.medium --node-ami auto --nodes 1 --nodes-min 1 --nodes-max 3 --asg-access Comprobamos que el pod de prueba est\u00e1 ahora running y asignado al nodo creado: [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 68s v1.17.11-eks-cfdc40 # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6m38s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Name: pod-test Namespace: default Priority: 0 Node: ip-192-168-38-128.eu-west-2.compute.internal/192.168.38.128 INGRESS AWS EKS Para exponerlo, crearemos un balanzador de carga, un ingress y un ingress controller. DOCS para crear el ingress controller nos dice que nuestro servicio(VPC) tiene que seguir una estructura de tag. Vemos los servicios VPC que se crearon automaticamente al crear el cluster y los nodos. Las subnets tambien tienen que seguir una estructura de tags. No obstante todos estos pasos al crearlos con EKSCTL ya vienen por defecto. IAM OIDC eksctl utils associate-iam-oidc-provider --region eu-west-2 --cluster test-cluster --approve Politica para crear recursos de balanceador de carga: [isx46410800@miguel ~]$ aws iam create-policy \\ > --policy-name ALBIngressControllerIAMPolicy \\ > --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json { \"Policy\": { \"PolicyName\": \"ALBIngressControllerIAMPolicy\", \"PolicyId\": \"ANPA5RIFOUI3IFJHOR5SB\", \"Arn\": \"arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy\", \"Path\": \"/\", \"DefaultVersionId\": \"v1\", \"AttachmentCount\": 0, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"CreateDate\": \"2020-10-21T17:06:33Z\", \"UpdateDate\": \"2020-10-21T17:06:33Z\" } } Creamos un service account para ingress con un clusterrole y un clusterrolebinding de ingress controller para balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml Creamos un service account para que nuestro ingress controller sea capaz de crear recursos en AWS: eksctl create iamserviceaccount \\ --region eu-west-2 \\ --name alb-ingress-controller \\ --namespace kube-system \\ --cluster test-cluster \\ --attach-policy-arn arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy \\ --override-existing-serviceaccounts \\ --approve La policy la vemos en IAM-POLICIES Resumen: creamos un service account que tiene un clusterrolebinding para ver los permisos de ingress y de balanzador de carga, por esto, de este ultimo, creamos una politica para que pueda crear recursos en AWS y en balanceador de carga. DEPLOY INGRESS CONTROLLER AWS Creamos un deployment que crea un pod de ingress controller con una imagen de aws ingress controller que lo que har\u00e1 es que si ve cambios, los modifica en el balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml Cambiamos unas lineas del deploy: kubectl edit deployment.apps/alb-ingress-controller -n kube-system spec: containers: - args: - --ingress-class=alb - --cluster-name=test-cluster Comprobamos que esto funciona: [isx46410800@miguel ~]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE alb-ingress-controller-868ddb9874-gzsvx 1/1 Running 0 41s aws-node-gcd69 1/1 Running 0 35m coredns-6ddcfb5bcf-h7qrx 1/1 Running 0 48m coredns-6ddcfb5bcf-t7wnz 1/1 Running 0 48m kube-proxy-jdnj5 1/1 Running 0 35m DEPLOY APP Creamos el ejemplo de aplicaci\u00f3n que es un juego, creamos un servicio, un deploy y namespaces: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml Comprobamos: [isx46410800@miguel ~]$ kubectl get all -n 2048-game NAME READY STATUS RESTARTS AGE pod/2048-deployment-dd74cc68d-88w46 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-gc9pp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-lw72w 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-wk8tp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-zlshx 1/1 Running 0 29s # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/service-2048 NodePort 10.100.179.203 <none> 80:30798/TCP 20s # NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/2048-deployment 5/5 5 5 30s # NAME DESIRED CURRENT READY AGE replicaset.apps/2048-deployment-dd74cc68d 5 5 5 30s Para comprobar que funciona la app internamente usamos: [isx46410800@miguel ~]$ kubectl port-forward pod/2048-deployment-dd74cc68d-88w46 -n 2048-game 7000:80 Forwarding from 127.0.0.1:7000 -> 80 Forwarding from [::1]:7000 -> 80 EXPONER LA APP EXTERNAMENTE Enrutamos con el ingress la app: [isx46410800@miguel ~]$ kubectl get ingress -n 2048-game NAME HOSTS ADDRESS PORTS AGE 2048-ingress * d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com 80 14s Si vamos a nuestro EC2 de amazon. a nuestro balanceador de carga veremos que nos sale la url en la que podemos ir a la aplicaci\u00f3n ya que la regla estaba asignada. MODIFICANDO REGLAS INGRESS Vemos que IPs apuntan al balanceador de carga que nos da la url del juego: [isx46410800@miguel ~]$ nslookup d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Server: 192.168.1.1 Address: 192.168.1.1#53 Non-authoritative answer: Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.134.190.250 Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.133.107.232 Las a\u00f1adimos a nuestro /etc/hosts: 18.134.190.250 app.aws.game.test 18.133.107.232 app.aws.game.test Cambiamos reglas para que utilicen el nombre y no la ip ni dns: kubectl edit ingress 2048-ingress -n 2048-game spec: rules: - host: app.aws.game.test http: paths: - path: /* backend: serviceName: service-2048 servicePort: 80 Ahora entraremos solo por nombre BORRAR TODO Borramos todo y vemos que no hay el balanceador de carga: kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml AWS HPA INSTALL HPA(Horizontal Pod Autoescaler) consulta unas metricas y se asocia a un deployment. Basado a unas metricas dice cuanta cantidad de pods creas, segun la carga que se pueda ir soportando. Solo escala por CPU. Se ha de instalar el Metrics Server : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml [isx46410800@miguel ~]$ kubectl get deployment metrics-server -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 6s CREAR UN HPA Ejemplo de una app: [isx46410800@miguel ~]$ kubectl apply -f https://k8s.io/examples/application/php-apache.yaml deployment.apps/php-apache created service/php-apache created [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 0/1 1 0 15s [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 105m php-apache ClusterIP 10.100.137.253 <none> 80/TCP 19s [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-h2xhh 1/1 Running 0 50s Ahora Escalamos. Esto quiere decir que si la carga pasa del 50% creee pods hasta un maximo de 10 pods: kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Comprobamos con kubectl get hpa : [isx46410800@miguel ~]$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 74s [isx46410800@miguel ~]$ kubectl get hpa -o yaml apiVersion: v1 items: - apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler AUTOESCALAR HPA Nuestra maquina de AWS es un t3.medium y tiene 2 cpus y 4 de ram. Creamos un container y dentro de el le hacemos muchas peticiones, veremos como se va cargando y se van creando pods para balancear esta carga: kubectl run -it --rm load-generator --image=busybox /bin/sh --generator=run-pod/v1 # while true; do wget -q -O- http://php-apache; done Vemos los pods y los hpa: [isx46410800@miguel ~]$ kubectl get pods -w NAME READY STATUS RESTARTS AGE apache-bench 1/1 Running 1 21m httpd 1/1 Running 0 2m10s load-generator 1/1 Running 0 20s php-apache-79544c9bd9-cnq6h 1/1 Running 0 43s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-ckcgb 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-m29bz 0/1 ContainerCreating 0 0s # [isx46410800@miguel ~]$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 35m php-apache Deployment/php-apache 54%/50% 1 10 1 35m php-apache Deployment/php-apache 250%/50% 1 10 1 36m php-apache Deployment/php-apache 250%/50% 1 10 4 36m php-apache Deployment/php-apache 250%/50% 1 10 5 36m php-apache Deployment/php-apache 74%/50% 1 10 5 37m php-apache Deployment/php-apache 74%/50% 1 10 8 37m php-apache Deployment/php-apache 68%/50% 1 10 8 38m php-apache Deployment/php-apache 68%/50% 1 10 8 39m php-apache Deployment/php-apache 0%/50% 1 10 8 40m CLUSTER AUTOSCALER Se dispara cuando el HPA dispara pods y no hay nodos donde colocarlos. Entonces se autoescala en nodos para ponerlos. Se dispara cuando desde fuera se hace un deploy y se llena el nodo. Si se dispara otro deploy, como no hay espacio, el cluster autoescaler crea otro nodo para poner los pods que falten por poner. La politica que se tiene que agregar al cluster de cluster autoscale se crea de por s\u00ed cuando creamos el cluster con la herramienta eksctl con la opcion --asg-access. DOCS autoscaler Trabaja como otro pod corriendo en mi cluster. Lo desplegamos: [isx46410800@miguel ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created # [isx46410800@miguel ~]$ kubectl get deploy -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE alb-ingress-controller 1/1 1 1 5h18m cluster-autoscaler 1/1 1 1 13s coredns 2/2 2 2 6h2m metrics-server 1/1 1 1 4h25m Editamos el deploy: kubectl -n kube-system edit deploy cluster-autoscaler - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/test-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false Borramos el HPA para que no haya conflictos. Editamos el deploy y ponemos 3 replicas: [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 3h55m [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-6zqcc 1/1 Running 0 5s php-apache-79544c9bd9-cnq6h 1/1 Running 0 3h56m php-apache-79544c9bd9-pfsrq 1/1 Running 0 5s Si editamos el deploy y a\u00f1adimos mas replicas, veremos que se nos crean varias maquinas, varios nodes. kubectl edit deploy php-apache Comprobamos: [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 114s php-apache-79544c9bd9-6zqcc 1/1 Running 0 6m59s php-apache-79544c9bd9-cnq6h 1/1 Running 0 4h3m php-apache-79544c9bd9-dlmrz 1/1 Running 0 114s php-apache-79544c9bd9-dq8f2 1/1 Running 0 3m29s php-apache-79544c9bd9-hbxnr 1/1 Running 0 3m29s php-apache-79544c9bd9-n594l 1/1 Running 0 114s php-apache-79544c9bd9-pfsrq 1/1 Running 0 6m59s php-apache-79544c9bd9-pv5cl 1/1 Running 0 114s php-apache-79544c9bd9-pzz4w 1/1 Running 0 114s php-apache-79544c9bd9-x4czh 1/1 Running 0 4m19s php-apache-79544c9bd9-zm7fj 1/1 Running 0 114s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-22-127.eu-west-2.compute.internal Ready <none> 41s v1.17.11-eks-cfdc40 ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h11m v1.17.11-eks-cfdc40 Ahora comprobamos que cuando no usa un nodo, el autoscale lo elimine automaticamente y va pasando pods a un solo nodo y dejar el minimo de maquinas running: [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 5m47s php-apache-79544c9bd9-dlmrz 1/1 Running 0 5m47s php-apache-79544c9bd9-n594l 1/1 Running 0 5m47s php-apache-79544c9bd9-pv5cl 1/1 Running 0 5m47s php-apache-79544c9bd9-pzz4w 1/1 Running 0 5m47s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h14m v1.17.11-eks-cfdc40 ELIMINAMOS TODO DE LA NUBE Vamos a AWS - CLOUD FORMATION y eliminamos todo.","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal","title":"KUBERNETES"},{"location":"kubernetes/#arquitectura","text":"MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc.","title":"ARQUITECTURA"},{"location":"kubernetes/#instalacion-minikubekubectl","text":"MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes","title":"INSTALACI\u00d3N MINIKUBE/KUBECTL"},{"location":"kubernetes/#pods-vs-contenedores","text":"Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.","title":"PODS VS CONTENEDORES"},{"location":"kubernetes/#pods","text":"","title":"PODS"},{"location":"kubernetes/#crear-pod","text":"Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso.","title":"CREAR POD"},{"location":"kubernetes/#logs-pods","text":"Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso.","title":"LOGS PODS"},{"location":"kubernetes/#api-resources","text":"Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources","title":"API-RESOURCES"},{"location":"kubernetes/#eliminar-pods","text":"Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all","title":"ELIMINAR PODS"},{"location":"kubernetes/#obtener-yaml-pod","text":"Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces.","title":"OBTENER YAML POD"},{"location":"kubernetes/#ip-pod","text":"Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80","title":"IP POD"},{"location":"kubernetes/#entrar-al-pod","text":"Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh","title":"ENTRAR AL POD"},{"location":"kubernetes/#crear-pod-yaml","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine","title":"CREAR POD YAML"},{"location":"kubernetes/#2-container-por-pod","text":"Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2","title":"2+ CONTAINER POR POD"},{"location":"kubernetes/#labels","text":"Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas.","title":"LABELS"},{"location":"kubernetes/#problemas-pods","text":"Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar.","title":"PROBLEMAS PODs"},{"location":"kubernetes/#replicasets","text":"Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.","title":"REPLICASETS"},{"location":"kubernetes/#crear-replicaset","text":"Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s","title":"CREAR REPLICASET"},{"location":"kubernetes/#eliminarmodificar","text":"En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s","title":"ELIMINAR/MODIFICAR"},{"location":"kubernetes/#logs","text":"Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test","title":"LOGS"},{"location":"kubernetes/#owner-refernce","text":"Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0","title":"OWNER REFERNCE"},{"location":"kubernetes/#adopcion-de-pods-planos","text":"Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.","title":"ADOPCI\u00d3N DE PODS PLANOS"},{"location":"kubernetes/#problemas","text":"Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.","title":"PROBLEMAS"},{"location":"kubernetes/#deployments","text":"Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets","title":"DEPLOYMENTS"},{"location":"kubernetes/#crear-deployment","text":"Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset","title":"CREAR DEPLOYMENT"},{"location":"kubernetes/#rolling-update","text":"Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge","title":"ROLLING UPDATE"},{"location":"kubernetes/#historial-de-deployments","text":"Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"HISTORIAL DE DEPLOYMENTS"},{"location":"kubernetes/#roll-backs","text":"Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0","title":"ROLL BACKS"},{"location":"kubernetes/#servicios","text":"Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.","title":"SERVICIOS"},{"location":"kubernetes/#crear-servicio","text":"Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.","title":"CREAR SERVICIO"},{"location":"kubernetes/#info-servicio","text":"Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.","title":"INFO SERVICIO"},{"location":"kubernetes/#endpoints","text":"Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none>","title":"ENDPOINTS"},{"location":"kubernetes/#dns","text":"Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888","title":"DNS"},{"location":"kubernetes/#servicio-cluster-ip","text":"IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)","title":"SERVICIO CLUSTER-IP"},{"location":"kubernetes/#servicio-node-port","text":"IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port.","title":"SERVICIO NODE-PORT"},{"location":"kubernetes/#servicio-load-balancer","text":"Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.","title":"SERVICIO LOAD BALANCER"},{"location":"kubernetes/#golang","text":"Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend.","title":"GOLANG"},{"location":"kubernetes/#crear-api-rest-go","text":"DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png]","title":"CREAR API REST GO"},{"location":"kubernetes/#cambios-mensaje-respuesta","text":"","title":"CAMBIOS MENSAJE RESPUESTA"},{"location":"kubernetes/#mensaje-1","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png]","title":"MENSAJE 1"},{"location":"kubernetes/#mensaje-2","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png]","title":"MENSAJE 2"},{"location":"kubernetes/#dockerfile-golang","text":"Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.","title":"DOCKERFILE GOLANG"},{"location":"kubernetes/#deployment-golang","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png]","title":"DEPLOYMENT GOLANG"},{"location":"kubernetes/#consumo-del-servicio","text":"Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ #","title":"CONSUMO DEL SERVICIO"},{"location":"kubernetes/#fronted","text":"Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html","title":"FRONTED"},{"location":"kubernetes/#manifiesto-fronted","text":"Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy","title":"MANIFIESTO FRONTED"},{"location":"kubernetes/#namespaces","text":"Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName","title":"NAMESPACES"},{"location":"kubernetes/#crear-namespace","text":"Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none>","title":"CREAR NAMESPACE"},{"location":"kubernetes/#asignar-namespaces","text":"Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s","title":"ASIGNAR NAMESPACES"},{"location":"kubernetes/#borrar-namespaces","text":"Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns","title":"BORRAR NAMESPACES"},{"location":"kubernetes/#deploy-namespaces","text":"Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m","title":"DEPLOY NAMESPACES"},{"location":"kubernetes/#dns-namespaces","text":"Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on","title":"DNS NAMESPACES"},{"location":"kubernetes/#contextos-namespaces","text":"Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d","title":"CONTEXTOS NAMESPACES"},{"location":"kubernetes/#limitar-ramcpu","text":"La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus.","title":"LIMITAR RAM/CPU"},{"location":"kubernetes/#limitsrequest","text":"Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique.","title":"LIMITS/REQUEST"},{"location":"kubernetes/#ram","text":"Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory.","title":"RAM"},{"location":"kubernetes/#cpu","text":"Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu.","title":"CPU"},{"location":"kubernetes/#qosquality-of-service","text":"Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request.","title":"QOS(Quality of Service)"},{"location":"kubernetes/#limitrange","text":"Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods","title":"LIMITRANGE"},{"location":"kubernetes/#valores-por-defecto","text":"Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi -","title":"VALORES POR DEFECTO"},{"location":"kubernetes/#valores-pod","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M.","title":"VALORES POD"},{"location":"kubernetes/#limites","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram","title":"LIMITES"},{"location":"kubernetes/#resource-quota","text":"Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod.","title":"RESOURCE QUOTA"},{"location":"kubernetes/#crear-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource.","title":"CREAR RQ"},{"location":"kubernetes/#deploy-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA.","title":"DEPLOY RQ"},{"location":"kubernetes/#limitar-no-pods-en-ns","text":"Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota.","title":"LIMITAR N\u00ba PODS EN NS"},{"location":"kubernetes/#probes","text":"Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP","title":"PROBES"},{"location":"kubernetes/#tipos-probes","text":"Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse.","title":"TIPOS PROBES"},{"location":"kubernetes/#crear-liveness-probe","text":"Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness","title":"CREAR LIVENESS PROBE"},{"location":"kubernetes/#liveness-tcp","text":"Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS TCP"},{"location":"kubernetes/#liveness-http","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS HTTP"},{"location":"kubernetes/#readiness-probe","text":"Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod.","title":"READINESS PROBE"},{"location":"kubernetes/#variables-y-configmap","text":"","title":"VARIABLES Y CONFIGMAP"},{"location":"kubernetes/#crear-variables","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1","title":"CREAR VARIABLES"},{"location":"kubernetes/#variables-referenciadas","text":"Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc.","title":"VARIABLES REFERENCIADAS"},{"location":"kubernetes/#configmap","text":"Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none>","title":"CONFIGMAP"},{"location":"kubernetes/#montando-volumen-configmap","text":"Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo.","title":"MONTANDO VOLUMEN CONFIGMAP"},{"location":"kubernetes/#volumen-env-configmap","text":"Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user","title":"VOLUMEN-ENV CONFIGMAP"},{"location":"kubernetes/#secrets","text":"Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen.","title":"SECRETS"},{"location":"kubernetes/#crear","text":"Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode","title":"CREAR"},{"location":"kubernetes/#manifiestos","text":"Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test","title":"MANIFIESTOS"},{"location":"kubernetes/#envsubts","text":"Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado.","title":"ENVSUBTS"},{"location":"kubernetes/#volume-secrets","text":"Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ #","title":"VOLUME SECRETS"},{"location":"kubernetes/#env-secrets","text":"Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456","title":"ENV SECRETS"},{"location":"kubernetes/#volumes","text":"Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"VOLUMES"},{"location":"kubernetes/#emptydir","text":"Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / #","title":"EMPTYDIR"},{"location":"kubernetes/#hostpath-pv","text":"En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none>","title":"HOSTPATH-PV"},{"location":"kubernetes/#pvc","text":"El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC.","title":"PVC"},{"location":"kubernetes/#pvc-pv","text":"Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector.","title":"PVC-PV"},{"location":"kubernetes/#pvc-pods","text":"De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql>","title":"PVC-PODS"},{"location":"kubernetes/#cloud-volumes","text":"Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC.","title":"CLOUD VOLUMES"},{"location":"kubernetes/#reclaim-policy","text":"Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"RECLAIM POLICY"},{"location":"kubernetes/#usersgroups-rbac","text":"RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster.","title":"USERS/GROUPS RBAC"},{"location":"kubernetes/#roles-vs-clusterroles","text":"En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo.","title":"ROLES vs CLUSTERROLES"},{"location":"kubernetes/#rolebinding-vs-clusterrolebinding","text":"Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1.","title":"ROLEBINDING vs CLUSTERROLEBINDING"},{"location":"kubernetes/#crear-users-groups","text":"Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\"","title":"CREAR USERS &amp; GROUPS"},{"location":"kubernetes/#habilitar-rbac","text":"Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC","title":"HABILITAR RBAC"},{"location":"kubernetes/#simplificamos-contexto","text":"Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel","title":"SIMPLIFICAMOS CONTEXTO"},{"location":"kubernetes/#crear-roles","text":"Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user.","title":"CREAR ROLES"},{"location":"kubernetes/#enlazar-role-user","text":"Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\"","title":"ENLAZAR ROLE &amp; USER"},{"location":"kubernetes/#config-maps","text":"Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again.","title":"CONFIG MAPS"},{"location":"kubernetes/#crear-clusterole","text":"Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m","title":"CREAR CLUSTEROLE"},{"location":"kubernetes/#crear-user-admin","text":"Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m","title":"CREAR USER ADMIN"},{"location":"kubernetes/#roles-a-grupos","text":"Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m","title":"ROLES A GRUPOS"},{"location":"kubernetes/#services-account","text":"Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr","title":"SERVICES ACCOUNT"},{"location":"kubernetes/#secret-sa","text":"Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc","title":"SECRET SA"},{"location":"kubernetes/#crear-sa","text":"Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s","title":"CREAR SA"},{"location":"kubernetes/#relacion-pod-sa","text":"Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/","title":"RELACION POD-SA"},{"location":"kubernetes/#requests","text":"A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones.","title":"REQUESTS"},{"location":"kubernetes/#request-jwt","text":"Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante.","title":"REQUEST JWT"},{"location":"kubernetes/#sa-deployment","text":"Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa","title":"SA DEPLOYMENT"},{"location":"kubernetes/#role-sa","text":"Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1,","title":"ROLE SA"},{"location":"kubernetes/#ingress","text":"Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada.","title":"INGRESS"},{"location":"kubernetes/#ingress-controller","text":"Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud.","title":"INGRESS CONTROLLER"},{"location":"kubernetes/#crear-ingress-controller","text":"Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s","title":"CREAR INGRESS CONTROLLER"},{"location":"kubernetes/#ip-ingress-controller","text":"Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602","title":"IP INGRESS CONTROLLER"},{"location":"kubernetes/#app-ingress-controller","text":"Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv","title":"APP INGRESS-CONTROLLER"},{"location":"kubernetes/#exponer-el-puerto-al-exterior","text":"Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080","title":"EXPONER EL PUERTO AL EXTERIOR"},{"location":"kubernetes/#2-apps-en-ic","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos:","title":"2 APPS EN IC"},{"location":"kubernetes/#aws-kubernetes","text":"Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: xxxx Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0","title":"AWS KUBERNETES"},{"location":"kubernetes/#crear-cluster-aws-eksctl","text":"docs install Creamos cluster master sin nodos: eksctl create cluster --name test-cluster --without-nodegroup --region eu-west-2 --zones eu-west-2a,eu-west-2b Vemos lo creado en el apartado EKS y CloudFormation : Eksctl lee de estos archivos para comunicarse: [isx46410800@miguel ~]$ cat .aws/credentials [default] aws_access_key_id = AKIA5RIFOUI3OMSWWHNM aws_secret_access_key = xxxxx [isx46410800@miguel ~]$ cat .aws/config [default] region = eu-west-2 Al crear el cluster nos crea un directorio ~/.kube/config Si eliminamos este directorio, como si no lo tuvieramos y nos queremos conectar a este cluster usamos la orden: aws eks --region eu-west-2 update-kubeconfig --name test-cluster Ahora si hacemos kubectl get svc y kubectl cluster-info vemos que estamos conectados y referenciados al cluster de AWS: [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7m11s [isx46410800@miguel ~]$ kubectl cluster-info Kubernetes master is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com CoreDNS is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ahora intentamos crear un POD pero vemos que no se acaba de crear porque no tenemos ningun nodo unido a nuestro CLUSTER: [isx46410800@miguel ~]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 0/1 Pending 0 10s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 15s default-scheduler no nodes available to schedule pods Ahora creamos nodos con eksctl con ami version kubernetes auto y asg access para ser escalable: eksctl create nodegroup --cluster test-cluster --region eu-west-2 --name test-workers --node-type t3.medium --node-ami auto --nodes 1 --nodes-min 1 --nodes-max 3 --asg-access Comprobamos que el pod de prueba est\u00e1 ahora running y asignado al nodo creado: [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 68s v1.17.11-eks-cfdc40 # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6m38s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Name: pod-test Namespace: default Priority: 0 Node: ip-192-168-38-128.eu-west-2.compute.internal/192.168.38.128","title":"CREAR CLUSTER AWS EKSCTL"},{"location":"kubernetes/#ingress-aws-eks","text":"Para exponerlo, crearemos un balanzador de carga, un ingress y un ingress controller. DOCS para crear el ingress controller nos dice que nuestro servicio(VPC) tiene que seguir una estructura de tag. Vemos los servicios VPC que se crearon automaticamente al crear el cluster y los nodos. Las subnets tambien tienen que seguir una estructura de tags. No obstante todos estos pasos al crearlos con EKSCTL ya vienen por defecto. IAM OIDC eksctl utils associate-iam-oidc-provider --region eu-west-2 --cluster test-cluster --approve Politica para crear recursos de balanceador de carga: [isx46410800@miguel ~]$ aws iam create-policy \\ > --policy-name ALBIngressControllerIAMPolicy \\ > --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json { \"Policy\": { \"PolicyName\": \"ALBIngressControllerIAMPolicy\", \"PolicyId\": \"ANPA5RIFOUI3IFJHOR5SB\", \"Arn\": \"arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy\", \"Path\": \"/\", \"DefaultVersionId\": \"v1\", \"AttachmentCount\": 0, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"CreateDate\": \"2020-10-21T17:06:33Z\", \"UpdateDate\": \"2020-10-21T17:06:33Z\" } } Creamos un service account para ingress con un clusterrole y un clusterrolebinding de ingress controller para balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml Creamos un service account para que nuestro ingress controller sea capaz de crear recursos en AWS: eksctl create iamserviceaccount \\ --region eu-west-2 \\ --name alb-ingress-controller \\ --namespace kube-system \\ --cluster test-cluster \\ --attach-policy-arn arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy \\ --override-existing-serviceaccounts \\ --approve La policy la vemos en IAM-POLICIES Resumen: creamos un service account que tiene un clusterrolebinding para ver los permisos de ingress y de balanzador de carga, por esto, de este ultimo, creamos una politica para que pueda crear recursos en AWS y en balanceador de carga.","title":"INGRESS AWS EKS"},{"location":"kubernetes/#deploy-ingress-controller-aws","text":"Creamos un deployment que crea un pod de ingress controller con una imagen de aws ingress controller que lo que har\u00e1 es que si ve cambios, los modifica en el balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml Cambiamos unas lineas del deploy: kubectl edit deployment.apps/alb-ingress-controller -n kube-system spec: containers: - args: - --ingress-class=alb - --cluster-name=test-cluster Comprobamos que esto funciona: [isx46410800@miguel ~]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE alb-ingress-controller-868ddb9874-gzsvx 1/1 Running 0 41s aws-node-gcd69 1/1 Running 0 35m coredns-6ddcfb5bcf-h7qrx 1/1 Running 0 48m coredns-6ddcfb5bcf-t7wnz 1/1 Running 0 48m kube-proxy-jdnj5 1/1 Running 0 35m","title":"DEPLOY INGRESS CONTROLLER AWS"},{"location":"kubernetes/#deploy-app","text":"Creamos el ejemplo de aplicaci\u00f3n que es un juego, creamos un servicio, un deploy y namespaces: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml Comprobamos: [isx46410800@miguel ~]$ kubectl get all -n 2048-game NAME READY STATUS RESTARTS AGE pod/2048-deployment-dd74cc68d-88w46 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-gc9pp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-lw72w 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-wk8tp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-zlshx 1/1 Running 0 29s # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/service-2048 NodePort 10.100.179.203 <none> 80:30798/TCP 20s # NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/2048-deployment 5/5 5 5 30s # NAME DESIRED CURRENT READY AGE replicaset.apps/2048-deployment-dd74cc68d 5 5 5 30s Para comprobar que funciona la app internamente usamos: [isx46410800@miguel ~]$ kubectl port-forward pod/2048-deployment-dd74cc68d-88w46 -n 2048-game 7000:80 Forwarding from 127.0.0.1:7000 -> 80 Forwarding from [::1]:7000 -> 80","title":"DEPLOY APP"},{"location":"kubernetes/#exponer-la-app-externamente","text":"Enrutamos con el ingress la app: [isx46410800@miguel ~]$ kubectl get ingress -n 2048-game NAME HOSTS ADDRESS PORTS AGE 2048-ingress * d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com 80 14s Si vamos a nuestro EC2 de amazon. a nuestro balanceador de carga veremos que nos sale la url en la que podemos ir a la aplicaci\u00f3n ya que la regla estaba asignada.","title":"EXPONER LA APP EXTERNAMENTE"},{"location":"kubernetes/#modificando-reglas-ingress","text":"Vemos que IPs apuntan al balanceador de carga que nos da la url del juego: [isx46410800@miguel ~]$ nslookup d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Server: 192.168.1.1 Address: 192.168.1.1#53 Non-authoritative answer: Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.134.190.250 Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.133.107.232 Las a\u00f1adimos a nuestro /etc/hosts: 18.134.190.250 app.aws.game.test 18.133.107.232 app.aws.game.test Cambiamos reglas para que utilicen el nombre y no la ip ni dns: kubectl edit ingress 2048-ingress -n 2048-game spec: rules: - host: app.aws.game.test http: paths: - path: /* backend: serviceName: service-2048 servicePort: 80 Ahora entraremos solo por nombre","title":"MODIFICANDO REGLAS INGRESS"},{"location":"kubernetes/#borrar-todo","text":"Borramos todo y vemos que no hay el balanceador de carga: kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml","title":"BORRAR TODO"},{"location":"kubernetes/#aws-hpa-install","text":"HPA(Horizontal Pod Autoescaler) consulta unas metricas y se asocia a un deployment. Basado a unas metricas dice cuanta cantidad de pods creas, segun la carga que se pueda ir soportando. Solo escala por CPU. Se ha de instalar el Metrics Server : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml [isx46410800@miguel ~]$ kubectl get deployment metrics-server -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 6s","title":"AWS HPA INSTALL"},{"location":"kubernetes/#crear-un-hpa","text":"Ejemplo de una app: [isx46410800@miguel ~]$ kubectl apply -f https://k8s.io/examples/application/php-apache.yaml deployment.apps/php-apache created service/php-apache created [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 0/1 1 0 15s [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 105m php-apache ClusterIP 10.100.137.253 <none> 80/TCP 19s [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-h2xhh 1/1 Running 0 50s Ahora Escalamos. Esto quiere decir que si la carga pasa del 50% creee pods hasta un maximo de 10 pods: kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Comprobamos con kubectl get hpa : [isx46410800@miguel ~]$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 74s [isx46410800@miguel ~]$ kubectl get hpa -o yaml apiVersion: v1 items: - apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler","title":"CREAR UN HPA"},{"location":"kubernetes/#autoescalar-hpa","text":"Nuestra maquina de AWS es un t3.medium y tiene 2 cpus y 4 de ram. Creamos un container y dentro de el le hacemos muchas peticiones, veremos como se va cargando y se van creando pods para balancear esta carga: kubectl run -it --rm load-generator --image=busybox /bin/sh --generator=run-pod/v1 # while true; do wget -q -O- http://php-apache; done Vemos los pods y los hpa: [isx46410800@miguel ~]$ kubectl get pods -w NAME READY STATUS RESTARTS AGE apache-bench 1/1 Running 1 21m httpd 1/1 Running 0 2m10s load-generator 1/1 Running 0 20s php-apache-79544c9bd9-cnq6h 1/1 Running 0 43s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-ckcgb 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-m29bz 0/1 ContainerCreating 0 0s # [isx46410800@miguel ~]$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 35m php-apache Deployment/php-apache 54%/50% 1 10 1 35m php-apache Deployment/php-apache 250%/50% 1 10 1 36m php-apache Deployment/php-apache 250%/50% 1 10 4 36m php-apache Deployment/php-apache 250%/50% 1 10 5 36m php-apache Deployment/php-apache 74%/50% 1 10 5 37m php-apache Deployment/php-apache 74%/50% 1 10 8 37m php-apache Deployment/php-apache 68%/50% 1 10 8 38m php-apache Deployment/php-apache 68%/50% 1 10 8 39m php-apache Deployment/php-apache 0%/50% 1 10 8 40m","title":"AUTOESCALAR HPA"},{"location":"kubernetes/#cluster-autoscaler","text":"Se dispara cuando el HPA dispara pods y no hay nodos donde colocarlos. Entonces se autoescala en nodos para ponerlos. Se dispara cuando desde fuera se hace un deploy y se llena el nodo. Si se dispara otro deploy, como no hay espacio, el cluster autoescaler crea otro nodo para poner los pods que falten por poner. La politica que se tiene que agregar al cluster de cluster autoscale se crea de por s\u00ed cuando creamos el cluster con la herramienta eksctl con la opcion --asg-access. DOCS autoscaler Trabaja como otro pod corriendo en mi cluster. Lo desplegamos: [isx46410800@miguel ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created # [isx46410800@miguel ~]$ kubectl get deploy -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE alb-ingress-controller 1/1 1 1 5h18m cluster-autoscaler 1/1 1 1 13s coredns 2/2 2 2 6h2m metrics-server 1/1 1 1 4h25m Editamos el deploy: kubectl -n kube-system edit deploy cluster-autoscaler - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/test-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false Borramos el HPA para que no haya conflictos. Editamos el deploy y ponemos 3 replicas: [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 3h55m [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-6zqcc 1/1 Running 0 5s php-apache-79544c9bd9-cnq6h 1/1 Running 0 3h56m php-apache-79544c9bd9-pfsrq 1/1 Running 0 5s Si editamos el deploy y a\u00f1adimos mas replicas, veremos que se nos crean varias maquinas, varios nodes. kubectl edit deploy php-apache Comprobamos: [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 114s php-apache-79544c9bd9-6zqcc 1/1 Running 0 6m59s php-apache-79544c9bd9-cnq6h 1/1 Running 0 4h3m php-apache-79544c9bd9-dlmrz 1/1 Running 0 114s php-apache-79544c9bd9-dq8f2 1/1 Running 0 3m29s php-apache-79544c9bd9-hbxnr 1/1 Running 0 3m29s php-apache-79544c9bd9-n594l 1/1 Running 0 114s php-apache-79544c9bd9-pfsrq 1/1 Running 0 6m59s php-apache-79544c9bd9-pv5cl 1/1 Running 0 114s php-apache-79544c9bd9-pzz4w 1/1 Running 0 114s php-apache-79544c9bd9-x4czh 1/1 Running 0 4m19s php-apache-79544c9bd9-zm7fj 1/1 Running 0 114s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-22-127.eu-west-2.compute.internal Ready <none> 41s v1.17.11-eks-cfdc40 ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h11m v1.17.11-eks-cfdc40 Ahora comprobamos que cuando no usa un nodo, el autoscale lo elimine automaticamente y va pasando pods a un solo nodo y dejar el minimo de maquinas running: [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 5m47s php-apache-79544c9bd9-dlmrz 1/1 Running 0 5m47s php-apache-79544c9bd9-n594l 1/1 Running 0 5m47s php-apache-79544c9bd9-pv5cl 1/1 Running 0 5m47s php-apache-79544c9bd9-pzz4w 1/1 Running 0 5m47s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h14m v1.17.11-eks-cfdc40","title":"CLUSTER AUTOSCALER"},{"location":"kubernetes/#eliminamos-todo-de-la-nube","text":"Vamos a AWS - CLOUD FORMATION y eliminamos todo.","title":"ELIMINAMOS TODO DE LA NUBE"},{"location":"ldap/","text":"LDAP Administra un servicio de directorios. Autenticaci\u00f3n: demuestra quien soy (auth) Autorizar: que derechos tiene (autz) Information Provider: la info de la cuenta de usuario Authenticaction Provider: quien da el password Es una base de datos no relacional: Jerarquica Distribuida Optimizada por las lecturas Forma de arbol Trabaja con identidades Ejemplo EDT: Escola utiliza LDAP/Kerberos y unix solo utiliza /etc/passwd. Una caja es una entidad y las caracteristicas de la caja son atributos. Formato ldif para los datos de LDAP Paquetes de instalaci\u00f3n: openldap-clients openldap-servers slapd ordenes de bajo nivel para el servidor /var/lib/ldap directorio donde se guarda las bbdd ldap.ldap /etc/openldap/slapd.d directorio de condiguraci\u00f3n formato ldap rootdn \"cn=Manager,dc=edt,dc=org\" es el root total de una bbdd. Puertos LDAP 389 Encender servicio /sbin/slapd -d CREACION Ejemplo ldap normal Dockerfile: # ldapserver FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"ldapserver\" RUN dnf install -y openldap-servers openldap-clients RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker CMD /opt/docker/startup.sh Install.sh: #! /bin/bash # Install ldap server rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* cp /opt/docker/DB_CONFIG /var/lib/ldap/. slaptest -f /opt/docker/slapd.conf -F /etc/openldap/slapd.d/ slapadd -F /etc/openldap/slapd.d/ -l /opt/docker/edt.org.ldif chown -R ldap.ldap /etc/openldap/slapd.d chown -R ldap.ldap /var/lib/ldap cp /opt/docker/ldap.conf /etc/openldap/. Startup.sh: #! /bin/bash bash /opt/docker/install.sh ulimit -n 1024 /sbin/slapd a=1 while [ $a -eq 1 ] do a=1 done FICHEROS LDAP Fichero slapd.conf. A partir de este file generamos el directorio de configuraci\u00f3n: # # See slapd.conf(5) for details on configuration options. # This file should NOT be world readable. # include /etc/openldap/schema/corba.schema include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/duaconf.schema include /etc/openldap/schema/dyngroup.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/java.schema include /etc/openldap/schema/misc.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/openldap.schema include /etc/openldap/schema/ppolicy.schema include /etc/openldap/schema/collective.schema # Allow LDAPv2 client connections. This is NOT the default. allow bind_v2 pidfile /var/run/openldap/slapd.pid #argsfile /var/run/openldap/slapd.args # ---------------------------------------------------------------------- database mdb suffix \"dc=edt,dc=org\" rootdn \"cn=Manager,dc=edt,dc=org\" rootpw secret directory /var/lib/ldap index objectClass eq,pres access to * by self write by * read # ---------------------------------------------------------------------- # ---------------------------------------------------------------------- database config rootdn \"cn=Sysadmin,cn=config\" rootpw {SSHA}5DfZc1WXeIwrP7C3fr23WLZiPZ5YHMgA # el passwd es syskey # ---------------------------------------------------------------------- # enable monitoring database monitor ldap.conf configuraci\u00f3n para conectar a ldap: # # LDAP Defaults # # See ldap.conf(5) for details # This file should be world readable but not world writable. #BASE dc=example,dc=com #URI ldap://ldap.example.com ldap://ldap-master.example.com:666 #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never TLS_CACERTDIR /etc/openldap/certs # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on URI ldap://ldapserver BASE dc=edt,dc=org Fichero edt.org.ldif fichero donde esta el arbol de root, subgrupos,usuarios, grupos: dn: dc=edt,dc=org dc: edt description: Escola del treball de Barcelona objectClass: dcObject objectClass: organization o: edt.org # dn: ou=maquines,dc=edt,dc=org ou: maquines description: Container per a maquines linux objectclass: organizationalunit # dn: ou=clients,dc=edt,dc=org ou: clients description: Container per a clients linux objectclass: organizationalunit # dn: ou=usuaris,dc=edt,dc=org ou: usuaris description: Container per usuaris del sistema linux objectclass: organizationalunit # dn: ou=grups,dc=edt,dc=org ou: grups description: Container per grups del sistema linux objectclass: organizationalunit # dn: cn=1asix,ou=grups,dc=edt,dc=org cn: 1asix gidNumber: 610 description: Grup de 1asix memberUid: user01 memberUid: user02 memberUid: user03 memberUid: user04 memberUid: user15 objectclass: posixGroup # dn: cn=2asix,ou=grups,dc=edt,dc=org cn: 2asix gidNumber: 611 description: Grup de 2asix memberUid: user06 memberUid: user07 memberUid: user08 memberUid: user09 memberUid: user10 objectclass: posixGroup # dn: cn=profesasix,ou=grups,dc=edt,dc=org cn: profesasix gidNumber: 612 description: Profes de asix memberUid: pere memberUid: anna memberUid: jordi memberUid: marta memberUid: pau objectclass: posixGroup # dn: cn=Pau Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pau Pou cn: Pauet Pou sn: Pou homephone: 555-222-2220 mail: pau@edt.org description: Watch out for this guy ou: Profes uid: pau uidNumber: 5000 gidNumber: 612 homeDirectory: /tmp/home/pau userPassword: {SSHA}NDkipesNQqTFDgGJfyraLz/csZAIlk2/ # dn: cn=Pere Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pere Pou sn: Pou homephone: 555-222-2221 mail: pere@edt.org description: Watch out for this guy ou: Profes uid: pere uidNumber: 5001 gidNumber: 612 homeDirectory: /tmp/home/pere userPassword: {SSHA}ghmtRL11YtXoUhIP7z6f7nb8RCNadFe+ DB_CONFIG configuracion de bbdd: # $OpenLDAP$ # Example DB_CONFIG file for use with slapd(8) BDB/HDB databases. # # See the Oracle Berkeley DB documentation # <http://www.oracle.com/technology/documentation/berkeley-db/db/ref/env/db_config.html> # for detail description of DB_CONFIG syntax and semantics. # # Hints can also be found in the OpenLDAP Software FAQ # <http://www.openldap.org/faq/index.cgi?file=2> # in particular: # <http://www.openldap.org/faq/index.cgi?file=1075> # Note: most DB_CONFIG settings will take effect only upon rebuilding # the DB environment. # one 0.25 GB cache set_cachesize 0 268435456 1 # Data Directory #set_data_dir db # Transaction Log settings set_lg_regionmax 262144 set_lg_bsize 2097152 #set_lg_dir logs # Note: special DB_CONFIG flags are no longer needed for \"quick\" # slapadd(8) or slapindex(8) access (see their -q option). SCHEMA Ejemplo de nombre.schema : attributetype ( 1.1.2.1.1 NAME 'x-equip' DESC 'equip del futbolista' EQUALITY caseIgnoreMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE ) attributetype ( 1.1.2.1.2 NAME 'x-dorsal' DESC 'dorsal del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.27 SINGLE-VALUE ) attributetype ( 1.1.2.1.3 NAME 'x-web' DESC 'pagina web del futbolista' EQUALITY caseExactMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 ) attributetype ( 1.1.2.1.4 NAME 'x-foto' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) attributetype ( 1.1.2.1.5 NAME 'x-lesionat' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.7 SINGLE-VALUE ) objectclass ( 1.1.2.2.1 NAME 'x-futbolistes' DESC 'futboleros' SUP inetOrgPerson STRUCTURAL MUST x-equip MAY ( x-dorsal $ x-web $ x-foto $ x-lesionat ) ) Creacion de un usuario con este schema: dn: cn=kaka,ou=Productes,dc=edt,dc=org objectclass: x-futbolistes cn: kaka sn: kaka x-equip: los pimientos x-dorsal: 7 x-web: www.kaka.com x-foto: //var/tmp/foto.jpg x-lesionat: FALSE A\u00f1adimos en el fichero slapd.conf el schema: include /opt/docker/futbolista-C.schema ACL En el slapd.conf est\u00e1 el user y pass del ACL. Ejemplos de ACL, son directrices y permisos que tienen ciertos grupos, usuarios etc: Implementar a la base de dades edt.org les seg\u00fcents ACLS: 1. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant de l\u2019administrador i t\u00e9 permisos per modificar-ho tot. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- **[implicitas aunque no salgan] [isx46410800@miguel-fedora27 ldapserver19:acl]$ ldapmodify -vx -c -h 172.17.0.2 -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi -f mod01.ldif ldap_initialize( ldap://172.17.0.2 ) replace mail: newmarta10@edt.org modifying entry \"cn=Marta Mas,ou=usuaris,dc=edt,dc=org\" ldap_modify: Insufficient access (50) replace mail: newmjordi10@edt.org modifying entry \"cn=Jordi Mas,ou=usuaris,dc=edt,dc=org\" modify complete # # 2. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant d\u2019administraci\u00f3. Tothom es pot modificar el seu propi email i homePhone. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- # # 3. Tot usuari es pot modificar el seu mail. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self write by * read [by * none] olcAccess: to * by * read [by * none] [acces to * by * none] --- # # 4. Tothom pot veure totes les dades de tothom, excepte els mail dels altres. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self read [by * none] olcAccess: to * by * read [acces to * by * none] --- [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna dn mail # # 5. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * read olcAccess: to * by * read --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword -- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --puedo ver todos los userpassword como ldapsearch -x -LLL ** by auth no es necesario porqie todo el mundo podemos ver el password y no requiere de identificacion # 6. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom, excepte els altres passwords. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth olcAccess: to * by * read --- **para que nadie lo vea, puedas cambiarlo, tienes que tener primero permiso de auth para autenticarte primero, sino serias un anonimo de fuera ----->puedo cambiar pass mio pero no otro [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s ann2 --->no puedo ver los userpassword con ldapsearch -x -LLL ni con [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword --> solo vere el de jordi por ser authorizado **sino ponemos el by auth, no podremos autenticarnos ya que al hacer la orden somos un user anonymous y hemos de hacer BIND con el -D para identificarnos y solo se consigue poniendo by auth -->** [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword ldap_bind: Invalid credentials (49)--> no deja sin el by auth # # 7. Tot usuari es pot modificar el seu propi password i tot usuari nom\u00e9s pot veure les seves pr\u00f2pies dades. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth [by * none] olcAccess: to * by self read by * search [by * none] ** permet la capacitat de llegir i navegar per tot el arbre ** si ponemos solo by self read, no podremos ver por toodo el contenido para poder ver sus datos y no es capaz de autenticarte. --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --> podemos cambiar nuestro pass pero no el de otro -->con el by * search tendremos acceso a los campos propios [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn: cn=Jordi Mas,ou=usuaris,dc=edt,dc=org objectClass: posixAccount **nos permite de todos los usuarios, ver solo el nuestro. en este caso jordi solo ve el suyo **como anonimo no sale nada porque no tiene datos dentro de la bbdd # 8. Tot usuari pot observar les seves pr\u00f2pies dades i modificar el seu propi password,email i homephone. L\u2019usuari \u201cAnna Pou\u201d pot modificar tots els atributs de tots excepte els passwords, que tampoc pot veure. L\u2019usuari \u201cPere Pou\u201d pot modificar els passwords de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by dn.exact=\"cn=Pere Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * auth [by * none] olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to * by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self read by * search [by * none] --- * by * search para poder ver todas las dades del arbol y hacer match cuando encuentre el suyo que entonces podra ver, el resto no. ORDENES LDAP rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* slaptest -F /etc/openldap/slapd.d/ -f /opt/docker/slapd.conf slapadd -f /etc/openldap/slapd.d -l /opt/docker/usuarios.ldif /sbin/slapd -d0 slapcat -n0 | grep dn ldapsearch -x -LLL -h ipLdap -b 'dc=edt,dc=org' ldapdelete -vx -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' ldapadd -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Sysadmin,cn=config' -w syskey 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif slappasswd -> genera passwd sha de tipo SHA slappasswd -h {md5/crypt} ldapwhoami -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna ldappassword -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s annanew ldapcompare -x -h ipLDAP 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' homePhone=555-222-222 slapacl -b 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' 'mail'","title":"LDAP"},{"location":"ldap/#ldap","text":"Administra un servicio de directorios. Autenticaci\u00f3n: demuestra quien soy (auth) Autorizar: que derechos tiene (autz) Information Provider: la info de la cuenta de usuario Authenticaction Provider: quien da el password Es una base de datos no relacional: Jerarquica Distribuida Optimizada por las lecturas Forma de arbol Trabaja con identidades Ejemplo EDT: Escola utiliza LDAP/Kerberos y unix solo utiliza /etc/passwd. Una caja es una entidad y las caracteristicas de la caja son atributos. Formato ldif para los datos de LDAP Paquetes de instalaci\u00f3n: openldap-clients openldap-servers slapd ordenes de bajo nivel para el servidor /var/lib/ldap directorio donde se guarda las bbdd ldap.ldap /etc/openldap/slapd.d directorio de condiguraci\u00f3n formato ldap rootdn \"cn=Manager,dc=edt,dc=org\" es el root total de una bbdd. Puertos LDAP 389 Encender servicio /sbin/slapd -d","title":"LDAP"},{"location":"ldap/#creacion","text":"Ejemplo ldap normal Dockerfile: # ldapserver FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"ldapserver\" RUN dnf install -y openldap-servers openldap-clients RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker CMD /opt/docker/startup.sh Install.sh: #! /bin/bash # Install ldap server rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* cp /opt/docker/DB_CONFIG /var/lib/ldap/. slaptest -f /opt/docker/slapd.conf -F /etc/openldap/slapd.d/ slapadd -F /etc/openldap/slapd.d/ -l /opt/docker/edt.org.ldif chown -R ldap.ldap /etc/openldap/slapd.d chown -R ldap.ldap /var/lib/ldap cp /opt/docker/ldap.conf /etc/openldap/. Startup.sh: #! /bin/bash bash /opt/docker/install.sh ulimit -n 1024 /sbin/slapd a=1 while [ $a -eq 1 ] do a=1 done","title":"CREACION"},{"location":"ldap/#ficheros-ldap","text":"Fichero slapd.conf. A partir de este file generamos el directorio de configuraci\u00f3n: # # See slapd.conf(5) for details on configuration options. # This file should NOT be world readable. # include /etc/openldap/schema/corba.schema include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/duaconf.schema include /etc/openldap/schema/dyngroup.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/java.schema include /etc/openldap/schema/misc.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/openldap.schema include /etc/openldap/schema/ppolicy.schema include /etc/openldap/schema/collective.schema # Allow LDAPv2 client connections. This is NOT the default. allow bind_v2 pidfile /var/run/openldap/slapd.pid #argsfile /var/run/openldap/slapd.args # ---------------------------------------------------------------------- database mdb suffix \"dc=edt,dc=org\" rootdn \"cn=Manager,dc=edt,dc=org\" rootpw secret directory /var/lib/ldap index objectClass eq,pres access to * by self write by * read # ---------------------------------------------------------------------- # ---------------------------------------------------------------------- database config rootdn \"cn=Sysadmin,cn=config\" rootpw {SSHA}5DfZc1WXeIwrP7C3fr23WLZiPZ5YHMgA # el passwd es syskey # ---------------------------------------------------------------------- # enable monitoring database monitor ldap.conf configuraci\u00f3n para conectar a ldap: # # LDAP Defaults # # See ldap.conf(5) for details # This file should be world readable but not world writable. #BASE dc=example,dc=com #URI ldap://ldap.example.com ldap://ldap-master.example.com:666 #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never TLS_CACERTDIR /etc/openldap/certs # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on URI ldap://ldapserver BASE dc=edt,dc=org Fichero edt.org.ldif fichero donde esta el arbol de root, subgrupos,usuarios, grupos: dn: dc=edt,dc=org dc: edt description: Escola del treball de Barcelona objectClass: dcObject objectClass: organization o: edt.org # dn: ou=maquines,dc=edt,dc=org ou: maquines description: Container per a maquines linux objectclass: organizationalunit # dn: ou=clients,dc=edt,dc=org ou: clients description: Container per a clients linux objectclass: organizationalunit # dn: ou=usuaris,dc=edt,dc=org ou: usuaris description: Container per usuaris del sistema linux objectclass: organizationalunit # dn: ou=grups,dc=edt,dc=org ou: grups description: Container per grups del sistema linux objectclass: organizationalunit # dn: cn=1asix,ou=grups,dc=edt,dc=org cn: 1asix gidNumber: 610 description: Grup de 1asix memberUid: user01 memberUid: user02 memberUid: user03 memberUid: user04 memberUid: user15 objectclass: posixGroup # dn: cn=2asix,ou=grups,dc=edt,dc=org cn: 2asix gidNumber: 611 description: Grup de 2asix memberUid: user06 memberUid: user07 memberUid: user08 memberUid: user09 memberUid: user10 objectclass: posixGroup # dn: cn=profesasix,ou=grups,dc=edt,dc=org cn: profesasix gidNumber: 612 description: Profes de asix memberUid: pere memberUid: anna memberUid: jordi memberUid: marta memberUid: pau objectclass: posixGroup # dn: cn=Pau Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pau Pou cn: Pauet Pou sn: Pou homephone: 555-222-2220 mail: pau@edt.org description: Watch out for this guy ou: Profes uid: pau uidNumber: 5000 gidNumber: 612 homeDirectory: /tmp/home/pau userPassword: {SSHA}NDkipesNQqTFDgGJfyraLz/csZAIlk2/ # dn: cn=Pere Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pere Pou sn: Pou homephone: 555-222-2221 mail: pere@edt.org description: Watch out for this guy ou: Profes uid: pere uidNumber: 5001 gidNumber: 612 homeDirectory: /tmp/home/pere userPassword: {SSHA}ghmtRL11YtXoUhIP7z6f7nb8RCNadFe+ DB_CONFIG configuracion de bbdd: # $OpenLDAP$ # Example DB_CONFIG file for use with slapd(8) BDB/HDB databases. # # See the Oracle Berkeley DB documentation # <http://www.oracle.com/technology/documentation/berkeley-db/db/ref/env/db_config.html> # for detail description of DB_CONFIG syntax and semantics. # # Hints can also be found in the OpenLDAP Software FAQ # <http://www.openldap.org/faq/index.cgi?file=2> # in particular: # <http://www.openldap.org/faq/index.cgi?file=1075> # Note: most DB_CONFIG settings will take effect only upon rebuilding # the DB environment. # one 0.25 GB cache set_cachesize 0 268435456 1 # Data Directory #set_data_dir db # Transaction Log settings set_lg_regionmax 262144 set_lg_bsize 2097152 #set_lg_dir logs # Note: special DB_CONFIG flags are no longer needed for \"quick\" # slapadd(8) or slapindex(8) access (see their -q option).","title":"FICHEROS LDAP"},{"location":"ldap/#schema","text":"Ejemplo de nombre.schema : attributetype ( 1.1.2.1.1 NAME 'x-equip' DESC 'equip del futbolista' EQUALITY caseIgnoreMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE ) attributetype ( 1.1.2.1.2 NAME 'x-dorsal' DESC 'dorsal del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.27 SINGLE-VALUE ) attributetype ( 1.1.2.1.3 NAME 'x-web' DESC 'pagina web del futbolista' EQUALITY caseExactMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 ) attributetype ( 1.1.2.1.4 NAME 'x-foto' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) attributetype ( 1.1.2.1.5 NAME 'x-lesionat' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.7 SINGLE-VALUE ) objectclass ( 1.1.2.2.1 NAME 'x-futbolistes' DESC 'futboleros' SUP inetOrgPerson STRUCTURAL MUST x-equip MAY ( x-dorsal $ x-web $ x-foto $ x-lesionat ) ) Creacion de un usuario con este schema: dn: cn=kaka,ou=Productes,dc=edt,dc=org objectclass: x-futbolistes cn: kaka sn: kaka x-equip: los pimientos x-dorsal: 7 x-web: www.kaka.com x-foto: //var/tmp/foto.jpg x-lesionat: FALSE A\u00f1adimos en el fichero slapd.conf el schema: include /opt/docker/futbolista-C.schema","title":"SCHEMA"},{"location":"ldap/#acl","text":"En el slapd.conf est\u00e1 el user y pass del ACL. Ejemplos de ACL, son directrices y permisos que tienen ciertos grupos, usuarios etc: Implementar a la base de dades edt.org les seg\u00fcents ACLS: 1. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant de l\u2019administrador i t\u00e9 permisos per modificar-ho tot. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- **[implicitas aunque no salgan] [isx46410800@miguel-fedora27 ldapserver19:acl]$ ldapmodify -vx -c -h 172.17.0.2 -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi -f mod01.ldif ldap_initialize( ldap://172.17.0.2 ) replace mail: newmarta10@edt.org modifying entry \"cn=Marta Mas,ou=usuaris,dc=edt,dc=org\" ldap_modify: Insufficient access (50) replace mail: newmjordi10@edt.org modifying entry \"cn=Jordi Mas,ou=usuaris,dc=edt,dc=org\" modify complete # # 2. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant d\u2019administraci\u00f3. Tothom es pot modificar el seu propi email i homePhone. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- # # 3. Tot usuari es pot modificar el seu mail. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self write by * read [by * none] olcAccess: to * by * read [by * none] [acces to * by * none] --- # # 4. Tothom pot veure totes les dades de tothom, excepte els mail dels altres. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self read [by * none] olcAccess: to * by * read [acces to * by * none] --- [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna dn mail # # 5. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * read olcAccess: to * by * read --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword -- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --puedo ver todos los userpassword como ldapsearch -x -LLL ** by auth no es necesario porqie todo el mundo podemos ver el password y no requiere de identificacion # 6. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom, excepte els altres passwords. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth olcAccess: to * by * read --- **para que nadie lo vea, puedas cambiarlo, tienes que tener primero permiso de auth para autenticarte primero, sino serias un anonimo de fuera ----->puedo cambiar pass mio pero no otro [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s ann2 --->no puedo ver los userpassword con ldapsearch -x -LLL ni con [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword --> solo vere el de jordi por ser authorizado **sino ponemos el by auth, no podremos autenticarnos ya que al hacer la orden somos un user anonymous y hemos de hacer BIND con el -D para identificarnos y solo se consigue poniendo by auth -->** [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword ldap_bind: Invalid credentials (49)--> no deja sin el by auth # # 7. Tot usuari es pot modificar el seu propi password i tot usuari nom\u00e9s pot veure les seves pr\u00f2pies dades. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth [by * none] olcAccess: to * by self read by * search [by * none] ** permet la capacitat de llegir i navegar per tot el arbre ** si ponemos solo by self read, no podremos ver por toodo el contenido para poder ver sus datos y no es capaz de autenticarte. --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --> podemos cambiar nuestro pass pero no el de otro -->con el by * search tendremos acceso a los campos propios [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn: cn=Jordi Mas,ou=usuaris,dc=edt,dc=org objectClass: posixAccount **nos permite de todos los usuarios, ver solo el nuestro. en este caso jordi solo ve el suyo **como anonimo no sale nada porque no tiene datos dentro de la bbdd # 8. Tot usuari pot observar les seves pr\u00f2pies dades i modificar el seu propi password,email i homephone. L\u2019usuari \u201cAnna Pou\u201d pot modificar tots els atributs de tots excepte els passwords, que tampoc pot veure. L\u2019usuari \u201cPere Pou\u201d pot modificar els passwords de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by dn.exact=\"cn=Pere Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * auth [by * none] olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to * by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self read by * search [by * none] --- * by * search para poder ver todas las dades del arbol y hacer match cuando encuentre el suyo que entonces podra ver, el resto no.","title":"ACL"},{"location":"ldap/#ordenes-ldap","text":"rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* slaptest -F /etc/openldap/slapd.d/ -f /opt/docker/slapd.conf slapadd -f /etc/openldap/slapd.d -l /opt/docker/usuarios.ldif /sbin/slapd -d0 slapcat -n0 | grep dn ldapsearch -x -LLL -h ipLdap -b 'dc=edt,dc=org' ldapdelete -vx -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' ldapadd -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Sysadmin,cn=config' -w syskey 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif slappasswd -> genera passwd sha de tipo SHA slappasswd -h {md5/crypt} ldapwhoami -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna ldappassword -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s annanew ldapcompare -x -h ipLDAP 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' homePhone=555-222-222 slapacl -b 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' 'mail'","title":"ORDENES LDAP"},{"location":"linux/","text":"Comandos LINUX Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd COMANDOS SYSADMIN Linux Commands frequently used by Linux Sysadmins \u2013 Part 1: 1. ip \u2013 from Iproute2, a collection of utilities for controlling TCP/IP networking and traffic control in Linux. 2. ls \u2013 list directory contents. 3. df \u2013 display disk space usage. 4. du \u2013 estimate file space usage. 5. free \u2013 display memory usage. 6. scp \u2013 securely Copy Files Using SCP, with examples. 7. find \u2013 locates files based on some user-specified criteria. 8. ncdu \u2013 a disk utility for Unix systems. 9. pstree \u2013 display a tree of processes. 10. last \u2013 show a listing of last logged in users. 11. w \u2013 show a list of currently logged in user sessions. 12. grep \u2013 Search a file for a pattern of characters, then display all matching lines. Linux Commands frequently used by Linux Sysadmins \u2013 Part 2: 13. uptime \u2013 shows system uptime and load average. 14. top \u2013 shows an overall system view. 15. vmstat \u2013 shows system memory, processes, interrupts, paging, block I/O, and CPU info. 16. htop \u2013 interactive process viewer and manager. 17. dstat \u2013 view processes, memory, paging, I/O, CPU, etc., in real-time. All-in-one for vmstat, iostat, netstat, and ifstat. 18. iftop \u2013 network traffic viewer. 19. nethogs \u2013 network traffic analyzer. 20. iotop \u2013 interactive I/O viewer. Get an overview of storage r/w activity. 21. iostat \u2013 for storage I/O statistics. 22. netstat \u2013 for network statistics. 23. ss \u2013 utility to investigate sockets. 24. atop \u2013 For Linux server performance analysis. 25. Glances and nmon \u2013 htop and top Alternatives: 26. ssh \u2013 secure command-line access to remote Linux systems. 27. sudo \u2013 execute commands with administrative privilege. 28. cd \u2013 directory navigation. 29. pwd \u2013 shows your current directory location. 30. cp \u2013 copying files and folders. 31. mv \u2013 moving files and folders. 32. rm \u2013 removing files and folders. 33. mkdir \u2013 create or make new directories. 34. touch \u2013 used to update the access date and/or modification date of a computer file or directory. 35. man \u2013 for reading system reference manuals. 36. apropos \u2013 Search man page names and descriptions. Linux Commands frequently used by Linux Sysadmins \u2013 Part 3: 37. rsync \u2013 remote file transfers and syncing. 38. tar \u2013 an archiving utility. 39. gzip \u2013 file compression and decompression. 40. b2zip \u2013 similar to gzip. It uses a different compression algorithm. 41. zip \u2013 for packaging and compressing (to archive) files. 42. locate \u2013 search files in Linux. 43. ps \u2013 information about the currently running processes. 44. Making use of Bash scripts. Example: ./bashscript.sh 45. cron \u2013 set up scheduled tasks to run. 46. nmcli \u2013 network management. 47. ping \u2013 send ICMP ECHO_REQUEST to network hosts. 48. traceroute \u2013 check the route packets take to a specified host. 49. mtr \u2013 network diagnostic tool. 50. nslookup \u2013 query Internet name servers (NS) interactively. 51. host \u2013 perform DNS lookups in Linux. 52. dig \u2013 DNS lookup utility. Linux Commands frequently used by Linux Sysadmins \u2013 Part 4: 53. wget \u2013 retrieve files over HTTP, HTTPS, FTP, and FTPS. 54. curl \u2013 transferring data using various network protocols. (supports more protocols than wget) 55. dd \u2013 convert and copy files. 56. fdisk \u2013 manipulate the disk partition table. 57. parted \u2013 for creating and manipulating partition tables. 58. blkid \u2013 command-line utility to locate/print block device attributes. 59. mkfs \u2013 build a Linux file system. 60. fsck \u2013 tool for checking the consistency of a file system. 61. whois \u2013 client for the whois directory service. 62. nc \u2013 command-line networking utility. (Also, see 60 Linux Networking commands and scripts.) 63. umask \u2013 set file mode creation mask. 64. chmod \u2013 change the access permissions of file system objects. 65. chown \u2013 change file owner and group. 66. chroot \u2013 run command or interactive shell with a special root directory. 67. useradd \u2013 create a new user or update default new user information. 68. userdel \u2013 used to delete a user account and all related files. 69. usermod \u2013 used to modify or change any attributes of an existing user account. Linux Commands frequently used by Linux Sysadmins \u2013 Part 5: 70. vi \u2013 text editor. 71. cat \u2013 display file contents. 72. tac \u2013 output file contents, in reverse. 73. more \u2013 display file contents one screen/page at a time. 74. less \u2013 similar to the more command with additional features. 75. tail \u2013 used to display the tail end of a text file or piped data. 76. dmesg \u2013 prints the message buffer of the kernel ring. 77. journalctl \u2013 query the systemd journal. 78. kill \u2013 terminate a process. 79. killall \u2013 Sends a kill signal to all instances of a process by name. 80. sleep \u2013 suspends program execution for a specified time. 81. wait \u2013 Suspend script execution until all jobs running in the background have been terminated. 82. nohup \u2013 Run Commands in the Background. 83. screen \u2013 hold a session open on a remote server. (also a full-screen window manager) 84. tmux \u2013 a terminal multiplexer. 85. passwd \u2013 change a user\u2019s password. 86. chpassword \u2013 87. mount / umount \u2013 provides access to an entire filesystem in one directory. 88. systemctl \u2013 Managing Services (Daemons). 89. clear \u2013 clears the screen of the terminal. 90. env -Run a command in a modified environment. Misc commands: 91. cheat \u2013 allows you to create and view interactive cheatsheets on the command-line.\u201d 92. tldr \u2013 Collaborative cheatsheets for console commands. 93. bashtop \u2013 the \u2018cool\u2019 top alternative. 94. bpytop \u2013 Python port of bashtop. This list of Linux Networking commands and scripts will receive ongoing updates, similar to the other lists on this blog\u2026 aria2 \u2013 downloading just about everything. Torrents included. arpwatch \u2013 Ethernet Activity Monitor. bmon \u2013 bandwidth monitor and rate estimator. bwm-ng \u2013 live network bandwidth monitor. curl \u2013 transferring data with URLs. (or try httpie) darkstat \u2013 captures network traffic, usage statistics. dhclient \u2013 Dynamic Host Configuration Protocol Client dig \u2013 query DNS servers for information. dstat \u2013 replacement for vmstat, iostat, mpstat, netstat and ifstat. ethtool \u2013 utility for controlling network drivers and hardware. gated \u2013 gateway routing daemon. host \u2013 DNS lookup utility. hping \u2013 TCP/IP packet assembler/analyzer. ibmonitor \u2013 shows bandwidth and total data transferred. ifstat \u2013 report network interfaces bandwidth. iftop \u2013 display bandwidth usage. ip (PDF file) \u2013 a command with more features that ifconfig (net-tools). iperf3 \u2013 network bandwidth measurement tool. (above screenshot Stacklinux VPS) iproute2 \u2013 collection of utilities for controlling TCP/IP. iptables \u2013 take control of network traffic. IPTraf \u2013 An IP Network Monitor. iputils \u2013 set of small useful utilities for Linux networking. iw \u2013 a new nl80211 based CLI configuration utility for wireless devices. jwhois (whois) \u2013 client for the whois service. \u201clsof -i\u201d \u2013 reveal information about your network sockets. mtr \u2013 network diagnostic tool. net-tools \u2013 utilities include: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool, iptunnel and ipmaddr. ncat \u2013 improved re-implementation of the venerable netcat. netcat \u2013 networking utility for reading/writing network connections. nethogs \u2013 a small \u2018net top\u2019 tool. Netperf \u2013 Network bandwidth Testing. netplan \u2013 Netplan is a utility for easily configuring networking on a linux system. netsniff-ng \u2013 Swiss army knife for daily Linux network plumbing. netwatch \u2013 monitoring Network Connections. ngrep \u2013 grep applied to the network layer. nload \u2013 display network usage. nmap \u2013 network discovery and security auditing. nmcli \u2013 a command-line tool for controlling NetworkManager and reporting network status. nmtui \u2013 provides a text interface to configure networking by controlling NetworkManager. nslookup \u2013 query Internet name servers interactively. ping \u2013 send icmp echo_request to network hosts. route \u2013 show / manipulate the IP routing table. slurm \u2013 network load monitor. snort \u2013 Network Intrusion Detection and Prevention System. smokeping \u2013 keeps track of your network latency. socat \u2013 establishes two bidirectional byte streams and transfers data between them. speedometer \u2013 Measure and display the rate of data across a network. speedtest-cli \u2013 test internet bandwidth using speedtest.net ss \u2013 utility to investigate sockets. ssh \u2013 secure system administration and file transfers over insecure networks. tcpdump \u2013 command-line packet analyzer. tcptrack \u2013 Displays information about tcp connections on a network interface. telnet \u2013 user interface to the TELNET protocol. tracepath \u2013 very similar function to traceroute. traceroute \u2013 print the route packets trace to network host. vnStat \u2013 network traffic monitor. websocat \u2013 Connection forwarder from/to web sockets to/from usual sockets, in style of socat. wget \u2013 retrieving files using HTTP, HTTPS, FTP and FTPS. Wireless Tools for Linux \u2013 includes iwconfig, iwlist, iwspy, iwpriv and ifrename. Wireshark \u2013 network protocol analyzer.","title":"Linux"},{"location":"linux/#comandos-linux","text":"Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd","title":"Comandos LINUX"},{"location":"linux/#comandos-sysadmin","text":"Linux Commands frequently used by Linux Sysadmins \u2013 Part 1: 1. ip \u2013 from Iproute2, a collection of utilities for controlling TCP/IP networking and traffic control in Linux. 2. ls \u2013 list directory contents. 3. df \u2013 display disk space usage. 4. du \u2013 estimate file space usage. 5. free \u2013 display memory usage. 6. scp \u2013 securely Copy Files Using SCP, with examples. 7. find \u2013 locates files based on some user-specified criteria. 8. ncdu \u2013 a disk utility for Unix systems. 9. pstree \u2013 display a tree of processes. 10. last \u2013 show a listing of last logged in users. 11. w \u2013 show a list of currently logged in user sessions. 12. grep \u2013 Search a file for a pattern of characters, then display all matching lines. Linux Commands frequently used by Linux Sysadmins \u2013 Part 2: 13. uptime \u2013 shows system uptime and load average. 14. top \u2013 shows an overall system view. 15. vmstat \u2013 shows system memory, processes, interrupts, paging, block I/O, and CPU info. 16. htop \u2013 interactive process viewer and manager. 17. dstat \u2013 view processes, memory, paging, I/O, CPU, etc., in real-time. All-in-one for vmstat, iostat, netstat, and ifstat. 18. iftop \u2013 network traffic viewer. 19. nethogs \u2013 network traffic analyzer. 20. iotop \u2013 interactive I/O viewer. Get an overview of storage r/w activity. 21. iostat \u2013 for storage I/O statistics. 22. netstat \u2013 for network statistics. 23. ss \u2013 utility to investigate sockets. 24. atop \u2013 For Linux server performance analysis. 25. Glances and nmon \u2013 htop and top Alternatives: 26. ssh \u2013 secure command-line access to remote Linux systems. 27. sudo \u2013 execute commands with administrative privilege. 28. cd \u2013 directory navigation. 29. pwd \u2013 shows your current directory location. 30. cp \u2013 copying files and folders. 31. mv \u2013 moving files and folders. 32. rm \u2013 removing files and folders. 33. mkdir \u2013 create or make new directories. 34. touch \u2013 used to update the access date and/or modification date of a computer file or directory. 35. man \u2013 for reading system reference manuals. 36. apropos \u2013 Search man page names and descriptions. Linux Commands frequently used by Linux Sysadmins \u2013 Part 3: 37. rsync \u2013 remote file transfers and syncing. 38. tar \u2013 an archiving utility. 39. gzip \u2013 file compression and decompression. 40. b2zip \u2013 similar to gzip. It uses a different compression algorithm. 41. zip \u2013 for packaging and compressing (to archive) files. 42. locate \u2013 search files in Linux. 43. ps \u2013 information about the currently running processes. 44. Making use of Bash scripts. Example: ./bashscript.sh 45. cron \u2013 set up scheduled tasks to run. 46. nmcli \u2013 network management. 47. ping \u2013 send ICMP ECHO_REQUEST to network hosts. 48. traceroute \u2013 check the route packets take to a specified host. 49. mtr \u2013 network diagnostic tool. 50. nslookup \u2013 query Internet name servers (NS) interactively. 51. host \u2013 perform DNS lookups in Linux. 52. dig \u2013 DNS lookup utility. Linux Commands frequently used by Linux Sysadmins \u2013 Part 4: 53. wget \u2013 retrieve files over HTTP, HTTPS, FTP, and FTPS. 54. curl \u2013 transferring data using various network protocols. (supports more protocols than wget) 55. dd \u2013 convert and copy files. 56. fdisk \u2013 manipulate the disk partition table. 57. parted \u2013 for creating and manipulating partition tables. 58. blkid \u2013 command-line utility to locate/print block device attributes. 59. mkfs \u2013 build a Linux file system. 60. fsck \u2013 tool for checking the consistency of a file system. 61. whois \u2013 client for the whois directory service. 62. nc \u2013 command-line networking utility. (Also, see 60 Linux Networking commands and scripts.) 63. umask \u2013 set file mode creation mask. 64. chmod \u2013 change the access permissions of file system objects. 65. chown \u2013 change file owner and group. 66. chroot \u2013 run command or interactive shell with a special root directory. 67. useradd \u2013 create a new user or update default new user information. 68. userdel \u2013 used to delete a user account and all related files. 69. usermod \u2013 used to modify or change any attributes of an existing user account. Linux Commands frequently used by Linux Sysadmins \u2013 Part 5: 70. vi \u2013 text editor. 71. cat \u2013 display file contents. 72. tac \u2013 output file contents, in reverse. 73. more \u2013 display file contents one screen/page at a time. 74. less \u2013 similar to the more command with additional features. 75. tail \u2013 used to display the tail end of a text file or piped data. 76. dmesg \u2013 prints the message buffer of the kernel ring. 77. journalctl \u2013 query the systemd journal. 78. kill \u2013 terminate a process. 79. killall \u2013 Sends a kill signal to all instances of a process by name. 80. sleep \u2013 suspends program execution for a specified time. 81. wait \u2013 Suspend script execution until all jobs running in the background have been terminated. 82. nohup \u2013 Run Commands in the Background. 83. screen \u2013 hold a session open on a remote server. (also a full-screen window manager) 84. tmux \u2013 a terminal multiplexer. 85. passwd \u2013 change a user\u2019s password. 86. chpassword \u2013 87. mount / umount \u2013 provides access to an entire filesystem in one directory. 88. systemctl \u2013 Managing Services (Daemons). 89. clear \u2013 clears the screen of the terminal. 90. env -Run a command in a modified environment. Misc commands: 91. cheat \u2013 allows you to create and view interactive cheatsheets on the command-line.\u201d 92. tldr \u2013 Collaborative cheatsheets for console commands. 93. bashtop \u2013 the \u2018cool\u2019 top alternative. 94. bpytop \u2013 Python port of bashtop. This list of Linux Networking commands and scripts will receive ongoing updates, similar to the other lists on this blog\u2026 aria2 \u2013 downloading just about everything. Torrents included. arpwatch \u2013 Ethernet Activity Monitor. bmon \u2013 bandwidth monitor and rate estimator. bwm-ng \u2013 live network bandwidth monitor. curl \u2013 transferring data with URLs. (or try httpie) darkstat \u2013 captures network traffic, usage statistics. dhclient \u2013 Dynamic Host Configuration Protocol Client dig \u2013 query DNS servers for information. dstat \u2013 replacement for vmstat, iostat, mpstat, netstat and ifstat. ethtool \u2013 utility for controlling network drivers and hardware. gated \u2013 gateway routing daemon. host \u2013 DNS lookup utility. hping \u2013 TCP/IP packet assembler/analyzer. ibmonitor \u2013 shows bandwidth and total data transferred. ifstat \u2013 report network interfaces bandwidth. iftop \u2013 display bandwidth usage. ip (PDF file) \u2013 a command with more features that ifconfig (net-tools). iperf3 \u2013 network bandwidth measurement tool. (above screenshot Stacklinux VPS) iproute2 \u2013 collection of utilities for controlling TCP/IP. iptables \u2013 take control of network traffic. IPTraf \u2013 An IP Network Monitor. iputils \u2013 set of small useful utilities for Linux networking. iw \u2013 a new nl80211 based CLI configuration utility for wireless devices. jwhois (whois) \u2013 client for the whois service. \u201clsof -i\u201d \u2013 reveal information about your network sockets. mtr \u2013 network diagnostic tool. net-tools \u2013 utilities include: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool, iptunnel and ipmaddr. ncat \u2013 improved re-implementation of the venerable netcat. netcat \u2013 networking utility for reading/writing network connections. nethogs \u2013 a small \u2018net top\u2019 tool. Netperf \u2013 Network bandwidth Testing. netplan \u2013 Netplan is a utility for easily configuring networking on a linux system. netsniff-ng \u2013 Swiss army knife for daily Linux network plumbing. netwatch \u2013 monitoring Network Connections. ngrep \u2013 grep applied to the network layer. nload \u2013 display network usage. nmap \u2013 network discovery and security auditing. nmcli \u2013 a command-line tool for controlling NetworkManager and reporting network status. nmtui \u2013 provides a text interface to configure networking by controlling NetworkManager. nslookup \u2013 query Internet name servers interactively. ping \u2013 send icmp echo_request to network hosts. route \u2013 show / manipulate the IP routing table. slurm \u2013 network load monitor. snort \u2013 Network Intrusion Detection and Prevention System. smokeping \u2013 keeps track of your network latency. socat \u2013 establishes two bidirectional byte streams and transfers data between them. speedometer \u2013 Measure and display the rate of data across a network. speedtest-cli \u2013 test internet bandwidth using speedtest.net ss \u2013 utility to investigate sockets. ssh \u2013 secure system administration and file transfers over insecure networks. tcpdump \u2013 command-line packet analyzer. tcptrack \u2013 Displays information about tcp connections on a network interface. telnet \u2013 user interface to the TELNET protocol. tracepath \u2013 very similar function to traceroute. traceroute \u2013 print the route packets trace to network host. vnStat \u2013 network traffic monitor. websocat \u2013 Connection forwarder from/to web sockets to/from usual sockets, in style of socat. wget \u2013 retrieving files using HTTP, HTTPS, FTP and FTPS. Wireless Tools for Linux \u2013 includes iwconfig, iwlist, iwspy, iwpriv and ifrename. Wireshark \u2013 network protocol analyzer.","title":"COMANDOS SYSADMIN"},{"location":"lvm/","text":"LVM Logical Volum Management 1 particion extensa, 4 primarias, 16 logicas. COMANDOS CREAMOS IMAGEN Y ASIGNAMOS A UN LOOP dd if=/dev/zero of=file.img bs=1024 count=1024 losetup /dev/loop0 file.img losetup -a/-d CREAMOS UN PHYSICAL VOLUM pvcreate /dev/loop0 pvdisplay /dev/loop0 CREAMOS UN VOLUME GROUP vgcreate nameVG /dev/loop0{...} vgdisplay nameVG blkid tree /dev/disk CREAMOS UN LOGICAL VOLUME lvcreate -L tama\u00f1o -n nameLV /dev/nomVG lvcreate -l 100%libre -n nameLV /dev/nomVG lvdisplay /dev/nameVG/nameLV FORMATEAMOS mkfs -t ext4 /dev/nameVG/nameLV mkdir /mnt/dades mount /dev/nameVG/nameLV /mnt/dades df -h -t ext4 EXTENDEMOS vgextend /dev/nameVG /dev/loop2 lvextend -L +30M /dev/nameVG/nameLV /dev/loop2 resize2fs /dev/nameVG/nameLV REDUCIMOS umount /dev/nameVG/nameLV e2fsck -f /dev/nameVG/nameLV resize2fs /dev/nameVG/nameLV 56M mount /dev/nameVG/nameLV /mnt/xxx lvreduce -L 56M -r /dev/nameVG/nameLV DESHACEMOS umount /mnt/dades lvremove /dev/nameVG/nameLV vgremove /dev/nameVG pvremove /dev/loop0 losetup -d /dev/loop0 EJERCICIO PRACTICA RAID RAID 1. Crear tres particions de 5GBytes al HD, corresponents a sda2, sda3 i sda4. Entramos en nuestra tabla de particiones de /dev/sda para crear las particiones: [root@i21 ~]# fdisk /dev/sda creamos 3 particiones primarias(mismos pasos para las 3): opcion n para nueva particion: Command (m for help): n indicamos que es primaria Partition type p primary (0 primary, 1 extended, 3 free) l logical (numbered from 5) Select (default p): p indicamos el numero de particion, enter para por defecto (este caso 2) Partition number (2-4, default 2): indicamos desde donde empieza(por defecto desde el primer sitio libre) First sector (429918208-468862127, default 429918208): indicamos la medida que queremos, en este caso 5GB Last sector, +sectors or +size{K,M,G,T,P} (429918208-468862127, default 468862127): +5G Created a new partition 2 of type 'Linux' and of size 5 GiB. Partition #2 contains a ext4 signature. borramos la firma Do you want to remove the signature? [Y]es/[N]o: y Disk /dev/sda: 223.6 GiB, 240057409536 bytes, 468862128 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x173e314d Device Boot Start End Sectors Size Id Type /dev/sda1 2048 429918207 429916160 205G 5 Extended /dev/sda2 429918208 440403967 10485760 5G 83 Linux /dev/sda3 440403968 450889727 10485760 5G 83 Linux /dev/sda4 450889728 461375487 10485760 5G 83 Linux /dev/sda5 4096 209719295 209715200 100G 83 Linux /dev/sda6 * 209721344 419436543 209715200 100G 83 Linux /dev/sda7 419438592 429918207 10479616 5G 82 Linux swap / Solaris Para finalizar apretamos \u2018w\u2019 para guardar y hacemos un partprobe [root@i21 ~]# partprobe Crear un RAID de nivell 1 utilitzant les particions anteriors. Usar dos discs m\u00e9s un de spare. Mostrar el raid: la descripci\u00f3 i el proc\u00e9s.. creamos el raid 1 con dos discos (sda2 y sda3) y uno de spare(sda4) [root@i21 ~]# mdadm -v --create /dev/md/raid --level=1 --raid-devices=2 /dev/sda2 /dev/sda3 --spare-devices=1 /dev/sda4 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 mdadm: size set to 5238784K Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md/raid started. Vemos la descripcion [root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid1 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 10:57:07 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 17 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 Vemos el proceso [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] md127 : active raid1 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 [2/2] [UU] unused devices: Assignar format al RAID i muntar-lo al directori /mnt/raid. Copiar-hi tot el directori /bin i posar-hi un fitxer xixa.dat de 3G. Mostrar amb df -h l'ocupaci\u00f3. formateamos [root@i21 ~]# mkfs -t ext4 /dev/md/raid mke2fs 1.43.5 (04-Aug-2017) Discarding device blocks: done Creating filesystem with 1309696 4k blocks and 327680 inodes Filesystem UUID: 983460ab-c0f1-41dc-91e0-7c99fa6a266c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done creamos el directorio en el cual montaremos el raid [root@i21 ~]# mkdir /mnt/raid lo montamos [root@i21 ~]# mount /dev/md/raid /mnt/raid vemos que est\u00e1 montado [root@i21 ~]# ll /mnt/raid/ total 16 drwx------. 2 root root 16384 Feb 13 11:02 lost+found copiamos el bin (estaba con simbolic link por lo que copiamos la info real) [root@i21 ~]# cp -r /usr/bin/ /mnt/raid/ creamos un file xixat.dat de 3g [root@i21 ~]# dd if=/dev/zero of=xixa.dat bs=1k count=3M 3145728+0 records in 3145728+0 records out 3221225472 bytes (3.2 GB, 3.0 GiB) copied, 4.41942 s, 729 MB/s copiamos este fichero a mnt/raid [root@i21 ~]# cp xixa.dat /mnt/raid/. vemos el contenido del directorio y su ocupacion [root@i21 ~]# ll /mnt/raid/ total 3145800 dr-xr-xr-x. 2 root root 53248 Feb 13 11:12 bin drwx------. 2 root root 16384 Feb 13 11:02 lost+found -rw-r--r--. 1 root root 3221225472 Feb 13 11:10 xixa.dat [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Passar el RAID a nivell 5. Contesta i mostra quants discs en total t\u00e9 el raid i quants d\u2019actius (proc i descripci\u00f3). pasamos a raid 5 [root@i21 ~]# mdadm --grow /dev/md/raid --level=5 mdadm: level of /dev/md/raid changed to raid5 vemos el proceso [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU] unused devices: vemos como est\u00e1 formado el raid5 [root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:15:04 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 18 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 tiene 3 discos , dos activos y uno en spare Fes els passos necessaris perqu\u00e8 el RAID tingui tres discs actius. Mostrar-ho amb proc i descripci\u00f3. creo un nuevo disco y lo asigno a un loop, despues lo a\u00f1ado al raid 5 [root@i21 ~]# dd if=/dev/zero of=disc04.img bs=1k count=5M 5242880+0 records in 5242880+0 records out 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 7.36486 s, 729 MB/s [root@i21 ~]# losetup /dev/loop0 disc04.img [root@i21 ~]# mdadm --grow /dev/md127 --level=5 mdadm: level of /dev/md127 changed to raid5 [root@i21 ~]# mdadm --grow /dev/md127 --raid-devices=3 --add /dev/loop0 mdadm: added /dev/loop0 provoco un fallo de este disco que estaba activo en el raid [root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid Active Devices : 2 Working Devices : 3 Failed Devices : 1 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Reshape Status : 97% complete Delta Devices : 1, (2->3) Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 45 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 3 7 0 2 faulty /dev/loop0 2 8 4 - spare /dev/sda4 borro el disco y lo vuelvo a\u00f1adir entonces el de spare ocupa su puesto y se activa y el nuevo disco se queda como spare [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --add /dev/loop0 mdadm: added /dev/loop0 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 loop0 3 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:33:48 2020 State : clean Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 67 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 3 7 0 - spare /dev/loop0 provocamos fallo del loop y lo borramos [root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid mostramos resultados [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: Contesta i mostra: l\u2019espai de disc disponible al raid, l\u2019espai actual i l\u2019ocupaci\u00f3 de disc que mostra df del raid muntat. espacio deisponible del raid [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 ocupacion en disco montado [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Fes els canvis pertinents per tal de que el sistema de fitxers ocupi totalment l\u2019espai disponible al raid, i mostra-ho. hacemos un resize2fs para que ocupe todo el espacio [root@i21 ~]# resize2fs /dev/md/raid resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/md/raid is mounted on /mnt/raid; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 2 The filesystem on /dev/md/raid is now 2619392 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Descriu les ordres a fer per tal de poder reiniciar el sistema i disposar del raid actiu. Cal fer-ho (reboot) i mostrar amb df l\u2019ocupaci\u00f3. copiamos en el fichero etc/mdadm.conf la configuracion del raid [root@i21 ~]# mdadm --examine -scan > /etc/mdadm.conf [root@i21 ~]# cat /etc/mdadm.conf ARRAY /dev/md/raid metadata=1.2 UUID=25fba14f:434b6e31:97a0d544:0dd4bfd1 name=i21:raid spares=1 en el fstab ponemos el montaje para que al reiniciarlo salga /dev/md/raid /mnt/raid ext4 defaults 0 0 hacemos el reboot [root@i21 ~]# df -h -t ext4 /dev/md/raid Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Un cop fet desmunta /mnt/raid i elimina el muntatge automatittzat del fstab. Eliminariamos la linea introducida en el fstab [root@i21 ~]# vim /etc/fstab Desmontariamos /mnt/raid [root@i21 ~]# umount /mnt/raid parariamos el raid mdadm \u2013stop /dev/md/raid eliminariamos el fichero de conf /etc/mdadm.conf hariamos un mdadm \u2013zero-superblock a cada particion LVM LVM Partint del RAID creat a l'apartat anterior (Raid 1 dels discs sda2, sda3 i sda4) fer: 1. Crear dins dos Volums L\u00f2gics anomenats hdsystem (1024 MBytes) i hddata (500 Mbytes). Mostrar clarament tots els passos per fer-ho. creamos el pv [root@i21 ~]# pvcreate /dev/md/raid WARNING: ext4 signature detected on /dev/md/raid at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/md/raid. Physical volume \"/dev/md/raid\" successfully created. [root@i21 ~]# vgcreate mydisc /dev/md/raid creamos el vg Volume group \"mydisc\" successfully created creamos las lv [root@i21 ~]# lvcreate -L 1024M -n hdsystem /dev/mydisc Logical volume \"hdsystem\" created. [root@i21 ~]# lvcreate -L 500M -n hddata /dev/mydisc Logical volume \"hddata\" created. Mostrar la informaci\u00f3 del volum f\u00edsic, el grup de volum i els volums l\u00f2gics. info del pv [root@i21 ~]# pvdisplay /dev/md/raid --- Physical volume --- PV Name /dev/md127 VG Name mydisc PV Size 9.99 GiB / not usable 4.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 2557 Free PE 2176 Allocated PE 381 PV UUID 1pbO3s-krID-iQhm-xNzZ-ocAi-N9vw-uXdicW info del vg [root@i21 ~]# vgdisplay /dev/mydisc --- Volume group --- VG Name mydisc System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size <9.99 GiB PE Size 4.00 MiB Total PE 2557 Alloc PE / Size 381 / <1.49 GiB Free PE / Size 2176 / 8.50 GiB VG UUID IzpMcC-71Ri-1Xwc-8Vsw-Pwj1-N6lY-R33fKC info dels lv [root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 0 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 0 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 Muntar a /mnt/hdsystem i a /mnt/hddata els corresponents Volums L\u00f2gics. Copiar-hi a hdsystem tot el contingut de /usr/share/man i a hddata de /usr/share/doc. Mostrar amb df -h l'ocupaci\u00f3. damos formato a las lv [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hdsystem mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 262144 4k blocks and 65536 inodes Filesystem UUID: 16687c70-a2f0-477b-b7cd-7cb1c4a172da Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hddata mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 512000 1k blocks and 128016 inodes Filesystem UUID: 002310bc-b92e-491c-bcb0-52f104a69323 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done creamos los directorios hdsystem y hddata a mnt [root@i21 ~]# mkdir /mnt/hddata [root@i21 ~]# mkdir /mnt/hdsystem montamos los lv [root@i21 ~]# mount /dev/mydisc/hdsystem /mnt/hdsystem [root@i21 ~]# mount /dev/mydisc/hddata /mnt/hddata copiamos los ficheros indicados en su directorio correspondiente y vemos que se haya copiado: [root@i21 ~]# cp -r /usr/share/man /mnt/hdsystem/. [root@i21 ~]# cp -r /usr/share/doc /mnt/hddata/. [root@i21 ~]# ll /mnt/hddata/ total 48 drwxr-xr-x. 1113 root root 34816 Feb 13 12:07 doc drwx------. 2 root root 12288 Feb 13 12:04 lost+found [root@i21 ~]# ll /mnt/hdsystem/ total 20 drwx------. 2 root root 16384 Feb 13 12:04 lost+found drwxr-xr-x. 47 root root 4096 Feb 13 12:06 man vemos la ocupacion [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Fer que aquests canvis siguin permanents en reiniciar el sistema. Mostra que es fa un reinici i els sistemes de fitxers estan muntats i quina \u00e9s la seva ocupaci\u00f3. copiamos las siguientes lineas en el fstab [root@i21 ~]# vim /etc/fstab /dev/mydisc/hddata /mnt/hddata ext4 defaults 0 0 /dev/mydisc/hdsystem /mnt/hdsystem ext4 defaults 0 0 reboot [root@i21 ~]# reboot ocupacion Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Aprofitant que encara hi ha espai disponible assignar un 50% de l'espai lliure al Volum L\u00f2gic hdsystem. Mostrar clarament aquests canvi en el Volum L\u00f2gic. extendemos la lv hdsystem [root@i21 ~]# lvextend -l +50%FREE /dev/mydisc/hdsystem Size of logical volume mydisc/hdsystem changed from 1.00 GiB (256 extents) to 5.25 GiB (1344 extents). Logical volume mydisc/hdsystem successfully resized. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem como vemos que el file sysrem sigue igual , hacemos un resize2fs para que ocupe todo el espacio [root@i21 ~]# resize2fs /dev/mydisc/hdsystem resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/mydisc/hdsystem is mounted on /mnt/system; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/mydisc/hdsystem is now 1376256 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem vemos que ha cambiado la lv [root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 [root@i21 ~]# lvdisplay /dev/mydisc/hdsystem --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 5.25 GiB Current LE 1344 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 [root@i21 ~]# lvdisplay /dev/mydisc/hddata --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 6. Usant l\u2019ordre df -h mostrar que el sisteme de fitxers muntat a /mnt/hdsystem s'ha ampliat fins al m\u00e0xim del seu espai disponible (i si no ho ha fet, fer-ho!). como hemos hecho antes un resize2fs, tenemos el maximo del espacio asignado. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem EXTRA Partint del RAID i el LVM creats en els apartats anteriors fer: 1. Escriu les ordres necess\u00e0ries per eliminar tota l\u2019automatitzaci\u00f3 de l\u2019arrancada. borramos las lineas del fstab [root@i21 ~]# vim /etc/fstab borramos el file de conf de raid para que no detecte nada [root@i21 ~]# rm -rf /etc/mdadm.conf desmontamos los directorios [root@i21 ~]# umount /mnt/raid [root@i21 ~]# umount /mnt/hdsystem [root@i21 ~]# umount /mnt/hddata Escriu les ordres necess\u00e0ries per eliminar els LVM. borramos las lv [root@i21 ~]# lvremove /dev/mydisc/hdsystem Do you really want to remove active logical volume mydisc/hdsystem? [y/n]: y Logical volume \"hdsystem\" successfully removed [root@i21 ~]# lvremove /dev/mydisc/hddata Do you really want to remove active logical volume mydisc/hddata? [y/n]: y Logical volume \"hddata\" successfully removed borramos las vg [root@i21 ~]# vgremove /dev/mydisc Volume group \"mydisc\" successfully removed booramos las pv [root@i21 ~]# pvremove /dev/md/raid Labels on physical volume \"/dev/md/raid\" successfully wiped. Escriu les ordres necess\u00e0ries per eliminar el raid. paramos el raid [root@i21 ~]# mdadm --stop /dev/md/raid mdadm: stopped /dev/md/raid Verifica que les particions sda2, sda3 i sda4 no tenen cap marca especial. hacemos un zero-superblock para eliminar todo tipo de marca [root@i21 ~]# mdadm --zero-superblock /dev/sda2 [root@i21 ~]# mdadm --zero-superblock /dev/sda3 [root@i21 ~]# mdadm --zero-superblock /dev/sda4 verificamos [root@i21 ~]# mdadm --examine /dev/sda2 mdadm: No md superblock detected on /dev/sda2. [root@i21 ~]# mdadm --examine /dev/sda3 mdadm: No md superblock detected on /dev/sda3. [root@i21 ~]# mdadm --examine /dev/sda4 mdadm: No md superblock detected on /dev/sda4.","title":"LVM/RAID"},{"location":"lvm/#lvm","text":"Logical Volum Management 1 particion extensa, 4 primarias, 16 logicas.","title":"LVM"},{"location":"lvm/#comandos","text":"CREAMOS IMAGEN Y ASIGNAMOS A UN LOOP dd if=/dev/zero of=file.img bs=1024 count=1024 losetup /dev/loop0 file.img losetup -a/-d CREAMOS UN PHYSICAL VOLUM pvcreate /dev/loop0 pvdisplay /dev/loop0 CREAMOS UN VOLUME GROUP vgcreate nameVG /dev/loop0{...} vgdisplay nameVG blkid tree /dev/disk CREAMOS UN LOGICAL VOLUME lvcreate -L tama\u00f1o -n nameLV /dev/nomVG lvcreate -l 100%libre -n nameLV /dev/nomVG lvdisplay /dev/nameVG/nameLV FORMATEAMOS mkfs -t ext4 /dev/nameVG/nameLV mkdir /mnt/dades mount /dev/nameVG/nameLV /mnt/dades df -h -t ext4 EXTENDEMOS vgextend /dev/nameVG /dev/loop2 lvextend -L +30M /dev/nameVG/nameLV /dev/loop2 resize2fs /dev/nameVG/nameLV REDUCIMOS umount /dev/nameVG/nameLV e2fsck -f /dev/nameVG/nameLV resize2fs /dev/nameVG/nameLV 56M mount /dev/nameVG/nameLV /mnt/xxx lvreduce -L 56M -r /dev/nameVG/nameLV DESHACEMOS umount /mnt/dades lvremove /dev/nameVG/nameLV vgremove /dev/nameVG pvremove /dev/loop0 losetup -d /dev/loop0","title":"COMANDOS"},{"location":"lvm/#ejercicio-practica","text":"","title":"EJERCICIO PRACTICA"},{"location":"lvm/#raid","text":"RAID 1. Crear tres particions de 5GBytes al HD, corresponents a sda2, sda3 i sda4.","title":"RAID"},{"location":"lvm/#entramos-en-nuestra-tabla-de-particiones-de-devsda-para-crear-las-particiones","text":"[root@i21 ~]# fdisk /dev/sda","title":"Entramos en nuestra tabla de particiones de /dev/sda para crear las particiones:"},{"location":"lvm/#creamos-3-particiones-primariasmismos-pasos-para-las-3","text":"","title":"creamos 3 particiones primarias(mismos pasos para las 3):"},{"location":"lvm/#opcion-n-para-nueva-particion","text":"Command (m for help): n","title":"opcion n para nueva particion:"},{"location":"lvm/#indicamos-que-es-primaria","text":"Partition type p primary (0 primary, 1 extended, 3 free) l logical (numbered from 5) Select (default p): p","title":"indicamos que es primaria"},{"location":"lvm/#indicamos-el-numero-de-particion-enter-para-por-defecto-este-caso-2","text":"Partition number (2-4, default 2):","title":"indicamos el numero de particion, enter para por defecto (este caso 2)"},{"location":"lvm/#indicamos-desde-donde-empiezapor-defecto-desde-el-primer-sitio-libre","text":"First sector (429918208-468862127, default 429918208):","title":"indicamos desde donde empieza(por defecto desde el primer sitio libre)"},{"location":"lvm/#indicamos-la-medida-que-queremos-en-este-caso-5gb","text":"Last sector, +sectors or +size{K,M,G,T,P} (429918208-468862127, default 468862127): +5G Created a new partition 2 of type 'Linux' and of size 5 GiB. Partition #2 contains a ext4 signature.","title":"indicamos la medida que queremos, en este caso 5GB"},{"location":"lvm/#borramos-la-firma","text":"Do you want to remove the signature? [Y]es/[N]o: y Disk /dev/sda: 223.6 GiB, 240057409536 bytes, 468862128 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x173e314d Device Boot Start End Sectors Size Id Type /dev/sda1 2048 429918207 429916160 205G 5 Extended /dev/sda2 429918208 440403967 10485760 5G 83 Linux /dev/sda3 440403968 450889727 10485760 5G 83 Linux /dev/sda4 450889728 461375487 10485760 5G 83 Linux /dev/sda5 4096 209719295 209715200 100G 83 Linux /dev/sda6 * 209721344 419436543 209715200 100G 83 Linux /dev/sda7 419438592 429918207 10479616 5G 82 Linux swap / Solaris","title":"borramos la firma"},{"location":"lvm/#para-finalizar-apretamos-w-para-guardar-y-hacemos-un-partprobe","text":"[root@i21 ~]# partprobe Crear un RAID de nivell 1 utilitzant les particions anteriors. Usar dos discs m\u00e9s un de spare. Mostrar el raid: la descripci\u00f3 i el proc\u00e9s..","title":"Para finalizar apretamos \u2018w\u2019 para guardar y hacemos un partprobe"},{"location":"lvm/#creamos-el-raid-1-con-dos-discos-sda2-y-sda3-y-uno-de-sparesda4","text":"[root@i21 ~]# mdadm -v --create /dev/md/raid --level=1 --raid-devices=2 /dev/sda2 /dev/sda3 --spare-devices=1 /dev/sda4 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 mdadm: size set to 5238784K Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md/raid started.","title":"creamos el raid 1 con dos discos (sda2 y sda3) y uno de spare(sda4)"},{"location":"lvm/#vemos-la-descripcion","text":"[root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid1 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 10:57:07 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 17 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4","title":"Vemos la descripcion"},{"location":"lvm/#vemos-el-proceso","text":"[root@i21 ~]# cat /proc/mdstat Personalities : [raid1] md127 : active raid1 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 [2/2] [UU] unused devices: Assignar format al RAID i muntar-lo al directori /mnt/raid. Copiar-hi tot el directori /bin i posar-hi un fitxer xixa.dat de 3G. Mostrar amb df -h l'ocupaci\u00f3.","title":"Vemos el proceso"},{"location":"lvm/#formateamos","text":"[root@i21 ~]# mkfs -t ext4 /dev/md/raid mke2fs 1.43.5 (04-Aug-2017) Discarding device blocks: done Creating filesystem with 1309696 4k blocks and 327680 inodes Filesystem UUID: 983460ab-c0f1-41dc-91e0-7c99fa6a266c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done","title":"formateamos"},{"location":"lvm/#creamos-el-directorio-en-el-cual-montaremos-el-raid","text":"[root@i21 ~]# mkdir /mnt/raid","title":"creamos el directorio en el cual montaremos el raid"},{"location":"lvm/#lo-montamos","text":"[root@i21 ~]# mount /dev/md/raid /mnt/raid","title":"lo montamos"},{"location":"lvm/#vemos-que-esta-montado","text":"[root@i21 ~]# ll /mnt/raid/ total 16 drwx------. 2 root root 16384 Feb 13 11:02 lost+found","title":"vemos que est\u00e1 montado"},{"location":"lvm/#copiamos-el-bin-estaba-con-simbolic-link-por-lo-que-copiamos-la-info-real","text":"[root@i21 ~]# cp -r /usr/bin/ /mnt/raid/","title":"copiamos el bin (estaba con simbolic link por lo que copiamos la info real)"},{"location":"lvm/#creamos-un-file-xixatdat-de-3g","text":"[root@i21 ~]# dd if=/dev/zero of=xixa.dat bs=1k count=3M 3145728+0 records in 3145728+0 records out 3221225472 bytes (3.2 GB, 3.0 GiB) copied, 4.41942 s, 729 MB/s","title":"creamos un file xixat.dat de 3g"},{"location":"lvm/#copiamos-este-fichero-a-mntraid","text":"[root@i21 ~]# cp xixa.dat /mnt/raid/.","title":"copiamos este fichero a mnt/raid"},{"location":"lvm/#vemos-el-contenido-del-directorio-y-su-ocupacion","text":"[root@i21 ~]# ll /mnt/raid/ total 3145800 dr-xr-xr-x. 2 root root 53248 Feb 13 11:12 bin drwx------. 2 root root 16384 Feb 13 11:02 lost+found -rw-r--r--. 1 root root 3221225472 Feb 13 11:10 xixa.dat [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Passar el RAID a nivell 5. Contesta i mostra quants discs en total t\u00e9 el raid i quants d\u2019actius (proc i descripci\u00f3).","title":"vemos el contenido del directorio y su ocupacion"},{"location":"lvm/#pasamos-a-raid-5","text":"[root@i21 ~]# mdadm --grow /dev/md/raid --level=5 mdadm: level of /dev/md/raid changed to raid5","title":"pasamos a raid 5"},{"location":"lvm/#vemos-el-proceso_1","text":"[root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU] unused devices:","title":"vemos el proceso"},{"location":"lvm/#vemos-como-esta-formado-el-raid5","text":"[root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:15:04 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 18 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 tiene 3 discos , dos activos y uno en spare Fes els passos necessaris perqu\u00e8 el RAID tingui tres discs actius. Mostrar-ho amb proc i descripci\u00f3.","title":"vemos como est\u00e1 formado el raid5"},{"location":"lvm/#creo-un-nuevo-disco-y-lo-asigno-a-un-loop-despues-lo-anado-al-raid-5","text":"[root@i21 ~]# dd if=/dev/zero of=disc04.img bs=1k count=5M 5242880+0 records in 5242880+0 records out 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 7.36486 s, 729 MB/s [root@i21 ~]# losetup /dev/loop0 disc04.img [root@i21 ~]# mdadm --grow /dev/md127 --level=5 mdadm: level of /dev/md127 changed to raid5 [root@i21 ~]# mdadm --grow /dev/md127 --raid-devices=3 --add /dev/loop0 mdadm: added /dev/loop0","title":"creo un nuevo disco y lo asigno a un loop, despues lo a\u00f1ado al raid 5"},{"location":"lvm/#provoco-un-fallo-de-este-disco-que-estaba-activo-en-el-raid","text":"[root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid Active Devices : 2 Working Devices : 3 Failed Devices : 1 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Reshape Status : 97% complete Delta Devices : 1, (2->3) Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 45 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 3 7 0 2 faulty /dev/loop0 2 8 4 - spare /dev/sda4","title":"provoco un fallo de este disco que estaba activo en el raid"},{"location":"lvm/#borro-el-disco-y-lo-vuelvo-anadir","text":"","title":"borro el disco y lo vuelvo a\u00f1adir"},{"location":"lvm/#entonces-el-de-spare-ocupa-su-puesto-y-se-activa-y-el-nuevo-disco-se-queda-como-spare","text":"[root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --add /dev/loop0 mdadm: added /dev/loop0 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 loop0 3 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:33:48 2020 State : clean Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 67 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 3 7 0 - spare /dev/loop0","title":"entonces el de spare ocupa su puesto y se activa y el nuevo disco se queda como spare"},{"location":"lvm/#provocamos-fallo-del-loop-y-lo-borramos","text":"[root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid","title":"provocamos fallo del loop y lo borramos"},{"location":"lvm/#mostramos-resultados","text":"[root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: Contesta i mostra: l\u2019espai de disc disponible al raid, l\u2019espai actual i l\u2019ocupaci\u00f3 de disc que mostra df del raid muntat.","title":"mostramos resultados"},{"location":"lvm/#espacio-deisponible-del-raid","text":"[root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4","title":"espacio deisponible del raid"},{"location":"lvm/#ocupacion-en-disco-montado","text":"[root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Fes els canvis pertinents per tal de que el sistema de fitxers ocupi totalment l\u2019espai disponible al raid, i mostra-ho.","title":"ocupacion en disco montado"},{"location":"lvm/#hacemos-un-resize2fs-para-que-ocupe-todo-el-espacio","text":"[root@i21 ~]# resize2fs /dev/md/raid resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/md/raid is mounted on /mnt/raid; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 2 The filesystem on /dev/md/raid is now 2619392 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Descriu les ordres a fer per tal de poder reiniciar el sistema i disposar del raid actiu. Cal fer-ho (reboot) i mostrar amb df l\u2019ocupaci\u00f3.","title":"hacemos un resize2fs para que ocupe todo el espacio"},{"location":"lvm/#copiamos-en-el-fichero-etcmdadmconf-la-configuracion-del-raid","text":"[root@i21 ~]# mdadm --examine -scan > /etc/mdadm.conf [root@i21 ~]# cat /etc/mdadm.conf ARRAY /dev/md/raid metadata=1.2 UUID=25fba14f:434b6e31:97a0d544:0dd4bfd1 name=i21:raid spares=1","title":"copiamos en el fichero etc/mdadm.conf la configuracion del raid"},{"location":"lvm/#en-el-fstab-ponemos-el-montaje-para-que-al-reiniciarlo-salga","text":"/dev/md/raid /mnt/raid ext4 defaults 0 0","title":"en el fstab ponemos el montaje para que al reiniciarlo salga"},{"location":"lvm/#hacemos-el-reboot","text":"[root@i21 ~]# df -h -t ext4 /dev/md/raid Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Un cop fet desmunta /mnt/raid i elimina el muntatge automatittzat del fstab.","title":"hacemos el reboot"},{"location":"lvm/#eliminariamos-la-linea-introducida-en-el-fstab","text":"[root@i21 ~]# vim /etc/fstab","title":"Eliminariamos la linea introducida en el fstab"},{"location":"lvm/#desmontariamos-mntraid","text":"[root@i21 ~]# umount /mnt/raid","title":"Desmontariamos /mnt/raid"},{"location":"lvm/#parariamos-el-raid","text":"mdadm \u2013stop /dev/md/raid","title":"parariamos el raid"},{"location":"lvm/#eliminariamos-el-fichero-de-conf-etcmdadmconf","text":"","title":"eliminariamos el fichero de conf /etc/mdadm.conf"},{"location":"lvm/#hariamos-un-mdadm-zero-superblock-a-cada-particion","text":"","title":"hariamos un mdadm \u2013zero-superblock a cada particion"},{"location":"lvm/#lvm_1","text":"LVM Partint del RAID creat a l'apartat anterior (Raid 1 dels discs sda2, sda3 i sda4) fer: 1. Crear dins dos Volums L\u00f2gics anomenats hdsystem (1024 MBytes) i hddata (500 Mbytes). Mostrar clarament tots els passos per fer-ho.","title":"LVM"},{"location":"lvm/#creamos-el-pv","text":"[root@i21 ~]# pvcreate /dev/md/raid WARNING: ext4 signature detected on /dev/md/raid at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/md/raid. Physical volume \"/dev/md/raid\" successfully created. [root@i21 ~]# vgcreate mydisc /dev/md/raid","title":"creamos el pv"},{"location":"lvm/#creamos-el-vg","text":"Volume group \"mydisc\" successfully created","title":"creamos el vg"},{"location":"lvm/#creamos-las-lv","text":"[root@i21 ~]# lvcreate -L 1024M -n hdsystem /dev/mydisc Logical volume \"hdsystem\" created. [root@i21 ~]# lvcreate -L 500M -n hddata /dev/mydisc Logical volume \"hddata\" created. Mostrar la informaci\u00f3 del volum f\u00edsic, el grup de volum i els volums l\u00f2gics.","title":"creamos las lv"},{"location":"lvm/#info-del-pv","text":"[root@i21 ~]# pvdisplay /dev/md/raid --- Physical volume --- PV Name /dev/md127 VG Name mydisc PV Size 9.99 GiB / not usable 4.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 2557 Free PE 2176 Allocated PE 381 PV UUID 1pbO3s-krID-iQhm-xNzZ-ocAi-N9vw-uXdicW","title":"info del pv"},{"location":"lvm/#info-del-vg","text":"[root@i21 ~]# vgdisplay /dev/mydisc --- Volume group --- VG Name mydisc System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size <9.99 GiB PE Size 4.00 MiB Total PE 2557 Alloc PE / Size 381 / <1.49 GiB Free PE / Size 2176 / 8.50 GiB VG UUID IzpMcC-71Ri-1Xwc-8Vsw-Pwj1-N6lY-R33fKC","title":"info del vg"},{"location":"lvm/#info-dels-lv","text":"[root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 0 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 0 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 Muntar a /mnt/hdsystem i a /mnt/hddata els corresponents Volums L\u00f2gics. Copiar-hi a hdsystem tot el contingut de /usr/share/man i a hddata de /usr/share/doc. Mostrar amb df -h l'ocupaci\u00f3.","title":"info dels lv"},{"location":"lvm/#damos-formato-a-las-lv","text":"[root@i21 ~]# mkfs -t ext4 /dev/mydisc/hdsystem mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 262144 4k blocks and 65536 inodes Filesystem UUID: 16687c70-a2f0-477b-b7cd-7cb1c4a172da Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hddata mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 512000 1k blocks and 128016 inodes Filesystem UUID: 002310bc-b92e-491c-bcb0-52f104a69323 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done","title":"damos formato a las lv"},{"location":"lvm/#creamos-los-directorios-hdsystem-y-hddata-a-mnt","text":"[root@i21 ~]# mkdir /mnt/hddata [root@i21 ~]# mkdir /mnt/hdsystem","title":"creamos los directorios hdsystem y hddata a mnt"},{"location":"lvm/#montamos-los-lv","text":"[root@i21 ~]# mount /dev/mydisc/hdsystem /mnt/hdsystem [root@i21 ~]# mount /dev/mydisc/hddata /mnt/hddata","title":"montamos los lv"},{"location":"lvm/#copiamos-los-ficheros-indicados-en-su-directorio-correspondiente-y-vemos-que-se-haya-copiado","text":"[root@i21 ~]# cp -r /usr/share/man /mnt/hdsystem/. [root@i21 ~]# cp -r /usr/share/doc /mnt/hddata/. [root@i21 ~]# ll /mnt/hddata/ total 48 drwxr-xr-x. 1113 root root 34816 Feb 13 12:07 doc drwx------. 2 root root 12288 Feb 13 12:04 lost+found [root@i21 ~]# ll /mnt/hdsystem/ total 20 drwx------. 2 root root 16384 Feb 13 12:04 lost+found drwxr-xr-x. 47 root root 4096 Feb 13 12:06 man","title":"copiamos los ficheros indicados en su directorio correspondiente y vemos que se haya copiado:"},{"location":"lvm/#vemos-la-ocupacion","text":"[root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Fer que aquests canvis siguin permanents en reiniciar el sistema. Mostra que es fa un reinici i els sistemes de fitxers estan muntats i quina \u00e9s la seva ocupaci\u00f3.","title":"vemos la ocupacion"},{"location":"lvm/#copiamos-las-siguientes-lineas-en-el-fstab","text":"[root@i21 ~]# vim /etc/fstab /dev/mydisc/hddata /mnt/hddata ext4 defaults 0 0 /dev/mydisc/hdsystem /mnt/hdsystem ext4 defaults 0 0","title":"copiamos las siguientes lineas en el fstab"},{"location":"lvm/#reboot","text":"[root@i21 ~]# reboot","title":"reboot"},{"location":"lvm/#ocupacion","text":"Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Aprofitant que encara hi ha espai disponible assignar un 50% de l'espai lliure al Volum L\u00f2gic hdsystem. Mostrar clarament aquests canvi en el Volum L\u00f2gic.","title":"ocupacion"},{"location":"lvm/#extendemos-la-lv-hdsystem","text":"[root@i21 ~]# lvextend -l +50%FREE /dev/mydisc/hdsystem Size of logical volume mydisc/hdsystem changed from 1.00 GiB (256 extents) to 5.25 GiB (1344 extents). Logical volume mydisc/hdsystem successfully resized. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem","title":"extendemos la lv hdsystem"},{"location":"lvm/#como-vemos-que-el-file-sysrem-sigue-igual-hacemos-un-resize2fs-para-que-ocupe-todo-el-espacio","text":"[root@i21 ~]# resize2fs /dev/mydisc/hdsystem resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/mydisc/hdsystem is mounted on /mnt/system; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/mydisc/hdsystem is now 1376256 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem","title":"como vemos que el file sysrem sigue igual , hacemos un resize2fs para que ocupe todo el espacio"},{"location":"lvm/#vemos-que-ha-cambiado-la-lv","text":"[root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 [root@i21 ~]# lvdisplay /dev/mydisc/hdsystem --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 5.25 GiB Current LE 1344 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 [root@i21 ~]# lvdisplay /dev/mydisc/hddata --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 6. Usant l\u2019ordre df -h mostrar que el sisteme de fitxers muntat a /mnt/hdsystem s'ha ampliat fins al m\u00e0xim del seu espai disponible (i si no ho ha fet, fer-ho!).","title":"vemos que ha cambiado la lv"},{"location":"lvm/#como-hemos-hecho-antes-un-resize2fs-tenemos-el-maximo-del-espacio-asignado","text":"[root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem","title":"como hemos hecho antes un resize2fs, tenemos el maximo del espacio asignado."},{"location":"lvm/#extra","text":"Partint del RAID i el LVM creats en els apartats anteriors fer: 1. Escriu les ordres necess\u00e0ries per eliminar tota l\u2019automatitzaci\u00f3 de l\u2019arrancada.","title":"EXTRA"},{"location":"lvm/#borramos-las-lineas-del-fstab","text":"[root@i21 ~]# vim /etc/fstab","title":"borramos las lineas del fstab"},{"location":"lvm/#borramos-el-file-de-conf-de-raid-para-que-no-detecte-nada","text":"[root@i21 ~]# rm -rf /etc/mdadm.conf","title":"borramos el file de conf de raid para que no detecte nada"},{"location":"lvm/#desmontamos-los-directorios","text":"[root@i21 ~]# umount /mnt/raid [root@i21 ~]# umount /mnt/hdsystem [root@i21 ~]# umount /mnt/hddata Escriu les ordres necess\u00e0ries per eliminar els LVM.","title":"desmontamos los directorios"},{"location":"lvm/#borramos-las-lv","text":"[root@i21 ~]# lvremove /dev/mydisc/hdsystem Do you really want to remove active logical volume mydisc/hdsystem? [y/n]: y Logical volume \"hdsystem\" successfully removed [root@i21 ~]# lvremove /dev/mydisc/hddata Do you really want to remove active logical volume mydisc/hddata? [y/n]: y Logical volume \"hddata\" successfully removed","title":"borramos las lv"},{"location":"lvm/#borramos-las-vg","text":"[root@i21 ~]# vgremove /dev/mydisc Volume group \"mydisc\" successfully removed","title":"borramos las vg"},{"location":"lvm/#booramos-las-pv","text":"[root@i21 ~]# pvremove /dev/md/raid Labels on physical volume \"/dev/md/raid\" successfully wiped. Escriu les ordres necess\u00e0ries per eliminar el raid.","title":"booramos las pv"},{"location":"lvm/#paramos-el-raid","text":"[root@i21 ~]# mdadm --stop /dev/md/raid mdadm: stopped /dev/md/raid Verifica que les particions sda2, sda3 i sda4 no tenen cap marca especial.","title":"paramos el raid"},{"location":"lvm/#hacemos-un-zero-superblock-para-eliminar-todo-tipo-de-marca","text":"[root@i21 ~]# mdadm --zero-superblock /dev/sda2 [root@i21 ~]# mdadm --zero-superblock /dev/sda3 [root@i21 ~]# mdadm --zero-superblock /dev/sda4","title":"hacemos un zero-superblock para eliminar todo tipo de marca"},{"location":"lvm/#verificamos","text":"[root@i21 ~]# mdadm --examine /dev/sda2 mdadm: No md superblock detected on /dev/sda2. [root@i21 ~]# mdadm --examine /dev/sda3 mdadm: No md superblock detected on /dev/sda3. [root@i21 ~]# mdadm --examine /dev/sda4 mdadm: No md superblock detected on /dev/sda4.","title":"verificamos"},{"location":"markdown/","text":"Comandos lenguaje MARKDOWN T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea","title":"Markdown"},{"location":"markdown/#comandos-lenguaje-markdown","text":"T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea","title":"Comandos lenguaje MARKDOWN"},{"location":"office/","text":"OFFICE 365 CURSO OFFICE 365 Microsoft 365 ayuda a los usuarios con la innovaci\u00f3n m\u00e1s reciente en experiencias de productividad nuevas y familiares, como Teams, Word, Excel, PowerPoint, Outlook y Windows. A diferencia de otros servicios de productividad, Microsoft 365 aprende de los usuarios y recopila informaci\u00f3n valiosa a trav\u00e9s de Microsoft Graph para ofrecer experiencias mejoradas que mejoran continuamente a lo largo del tiempo y mantienen protegidos a los usuarios. Microsoft 365 ayuda a las organizaciones con las siguientes caracter\u00edsticas: Productividad y trabajo en equipo: Incluye mensajer\u00eda instant\u00e1nea y reuniones en l\u00ednea con Microsoft Teams, correo electr\u00f3nico y calendarios con Outlook, aplicaciones conocidas de Office en todos los dispositivos, almacenamiento avanzado y uso compartido de archivos con OneDrive para la Empresa, sitios de grupo e intranet, y redes sociales empresariales con Yammer. Administraci\u00f3n de empresaS: Incluye administraci\u00f3n de TI simplificada con Microsoft Endpoint Manager, automatizaci\u00f3n de procesos empresariales, extensibilidad con Teams y Power Platform, telefon\u00eda empresarial y sistema telef\u00f3nico con Teams, administraci\u00f3n de flujos de trabajo y Formularios, inteligencia empresarial con Workplace Analytics y administraci\u00f3n del trabajo con Project Online. Seguridad y cumplimiento: Incluye soluciones de administraci\u00f3n de identidad y acceso, control y protecci\u00f3n de la informaci\u00f3n, protecci\u00f3n contra amenazas, administraci\u00f3n de seguridad, administraci\u00f3n de riesgos internos, administraci\u00f3n de cumplimiento y eDiscovery. Algunos de los componentes de Microsoft 365, como las Aplicaciones de Microsoft 365 y Windows, se entregan con el modelo de software como servicio (SaaS). SaaS es un software que un proveedor de servicios en la nube (CSP) hospeda y administra de forma centralizada para clientes. En general, los CSP proporcionan una versi\u00f3n de una aplicaci\u00f3n para todos los clientes que otorgan mediante una suscripci\u00f3n mensual o anual. Ventajas: Permitir el trabajo en equipo y simplificar el flujo de trabajo:Colaborar, reunirse, llamar y conectar aplicaciones empresariales en un \u00fanico sitio con Microsoft Teams. Productividad desde cualquier lugar: Pase f\u00e1cilmente de ordenadores a dispositivos m\u00f3viles con aplicaciones m\u00f3viles innovadoras y eficaces. Mayor productividad con las herramientas habilitadas para IA: Potencie la creatividad, descubra nuevas perspectivas, mejore las b\u00fasquedas y obtenga asistencia personalizada con caracter\u00edsticas de inteligencia integradas. Productividad en la organizaci\u00f3n: Las organizaciones siempre intentan destacar en un entorno comercial en constante evoluci\u00f3n. Quieren impulsar el crecimiento, reducir los costos y servir mejor a sus clientes. Quieren desbloquear el potencial de sus empleados, impulsar la automatizaci\u00f3n de procesos, capturar el conocimiento colectivo de su organizaci\u00f3n y evitar posibles riesgos de seguridad, cumplimiento normativo y privacidad que puedan interferir en el progreso. Aumentar los conocimientos de la organizaci\u00f3n: Convierta r\u00e1pidamente los datos en perspectivas y ofrezca a los empleados la informaci\u00f3n y la experiencia que necesitan para realizar su trabajo con Workplace Analytics. Administrar todos los puntos de conexi\u00f3n: Implemente una soluci\u00f3n de administraci\u00f3n fluida de un extremo a otro y mejore la visibilidad entre todos los dispositivos conectados con Microsoft Endpoint Manager. Proteger su empresa: Eleve y modernice su seguridad, administre los riesgos y cumpla los est\u00e1ndares de cumplimiento en la nube de confianza de Microsoft. SUSCRIPCIONES Microsoft 365 Enterprise Microsoft 365 Enterprise ofrece servicios de clase empresarial para organizaciones que quieren una soluci\u00f3n de productividad que incluya caracter\u00edsticas seguras de protecci\u00f3n contra amenazas, seguridad, cumplimiento de normas y an\u00e1lisis. Hay tres planes disponibles de Microsoft 365 Enterprise, que le permiten ajustar a\u00fan m\u00e1s lo que se incluye en su implementaci\u00f3n: E3, E5 y F3 (anteriormente conocido como F1). E5 incluye las mismas caracter\u00edsticas que E3 y las herramientas m\u00e1s recientes de protecci\u00f3n contra amenazas avanzada, seguridad y colaboraci\u00f3n. F3 est\u00e1 dise\u00f1ado para trabajadores de primera l\u00ednea mediante recursos y herramientas dedicadas que les permiten dar lo mejor de s\u00ed. Microsoft 365 para empresas Microsoft 365 para empresas est\u00e1 dise\u00f1ado para peque\u00f1as y medianas organizaciones. Como Microsoft 365 Enterprise, Microsoft 365 para empresas ofrece el conjunto completo de herramientas de productividad de Office 365 e incluye caracter\u00edsticas de seguridad y administraci\u00f3n de dispositivos. No incluye algunas de las herramientas de protecci\u00f3n de la informaci\u00f3n, cumplimiento y an\u00e1lisis m\u00e1s avanzadas disponibles para los suscriptores de Enterprise. Se ha dise\u00f1ado para organizaciones que necesitan hasta 300 licencias. Si su organizaci\u00f3n es m\u00e1s grande, tendr\u00e1 que suscribirse a un plan de Microsoft 365 Enterprise. Microsoft 365 Educaci\u00f3n Microsoft 365 Educaci\u00f3n est\u00e1 disponible para organizaciones educativas y permite a los profesores dar rienda suelta a la creatividad, fomentar el trabajo en equipo y proporcionar una experiencia segura y sencilla en una \u00fanica soluci\u00f3n econ\u00f3mica dise\u00f1ada para el \u00e1mbito educativo. Las licencias acad\u00e9micas se pueden modificar para adaptarse a las necesidades de cualquier instituci\u00f3n, incluidas soluciones de productividad y seguridad para profesores, miembros del personal y estudiantes. Microsoft 365 Hogar Microsoft 365 Hogar tiene el prop\u00f3sito de ofrecer las mismas ventajas de productividad en su vida personal y familiar. Microsoft 365 Hogar tiene dos planes: Microsoft 365 Familia y Microsoft 365 Personal. Office Home y Estudiantes 2019 est\u00e1n disponibles como compras de pago \u00fanico, pero no incluyen ninguno de los beneficios de la nube de Microsoft 365. Compare planes para ver el plan que mejor le convenga. CREACIONES GRUPOS y AD Ejercicio 1: Iniciar sesi\u00f3n en el espacio empresarial Abra Microsoft Edge. Vaya a www.office.com. Inicie sesi\u00f3n con las credenciales de la cuenta de administrador global de su espacio empresarial de Office 365. Consulte la introducci\u00f3n al Laboratorio para adquirir un espacio empresarial de prueba de Office 365. Haga clic en el icono Administraci\u00f3n. Ejercicio 2: Explorar el Centro de administraci\u00f3n de Microsoft 365 En el Centro de administraci\u00f3n de Microsoft 365, en el panel de navegaci\u00f3n, seleccione Mostrar todo. Expanda Usuarios y seleccione Usuarios activos. Vea las cuentas disponibles. Haga clic en el primer nombre de usuario de la lista para seleccionarlo. Se abre una hoja en la que se muestra informaci\u00f3n m\u00e1s detallada sobre la cuenta. Para cerrar la hoja, seleccione la X en la esquina superior derecha de la hoja. Expanda Grupos y seleccione Grupos. Si usa una versi\u00f3n de prueba de espacio empresarial de Office 365 reci\u00e9n creada, esta p\u00e1gina probablemente estar\u00e1 vac\u00eda. Si a\u00fan no tiene grupos, haga clic en Agregar un grupo para agregar uno. Expanda Facturaci\u00f3n y seleccione Licencias. Se deber\u00eda mostrar al menos un conjunto de licencias. Ejercicio 3: Explorar el Centro de administraci\u00f3n de Azure Active Directory Expanda centros de administraci\u00f3n y seleccione Azure Active Directory. Observe que se abre una pesta\u00f1a nueva en Microsoft Edge. En el Centro de administraci\u00f3n de Azure Active Directory, en el Panel, seleccione Azure Active Directory en el panel de navegaci\u00f3n. Haga clic en Usuarios. Observe que se muestran las mismas cuentas de usuario de Office 365. Cierre la hoja Usuarios: Todos los usuarios. Observe que en el panel del \u00e1rea Usuarios y grupos se muestra el grupo que cre\u00f3 anteriormente. Puede ver los mismos grupos de Office 365. Puede hacer clic en Buscar un grupo en el \u00e1rea Tareas r\u00e1pidas para buscar un grupo espec\u00edfico. Cierre la hoja Grupos: Todos los grupos. En el panel del Centro de administraci\u00f3n de Azure Active Directory, haga clic en Personalizaci\u00f3n de marca de la empresa. Observe las opciones configuradas para la personalizaci\u00f3n de marca. Cierre la hoja de personalizaci\u00f3n de marca de la empresa. APLICACIONES 365 \u2013 Word \u2013 El famoso procesador de textos \u2013 Excel \u2013 La potente hoja de c\u00e1lculo \u2013 Outlook \u2013 El servicio de correo de Microsoft \u2013 PowerPoint \u2013 El programa para realizar presentaciones \u2013 Access \u2013 Para la creaci\u00f3n de bases de datos \u2013 Skype \u2013 el conocido programa para llamadas VOIP y videollamadas \u2013 OneNote \u2013 una magn\u00edfica aplicaci\u00f3n para tomar notas \u2013 OneDrive \u2013 El servicio en la nube de Microsoft, gratuito pero que con Office 365 se incluye un TB adicional. \u2013 Publisher \u2013 Un software de autoedici\u00f3n para crear folletos, boletines, etc. *** EXCHANGE + Trabaja de forma m\u00e1s inteligente con calendario y correo electr\u00f3nico de categor\u00eda empresarial. + Exchange te ayuda a colaborar en los documentos cr\u00edticos y te proporciona la Bandeja de entrada Prioritarios, que muestra primero los mensajes importantes y se adapta a tu estilo de trabajo para que mejores tu productividad. + Obt\u00e9n acceso a una bandeja de entrada m\u00e1s personalizada con caracter\u00edsticas \u00fatiles y una forma m\u00e1s inteligente y organizada de ver el correo electr\u00f3nico y de realizar con \u00e9l las acciones oportunas. Las mejoras de b\u00fasqueda te ofrecen resultados m\u00e1s r\u00e1pidos y completos. Con los complementos, obtendr\u00e1s una personalizaci\u00f3n y una extensibilidad avanzadas, que te pondr\u00e1n en contacto con servicios modernos y aplicaciones internas de l\u00ednea de negocio. + Organiza tu tiempo con un sistema de calendario que va m\u00e1s all\u00e1 de la programaci\u00f3n b\u00e1sica de citas y compromisos. Captura autom\u00e1ticamente los eventos del correo electr\u00f3nico, como vuelos y reservas de hotel. Y obt\u00e9n sugerencias sobre los lugares en los que puedes reunirte en funci\u00f3n de cu\u00e1l sea tu ubicaci\u00f3n. Aplicaciones web \u00bfCu\u00e1les son todas las aplicaciones que vienen con Office 365? La gran novedad de Office 365 es que algunas de estas aplicaciones tienen una versi\u00f3n web que no son exactamente como la versi\u00f3n de escritorio. Por un lado, no utilizan nuestro disco duro para archivar los documentos creados, si no que se almacenan autom\u00e1ticamente en OneDrive, por lo que podemos acceder a ellos desde cualquier lugar y dispositivo. Por otra parte, las aplicaciones web est\u00e1n conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, favoreciendo la sincronizaci\u00f3n de contenidos. Por otra parte, estas aplicaciones web est\u00e1n mejor conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, con una mejor sincronizaci\u00f3n. Estas son las aplicaciones web que incluye Office 365: Word, Excel, OneNote, Skype y PowerPoint \u2013 b\u00e1sicamente son iguales a las versiones de escritorio, con algunas peque\u00f1as diferencias. Outlook \u2013 Aqu\u00ed comienzan las diferencias: algunas funciones de la versi\u00f3n de escritorio aqu\u00ed aparecen como funciones independientes: \u2013 Contactos \u2013 Calendario \u2013 Tareas Sway \u2013 Una aplicaci\u00f3n para crear informes, presentaciones, recursos de aprendizaje, etc. Flow \u2013 Una herramienta para automatizar tareas, capaz de trabajar con m\u00e1s de 200 servicios entre los que se incluyen Facebook, Instagram, Twitter, Dropbox, etc. Estas son las aplicaciones que podemos encontrar en Office 365. Adem\u00e1s, existen m\u00e1s servicios a\u00f1adidos como soporte t\u00e9cnico, actualizaciones autom\u00e1ticas, etc. APLICACIONES ADICIONALES Aplicaciones y servicios adicionales: Access (solo PC) Advanced Threat Analytics1 Bookings Cloud App Security Microsoft Edge Enterprise Mobility + Security Exchange Forms Intune Microsoft 365 Defender Microsoft Defender para punto de conexi\u00f3n Microsoft Defender for Identity Microsoft Defender para Office 365 Editor Microsoft Microsoft Family Safety Microsoft Lists Microsoft Stream Microsoft To Do Aplicaciones m\u00f3viles MyAnalytics Planner Power Apps Power Automate Project Publisher (solo PC) Microsoft Endpoint Manager SharePoint SharePoint Syntex Skype Skype Empresarial Sway Visio Whiteboard Windows Workplace Analytics Yammer","title":"Office 365"},{"location":"office/#office-365","text":"CURSO OFFICE 365 Microsoft 365 ayuda a los usuarios con la innovaci\u00f3n m\u00e1s reciente en experiencias de productividad nuevas y familiares, como Teams, Word, Excel, PowerPoint, Outlook y Windows. A diferencia de otros servicios de productividad, Microsoft 365 aprende de los usuarios y recopila informaci\u00f3n valiosa a trav\u00e9s de Microsoft Graph para ofrecer experiencias mejoradas que mejoran continuamente a lo largo del tiempo y mantienen protegidos a los usuarios. Microsoft 365 ayuda a las organizaciones con las siguientes caracter\u00edsticas: Productividad y trabajo en equipo: Incluye mensajer\u00eda instant\u00e1nea y reuniones en l\u00ednea con Microsoft Teams, correo electr\u00f3nico y calendarios con Outlook, aplicaciones conocidas de Office en todos los dispositivos, almacenamiento avanzado y uso compartido de archivos con OneDrive para la Empresa, sitios de grupo e intranet, y redes sociales empresariales con Yammer. Administraci\u00f3n de empresaS: Incluye administraci\u00f3n de TI simplificada con Microsoft Endpoint Manager, automatizaci\u00f3n de procesos empresariales, extensibilidad con Teams y Power Platform, telefon\u00eda empresarial y sistema telef\u00f3nico con Teams, administraci\u00f3n de flujos de trabajo y Formularios, inteligencia empresarial con Workplace Analytics y administraci\u00f3n del trabajo con Project Online. Seguridad y cumplimiento: Incluye soluciones de administraci\u00f3n de identidad y acceso, control y protecci\u00f3n de la informaci\u00f3n, protecci\u00f3n contra amenazas, administraci\u00f3n de seguridad, administraci\u00f3n de riesgos internos, administraci\u00f3n de cumplimiento y eDiscovery. Algunos de los componentes de Microsoft 365, como las Aplicaciones de Microsoft 365 y Windows, se entregan con el modelo de software como servicio (SaaS). SaaS es un software que un proveedor de servicios en la nube (CSP) hospeda y administra de forma centralizada para clientes. En general, los CSP proporcionan una versi\u00f3n de una aplicaci\u00f3n para todos los clientes que otorgan mediante una suscripci\u00f3n mensual o anual. Ventajas: Permitir el trabajo en equipo y simplificar el flujo de trabajo:Colaborar, reunirse, llamar y conectar aplicaciones empresariales en un \u00fanico sitio con Microsoft Teams. Productividad desde cualquier lugar: Pase f\u00e1cilmente de ordenadores a dispositivos m\u00f3viles con aplicaciones m\u00f3viles innovadoras y eficaces. Mayor productividad con las herramientas habilitadas para IA: Potencie la creatividad, descubra nuevas perspectivas, mejore las b\u00fasquedas y obtenga asistencia personalizada con caracter\u00edsticas de inteligencia integradas. Productividad en la organizaci\u00f3n: Las organizaciones siempre intentan destacar en un entorno comercial en constante evoluci\u00f3n. Quieren impulsar el crecimiento, reducir los costos y servir mejor a sus clientes. Quieren desbloquear el potencial de sus empleados, impulsar la automatizaci\u00f3n de procesos, capturar el conocimiento colectivo de su organizaci\u00f3n y evitar posibles riesgos de seguridad, cumplimiento normativo y privacidad que puedan interferir en el progreso. Aumentar los conocimientos de la organizaci\u00f3n: Convierta r\u00e1pidamente los datos en perspectivas y ofrezca a los empleados la informaci\u00f3n y la experiencia que necesitan para realizar su trabajo con Workplace Analytics. Administrar todos los puntos de conexi\u00f3n: Implemente una soluci\u00f3n de administraci\u00f3n fluida de un extremo a otro y mejore la visibilidad entre todos los dispositivos conectados con Microsoft Endpoint Manager. Proteger su empresa: Eleve y modernice su seguridad, administre los riesgos y cumpla los est\u00e1ndares de cumplimiento en la nube de confianza de Microsoft.","title":"OFFICE 365"},{"location":"office/#suscripciones","text":"Microsoft 365 Enterprise Microsoft 365 Enterprise ofrece servicios de clase empresarial para organizaciones que quieren una soluci\u00f3n de productividad que incluya caracter\u00edsticas seguras de protecci\u00f3n contra amenazas, seguridad, cumplimiento de normas y an\u00e1lisis. Hay tres planes disponibles de Microsoft 365 Enterprise, que le permiten ajustar a\u00fan m\u00e1s lo que se incluye en su implementaci\u00f3n: E3, E5 y F3 (anteriormente conocido como F1). E5 incluye las mismas caracter\u00edsticas que E3 y las herramientas m\u00e1s recientes de protecci\u00f3n contra amenazas avanzada, seguridad y colaboraci\u00f3n. F3 est\u00e1 dise\u00f1ado para trabajadores de primera l\u00ednea mediante recursos y herramientas dedicadas que les permiten dar lo mejor de s\u00ed. Microsoft 365 para empresas Microsoft 365 para empresas est\u00e1 dise\u00f1ado para peque\u00f1as y medianas organizaciones. Como Microsoft 365 Enterprise, Microsoft 365 para empresas ofrece el conjunto completo de herramientas de productividad de Office 365 e incluye caracter\u00edsticas de seguridad y administraci\u00f3n de dispositivos. No incluye algunas de las herramientas de protecci\u00f3n de la informaci\u00f3n, cumplimiento y an\u00e1lisis m\u00e1s avanzadas disponibles para los suscriptores de Enterprise. Se ha dise\u00f1ado para organizaciones que necesitan hasta 300 licencias. Si su organizaci\u00f3n es m\u00e1s grande, tendr\u00e1 que suscribirse a un plan de Microsoft 365 Enterprise. Microsoft 365 Educaci\u00f3n Microsoft 365 Educaci\u00f3n est\u00e1 disponible para organizaciones educativas y permite a los profesores dar rienda suelta a la creatividad, fomentar el trabajo en equipo y proporcionar una experiencia segura y sencilla en una \u00fanica soluci\u00f3n econ\u00f3mica dise\u00f1ada para el \u00e1mbito educativo. Las licencias acad\u00e9micas se pueden modificar para adaptarse a las necesidades de cualquier instituci\u00f3n, incluidas soluciones de productividad y seguridad para profesores, miembros del personal y estudiantes. Microsoft 365 Hogar Microsoft 365 Hogar tiene el prop\u00f3sito de ofrecer las mismas ventajas de productividad en su vida personal y familiar. Microsoft 365 Hogar tiene dos planes: Microsoft 365 Familia y Microsoft 365 Personal. Office Home y Estudiantes 2019 est\u00e1n disponibles como compras de pago \u00fanico, pero no incluyen ninguno de los beneficios de la nube de Microsoft 365. Compare planes para ver el plan que mejor le convenga.","title":"SUSCRIPCIONES"},{"location":"office/#creaciones-grupos-y-ad","text":"Ejercicio 1: Iniciar sesi\u00f3n en el espacio empresarial Abra Microsoft Edge. Vaya a www.office.com. Inicie sesi\u00f3n con las credenciales de la cuenta de administrador global de su espacio empresarial de Office 365. Consulte la introducci\u00f3n al Laboratorio para adquirir un espacio empresarial de prueba de Office 365. Haga clic en el icono Administraci\u00f3n. Ejercicio 2: Explorar el Centro de administraci\u00f3n de Microsoft 365 En el Centro de administraci\u00f3n de Microsoft 365, en el panel de navegaci\u00f3n, seleccione Mostrar todo. Expanda Usuarios y seleccione Usuarios activos. Vea las cuentas disponibles. Haga clic en el primer nombre de usuario de la lista para seleccionarlo. Se abre una hoja en la que se muestra informaci\u00f3n m\u00e1s detallada sobre la cuenta. Para cerrar la hoja, seleccione la X en la esquina superior derecha de la hoja. Expanda Grupos y seleccione Grupos. Si usa una versi\u00f3n de prueba de espacio empresarial de Office 365 reci\u00e9n creada, esta p\u00e1gina probablemente estar\u00e1 vac\u00eda. Si a\u00fan no tiene grupos, haga clic en Agregar un grupo para agregar uno. Expanda Facturaci\u00f3n y seleccione Licencias. Se deber\u00eda mostrar al menos un conjunto de licencias. Ejercicio 3: Explorar el Centro de administraci\u00f3n de Azure Active Directory Expanda centros de administraci\u00f3n y seleccione Azure Active Directory. Observe que se abre una pesta\u00f1a nueva en Microsoft Edge. En el Centro de administraci\u00f3n de Azure Active Directory, en el Panel, seleccione Azure Active Directory en el panel de navegaci\u00f3n. Haga clic en Usuarios. Observe que se muestran las mismas cuentas de usuario de Office 365. Cierre la hoja Usuarios: Todos los usuarios. Observe que en el panel del \u00e1rea Usuarios y grupos se muestra el grupo que cre\u00f3 anteriormente. Puede ver los mismos grupos de Office 365. Puede hacer clic en Buscar un grupo en el \u00e1rea Tareas r\u00e1pidas para buscar un grupo espec\u00edfico. Cierre la hoja Grupos: Todos los grupos. En el panel del Centro de administraci\u00f3n de Azure Active Directory, haga clic en Personalizaci\u00f3n de marca de la empresa. Observe las opciones configuradas para la personalizaci\u00f3n de marca. Cierre la hoja de personalizaci\u00f3n de marca de la empresa.","title":"CREACIONES GRUPOS y AD"},{"location":"office/#aplicaciones-365","text":"\u2013 Word \u2013 El famoso procesador de textos \u2013 Excel \u2013 La potente hoja de c\u00e1lculo \u2013 Outlook \u2013 El servicio de correo de Microsoft \u2013 PowerPoint \u2013 El programa para realizar presentaciones \u2013 Access \u2013 Para la creaci\u00f3n de bases de datos \u2013 Skype \u2013 el conocido programa para llamadas VOIP y videollamadas \u2013 OneNote \u2013 una magn\u00edfica aplicaci\u00f3n para tomar notas \u2013 OneDrive \u2013 El servicio en la nube de Microsoft, gratuito pero que con Office 365 se incluye un TB adicional. \u2013 Publisher \u2013 Un software de autoedici\u00f3n para crear folletos, boletines, etc. *** EXCHANGE + Trabaja de forma m\u00e1s inteligente con calendario y correo electr\u00f3nico de categor\u00eda empresarial. + Exchange te ayuda a colaborar en los documentos cr\u00edticos y te proporciona la Bandeja de entrada Prioritarios, que muestra primero los mensajes importantes y se adapta a tu estilo de trabajo para que mejores tu productividad. + Obt\u00e9n acceso a una bandeja de entrada m\u00e1s personalizada con caracter\u00edsticas \u00fatiles y una forma m\u00e1s inteligente y organizada de ver el correo electr\u00f3nico y de realizar con \u00e9l las acciones oportunas. Las mejoras de b\u00fasqueda te ofrecen resultados m\u00e1s r\u00e1pidos y completos. Con los complementos, obtendr\u00e1s una personalizaci\u00f3n y una extensibilidad avanzadas, que te pondr\u00e1n en contacto con servicios modernos y aplicaciones internas de l\u00ednea de negocio. + Organiza tu tiempo con un sistema de calendario que va m\u00e1s all\u00e1 de la programaci\u00f3n b\u00e1sica de citas y compromisos. Captura autom\u00e1ticamente los eventos del correo electr\u00f3nico, como vuelos y reservas de hotel. Y obt\u00e9n sugerencias sobre los lugares en los que puedes reunirte en funci\u00f3n de cu\u00e1l sea tu ubicaci\u00f3n. Aplicaciones web \u00bfCu\u00e1les son todas las aplicaciones que vienen con Office 365? La gran novedad de Office 365 es que algunas de estas aplicaciones tienen una versi\u00f3n web que no son exactamente como la versi\u00f3n de escritorio. Por un lado, no utilizan nuestro disco duro para archivar los documentos creados, si no que se almacenan autom\u00e1ticamente en OneDrive, por lo que podemos acceder a ellos desde cualquier lugar y dispositivo. Por otra parte, las aplicaciones web est\u00e1n conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, favoreciendo la sincronizaci\u00f3n de contenidos. Por otra parte, estas aplicaciones web est\u00e1n mejor conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, con una mejor sincronizaci\u00f3n. Estas son las aplicaciones web que incluye Office 365: Word, Excel, OneNote, Skype y PowerPoint \u2013 b\u00e1sicamente son iguales a las versiones de escritorio, con algunas peque\u00f1as diferencias. Outlook \u2013 Aqu\u00ed comienzan las diferencias: algunas funciones de la versi\u00f3n de escritorio aqu\u00ed aparecen como funciones independientes: \u2013 Contactos \u2013 Calendario \u2013 Tareas Sway \u2013 Una aplicaci\u00f3n para crear informes, presentaciones, recursos de aprendizaje, etc. Flow \u2013 Una herramienta para automatizar tareas, capaz de trabajar con m\u00e1s de 200 servicios entre los que se incluyen Facebook, Instagram, Twitter, Dropbox, etc. Estas son las aplicaciones que podemos encontrar en Office 365. Adem\u00e1s, existen m\u00e1s servicios a\u00f1adidos como soporte t\u00e9cnico, actualizaciones autom\u00e1ticas, etc.","title":"APLICACIONES 365"},{"location":"office/#aplicaciones-adicionales","text":"Aplicaciones y servicios adicionales: Access (solo PC) Advanced Threat Analytics1 Bookings Cloud App Security Microsoft Edge Enterprise Mobility + Security Exchange Forms Intune Microsoft 365 Defender Microsoft Defender para punto de conexi\u00f3n Microsoft Defender for Identity Microsoft Defender para Office 365 Editor Microsoft Microsoft Family Safety Microsoft Lists Microsoft Stream Microsoft To Do Aplicaciones m\u00f3viles MyAnalytics Planner Power Apps Power Automate Project Publisher (solo PC) Microsoft Endpoint Manager SharePoint SharePoint Syntex Skype Skype Empresarial Sway Visio Whiteboard Windows Workplace Analytics Yammer","title":"APLICACIONES ADICIONALES"},{"location":"pam/","text":"PAM Conjunto de librerias que permiten la autenticaci\u00f3n de aplicaciones en el sistema. Dan una API para dar ciertos privilegios a programas para su autenticacaci\u00f3n. Los clientes de PAM son las aplicaciones que necesitan la autenticaci\u00f3n. API cuando se proporcionan preguntas y te retorna respuestas. APP Pam Aware es una aplicaci\u00f3n construida para utilizar pam a traves de su API. Ir\u00e1 a mirar el fichero de /etc/pam.d Modulos plugables en /usr/lib64/security Linea de un file PAM: type - control - module_path - module_arguments Types: auth, account, password, session. Control: required, requisite, sufficient, optional, include, substack. pam_echo: siempre da SUCCESS pam_permit: permite cambiar chfn sin poner paasswd pam_deny: niega que puedas cambiar pam_unix: permite si es un user valid ldd /usr/lib/chfn ver dependencias Calidad de un password: /etc/pam.d/passwd // /etc/security/pwquality.conf Time: /etc/security/time.conf Definimos volumen a compartir en: /etc/security/pam_mount.conf.xml Comprobamos conexion con LDAP con getent passwd user/ getent group group Ficheros importantes de nscd.conf, nslcd.conf y nsswitch.conf INSTALACION Dockerfile: # hostpam FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"hostpam:scratch\" RUN dnf install -y vim util-linux-user-2.30.2-3.fc27.x86_64 finger passwd pam_mount nss-pam-ldapd authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker Startup.sh: #! /bin/bash bash /opt/docker/install.sh /sbin/nscd /sbin/nslcd -d Install.sh: #! /bin/bash useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 cp /opt/docker/login.defs /etc/login.defs cp /opt/docker/nslcd.conf /etc/nslcd.conf cp /opt/docker/nslcd.conf /etc/nscd.conf cp /opt/docker/nsswitch.conf /etc/nsswitch.conf authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall #bash /opt/docker/auth.sh FICHEROS nsswitch.conf: # # /etc/nsswitch.conf # # An example Name Service Switch config file. This file should be # sorted with the most-used services at the beginning. # # The entry '[NOTFOUND=return]' means that the search for an # entry should stop if the search in the previous entry turned # up nothing. Note that if the search failed due to some other reason # (like no NIS server responding) then the search continues with the # next entry. # # Valid entries include: # # nisplus Use NIS+ (NIS version 3) # nis Use NIS (NIS version 2), also called YP # dns Use DNS (Domain Name Service) # files Use the local files # db Use the local database (.db) files # compat Use NIS on compat mode # hesiod Use Hesiod for user lookups # [NOTFOUND=return] Stop searching if not found so far # # To use db, put the \"db\" in front of \"files\" for entries you want to be # looked up first in the databases # # Example: #passwd: db files nisplus nis #shadow: db files nisplus nis #group: db files nisplus nis passwd: files ldap systemd shadow: files ldap group: files ldap systemd #hosts: db files nisplus nis dns hosts: files dns myhostname # Example - obey only what nisplus tells us... #services: nisplus [NOTFOUND=return] files #networks: nisplus [NOTFOUND=return] files #protocols: nisplus [NOTFOUND=return] files #rpc: nisplus [NOTFOUND=return] files #ethers: nisplus [NOTFOUND=return] files #netmasks: nisplus [NOTFOUND=return] files bootparams: nisplus [NOTFOUND=return] files ethers: files netmasks: files networks: files protocols: files rpc: files services: files sss netgroup: nisplus sss publickey: nisplus automount: files nisplus aliases: files nisplus nslcd.conf: # This is the configuration file for the LDAP nameservice # switch library's nslcd daemon. It configures the mapping # between NSS names (see /etc/nsswitch.conf) and LDAP # information in the directory. # See the manual page nslcd.conf(5) for more information. # The user and group nslcd should run as. uid nslcd gid ldap # The uri pointing to the LDAP server to use for name lookups. # Multiple entries may be specified. The address that is used # here should be resolvable without using LDAP (obviously). #uri ldap://127.0.0.1/ #uri ldaps://127.0.0.1/ #uri ldapi://%2fvar%2frun%2fldapi_sock/ # Note: %2f encodes the '/' used as directory separator uri ldap://ldapserver # The LDAP version to use (defaults to 3 # if supported by client library) #ldap_version 3 # The distinguished name of the search base. base dc=edt,dc=org nscd.conf no hacemos nada Login.defs.conf: # # Please note that the parameters in this configuration file control the # behavior of the tools from the shadow-utils component. None of these # tools uses the PAM mechanism, and the utilities that use PAM (such as the # passwd command) should therefore be configured elsewhere. Refer to # /etc/pam.d/system-auth for more information. # # *REQUIRED* # Directory where mailboxes reside, _or_ name of file, relative to the # home directory. If you _do_ define both, MAIL_DIR takes precedence. # QMAIL_DIR is for Qmail # #QMAIL_DIR Maildir MAIL_DIR /var/spool/mail #MAIL_FILE .mail # Password aging controls: # # PASS_MAX_DAYS Maximum number of days a password may be used. # PASS_MIN_DAYS Minimum number of days allowed between password changes. # PASS_MIN_LEN Minimum acceptable password length. # PASS_WARN_AGE Number of days warning given before a password expires. # PASS_MAX_DAYS 99999 PASS_MIN_DAYS 0 PASS_MIN_LEN 5 PASS_WARN_AGE 7 # # Min/max values for automatic uid selection in useradd # UID_MIN 1000 UID_MAX 60000 # System accounts SYS_UID_MIN 201 SYS_UID_MAX 999 # # Min/max values for automatic gid selection in groupadd # GID_MIN 1000 GID_MAX 60000 # System accounts SYS_GID_MIN 201 SYS_GID_MAX 999 # # If defined, this command is run when removing a user. # It should remove any at/cron/print jobs etc. owned by # the user to be removed (passed as the first argument). # #USERDEL_CMD /usr/sbin/userdel_local # # If useradd should create home directories for users by default # On RH systems, we do. This option is overridden with the -m flag on # useradd command line. # CREATE_HOME yes # The permission mask is initialized to this value. If not specified, # the permission mask will be initialized to 022. UMASK 077 # This enables userdel to remove user groups if no members exist. # USERGROUPS_ENAB yes # Use SHA512 to encrypt password. ENCRYPT_METHOD SHA512 CHFN_RESTRICT no","title":"PAM"},{"location":"pam/#pam","text":"Conjunto de librerias que permiten la autenticaci\u00f3n de aplicaciones en el sistema. Dan una API para dar ciertos privilegios a programas para su autenticacaci\u00f3n. Los clientes de PAM son las aplicaciones que necesitan la autenticaci\u00f3n. API cuando se proporcionan preguntas y te retorna respuestas. APP Pam Aware es una aplicaci\u00f3n construida para utilizar pam a traves de su API. Ir\u00e1 a mirar el fichero de /etc/pam.d Modulos plugables en /usr/lib64/security Linea de un file PAM: type - control - module_path - module_arguments Types: auth, account, password, session. Control: required, requisite, sufficient, optional, include, substack. pam_echo: siempre da SUCCESS pam_permit: permite cambiar chfn sin poner paasswd pam_deny: niega que puedas cambiar pam_unix: permite si es un user valid ldd /usr/lib/chfn ver dependencias Calidad de un password: /etc/pam.d/passwd // /etc/security/pwquality.conf Time: /etc/security/time.conf Definimos volumen a compartir en: /etc/security/pam_mount.conf.xml Comprobamos conexion con LDAP con getent passwd user/ getent group group Ficheros importantes de nscd.conf, nslcd.conf y nsswitch.conf","title":"PAM"},{"location":"pam/#instalacion","text":"Dockerfile: # hostpam FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"hostpam:scratch\" RUN dnf install -y vim util-linux-user-2.30.2-3.fc27.x86_64 finger passwd pam_mount nss-pam-ldapd authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker Startup.sh: #! /bin/bash bash /opt/docker/install.sh /sbin/nscd /sbin/nslcd -d Install.sh: #! /bin/bash useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 cp /opt/docker/login.defs /etc/login.defs cp /opt/docker/nslcd.conf /etc/nslcd.conf cp /opt/docker/nslcd.conf /etc/nscd.conf cp /opt/docker/nsswitch.conf /etc/nsswitch.conf authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall #bash /opt/docker/auth.sh","title":"INSTALACION"},{"location":"pam/#ficheros","text":"nsswitch.conf: # # /etc/nsswitch.conf # # An example Name Service Switch config file. This file should be # sorted with the most-used services at the beginning. # # The entry '[NOTFOUND=return]' means that the search for an # entry should stop if the search in the previous entry turned # up nothing. Note that if the search failed due to some other reason # (like no NIS server responding) then the search continues with the # next entry. # # Valid entries include: # # nisplus Use NIS+ (NIS version 3) # nis Use NIS (NIS version 2), also called YP # dns Use DNS (Domain Name Service) # files Use the local files # db Use the local database (.db) files # compat Use NIS on compat mode # hesiod Use Hesiod for user lookups # [NOTFOUND=return] Stop searching if not found so far # # To use db, put the \"db\" in front of \"files\" for entries you want to be # looked up first in the databases # # Example: #passwd: db files nisplus nis #shadow: db files nisplus nis #group: db files nisplus nis passwd: files ldap systemd shadow: files ldap group: files ldap systemd #hosts: db files nisplus nis dns hosts: files dns myhostname # Example - obey only what nisplus tells us... #services: nisplus [NOTFOUND=return] files #networks: nisplus [NOTFOUND=return] files #protocols: nisplus [NOTFOUND=return] files #rpc: nisplus [NOTFOUND=return] files #ethers: nisplus [NOTFOUND=return] files #netmasks: nisplus [NOTFOUND=return] files bootparams: nisplus [NOTFOUND=return] files ethers: files netmasks: files networks: files protocols: files rpc: files services: files sss netgroup: nisplus sss publickey: nisplus automount: files nisplus aliases: files nisplus nslcd.conf: # This is the configuration file for the LDAP nameservice # switch library's nslcd daemon. It configures the mapping # between NSS names (see /etc/nsswitch.conf) and LDAP # information in the directory. # See the manual page nslcd.conf(5) for more information. # The user and group nslcd should run as. uid nslcd gid ldap # The uri pointing to the LDAP server to use for name lookups. # Multiple entries may be specified. The address that is used # here should be resolvable without using LDAP (obviously). #uri ldap://127.0.0.1/ #uri ldaps://127.0.0.1/ #uri ldapi://%2fvar%2frun%2fldapi_sock/ # Note: %2f encodes the '/' used as directory separator uri ldap://ldapserver # The LDAP version to use (defaults to 3 # if supported by client library) #ldap_version 3 # The distinguished name of the search base. base dc=edt,dc=org nscd.conf no hacemos nada Login.defs.conf: # # Please note that the parameters in this configuration file control the # behavior of the tools from the shadow-utils component. None of these # tools uses the PAM mechanism, and the utilities that use PAM (such as the # passwd command) should therefore be configured elsewhere. Refer to # /etc/pam.d/system-auth for more information. # # *REQUIRED* # Directory where mailboxes reside, _or_ name of file, relative to the # home directory. If you _do_ define both, MAIL_DIR takes precedence. # QMAIL_DIR is for Qmail # #QMAIL_DIR Maildir MAIL_DIR /var/spool/mail #MAIL_FILE .mail # Password aging controls: # # PASS_MAX_DAYS Maximum number of days a password may be used. # PASS_MIN_DAYS Minimum number of days allowed between password changes. # PASS_MIN_LEN Minimum acceptable password length. # PASS_WARN_AGE Number of days warning given before a password expires. # PASS_MAX_DAYS 99999 PASS_MIN_DAYS 0 PASS_MIN_LEN 5 PASS_WARN_AGE 7 # # Min/max values for automatic uid selection in useradd # UID_MIN 1000 UID_MAX 60000 # System accounts SYS_UID_MIN 201 SYS_UID_MAX 999 # # Min/max values for automatic gid selection in groupadd # GID_MIN 1000 GID_MAX 60000 # System accounts SYS_GID_MIN 201 SYS_GID_MAX 999 # # If defined, this command is run when removing a user. # It should remove any at/cron/print jobs etc. owned by # the user to be removed (passed as the first argument). # #USERDEL_CMD /usr/sbin/userdel_local # # If useradd should create home directories for users by default # On RH systems, we do. This option is overridden with the -m flag on # useradd command line. # CREATE_HOME yes # The permission mask is initialized to this value. If not specified, # the permission mask will be initialized to 022. UMASK 077 # This enables userdel to remove user groups if no members exist. # USERGROUPS_ENAB yes # Use SHA512 to encrypt password. ENCRYPT_METHOD SHA512 CHFN_RESTRICT no","title":"FICHEROS"},{"location":"python/","text":"Programaci\u00f3n PYTHON Shebang # !/usr/bin/python3 # -*-coding: utf-8-*- Server Python python -m SimpleHTTPDServer 80 Mostrar algo print(\"Uso windows\") Mostrar algo con formato print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}') Importar librerias import platform from xxxx import xx Variables x=100 y=True z=\"Miguel\" Condicional x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\") Bucle for (iterar elementos) food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i) Bucle while food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1 Funciones def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2 Test main if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x) Objetos class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2() Mayusculas, minusculas, letra capital mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize() Ver tipo de dato print(type(w)) #float Listas x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print() Tuplas t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e) Diccionarios dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}') Rangos r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e) List Comprension # de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2) Len len(*args/lista) Objetos # definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\")) Ficheros Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo M\u00f3dulos import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day)","title":"Python"},{"location":"python/#programacion-python","text":"","title":"Programaci\u00f3n PYTHON"},{"location":"python/#shebang","text":"# !/usr/bin/python3 # -*-coding: utf-8-*-","title":"Shebang"},{"location":"python/#server-python","text":"python -m SimpleHTTPDServer 80","title":"Server Python"},{"location":"python/#mostrar-algo","text":"print(\"Uso windows\")","title":"Mostrar algo"},{"location":"python/#mostrar-algo-con-formato","text":"print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}')","title":"Mostrar algo con formato"},{"location":"python/#importar-librerias","text":"import platform from xxxx import xx","title":"Importar librerias"},{"location":"python/#variables","text":"x=100 y=True z=\"Miguel\"","title":"Variables"},{"location":"python/#condicional","text":"x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\")","title":"Condicional"},{"location":"python/#bucle-for-iterar-elementos","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i)","title":"Bucle for (iterar elementos)"},{"location":"python/#bucle-while","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1","title":"Bucle while"},{"location":"python/#funciones","text":"def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2","title":"Funciones"},{"location":"python/#test-main","text":"if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x)","title":"Test main"},{"location":"python/#objetos","text":"class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2()","title":"Objetos"},{"location":"python/#mayusculas-minusculas-letra-capital","text":"mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize()","title":"Mayusculas, minusculas, letra capital"},{"location":"python/#ver-tipo-de-dato","text":"print(type(w)) #float","title":"Ver tipo de dato"},{"location":"python/#listas","text":"x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print()","title":"Listas"},{"location":"python/#tuplas","text":"t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e)","title":"Tuplas"},{"location":"python/#diccionarios","text":"dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}')","title":"Diccionarios"},{"location":"python/#rangos","text":"r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e)","title":"Rangos"},{"location":"python/#list-comprension","text":"# de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2)","title":"List Comprension"},{"location":"python/#len","text":"len(*args/lista)","title":"Len"},{"location":"python/#objetos_1","text":"# definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\"))","title":"Objetos"},{"location":"python/#ficheros","text":"Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo","title":"Ficheros"},{"location":"python/#modulos","text":"import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day)","title":"M\u00f3dulos"},{"location":"routing/","text":"ROUTING DATOS REDES Switch: permite conectar muchos PCs en una misma red Local. Router: permite conectar a otras redes Patch Panel: ayuda al montaje de los switch LAN: Local Area Network, red individual o local WAN: Wide Area Network, conecta dos o m\u00e1s LAN con TSP Internet: interconexion de redes por ISP Modelo TCP/IP: capa acceso a la red(MAC), capa internet(IP), capa transporte(puertos O y D - TCP/UDP) y capa de aplicaci\u00f3n(-/HTTP/FTP) ip a / ip r ifconfig /etc/resolv.conf // nslookup ip/web netstat -putano nc -l 2000 // nc ip/host 2000 traceroute ip/dominio Entre router cables serials y entre router y pc fast ethernet. IPS PUBLICAS/PRIVADAS Estas direcciones IP privadas son: 10.0.0.0 \u2013 10.255.255.255 172.16.0.0 \u2013 172.31.255.255 192.168.0.0 \u2013 192.168.255.255 169.254.0.0 \u2013 169.254.255.255 M\u00e1scaras: 255.0.0.0 / 255.255.0.0 / 255.255.255.0 Cuando una m\u00e1quina con una direcci\u00f3n IP privada quiere conectarse a Internet, deber\u00e1 sustituir esa direcci\u00f3n IP privada en una IP p\u00fablica. Este proceso es conocido como NAT (Network Address Translation). Si tenemos una red con muchos dispositivos con direcciones IP privadas, los routers o los firewalls se encargan de hacer salir a todos esos dispositivos que lo requieran por la misma direcci\u00f3n IP p\u00fablica (a veces puede ser un pool). Cuando el tr\u00e1fico vuelve, estos son capaces de deshacer el cambio de manera que pueden mantenerse todas las comunicaciones. Cuando un dispositivo de red quiere conectarse a trav\u00e9s de Internet normalmente tendr\u00e1 un router para salir a Internet. Este router tiene una direcci\u00f3n IP y a esta direcci\u00f3n IP del router (o firewall) se le denomina \u00abDefault Gateway\u00bb (Puerta de enlace Predeterminada). CALCULO DE IP DE RED, BITS DE HOSTS Y BITS DE RED C\u00e1lculo es IP / MASCARA = RED Se corta el bit para la red hasta que es diferente el bit de mascara y de la ip. Se pone 1 cuando hay 1 arriba y abajo, 0 si no coindicen Ejemplo: # red 10.20.192.7/19 10 . 20.110|00000.00000111 255.255.111|00000.00000000 -------------------------- 10.255.11000000.00000000 10.255.192.0 / 19 --------------------------- primer host 10.255.192.1 /19 10.255.110|00000.00000001 /19 ultimo host 10.255.223.254 /19 10.255.110|11111.11111110 /19 red 10.255.192.0/19 broadcast 10.255.223.255/19 SUBNETTING Numero de dispositivos por red: 2**bitsHosts - 2 Numero de bits para redes: 2**x = numero de subredes Ejemplo: # red 192.168.100.32/20 se quiere 3 subredes(2**2=4) # se necesitaran 2 bits mas de redes para hacer las subredes 192.168.0110|01 00.00100000 /20 192.168.0110|00|00.00000000 /22 >> 192.168.96.0 /22 192.168.0110|01|00.00000000 /22 >> 192.168.100.0 /22 192.168.0110|10|00.00000000 /22 >> 192.168.104.0 /22 192.168.0110|11|00.00000000 /22 >> 192.168.108.0 /22 VLSM - VLANS Mascaras de subred de tama\u00f1o variable. Division de subredes con diferentes dispositivos por cada subred. A partir de una red madre, se va dividiendo seguidamente Ejemplo: # red 192.168.224.0 /20 para dispositivos de 700,200,200,50,2,2 700 >> 2**10-2 200 >> 2**8-2 200 >> 2**8-2 50 >> 2**6-1 2 >> 2**2-2 2 >> 2**2-2 --700-- 192.168.1110|00 00.00000000 /20 192.168.1110 00|00.00000000 /22 >> 192.168.224.0 /22 red 192.168.1110 00|11.11111111 /22 >> 192.168.227.255 /22 broadcast --200-- 192.168.11100100.|00000000 /24 >> 192.168.228.0 /24 red 192.168.11100100.|11111111 /24 >> 192.168.228.255 /24 broadcast --200-- 192.168.11100101.|00000000 /24 >> 192.168.229.0 /24 red 192.168.11100101.|11111111 /24 >> 192.168.229.255 /24 broadcast --50-- 192.168.11100110.00|000000 /26 >> 192.168.230.0 /26 red 192.168.11100110.00|111111 /26 >> 192.168.230.63 /26 broadcast --2-- 192.168.230.010000|00 /30 >> 192.168.230.64 /30 red 192.168.230.010000|11 /30 >> 192.168.230.67 /30 broadcast --2-- 192.168.230.010001|00 /30 >> 192.168.230.68 /30 red 192.168.230.010001|11 /30 >> 192.168.230.71 /30 broadcast CONF ROUTER De una red, ponemos la .1 para el router y la .2 para el primer PC. De una red entre 2 routers, la .1 para uno la .2 para otro. Para configurar la ruta de un router a otro, se indica la red y mascara de la red de destino y la ip del siguiente router por el que tiene que pasar en su entrada de este router. CONFIGURACION ROUTER **CONFIGURACION DE LAS INTERFACES** # //PUERTO SERIAL ENTRE ROUTERS\\\\ Router>enable Router#configure terminal Router(config)#interface serial 0/1/0 (1/0 tambien) Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # //PUERTO FAST ETHERNET ROUTERS-PCS\\\\ Router>enable Router#configure terminal Router(config)#interface fa0/0 Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # ##PARA MANTENER GRABADA LA INFO DEL ROUTER## Router#copy running-config startup-config # # **CONFIGURACION DHCP PARA IP PCS** Router>enable Router#configure terminal Router(config)#ip dhcp excluded-address 172.16.3.1 Router(config)#ip dhcp pool xarxa1 Router(dhcp-config)#network 172.16.3.0 255.255.255.0 Router(dhcp-config)#default-router 172.16.3.1 Router#copy running-config startup-config # # **VER TABLAS DE ENRUTAMIENTO DEL ROUTER** Router#show ip route **CREAR LAS TABLAS DE ENRUTAMIENTO** Router(config)#ip route 172.16.3.0 255.255.255.0 172.16.2.1 (RED DE DESTINO, MASCARA DESTINO, ROUTER POR EL QUE PASAR) # # **COMPROBACION CONEXIONES** ping + ip destino, desde pc origen **VER INTERFACES CONFIGURADAS** router1#show interfaces fastEthernet 0/0 router1#show interfaces serial 0/0/0 Pautas: -VLSM(NUMERO HOSTS 2N-2 O SUBXARXES) Y RUTES RESUM - CONFIGURACION DE SWITCHOS CERRANDO PUERTOS Y HABILITANDO LOS QUE SE USAN PARA PC/ROUTER A TRAVES DE LAS MACS CREANDO VLAN POR CADA PUERTO EN CONCRETO - CONECTAMOS TODOS LOS CABLES SIGUIENDO EL ESQUEMA DE LOS SWITCHOS - PONEMOS LAS IPS DE CADA INTERFAZ Y DHCP EN LOS PCS, NO SERVIDOR. - TABLAS DE ENRUTAMIENTO ESTATICAS( TODAS LAS REDES POR ROUTER Y POR DEFECTO) DINAMICAS(RIP/RIP2) PARA CADA LO QUE TIENE CONECTADA A CADA EXTREMO ROUTER / PC / DHCP Ejemplo: ROUTER 7: PC0: Router>enable Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa1/0 Router(config-if)#ip address 192.168.12.129 255.255.255.192 Router(config-if)#no shutdown router(config-if)# %LINK-5-CHANGED: Interface FastEthernet1/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet1/0, changed state to up # PC1: Router(config)#interface fa6/0 %Invalid interface type and number Router(config)#interface ethernet 6/0 Router(config-if)#ip address 192.168.0.1 255.255.248.0 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface Ethernet6/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet6/0, changed state to up # WAN2: Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa0/0 Router(config-if)#ip address 172.16.240.5 255.255.255.252 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up # WAN3: Router(config-if)#interface serial 2/0 Router(config-if)#ip address 172.16.240.9 255.255.255.252 Router(config-if)#no shutdown %LINK-5-CHANGED: Interface Serial2/0, changed state to down # DHCP: DEP.CF Router(config)#ip dhcp excluded-address 192.168.12.129 Router(config)#ip dhcp pool depcf Router(dhcp-config)#network 192.168.12.128 255.255.255.192 Router(dhcp-config)#default-router 192.168.12.129 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK] # AULESCF Router(config)#ip dhcp excluded-address 192.168.0.1 Router(config)#ip dhcp pool aulescf Router(dhcp-config)#network 192.168.0.0 255.255.248.0 Router(dhcp-config)#default-router 192.168.0.1 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK]","title":"Routing"},{"location":"routing/#routing","text":"","title":"ROUTING"},{"location":"routing/#datos-redes","text":"Switch: permite conectar muchos PCs en una misma red Local. Router: permite conectar a otras redes Patch Panel: ayuda al montaje de los switch LAN: Local Area Network, red individual o local WAN: Wide Area Network, conecta dos o m\u00e1s LAN con TSP Internet: interconexion de redes por ISP Modelo TCP/IP: capa acceso a la red(MAC), capa internet(IP), capa transporte(puertos O y D - TCP/UDP) y capa de aplicaci\u00f3n(-/HTTP/FTP) ip a / ip r ifconfig /etc/resolv.conf // nslookup ip/web netstat -putano nc -l 2000 // nc ip/host 2000 traceroute ip/dominio Entre router cables serials y entre router y pc fast ethernet.","title":"DATOS REDES"},{"location":"routing/#ips-publicasprivadas","text":"Estas direcciones IP privadas son: 10.0.0.0 \u2013 10.255.255.255 172.16.0.0 \u2013 172.31.255.255 192.168.0.0 \u2013 192.168.255.255 169.254.0.0 \u2013 169.254.255.255 M\u00e1scaras: 255.0.0.0 / 255.255.0.0 / 255.255.255.0 Cuando una m\u00e1quina con una direcci\u00f3n IP privada quiere conectarse a Internet, deber\u00e1 sustituir esa direcci\u00f3n IP privada en una IP p\u00fablica. Este proceso es conocido como NAT (Network Address Translation). Si tenemos una red con muchos dispositivos con direcciones IP privadas, los routers o los firewalls se encargan de hacer salir a todos esos dispositivos que lo requieran por la misma direcci\u00f3n IP p\u00fablica (a veces puede ser un pool). Cuando el tr\u00e1fico vuelve, estos son capaces de deshacer el cambio de manera que pueden mantenerse todas las comunicaciones. Cuando un dispositivo de red quiere conectarse a trav\u00e9s de Internet normalmente tendr\u00e1 un router para salir a Internet. Este router tiene una direcci\u00f3n IP y a esta direcci\u00f3n IP del router (o firewall) se le denomina \u00abDefault Gateway\u00bb (Puerta de enlace Predeterminada).","title":"IPS PUBLICAS/PRIVADAS"},{"location":"routing/#calculo-de-ip-de-red-bits-de-hosts-y-bits-de-red","text":"C\u00e1lculo es IP / MASCARA = RED Se corta el bit para la red hasta que es diferente el bit de mascara y de la ip. Se pone 1 cuando hay 1 arriba y abajo, 0 si no coindicen Ejemplo: # red 10.20.192.7/19 10 . 20.110|00000.00000111 255.255.111|00000.00000000 -------------------------- 10.255.11000000.00000000 10.255.192.0 / 19 --------------------------- primer host 10.255.192.1 /19 10.255.110|00000.00000001 /19 ultimo host 10.255.223.254 /19 10.255.110|11111.11111110 /19 red 10.255.192.0/19 broadcast 10.255.223.255/19","title":"CALCULO DE IP DE RED, BITS DE HOSTS Y BITS DE RED"},{"location":"routing/#subnetting","text":"Numero de dispositivos por red: 2**bitsHosts - 2 Numero de bits para redes: 2**x = numero de subredes Ejemplo: # red 192.168.100.32/20 se quiere 3 subredes(2**2=4) # se necesitaran 2 bits mas de redes para hacer las subredes 192.168.0110|01 00.00100000 /20 192.168.0110|00|00.00000000 /22 >> 192.168.96.0 /22 192.168.0110|01|00.00000000 /22 >> 192.168.100.0 /22 192.168.0110|10|00.00000000 /22 >> 192.168.104.0 /22 192.168.0110|11|00.00000000 /22 >> 192.168.108.0 /22","title":"SUBNETTING"},{"location":"routing/#vlsm-vlans","text":"Mascaras de subred de tama\u00f1o variable. Division de subredes con diferentes dispositivos por cada subred. A partir de una red madre, se va dividiendo seguidamente Ejemplo: # red 192.168.224.0 /20 para dispositivos de 700,200,200,50,2,2 700 >> 2**10-2 200 >> 2**8-2 200 >> 2**8-2 50 >> 2**6-1 2 >> 2**2-2 2 >> 2**2-2 --700-- 192.168.1110|00 00.00000000 /20 192.168.1110 00|00.00000000 /22 >> 192.168.224.0 /22 red 192.168.1110 00|11.11111111 /22 >> 192.168.227.255 /22 broadcast --200-- 192.168.11100100.|00000000 /24 >> 192.168.228.0 /24 red 192.168.11100100.|11111111 /24 >> 192.168.228.255 /24 broadcast --200-- 192.168.11100101.|00000000 /24 >> 192.168.229.0 /24 red 192.168.11100101.|11111111 /24 >> 192.168.229.255 /24 broadcast --50-- 192.168.11100110.00|000000 /26 >> 192.168.230.0 /26 red 192.168.11100110.00|111111 /26 >> 192.168.230.63 /26 broadcast --2-- 192.168.230.010000|00 /30 >> 192.168.230.64 /30 red 192.168.230.010000|11 /30 >> 192.168.230.67 /30 broadcast --2-- 192.168.230.010001|00 /30 >> 192.168.230.68 /30 red 192.168.230.010001|11 /30 >> 192.168.230.71 /30 broadcast","title":"VLSM - VLANS"},{"location":"routing/#conf-router","text":"De una red, ponemos la .1 para el router y la .2 para el primer PC. De una red entre 2 routers, la .1 para uno la .2 para otro. Para configurar la ruta de un router a otro, se indica la red y mascara de la red de destino y la ip del siguiente router por el que tiene que pasar en su entrada de este router. CONFIGURACION ROUTER **CONFIGURACION DE LAS INTERFACES** # //PUERTO SERIAL ENTRE ROUTERS\\\\ Router>enable Router#configure terminal Router(config)#interface serial 0/1/0 (1/0 tambien) Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # //PUERTO FAST ETHERNET ROUTERS-PCS\\\\ Router>enable Router#configure terminal Router(config)#interface fa0/0 Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # ##PARA MANTENER GRABADA LA INFO DEL ROUTER## Router#copy running-config startup-config # # **CONFIGURACION DHCP PARA IP PCS** Router>enable Router#configure terminal Router(config)#ip dhcp excluded-address 172.16.3.1 Router(config)#ip dhcp pool xarxa1 Router(dhcp-config)#network 172.16.3.0 255.255.255.0 Router(dhcp-config)#default-router 172.16.3.1 Router#copy running-config startup-config # # **VER TABLAS DE ENRUTAMIENTO DEL ROUTER** Router#show ip route **CREAR LAS TABLAS DE ENRUTAMIENTO** Router(config)#ip route 172.16.3.0 255.255.255.0 172.16.2.1 (RED DE DESTINO, MASCARA DESTINO, ROUTER POR EL QUE PASAR) # # **COMPROBACION CONEXIONES** ping + ip destino, desde pc origen **VER INTERFACES CONFIGURADAS** router1#show interfaces fastEthernet 0/0 router1#show interfaces serial 0/0/0 Pautas: -VLSM(NUMERO HOSTS 2N-2 O SUBXARXES) Y RUTES RESUM - CONFIGURACION DE SWITCHOS CERRANDO PUERTOS Y HABILITANDO LOS QUE SE USAN PARA PC/ROUTER A TRAVES DE LAS MACS CREANDO VLAN POR CADA PUERTO EN CONCRETO - CONECTAMOS TODOS LOS CABLES SIGUIENDO EL ESQUEMA DE LOS SWITCHOS - PONEMOS LAS IPS DE CADA INTERFAZ Y DHCP EN LOS PCS, NO SERVIDOR. - TABLAS DE ENRUTAMIENTO ESTATICAS( TODAS LAS REDES POR ROUTER Y POR DEFECTO) DINAMICAS(RIP/RIP2) PARA CADA LO QUE TIENE CONECTADA A CADA EXTREMO","title":"CONF ROUTER"},{"location":"routing/#router-pc-dhcp","text":"Ejemplo: ROUTER 7: PC0: Router>enable Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa1/0 Router(config-if)#ip address 192.168.12.129 255.255.255.192 Router(config-if)#no shutdown router(config-if)# %LINK-5-CHANGED: Interface FastEthernet1/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet1/0, changed state to up # PC1: Router(config)#interface fa6/0 %Invalid interface type and number Router(config)#interface ethernet 6/0 Router(config-if)#ip address 192.168.0.1 255.255.248.0 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface Ethernet6/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet6/0, changed state to up # WAN2: Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa0/0 Router(config-if)#ip address 172.16.240.5 255.255.255.252 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up # WAN3: Router(config-if)#interface serial 2/0 Router(config-if)#ip address 172.16.240.9 255.255.255.252 Router(config-if)#no shutdown %LINK-5-CHANGED: Interface Serial2/0, changed state to down # DHCP: DEP.CF Router(config)#ip dhcp excluded-address 192.168.12.129 Router(config)#ip dhcp pool depcf Router(dhcp-config)#network 192.168.12.128 255.255.255.192 Router(dhcp-config)#default-router 192.168.12.129 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK] # AULESCF Router(config)#ip dhcp excluded-address 192.168.0.1 Router(config)#ip dhcp pool aulescf Router(dhcp-config)#network 192.168.0.0 255.255.248.0 Router(dhcp-config)#default-router 192.168.0.1 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK]","title":"ROUTER / PC / DHCP"},{"location":"samba/","text":"SAMBA Dos demonios: smbd y nmbd. Fichero de conf smb.conf. Topologia de red Peer to Peer: una red entre iguales, a su bola. Cliente/servidor. Puertos 139(netbios-ssn) y 445(microsoft-ds) testparm test de configuracion de samba smbtree hace un mensaje de broadcaste y salen los que contesten. Forma: \\\\server\\recurs INSTALACION Dockerfile: # Version: 0.0.1 # @edt M06 2019-2020 # samba # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"SAMBA server 2019-2020 - PAM\" RUN dnf -y install procps samba samba-client nss-pam-ldapd cifs-utils passwd pam_mount authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] Authconfig.sh: #! /bin/bash authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall Install.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # instal.lacio # ------------------------------------- # creacio usuaris locals useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 # Configuraci\u00f3 client autenticaci\u00f3 ldap bash /opt/docker/auth.sh # SAMBA ## Configuracio shares Samba mkdir /var/lib/samba/public chmod 777 /var/lib/samba/public cp /opt/docker/* /var/lib/samba/public/. mkdir /var/lib/samba/privat #chmod 777 /var/lib/samba/privat cp /opt/docker/*.md /var/lib/samba/privat/. cp /opt/docker/smb.conf /etc/samba/smb.conf ## creacio usuaris unix locals i samba locals useradd lila useradd roc useradd patipla useradd pla echo -e \"lila\\nlila\" | smbpasswd -a lila echo -e \"roc\\nroc\" | smbpasswd -a roc echo -e \"patipla\\npatipla\" | smbpasswd -a patipla echo -e \"pla\\npla\" | smbpasswd -a pla Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis ldap /sbin/nscd && echo \"nscd Ok\" /sbin/nslcd && echo \"nslcd Ok\" # activar els serveis samba /usr/sbin/smbd && echo \"smb Ok\" # creacion de users samba dels users ldap, creando sus cuentas y directorios bash /opt/docker/usersSambaUnixLdap.sh # servei per a detached /usr/sbin/nmbd -F usersldap.sh: llistaUsers=\"pere marta anna pau pere jordi\" for user in $llistaUsers do echo -e \"$user\\n$user\" | smbpasswd -a $user line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir && echo \"directori home creat\" cp -ra /etc/skel/. $homedir && echo \"skel copiado\" chown -R $uid.$gid $homedir && echo \"chown ok\" fi done for user in user{01..05} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done for user in user{06..10} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done FICHEROS smb.conf: [global] workgroup = MYGROUP server string = Samba Server Version %v log file = /var/log/samba/log.%m max log size = 50 security = user passdb backend = tdbsam load printers = yes cups options = raw [homes] comment = Home Directories browseable = no writable = yes ; valid users = %S ; valid users = MYDOMAIN\\%S [printers] comment = All Printers path = /var/spool/samba browseable = no guest ok = no writable = no printable = yes [documentation] comment = Documentaci\u00f3 doc del container path = /usr/share/doc public = yes browseable = yes writable = no printable = no guest ok = yes [manpages] comment = Documentaci\u00f3 man del container path = /usr/share/man public = yes browseable = yes writable = no printable = no guest ok = yes [public] comment = Share de contingut public path = /var/lib/samba/public public = yes browseable = yes writable = yes printable = no guest ok = yes [privat] comment = Share d'acc\u00e9s privat path = /var/lib/samba/privat public = no browseable = no writable = yes printable = no guest ok = yes ORDENES smbtree smbtree -D smbtree -S smbclient (-N) //server/recurs smbclient //server/recurs -U user%password smbget -R smb://server/recurs/file mount -t cifs //server/share / mnt -o guest /etc/samba/smb.conf /etc/samba/lmhosts useradd lila -> smbpasswd -a lila borrar: smbpasswd -x lila smbclient -d0 //samba/public -u lila pdbedit // pdbedit -Lv","title":"SAMBA"},{"location":"samba/#samba","text":"Dos demonios: smbd y nmbd. Fichero de conf smb.conf. Topologia de red Peer to Peer: una red entre iguales, a su bola. Cliente/servidor. Puertos 139(netbios-ssn) y 445(microsoft-ds) testparm test de configuracion de samba smbtree hace un mensaje de broadcaste y salen los que contesten. Forma: \\\\server\\recurs","title":"SAMBA"},{"location":"samba/#instalacion","text":"Dockerfile: # Version: 0.0.1 # @edt M06 2019-2020 # samba # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"SAMBA server 2019-2020 - PAM\" RUN dnf -y install procps samba samba-client nss-pam-ldapd cifs-utils passwd pam_mount authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] Authconfig.sh: #! /bin/bash authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall Install.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # instal.lacio # ------------------------------------- # creacio usuaris locals useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 # Configuraci\u00f3 client autenticaci\u00f3 ldap bash /opt/docker/auth.sh # SAMBA ## Configuracio shares Samba mkdir /var/lib/samba/public chmod 777 /var/lib/samba/public cp /opt/docker/* /var/lib/samba/public/. mkdir /var/lib/samba/privat #chmod 777 /var/lib/samba/privat cp /opt/docker/*.md /var/lib/samba/privat/. cp /opt/docker/smb.conf /etc/samba/smb.conf ## creacio usuaris unix locals i samba locals useradd lila useradd roc useradd patipla useradd pla echo -e \"lila\\nlila\" | smbpasswd -a lila echo -e \"roc\\nroc\" | smbpasswd -a roc echo -e \"patipla\\npatipla\" | smbpasswd -a patipla echo -e \"pla\\npla\" | smbpasswd -a pla Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis ldap /sbin/nscd && echo \"nscd Ok\" /sbin/nslcd && echo \"nslcd Ok\" # activar els serveis samba /usr/sbin/smbd && echo \"smb Ok\" # creacion de users samba dels users ldap, creando sus cuentas y directorios bash /opt/docker/usersSambaUnixLdap.sh # servei per a detached /usr/sbin/nmbd -F usersldap.sh: llistaUsers=\"pere marta anna pau pere jordi\" for user in $llistaUsers do echo -e \"$user\\n$user\" | smbpasswd -a $user line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir && echo \"directori home creat\" cp -ra /etc/skel/. $homedir && echo \"skel copiado\" chown -R $uid.$gid $homedir && echo \"chown ok\" fi done for user in user{01..05} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done for user in user{06..10} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done","title":"INSTALACION"},{"location":"samba/#ficheros","text":"smb.conf: [global] workgroup = MYGROUP server string = Samba Server Version %v log file = /var/log/samba/log.%m max log size = 50 security = user passdb backend = tdbsam load printers = yes cups options = raw [homes] comment = Home Directories browseable = no writable = yes ; valid users = %S ; valid users = MYDOMAIN\\%S [printers] comment = All Printers path = /var/spool/samba browseable = no guest ok = no writable = no printable = yes [documentation] comment = Documentaci\u00f3 doc del container path = /usr/share/doc public = yes browseable = yes writable = no printable = no guest ok = yes [manpages] comment = Documentaci\u00f3 man del container path = /usr/share/man public = yes browseable = yes writable = no printable = no guest ok = yes [public] comment = Share de contingut public path = /var/lib/samba/public public = yes browseable = yes writable = yes printable = no guest ok = yes [privat] comment = Share d'acc\u00e9s privat path = /var/lib/samba/privat public = no browseable = no writable = yes printable = no guest ok = yes","title":"FICHEROS"},{"location":"samba/#ordenes","text":"smbtree smbtree -D smbtree -S smbclient (-N) //server/recurs smbclient //server/recurs -U user%password smbget -R smb://server/recurs/file mount -t cifs //server/share / mnt -o guest /etc/samba/smb.conf /etc/samba/lmhosts useradd lila -> smbpasswd -a lila borrar: smbpasswd -x lila smbclient -d0 //samba/public -u lila pdbedit // pdbedit -Lv","title":"ORDENES"},{"location":"sap/","text":"SAP CURSO Systeme Anwendungen und Produkte que significa en espa\u00f1ol 'sistemas, aplicaciones y productos'. SAP ERP\u200b es un software de planificaci\u00f3n de recursos empresariales desarrollado por la compa\u00f1\u00eda alemana SAP SE. SAP ERP incorpora las funciones empresariales claves de una organizaci\u00f3n. As\u00ed, si tuvi\u00e9ramos que definir qu\u00e9 es SAP dir\u00edamos que es un software ERP (Enterprise Resource Planning), que permite planificar y gestionar los recursos de todas las \u00e1reas de la empresa: desde log\u00edstica a contabilidad, pasando por el departamento comercial y de m\u00e1rketing, finanzas, producci\u00f3n, gesti\u00f3n de proyectos, de la calidad, mantenimiento o direcci\u00f3n y administraci\u00f3n general. ERP Enterprise Resource Planning, planificaci\u00f3n de recursos empresariales. Es un conjunto de programas integrados que apoya las principales actividades organizacionales tales como finanza, contabilidad, log\u00edstica, producci\u00f3n, ventas y recursos humanos. Caracteristicas: Procesos estandatizados En linea Bases de datos unica Integrar todos los datos y procesos en un sistema \u00fanico. Configurable Modular SAP System Applications and Products in data processing. Sistema, aplicaciones y productos en el procesamiento de datos. ARQUITECTURA Estructura cliente - servidor Estrucutra de ambiente de desarrollo - calidad - producci\u00f3n. SAP GUI es el programa que se instala el ordenador para porder acceder en remoto al servidor en una empresa. SAP LOGON es el programa que se utiliza para entrar o loguearse en SAP. Es como el iconito al clicar dos veces para conectarse y elegir a que servidor conectarse. MODULOS SAP Formado por varios modulos interconectados entre ellos y que si hay datos en uno se referencian en los otros automaticamente. Modulos: FI (Finanzas) CO (controling de costes) MM (Materiales) SD (venta y distribucion) PP ( Produccion) PM (mantemiento planta) QM (calidad) PS (proyectos) HR (rrhh) ABAP (programacion y desarrollo) CONCEPTOS BASICOS Customizing: configuraci\u00f3n que representa la estructura legal y los procesos de negocio de la empresa. Unidad Organizativa: representa la estructura jerarquica de la empresa en el sistema. Datos maestros: datos requeridos para realizar transacciones del proceso en el sistema(clientes, materiales, proveedores...) Batch Input: para la entrada de grandes cantidades de registros. Documentos: cada transacci\u00f3n registra datos en la bbdd, crea un documento con id unico. Transacci\u00f3n: procesos de negocio en el sistema SAP. Sistemas de informaci\u00f3n: transacciones guardadas. Z: programas desarrollados aparte. MODO SAP: cada pantalla del SAP. WORKFLOW: flujo de trabajo para optimizar procesos. DATOS MAESTROS: info que cambia poco y son campos obligatorios para rellenar(clientes, precios, proveedores..) ESTRUCTURA ORGANIZATIVA CONEXION A SAP PANTALLA EASSY SAP FAVORITOS BARRA HERRAMIENTAS BARRA DE MENU MODOS COMANDOS DE USO FRECUENTE En el cuadro de busqueda: /N - termina la transacci\u00f3n /O - ver modos abiertos /I - cierra la transaccion abierta /Ntransacci\u00f3n - lleva a la trans indicada y cierra la actual. /NEND - salir del sistema pero avisa de cosas abiertas. /NEX - cierra totalmente CAMPOS DIFERENTES","title":"SAP"},{"location":"sap/#sap-curso","text":"Systeme Anwendungen und Produkte que significa en espa\u00f1ol 'sistemas, aplicaciones y productos'. SAP ERP\u200b es un software de planificaci\u00f3n de recursos empresariales desarrollado por la compa\u00f1\u00eda alemana SAP SE. SAP ERP incorpora las funciones empresariales claves de una organizaci\u00f3n. As\u00ed, si tuvi\u00e9ramos que definir qu\u00e9 es SAP dir\u00edamos que es un software ERP (Enterprise Resource Planning), que permite planificar y gestionar los recursos de todas las \u00e1reas de la empresa: desde log\u00edstica a contabilidad, pasando por el departamento comercial y de m\u00e1rketing, finanzas, producci\u00f3n, gesti\u00f3n de proyectos, de la calidad, mantenimiento o direcci\u00f3n y administraci\u00f3n general.","title":"SAP CURSO"},{"location":"sap/#erp","text":"Enterprise Resource Planning, planificaci\u00f3n de recursos empresariales. Es un conjunto de programas integrados que apoya las principales actividades organizacionales tales como finanza, contabilidad, log\u00edstica, producci\u00f3n, ventas y recursos humanos. Caracteristicas: Procesos estandatizados En linea Bases de datos unica Integrar todos los datos y procesos en un sistema \u00fanico. Configurable Modular","title":"ERP"},{"location":"sap/#sap","text":"System Applications and Products in data processing. Sistema, aplicaciones y productos en el procesamiento de datos.","title":"SAP"},{"location":"sap/#arquitectura","text":"Estructura cliente - servidor Estrucutra de ambiente de desarrollo - calidad - producci\u00f3n. SAP GUI es el programa que se instala el ordenador para porder acceder en remoto al servidor en una empresa. SAP LOGON es el programa que se utiliza para entrar o loguearse en SAP. Es como el iconito al clicar dos veces para conectarse y elegir a que servidor conectarse.","title":"ARQUITECTURA"},{"location":"sap/#modulos-sap","text":"Formado por varios modulos interconectados entre ellos y que si hay datos en uno se referencian en los otros automaticamente. Modulos: FI (Finanzas) CO (controling de costes) MM (Materiales) SD (venta y distribucion) PP ( Produccion) PM (mantemiento planta) QM (calidad) PS (proyectos) HR (rrhh) ABAP (programacion y desarrollo)","title":"MODULOS SAP"},{"location":"sap/#conceptos-basicos","text":"Customizing: configuraci\u00f3n que representa la estructura legal y los procesos de negocio de la empresa. Unidad Organizativa: representa la estructura jerarquica de la empresa en el sistema. Datos maestros: datos requeridos para realizar transacciones del proceso en el sistema(clientes, materiales, proveedores...) Batch Input: para la entrada de grandes cantidades de registros. Documentos: cada transacci\u00f3n registra datos en la bbdd, crea un documento con id unico. Transacci\u00f3n: procesos de negocio en el sistema SAP. Sistemas de informaci\u00f3n: transacciones guardadas. Z: programas desarrollados aparte. MODO SAP: cada pantalla del SAP. WORKFLOW: flujo de trabajo para optimizar procesos. DATOS MAESTROS: info que cambia poco y son campos obligatorios para rellenar(clientes, precios, proveedores..)","title":"CONCEPTOS BASICOS"},{"location":"sap/#estructura-organizativa","text":"","title":"ESTRUCTURA ORGANIZATIVA"},{"location":"sap/#conexion-a-sap","text":"","title":"CONEXION A SAP"},{"location":"sap/#pantalla-eassy-sap","text":"","title":"PANTALLA EASSY SAP"},{"location":"sap/#favoritos","text":"","title":"FAVORITOS"},{"location":"sap/#barra-herramientas","text":"","title":"BARRA HERRAMIENTAS"},{"location":"sap/#barra-de-menu","text":"","title":"BARRA DE MENU"},{"location":"sap/#modos","text":"","title":"MODOS"},{"location":"sap/#comandos-de-uso-frecuente","text":"En el cuadro de busqueda: /N - termina la transacci\u00f3n /O - ver modos abiertos /I - cierra la transaccion abierta /Ntransacci\u00f3n - lleva a la trans indicada y cierra la actual. /NEND - salir del sistema pero avisa de cosas abiertas. /NEX - cierra totalmente","title":"COMANDOS DE USO FRECUENTE"},{"location":"sap/#campos-diferentes","text":"","title":"CAMPOS DIFERENTES"},{"location":"tomcat/","text":"APACHE TOMCAT DOCUMENTACI\u00d3N TOMCAT Instalaci\u00f3n: Seguimos los pasos de esta web : sudo apt update sudo apt install default-jdk sudo groupadd tomcat sudo useradd -s /bin/false -g tomcat -d /home/ubuntu/tomcat tomcat cd /tmp curl -O paste_the_copied_link_here sudo mkdir /home/ubuntu/tomcat sudo tar xzvf apache-tomcat-*tar.gz -C /home/ubuntu/tomcat --strip-components=1 ubuntu@ip-172-31-38-253:~/tomcat$ cd /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chgrp -R tomcat /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod -R g+r conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod g+x conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chown -R tomcat webapps/ work/ temp/ logs/ ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo update-java-alternatives -l java-1.11.0-openjdk-amd64 1111 /usr/lib/jvm/java-1.11.0-openjdk-amd64 ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vim /etc/systemd/system/tomcat.service ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl daemon-reload ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl start tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl status tomcat \u25cf tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2021-04-08 11:00:09 UTC; 4s ago Process: 4655 ExecStart=/home/ubuntu/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 4662 (java) Tasks: 30 (limit: 1160) Memory: 128.6M CGroup: /system.slice/tomcat.service \u2514\u25004662 /usr/lib/jvm/java-1.11.0-openjdk-amd64/bin/java -Djava.util.logging.config.file=/home/ubuntu/tomcat/conf/> Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Starting Apache Tomcat Web Application Container... Apr 08 11:00:09 ip-172-31-38-253 startup.sh[4655]: Tomcat started. Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Started Apache Tomcat Web Application Container. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo ufw allow 8080 Rules updated Rules updated (v6) ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat sudo systemctl enable tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl enable tomcat Created symlink /etc/systemd/system/multi-user.target.wants/tomcat.service \u2192 /etc/systemd/system/tomcat.service. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/conf/tomcat-users.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/host-manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat Podemos poner en -bashrc la variable JAVA_HOME de /usr/lib/lvm/java... para que tengamos la variable configurada. Levantamos servicios o despliegues con: ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo bin/catalina.sh // bin/startup.sh start Comprobamos que est\u00e1 funcionando con: root@ip-172-31-29-160:/home/ubuntu/tomcat/bin# ps ax | grep java DIRECTORIOS TOMCAT Tomcat directorios: /bin - Startup, shutdown, and other scripts. The *.sh files (for Unix systems) are functional duplicates of the *.bat files (for Windows systems). Since the Win32 command-line lacks certain functionality, there are some additional files in here. /conf - Configuration files and related DTDs. The most important file in here is server.xml. It is the main configuration file for the container. /logs - Log files are here by default. /webapps - This is where your webapps go. CATALINA_HOME and CATALINA_BASE: CATALINA_HOME: Represents the root of your Tomcat installation, for example /home/tomcat/apache-tomcat-9.0.10 or C:\\Program Files\\apache-tomcat-9.0.10. CATALINA_BASE: Represents the root of a runtime configuration of a specific Tomcat instance. If you want to have multiple Tomcat instances on one machine, use the CATALINA_BASE property. If you set the properties to different locations, the CATALINA_HOME location contains static sources, such as .jar files, or binary files. The CATALINA_BASE location contains configuration files, log files, deployed applications, and other runtime requirements. Ejemplo APP: Sample Application The example app has been packaged as a war file and can be downloaded here (Note: make sure your browser doesn't change file extension or append a new one). The easiest way to run this application is simply to move the war file to your CATALINA_BASE/webapps directory. A default Tomcat install will automatically expand and deploy the application for you. You can view it with the following URL (assuming that you're running tomcat on port 8080 which is the default): http://localhost:8080/sample If you just want to browse the contents, you can unpack the war file with the jar command. jar -xvf sample.war Note: CATALINA_BASE is usually the directory in which you unpacked the Tomcat distribution. For more information on CATALINA_HOME, CATALINA_BASE and the difference between them see RUNNING.txt in the directory you unpacked your Tomcat distribution. Copiamos el ejemplo de demo app: [isx46410800@miguel .ssh]$ scp -i mykeypair.pem /home/isx46410800/Documents/tomcat/DemoApp.tar.gz ubuntu@3.8.187.27:/home/ubuntu DemoApp.tar.gz 100% 0 0.0KB/s 00:00 Estructura: el cliente se conecta al servidor por diferentes puertos que hace peticiones como servicios. el engine se encarga de gestionar estas peticiones a trav\u00e9s del host que contiene todas las aplicaciones que hay dentro del contexto. El fichero server.xml contiene la configuraci\u00f3n global, central de tomcat. Catalina es el motor de los servlets, tomcat en s\u00ed. Catalina.properties se pueden configurar variables, cosas del sistema, tiene algunas librer\u00edas o paquetes. Se pueden cambiar o a\u00f1adir propiedades Con la orden catalina.sh (nos sale por poner la variable JAVA_HOME en el path): root@ip-172-31-29-160:/home/ubuntu/tomcat# catalina.sh version Using CATALINA_BASE: /home/ubuntu/tomcat Using CATALINA_HOME: /home/ubuntu/tomcat Using CATALINA_TMPDIR: /home/ubuntu/tomcat/temp Using JRE_HOME: /usr Using CLASSPATH: /home/ubuntu/tomcat/bin/bootstrap.jar:/home/ubuntu/tomcat/bin/tomcat-juli.jar Using CATALINA_OPTS: NOTE: Picked up JDK_JAVA_OPTIONS: --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED Server version: Apache Tomcat/9.0.45 Server built: Mar 30 2021 10:29:04 UTC Server number: 9.0.45.0 OS Name: Linux OS Version: 5.4.0-1038-aws Architecture: amd64 JVM Version: 11.0.10+9-Ubuntu-0ubuntu1.20.04 JVM Vendor: Ubuntu Catalina base nos dice cual es la intancia activa, si hubiera varias instancias se deber\u00eda poner cual es la activa. Tambien podemos poner la variable CATALINA_HOME en el bashrc para que nos pille ya el directorio de tomcat. export PATH=$PATH:/home/ubuntu/tomcat/bin export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ export CATALINA_HOME=/home/ubuntu/tomcat Podemos usar catalina.sh run para iniciar en foreground o start en background. PUERTO SHUTDOWN Podemos ver la doc del server.xml para diferentes configuraciones o objetos que se pueden poner. Unas buenas pr\u00e1cticas es desactivar el puerto de shutdown para que no se pueda hacer ni por tcp ni por remoto. Si recibimos una cadena de SHUTDOWN por cierto puerto, nos matan el tomcat. <Server port=\"8005\" shutdown=\"SHUTDOWN\"> --- root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# telnet localhost 8005 Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. SHUTDOWN Connection closed by foreign host. root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# ps ax | grep java 15743 pts/0 S+ 0:00 grep --color=auto java Para que no pase eso, segun la docu ponemos puerto -1 o cambiamos la string de shutdown por algo dificil de acertar: <Server port=\"-1\" shutdown=\"SHUTDOWN\"> JAVA_OPTS Y CATALINA_OPTS JAVA_OPTS es lo que coge java para todas las cosas java y CATALINA_OPTS son solo para cosas relacionadas con Tomcat. DESPLIEGUE APPS Siempre que desplegamos algo tenemos que hacerlos en el directorio webapps de tomcat. Podemos hacerlo copiando directamente el .war o el directorio con todo el contenido. Al rato veremos que se nos crea el directorio autodesplegado en webapps con el contenido del war copiado: [isx46410800@miguel tomcat]$ sudo cp ejemploWAR/ejemplo.war /home/tomcat/tomcat/webapps/. Con un directorio: [isx46410800@miguel tomcat]$ ll ejemploWARdir/web1/ total 20 -rw-r--r--. 1 isx46410800 isx46410800 309 Jul 5 2020 hello.jsp drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 images -rw-r--r--. 1 isx46410800 isx46410800 470 Jul 5 2020 index.html drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 META-INF drwxr-xr-x. 3 isx46410800 isx46410800 4096 Jul 5 2020 WEB-INF [isx46410800@miguel tomcat]$ sudo cp -r ejemploWARdir/web1/ /home/tomcat/tomcat/webapps/. Si cambiamos algo, modificamos en el c\u00f3digo fuente el index.html y hello.jsp. Luego tenemos que generar de nuevo el .jar en el directorio de todo el codigo fuente con jar cvf ejemplo.war * y esto lo copiamos en el webapps. Para quitar despliegues con solo eliminar el .war o el directorio, ya se hace undeploy. INTEGRACION ECLIPSE CON TOMCAT Instale Eclipse IDE en CentOS, RHEL y Fedora: Se requiere una versi\u00f3n de Java 9 o superior para instalar Eclipse IDE y la forma m\u00e1s sencilla de instalar Oracle Java JDK desde los repositorios predeterminados. yum install java-11-openjdk-devel java -version A continuaci\u00f3n, abra un navegador, navegue hasta la p\u00e1gina de descarga oficial de Eclipse y descargue la \u00faltima versi\u00f3n del paquete tar espec\u00edfico para su arquitectura de distribuci\u00f3n de Linux instalada. Alternativamente, tambi\u00e9n puede descargar el archivo de instalaci\u00f3n de Eclipse IDE en su sistema a trav\u00e9s de la utilidad wget, emitiendo el siguiente comando. wget http://ftp.yz.yamagata-u.ac.jp/pub/eclipse/oomph/epp/2020-06/R/eclipse-inst-linux64.tar.gz Una vez que se complete la descarga, navegue hasta el directorio donde se descarg\u00f3 el paquete de archivo y emita los siguientes comandos para comenzar a instalar Eclipse IDE. tar -xvf eclipse-inst-linux64.tar.gz cd eclipse-installer/ sudo ./eclipse-inst TOMCAT MANAGER A\u00f1adimos en el fichero conf/tomcat-users.xml un usuario con privilegios para poder entrar: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui\"/> Para entrar: http://localhost:8888/manager/html Para desplegar una app vamos a browse seleccionamos el fichero .war y deploy. Despues se puede parar, undeploy, expirar sesion etc. Tambien se puede desplegar un dir(con solo todo el codigo descomprimido) o war con un path diferente nombre y no coja el nombre del archivo war o del directorio. Para utilizar el modo comando se ha de a\u00f1adir en el fichero de conf/tomcat-users.xml: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui,manager-script\"/> Se entra en http://localhost:8888/manager/text/list Algunos comandos: - http://localhost:8888/manager/text/serverinfo - http://localhost:8888/manager/text/list - http://localhost:8888/manager/text/vminfo - http://localhost:8888/manager/text/sessions?path=/app1 - http://localhost:8888/manager/text/threaddump - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1&path=/app3 - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war&path=/app2 - http://localhost:8888/manager/text/stop?path=/app2 - http://localhost:8888/manager/text/start?path=/app2 - http://localhost:8888/manager/text/expire?path=/app1&iddle=1 - http://localhost:8888/manager/text/undeploy?path=/app3 LOGS Fichero clave: conf/logging.properties. Los handers son los indicadores de a donde enrutamos las entradas de los logs de tomcat. handlers = 1catalina.org.apache.juli.AsyncFileHandler, 2localhost.org.apache.juli.AsyncFileHandler, 3manager.org.apache.juli.AsyncFileHandler, 4host-manager.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # .handlers = 1catalina.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # ############################################################ # Handler specific properties. # Describes specific configuration info for Handlers. ############################################################ # 1catalina.org.apache.juli.AsyncFileHandler.level = FINE 1catalina.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina. 1catalina.org.apache.juli.AsyncFileHandler.maxDays = 90 1catalina.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 2localhost.org.apache.juli.AsyncFileHandler.level = FINE 2localhost.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 2localhost.org.apache.juli.AsyncFileHandler.prefix = localhost. 2localhost.org.apache.juli.AsyncFileHandler.maxDays = 90 2localhost.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 4host-manager.org.apache.juli.AsyncFileHandler.level = FINE 4host-manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager. 4host-manager.org.apache.juli.AsyncFileHandler.maxDays = 90 4host-manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter java.util.logging.ConsoleHandler.encoding = UTF-8 el .handlers es el que coge de predeterminado Documentaci\u00f3n paquete JULI Estos handers del archivo coinciden con lo que tenemos en /logs: [tomcat@miguel tomcat]$ ll logs/ total 100 -rw-r-----. 1 tomcat tomcat 16752 may 19 13:55 catalina.2021-05-19.log -rw-r-----. 1 tomcat tomcat 15599 may 20 20:53 catalina.2021-05-20.log -rw-r-----. 1 tomcat tomcat 32351 may 20 20:53 catalina.out -rw-r-----. 1 tomcat tomcat 175 may 19 13:28 host-manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 0 may 20 20:23 host-manager.2021-05-20.log -rw-r-----. 1 tomcat tomcat 1121 may 19 13:28 localhost.2021-05-19.log -rw-r-----. 1 tomcat tomcat 1122 may 20 20:37 localhost.2021-05-20.log -rw-r-----. 1 tomcat tomcat 2034 may 19 14:14 localhost_access_log.2021-05-19.txt -rw-r-----. 1 tomcat tomcat 3656 may 20 20:53 localhost_access_log.2021-05-20.txt -rw-r-----. 1 tomcat tomcat 0 may 19 13:25 manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 4328 may 20 20:53 manager.2021-05-20.log Podemos personaliar uno: 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs/mis-logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 3manager.org.apache.juli.AsyncFileHandler.formatter = java.util.logging.XMLFormatter Vemos que se genera contenido en el personalizado: [tomcat@miguel logs]$ ll mis-logs/ total 0 -rw-r-----. 1 tomcat tomcat 0 may 21 00:23 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/ cat: mis-logs/: Es un directorio [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE log SYSTEM \"logger.dtd\"> <log> <record> <date>2021-05-21T00:25:02</date> <millis>1621549502823</millis> <sequence>46</sequence> <logger>org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager]</logger> <level>INFO</level> <class>org.apache.catalina.core.ApplicationContext</class> <method>log</method> <thread>17</thread> <message>HTMLManager: init: Associated with Deployer 'Catalina:type=Deployer,host=localhost'</message> </record> VALVES Las valvulas son sistemas de seguridad para la hora de entrar a una aplicaci\u00f3n etc, segun en que capa de servicio est\u00e1 especificada. DOC VALVES Una personalizada: <Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --> <!-- <Valve className=\"org.apache.catalina.authenticator.SingleSignOn\" /> --> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs/mis-logs\" prefix=\"localhost_access_log\" suffix=\".log\" pattern=\"%h %l %u %t &quot;%r&quot; %s %b %B\" /> </Host> Vemos el contenido: [tomcat@miguel logs]$ ll mis-logs/ total 12 -rw-r-----. 1 tomcat tomcat 217 may 21 00:41 localhost_access_log.2021-05-21.log -rw-r-----. 1 tomcat tomcat 5539 may 21 00:41 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/localhost_access_log.2021-05-21.log 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:22 +0200] \"GET /manager/text/stop?path=/app2 HTTP/1.1\" 200 70 70 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:25 +0200] \"GET /manager/text/start?path=/app2 HTTP/1.1\" 200 73 73 Ahora hacemos un ejemplo que permita o no unas ips o puertos: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\"/> solo acceden los que tienen la ip de localhost 127.0.0.1. Si ponemos nuestra ip:8888 no nos deja porque no hemos entrado por la de localhost indicada en el valve. JDBC BBDD Instalamos la herramienta probe para ver de otra manera el contenido de tomcat como una consola. Instalamos mariadb como bbdd mysql. MariaDB [seguridad]> select * from usuarios; +----------+ | nombre | +----------+ | Miguel | | Cristina | | Isabel | +----------+ 3 rows in set (0.000 sec) MariaDB [seguridad]> create user 'desa'@'localhost' identified by 'desa'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> grant all on seguridad.* to 'desa'@'localhost'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> commit; Instalamos el driver jdbc de mariadb en nuestro caso en: [isx46410800@miguel tomcat]$ sudo cp ~/Downloads/mariadb-java-client-2.6.1-sources.jar /home/tomcat/lib Configuramos el recurso de bbdd en tomcat que utilice ese driver de jdbc: <Context path=\"/jdbc\" > <Resource name=\"jdbc/cursoDB\" auth=\"Container\" type=\"javax.sql.DataSource\" username=\"desa\" password=\"desa\" driverClassName=\"org.mariadb.jdbc.Driver\" url=\"jdbc:mariadb://localhost:3306/seguridad\" initialSize=\"4\" maxActive=\"15\" maxIdle=\"3\"/> </Context> REALM Son otras opciones de seguridad para en contra de Tomcat, proteger recursos, impedir accesos indeseados... <!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --> <Realm className=\"org.apache.catalina.realm.LockOutRealm\"> <!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key \"UserDatabase\". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --> <Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/> </Realm> Sirven para proteger recursos webs y se basa en usuarios y roles asociados a las aplicaciones. Creamos un realm: <Context path=\"/ejeRealm\"> <Realm className=\"org.apache.catalina.realm.MemoryRealm\" pathname=\"conf/usuarios.xml\"/> </Context> en usuarios.xml: <!xml version=\"1.0\" enconding=\"UTF-8\"?> <tomcat-users> <user username=\"usu1\" password=\"usu1\" roles=\"usuario\"/> </tomcat-users> Y con la app que pasamos, indicamos que solo podr\u00e1 entrar segun el usuario o roles indicados en la app. APACHE CON TOMCAT Descargamos el conector mod_jk Luego tenemos que instalar como root una serie de paquetes como el paquete de http de desarrollador, el framework runtime apr para trabajar como dev, el compilador de c... [root@miguel tomcat]# dnf install httpd-devel apr apr-devel apr-util apr-util-devel gcc make libtool Despues vamos como root al directorio para compilar un arhcivo y meterlo en el tomcat lo que necesitamos. 1- /home/isx46410800/Downloads/tomcat-connectors-1.2.48-src/native 2- Configure para configurar, preparar librerias, el codigo fuente 3- make para compilar 4- make-install para instalar 5- tendremos el fichero mod_jk.so para poder conectar el apache con el tomcat, y puedan entenderse entre ellos. [root@miguel native]# #./configure --with-apxs=/bin/apxs [root@miguel native]# whereis apxs apxs: /usr/bin/apxs /usr/share/man/man1/apxs.1.gz [root@miguel native]# make [root@miguel apache-2.0]# cd apache-2.0/ [root@miguel apache-2.0]# ll mod_jk.s* -rwxr-xr-x. 1 root root 1961464 may 22 15:49 mod_jk.so [root@miguel apache-2.0]# cp mod_jk.so /etc/httpd/modules/ Despues a\u00f1adimos un archivo a la configuracion de apache en /etc/httpd/conf/httpd.conf: IncludeOptional conf/mod_jk.conf [root@miguel conf]# cat mod_jk.conf # Cargamos el modulo LoadModule jk_module modules/mod_jk.so # Indicamos dinde esta el fichero workers.properties JkWorkersFile conf/workers.properties # Podemos generar ficheros de log JkShmFile logs/mod_jk.shm JkLogFile logs/mod_jk.log JkLogLevel info # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus <Location /jkmanager/> </Location> # Mapeamos las URL contra un conjunto de workers JkMount /* MisTomcat Creamos nuestros ficheros de workers.properties de lo que ser\u00e1 nuestros tomcats diferentes, clusters etc(conf/workers.properties). # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status #worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 #worker.worker2.type=ajp13 #worker.worker2.host=localhost #worker.worker2.port=9010 #worker.worker3.type=ajp13 #worker.worker3.host=localhost #worker.worker3.port=10011 # Workers en el Load Balancer worker.MisTomcat.balance_workers=worker1 #worker.MisTomcat.balance_workers=worker1,worker2,worker3 Por ahora comentamos los workers2 y 3 porque solo trabajamos con uno. La info de el puerto y tipo ajp13 la vemos en la info de conf/server.xml y descomentamos la linea de ajp, indicamos los datos y a\u00f1adimos un parametro para que no de fallo: <!-- Define an AJP 1.3 Connector on port 8009 --> <Connector protocol=\"AJP/1.3\" address=\"::1\" port=\"8009\" secretRequired=\"false\" redirectPort=\"8443\" /> Ahora vemos que al pasarle al localhost sin puerto algo, nos manda al tomcat: CLUSTERS Instalaremos dos tomcats m\u00e1s y en cada uno de ellos modificamos el puertos y el ajp correspondiente: [tomcat@miguel ~]$ vim tomcat1/conf/server.xml [tomcat@miguel ~]$ vim tomcat2/conf/server.xml Eliminamos la variable del .bashrc que apuntaba cual era el tomcat, ya que ahora al tener 3 no podemos indicar que sea el primero solo. Tambi\u00e9n podemos abrir tres terminales y exportar la variable con el home correspondiente: [tomcat@miguel tomcat1]$ export CATALINA_HOME=/home/tomcat/tomcat1 Despues arrancamos cada maquina desde su bin para que pille esa variable: [tomcat@miguel bin]$ ./catalina.sh run Ahora modificamos el worker.properties de apache para que haya 3 workers con su ajp correspondiente, en modo balanceador de carga y para que mantenga la sesion con el el sticky sesion: # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 worker.worker2.type=ajp13 worker.worker2.host=localhost worker.worker2.port=9009 worker.worker3.type=ajp13 worker.worker3.host=localhost worker.worker3.port=7009 # Workers en el Load Balancer #worker.MisTomcat.balance_workers=worker1 worker.MisTomcat.balance_workers=worker1,worker2,worker3 worker.MisTomcat.sticky_session=true Paramos y arrancamos de nuevo el apache. Para probar el entorno de cluster copiamos el war de cluster para saber en cual estoy en los 3 tomcats: [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat1/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat2/webapps/. Probamos: http://localhost/cluster/hello.jsp Probamos la parte del jkstatus como worker: # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus Del worker properties: # Activar los workers worker.jkstatus.type=status Comprobamos con http://localhost/jkmanager/ PERSISTENCIA EN TOMCAT Sirve para cuando estamos en una session alojada por ejemplo en tomcat 1, todo se quede alojado ahi y no recargue en otro tomcat y perdamos todo lo de la sesion. En el server.xml de cada uno a\u00f1adimos el jvmroute. Lo que hace es identificar cada sesion con un ID de sesion y lo asigna al worker que ya indicamos el el worker.properties: <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker1\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker2\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker3\"> Descomentamos tambi\u00e9n la opcion de cluster : <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/> Si vamos al navegador y ponemos ajustes - web developer -storage inspector, veremos que nos sale siempre el mismo id worker de la sesion. VIRTUALHOSTS Estos crea dentro de un mismo tomcat, diferentes dominios repartidos para que segun lo que sea, trabaja una cosa u otra. En server.xml ponemos: #Virtual host 1 <Host name=\"empresa1.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa1.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"example_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa1\" debug=\"0\" reloadable=\"true\"/> </Host> #Virtual host 2 <Host name=\"empresa2.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa2.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"mydomain_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa2\" debug=\"0\" reloadable=\"true\"/> </Host> el alias es a lo que responde la petici\u00f3n, con este nombre referido. Para probar estos alias tenemos que ponerlos en el etc/hosts, dns o en el hosting. 192.168.1.113 www.empresa1.com 192.168.1.113 www.empresa2.com Copiamos la app de prueba de virtual host y comprobamos que por los dos alias te responde segun lo indicado en el server.xml Tenemos una herramienta que es el VHOST MANAGER. Para ello necesitamos crear un usuario con los permisos en el tomcat-users.xml: <user username=\"admin1\" password=\"jupiter\" roles=\"admin-gui,admin-script\"/> Rebotamos y entramos desde inicio - host manager o http://localhost:8888/host-manager/html Aqu\u00ed podemos eliminar, a\u00f1adirlo con opciones indicadas y para que se guarde los cambios a final de pagina hemos de a\u00f1adir la siguiente directiz en server.xml. <Listener className=\"org.apache.catalina.storeconfig.StoreConfigLifecycleListener\"/>","title":"Tomcat"},{"location":"tomcat/#apache-tomcat","text":"DOCUMENTACI\u00d3N TOMCAT","title":"APACHE TOMCAT"},{"location":"tomcat/#instalacion","text":"Seguimos los pasos de esta web : sudo apt update sudo apt install default-jdk sudo groupadd tomcat sudo useradd -s /bin/false -g tomcat -d /home/ubuntu/tomcat tomcat cd /tmp curl -O paste_the_copied_link_here sudo mkdir /home/ubuntu/tomcat sudo tar xzvf apache-tomcat-*tar.gz -C /home/ubuntu/tomcat --strip-components=1 ubuntu@ip-172-31-38-253:~/tomcat$ cd /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chgrp -R tomcat /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod -R g+r conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod g+x conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chown -R tomcat webapps/ work/ temp/ logs/ ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo update-java-alternatives -l java-1.11.0-openjdk-amd64 1111 /usr/lib/jvm/java-1.11.0-openjdk-amd64 ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vim /etc/systemd/system/tomcat.service ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl daemon-reload ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl start tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl status tomcat \u25cf tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2021-04-08 11:00:09 UTC; 4s ago Process: 4655 ExecStart=/home/ubuntu/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 4662 (java) Tasks: 30 (limit: 1160) Memory: 128.6M CGroup: /system.slice/tomcat.service \u2514\u25004662 /usr/lib/jvm/java-1.11.0-openjdk-amd64/bin/java -Djava.util.logging.config.file=/home/ubuntu/tomcat/conf/> Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Starting Apache Tomcat Web Application Container... Apr 08 11:00:09 ip-172-31-38-253 startup.sh[4655]: Tomcat started. Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Started Apache Tomcat Web Application Container. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo ufw allow 8080 Rules updated Rules updated (v6) ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat sudo systemctl enable tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl enable tomcat Created symlink /etc/systemd/system/multi-user.target.wants/tomcat.service \u2192 /etc/systemd/system/tomcat.service. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/conf/tomcat-users.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/host-manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat Podemos poner en -bashrc la variable JAVA_HOME de /usr/lib/lvm/java... para que tengamos la variable configurada. Levantamos servicios o despliegues con: ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo bin/catalina.sh // bin/startup.sh start Comprobamos que est\u00e1 funcionando con: root@ip-172-31-29-160:/home/ubuntu/tomcat/bin# ps ax | grep java","title":"Instalaci\u00f3n:"},{"location":"tomcat/#directorios-tomcat","text":"Tomcat directorios: /bin - Startup, shutdown, and other scripts. The *.sh files (for Unix systems) are functional duplicates of the *.bat files (for Windows systems). Since the Win32 command-line lacks certain functionality, there are some additional files in here. /conf - Configuration files and related DTDs. The most important file in here is server.xml. It is the main configuration file for the container. /logs - Log files are here by default. /webapps - This is where your webapps go. CATALINA_HOME and CATALINA_BASE: CATALINA_HOME: Represents the root of your Tomcat installation, for example /home/tomcat/apache-tomcat-9.0.10 or C:\\Program Files\\apache-tomcat-9.0.10. CATALINA_BASE: Represents the root of a runtime configuration of a specific Tomcat instance. If you want to have multiple Tomcat instances on one machine, use the CATALINA_BASE property. If you set the properties to different locations, the CATALINA_HOME location contains static sources, such as .jar files, or binary files. The CATALINA_BASE location contains configuration files, log files, deployed applications, and other runtime requirements. Ejemplo APP: Sample Application The example app has been packaged as a war file and can be downloaded here (Note: make sure your browser doesn't change file extension or append a new one). The easiest way to run this application is simply to move the war file to your CATALINA_BASE/webapps directory. A default Tomcat install will automatically expand and deploy the application for you. You can view it with the following URL (assuming that you're running tomcat on port 8080 which is the default): http://localhost:8080/sample If you just want to browse the contents, you can unpack the war file with the jar command. jar -xvf sample.war Note: CATALINA_BASE is usually the directory in which you unpacked the Tomcat distribution. For more information on CATALINA_HOME, CATALINA_BASE and the difference between them see RUNNING.txt in the directory you unpacked your Tomcat distribution. Copiamos el ejemplo de demo app: [isx46410800@miguel .ssh]$ scp -i mykeypair.pem /home/isx46410800/Documents/tomcat/DemoApp.tar.gz ubuntu@3.8.187.27:/home/ubuntu DemoApp.tar.gz 100% 0 0.0KB/s 00:00 Estructura: el cliente se conecta al servidor por diferentes puertos que hace peticiones como servicios. el engine se encarga de gestionar estas peticiones a trav\u00e9s del host que contiene todas las aplicaciones que hay dentro del contexto. El fichero server.xml contiene la configuraci\u00f3n global, central de tomcat. Catalina es el motor de los servlets, tomcat en s\u00ed. Catalina.properties se pueden configurar variables, cosas del sistema, tiene algunas librer\u00edas o paquetes. Se pueden cambiar o a\u00f1adir propiedades Con la orden catalina.sh (nos sale por poner la variable JAVA_HOME en el path): root@ip-172-31-29-160:/home/ubuntu/tomcat# catalina.sh version Using CATALINA_BASE: /home/ubuntu/tomcat Using CATALINA_HOME: /home/ubuntu/tomcat Using CATALINA_TMPDIR: /home/ubuntu/tomcat/temp Using JRE_HOME: /usr Using CLASSPATH: /home/ubuntu/tomcat/bin/bootstrap.jar:/home/ubuntu/tomcat/bin/tomcat-juli.jar Using CATALINA_OPTS: NOTE: Picked up JDK_JAVA_OPTIONS: --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED Server version: Apache Tomcat/9.0.45 Server built: Mar 30 2021 10:29:04 UTC Server number: 9.0.45.0 OS Name: Linux OS Version: 5.4.0-1038-aws Architecture: amd64 JVM Version: 11.0.10+9-Ubuntu-0ubuntu1.20.04 JVM Vendor: Ubuntu Catalina base nos dice cual es la intancia activa, si hubiera varias instancias se deber\u00eda poner cual es la activa. Tambien podemos poner la variable CATALINA_HOME en el bashrc para que nos pille ya el directorio de tomcat. export PATH=$PATH:/home/ubuntu/tomcat/bin export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ export CATALINA_HOME=/home/ubuntu/tomcat Podemos usar catalina.sh run para iniciar en foreground o start en background.","title":"DIRECTORIOS TOMCAT"},{"location":"tomcat/#puerto-shutdown","text":"Podemos ver la doc del server.xml para diferentes configuraciones o objetos que se pueden poner. Unas buenas pr\u00e1cticas es desactivar el puerto de shutdown para que no se pueda hacer ni por tcp ni por remoto. Si recibimos una cadena de SHUTDOWN por cierto puerto, nos matan el tomcat. <Server port=\"8005\" shutdown=\"SHUTDOWN\"> --- root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# telnet localhost 8005 Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. SHUTDOWN Connection closed by foreign host. root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# ps ax | grep java 15743 pts/0 S+ 0:00 grep --color=auto java Para que no pase eso, segun la docu ponemos puerto -1 o cambiamos la string de shutdown por algo dificil de acertar: <Server port=\"-1\" shutdown=\"SHUTDOWN\">","title":"PUERTO SHUTDOWN"},{"location":"tomcat/#java_opts-y-catalina_opts","text":"JAVA_OPTS es lo que coge java para todas las cosas java y CATALINA_OPTS son solo para cosas relacionadas con Tomcat.","title":"JAVA_OPTS Y CATALINA_OPTS"},{"location":"tomcat/#despliegue-apps","text":"Siempre que desplegamos algo tenemos que hacerlos en el directorio webapps de tomcat. Podemos hacerlo copiando directamente el .war o el directorio con todo el contenido. Al rato veremos que se nos crea el directorio autodesplegado en webapps con el contenido del war copiado: [isx46410800@miguel tomcat]$ sudo cp ejemploWAR/ejemplo.war /home/tomcat/tomcat/webapps/. Con un directorio: [isx46410800@miguel tomcat]$ ll ejemploWARdir/web1/ total 20 -rw-r--r--. 1 isx46410800 isx46410800 309 Jul 5 2020 hello.jsp drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 images -rw-r--r--. 1 isx46410800 isx46410800 470 Jul 5 2020 index.html drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 META-INF drwxr-xr-x. 3 isx46410800 isx46410800 4096 Jul 5 2020 WEB-INF [isx46410800@miguel tomcat]$ sudo cp -r ejemploWARdir/web1/ /home/tomcat/tomcat/webapps/. Si cambiamos algo, modificamos en el c\u00f3digo fuente el index.html y hello.jsp. Luego tenemos que generar de nuevo el .jar en el directorio de todo el codigo fuente con jar cvf ejemplo.war * y esto lo copiamos en el webapps. Para quitar despliegues con solo eliminar el .war o el directorio, ya se hace undeploy.","title":"DESPLIEGUE APPS"},{"location":"tomcat/#integracion-eclipse-con-tomcat","text":"Instale Eclipse IDE en CentOS, RHEL y Fedora: Se requiere una versi\u00f3n de Java 9 o superior para instalar Eclipse IDE y la forma m\u00e1s sencilla de instalar Oracle Java JDK desde los repositorios predeterminados. yum install java-11-openjdk-devel java -version A continuaci\u00f3n, abra un navegador, navegue hasta la p\u00e1gina de descarga oficial de Eclipse y descargue la \u00faltima versi\u00f3n del paquete tar espec\u00edfico para su arquitectura de distribuci\u00f3n de Linux instalada. Alternativamente, tambi\u00e9n puede descargar el archivo de instalaci\u00f3n de Eclipse IDE en su sistema a trav\u00e9s de la utilidad wget, emitiendo el siguiente comando. wget http://ftp.yz.yamagata-u.ac.jp/pub/eclipse/oomph/epp/2020-06/R/eclipse-inst-linux64.tar.gz Una vez que se complete la descarga, navegue hasta el directorio donde se descarg\u00f3 el paquete de archivo y emita los siguientes comandos para comenzar a instalar Eclipse IDE. tar -xvf eclipse-inst-linux64.tar.gz cd eclipse-installer/ sudo ./eclipse-inst","title":"INTEGRACION ECLIPSE CON TOMCAT"},{"location":"tomcat/#tomcat-manager","text":"A\u00f1adimos en el fichero conf/tomcat-users.xml un usuario con privilegios para poder entrar: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui\"/> Para entrar: http://localhost:8888/manager/html Para desplegar una app vamos a browse seleccionamos el fichero .war y deploy. Despues se puede parar, undeploy, expirar sesion etc. Tambien se puede desplegar un dir(con solo todo el codigo descomprimido) o war con un path diferente nombre y no coja el nombre del archivo war o del directorio. Para utilizar el modo comando se ha de a\u00f1adir en el fichero de conf/tomcat-users.xml: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui,manager-script\"/> Se entra en http://localhost:8888/manager/text/list Algunos comandos: - http://localhost:8888/manager/text/serverinfo - http://localhost:8888/manager/text/list - http://localhost:8888/manager/text/vminfo - http://localhost:8888/manager/text/sessions?path=/app1 - http://localhost:8888/manager/text/threaddump - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1&path=/app3 - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war&path=/app2 - http://localhost:8888/manager/text/stop?path=/app2 - http://localhost:8888/manager/text/start?path=/app2 - http://localhost:8888/manager/text/expire?path=/app1&iddle=1 - http://localhost:8888/manager/text/undeploy?path=/app3","title":"TOMCAT MANAGER"},{"location":"tomcat/#logs","text":"Fichero clave: conf/logging.properties. Los handers son los indicadores de a donde enrutamos las entradas de los logs de tomcat. handlers = 1catalina.org.apache.juli.AsyncFileHandler, 2localhost.org.apache.juli.AsyncFileHandler, 3manager.org.apache.juli.AsyncFileHandler, 4host-manager.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # .handlers = 1catalina.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # ############################################################ # Handler specific properties. # Describes specific configuration info for Handlers. ############################################################ # 1catalina.org.apache.juli.AsyncFileHandler.level = FINE 1catalina.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina. 1catalina.org.apache.juli.AsyncFileHandler.maxDays = 90 1catalina.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 2localhost.org.apache.juli.AsyncFileHandler.level = FINE 2localhost.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 2localhost.org.apache.juli.AsyncFileHandler.prefix = localhost. 2localhost.org.apache.juli.AsyncFileHandler.maxDays = 90 2localhost.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 4host-manager.org.apache.juli.AsyncFileHandler.level = FINE 4host-manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager. 4host-manager.org.apache.juli.AsyncFileHandler.maxDays = 90 4host-manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter java.util.logging.ConsoleHandler.encoding = UTF-8 el .handlers es el que coge de predeterminado Documentaci\u00f3n paquete JULI Estos handers del archivo coinciden con lo que tenemos en /logs: [tomcat@miguel tomcat]$ ll logs/ total 100 -rw-r-----. 1 tomcat tomcat 16752 may 19 13:55 catalina.2021-05-19.log -rw-r-----. 1 tomcat tomcat 15599 may 20 20:53 catalina.2021-05-20.log -rw-r-----. 1 tomcat tomcat 32351 may 20 20:53 catalina.out -rw-r-----. 1 tomcat tomcat 175 may 19 13:28 host-manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 0 may 20 20:23 host-manager.2021-05-20.log -rw-r-----. 1 tomcat tomcat 1121 may 19 13:28 localhost.2021-05-19.log -rw-r-----. 1 tomcat tomcat 1122 may 20 20:37 localhost.2021-05-20.log -rw-r-----. 1 tomcat tomcat 2034 may 19 14:14 localhost_access_log.2021-05-19.txt -rw-r-----. 1 tomcat tomcat 3656 may 20 20:53 localhost_access_log.2021-05-20.txt -rw-r-----. 1 tomcat tomcat 0 may 19 13:25 manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 4328 may 20 20:53 manager.2021-05-20.log Podemos personaliar uno: 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs/mis-logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 3manager.org.apache.juli.AsyncFileHandler.formatter = java.util.logging.XMLFormatter Vemos que se genera contenido en el personalizado: [tomcat@miguel logs]$ ll mis-logs/ total 0 -rw-r-----. 1 tomcat tomcat 0 may 21 00:23 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/ cat: mis-logs/: Es un directorio [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE log SYSTEM \"logger.dtd\"> <log> <record> <date>2021-05-21T00:25:02</date> <millis>1621549502823</millis> <sequence>46</sequence> <logger>org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager]</logger> <level>INFO</level> <class>org.apache.catalina.core.ApplicationContext</class> <method>log</method> <thread>17</thread> <message>HTMLManager: init: Associated with Deployer 'Catalina:type=Deployer,host=localhost'</message> </record>","title":"LOGS"},{"location":"tomcat/#valves","text":"Las valvulas son sistemas de seguridad para la hora de entrar a una aplicaci\u00f3n etc, segun en que capa de servicio est\u00e1 especificada. DOC VALVES Una personalizada: <Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --> <!-- <Valve className=\"org.apache.catalina.authenticator.SingleSignOn\" /> --> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs/mis-logs\" prefix=\"localhost_access_log\" suffix=\".log\" pattern=\"%h %l %u %t &quot;%r&quot; %s %b %B\" /> </Host> Vemos el contenido: [tomcat@miguel logs]$ ll mis-logs/ total 12 -rw-r-----. 1 tomcat tomcat 217 may 21 00:41 localhost_access_log.2021-05-21.log -rw-r-----. 1 tomcat tomcat 5539 may 21 00:41 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/localhost_access_log.2021-05-21.log 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:22 +0200] \"GET /manager/text/stop?path=/app2 HTTP/1.1\" 200 70 70 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:25 +0200] \"GET /manager/text/start?path=/app2 HTTP/1.1\" 200 73 73 Ahora hacemos un ejemplo que permita o no unas ips o puertos: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\"/> solo acceden los que tienen la ip de localhost 127.0.0.1. Si ponemos nuestra ip:8888 no nos deja porque no hemos entrado por la de localhost indicada en el valve.","title":"VALVES"},{"location":"tomcat/#jdbc-bbdd","text":"Instalamos la herramienta probe para ver de otra manera el contenido de tomcat como una consola. Instalamos mariadb como bbdd mysql. MariaDB [seguridad]> select * from usuarios; +----------+ | nombre | +----------+ | Miguel | | Cristina | | Isabel | +----------+ 3 rows in set (0.000 sec) MariaDB [seguridad]> create user 'desa'@'localhost' identified by 'desa'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> grant all on seguridad.* to 'desa'@'localhost'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> commit; Instalamos el driver jdbc de mariadb en nuestro caso en: [isx46410800@miguel tomcat]$ sudo cp ~/Downloads/mariadb-java-client-2.6.1-sources.jar /home/tomcat/lib Configuramos el recurso de bbdd en tomcat que utilice ese driver de jdbc: <Context path=\"/jdbc\" > <Resource name=\"jdbc/cursoDB\" auth=\"Container\" type=\"javax.sql.DataSource\" username=\"desa\" password=\"desa\" driverClassName=\"org.mariadb.jdbc.Driver\" url=\"jdbc:mariadb://localhost:3306/seguridad\" initialSize=\"4\" maxActive=\"15\" maxIdle=\"3\"/> </Context>","title":"JDBC BBDD"},{"location":"tomcat/#realm","text":"Son otras opciones de seguridad para en contra de Tomcat, proteger recursos, impedir accesos indeseados... <!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --> <Realm className=\"org.apache.catalina.realm.LockOutRealm\"> <!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key \"UserDatabase\". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --> <Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/> </Realm> Sirven para proteger recursos webs y se basa en usuarios y roles asociados a las aplicaciones. Creamos un realm: <Context path=\"/ejeRealm\"> <Realm className=\"org.apache.catalina.realm.MemoryRealm\" pathname=\"conf/usuarios.xml\"/> </Context> en usuarios.xml: <!xml version=\"1.0\" enconding=\"UTF-8\"?> <tomcat-users> <user username=\"usu1\" password=\"usu1\" roles=\"usuario\"/> </tomcat-users> Y con la app que pasamos, indicamos que solo podr\u00e1 entrar segun el usuario o roles indicados en la app.","title":"REALM"},{"location":"tomcat/#apache-con-tomcat","text":"Descargamos el conector mod_jk Luego tenemos que instalar como root una serie de paquetes como el paquete de http de desarrollador, el framework runtime apr para trabajar como dev, el compilador de c... [root@miguel tomcat]# dnf install httpd-devel apr apr-devel apr-util apr-util-devel gcc make libtool Despues vamos como root al directorio para compilar un arhcivo y meterlo en el tomcat lo que necesitamos. 1- /home/isx46410800/Downloads/tomcat-connectors-1.2.48-src/native 2- Configure para configurar, preparar librerias, el codigo fuente 3- make para compilar 4- make-install para instalar 5- tendremos el fichero mod_jk.so para poder conectar el apache con el tomcat, y puedan entenderse entre ellos. [root@miguel native]# #./configure --with-apxs=/bin/apxs [root@miguel native]# whereis apxs apxs: /usr/bin/apxs /usr/share/man/man1/apxs.1.gz [root@miguel native]# make [root@miguel apache-2.0]# cd apache-2.0/ [root@miguel apache-2.0]# ll mod_jk.s* -rwxr-xr-x. 1 root root 1961464 may 22 15:49 mod_jk.so [root@miguel apache-2.0]# cp mod_jk.so /etc/httpd/modules/ Despues a\u00f1adimos un archivo a la configuracion de apache en /etc/httpd/conf/httpd.conf: IncludeOptional conf/mod_jk.conf [root@miguel conf]# cat mod_jk.conf # Cargamos el modulo LoadModule jk_module modules/mod_jk.so # Indicamos dinde esta el fichero workers.properties JkWorkersFile conf/workers.properties # Podemos generar ficheros de log JkShmFile logs/mod_jk.shm JkLogFile logs/mod_jk.log JkLogLevel info # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus <Location /jkmanager/> </Location> # Mapeamos las URL contra un conjunto de workers JkMount /* MisTomcat Creamos nuestros ficheros de workers.properties de lo que ser\u00e1 nuestros tomcats diferentes, clusters etc(conf/workers.properties). # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status #worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 #worker.worker2.type=ajp13 #worker.worker2.host=localhost #worker.worker2.port=9010 #worker.worker3.type=ajp13 #worker.worker3.host=localhost #worker.worker3.port=10011 # Workers en el Load Balancer worker.MisTomcat.balance_workers=worker1 #worker.MisTomcat.balance_workers=worker1,worker2,worker3 Por ahora comentamos los workers2 y 3 porque solo trabajamos con uno. La info de el puerto y tipo ajp13 la vemos en la info de conf/server.xml y descomentamos la linea de ajp, indicamos los datos y a\u00f1adimos un parametro para que no de fallo: <!-- Define an AJP 1.3 Connector on port 8009 --> <Connector protocol=\"AJP/1.3\" address=\"::1\" port=\"8009\" secretRequired=\"false\" redirectPort=\"8443\" /> Ahora vemos que al pasarle al localhost sin puerto algo, nos manda al tomcat:","title":"APACHE CON TOMCAT"},{"location":"tomcat/#clusters","text":"Instalaremos dos tomcats m\u00e1s y en cada uno de ellos modificamos el puertos y el ajp correspondiente: [tomcat@miguel ~]$ vim tomcat1/conf/server.xml [tomcat@miguel ~]$ vim tomcat2/conf/server.xml Eliminamos la variable del .bashrc que apuntaba cual era el tomcat, ya que ahora al tener 3 no podemos indicar que sea el primero solo. Tambi\u00e9n podemos abrir tres terminales y exportar la variable con el home correspondiente: [tomcat@miguel tomcat1]$ export CATALINA_HOME=/home/tomcat/tomcat1 Despues arrancamos cada maquina desde su bin para que pille esa variable: [tomcat@miguel bin]$ ./catalina.sh run Ahora modificamos el worker.properties de apache para que haya 3 workers con su ajp correspondiente, en modo balanceador de carga y para que mantenga la sesion con el el sticky sesion: # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 worker.worker2.type=ajp13 worker.worker2.host=localhost worker.worker2.port=9009 worker.worker3.type=ajp13 worker.worker3.host=localhost worker.worker3.port=7009 # Workers en el Load Balancer #worker.MisTomcat.balance_workers=worker1 worker.MisTomcat.balance_workers=worker1,worker2,worker3 worker.MisTomcat.sticky_session=true Paramos y arrancamos de nuevo el apache. Para probar el entorno de cluster copiamos el war de cluster para saber en cual estoy en los 3 tomcats: [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat1/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat2/webapps/. Probamos: http://localhost/cluster/hello.jsp Probamos la parte del jkstatus como worker: # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus Del worker properties: # Activar los workers worker.jkstatus.type=status Comprobamos con http://localhost/jkmanager/","title":"CLUSTERS"},{"location":"tomcat/#persistencia-en-tomcat","text":"Sirve para cuando estamos en una session alojada por ejemplo en tomcat 1, todo se quede alojado ahi y no recargue en otro tomcat y perdamos todo lo de la sesion. En el server.xml de cada uno a\u00f1adimos el jvmroute. Lo que hace es identificar cada sesion con un ID de sesion y lo asigna al worker que ya indicamos el el worker.properties: <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker1\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker2\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker3\"> Descomentamos tambi\u00e9n la opcion de cluster : <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/> Si vamos al navegador y ponemos ajustes - web developer -storage inspector, veremos que nos sale siempre el mismo id worker de la sesion.","title":"PERSISTENCIA EN TOMCAT"},{"location":"tomcat/#virtualhosts","text":"Estos crea dentro de un mismo tomcat, diferentes dominios repartidos para que segun lo que sea, trabaja una cosa u otra. En server.xml ponemos: #Virtual host 1 <Host name=\"empresa1.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa1.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"example_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa1\" debug=\"0\" reloadable=\"true\"/> </Host> #Virtual host 2 <Host name=\"empresa2.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa2.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"mydomain_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa2\" debug=\"0\" reloadable=\"true\"/> </Host> el alias es a lo que responde la petici\u00f3n, con este nombre referido. Para probar estos alias tenemos que ponerlos en el etc/hosts, dns o en el hosting. 192.168.1.113 www.empresa1.com 192.168.1.113 www.empresa2.com Copiamos la app de prueba de virtual host y comprobamos que por los dos alias te responde segun lo indicado en el server.xml Tenemos una herramienta que es el VHOST MANAGER. Para ello necesitamos crear un usuario con los permisos en el tomcat-users.xml: <user username=\"admin1\" password=\"jupiter\" roles=\"admin-gui,admin-script\"/> Rebotamos y entramos desde inicio - host manager o http://localhost:8888/host-manager/html Aqu\u00ed podemos eliminar, a\u00f1adirlo con opciones indicadas y para que se guarde los cambios a final de pagina hemos de a\u00f1adir la siguiente directiz en server.xml. <Listener className=\"org.apache.catalina.storeconfig.StoreConfigLifecycleListener\"/>","title":"VIRTUALHOSTS"},{"location":"vscode/","text":"VISUAL STUDIO CODE Partimos del curso gratis con tips de VSCODE EDICIONES Y TIPS B\u00c1SICOS SELECCIONAR ALGO Se apreta SHIFT + FLECHAS QUITAR ESPACIOS Se apreta SHIFT + TAB QUITAR BARRA LATERAL Se apreta CONTROL + b ORDENAR LINEAS Se apreta ALT + FLECHA ARRIBA/ABAJO en cualquier parte de la frase y movemos para cambiar el orden: <ul> <li>L\u00ednea 1</li> <li>L\u00ednea 2</li> <li>L\u00ednea 3</li> <li>L\u00ednea 4</li> <li>L\u00ednea 5</li> <li>L\u00ednea 6</li> <li>L\u00ednea 7</li> </ul> Para lineas de m\u00e1s de una linea se SELECCIONA TODAS LAS LINEAS y se apreta ALT + FLECHA ARRIBA/ABAJO: <ul> <li> <span>l\u00ednea 1</span> <span>Nada importante 1</span> </li> <li> <span>l\u00ednea 2</span> <span>Nada importante 2</span> </li> <li> <li> <span>l\u00ednea 3</span> <span>Nada importante 3</span> </li> <span>l\u00ednea 4</span> <span>Nada importante 4</span> </li> </ul> COMENTAR C\u00d3DIGO Se apreta CONTROL + SHIFT + A o CONTROL + SHIFT + / Para trozos en medio de frases solo la opci\u00f3n de CONTROL + SHIFT + A. CREAR UNA RUTA DE ARCHIVO Apretamos CONTROL + CLICK y creamos file de la ruta clicada y se crea todos los direcotorios y ficheros: <script src=\"assets/js/app.js\"></script> Tambi\u00e9n sirve para ir a una funci\u00f3n concreta de un archivo enlazado. O vista previa sin salir del documento con SHIFT + F12. BORRAR LINEAS Borra una linea CONTROL + SHIFT + K. Borrar todo que se llame igual CONTROL + SHIFT + L y CONTROL + SHIFT + K. REHACER / DESHACER CONTROL + Z deshacer. CONTROL + SHIFT + Z rehacer. ZEN MODE Se apreta CONTROL + K y luego Z. NAVEGACI\u00d3N PESTA\u00d1AS Ctrl + W Cerrar tab Ctrl + K Ctrl + W Cerrar todas Ctrl + Shift + T Reabrir anterior Ctrl + TAB Cambiar de tab ABRIR TERMINAL Se apreta CONTROL + ` o icono abajo derecha de >. Aqui se puede usar como bash normal, abrir mas terminales, matar terminal, etc. LLAMAR A LA PALETA DE BUSQUEDA Se apreta CONTROL + SHIFT + P. Ejemplo de wrap with abreviation y despues como queremos encapsular unas palabras: code, ul>li... CONFIGURAR UN SHORTCUT. Manage -> keyboard shorcuts -> buscamos por ejemos wrap y ponemos la combinaci\u00f3n que queremos. Tambien CONTROL + K + CONTROL + S. MULTICURSORES Y EDICI\u00d3N R\u00c1PIDA CLONAR LINEAS Shortcut COPY LINE DOWN Y UP. CREAR MULTICURSOR ARRIBA/ABAJO Se apreta CONTROL + SHIFT + FLECHAS y escribes una sola vez que sirve para todo. MULTICURSOS CON COPY Partimos de: <span>amarillo</span> <span>rojo</span> <span>verde</span> <span>naranja</span> <span>morado</span> <span>negro</span> <span>blanco</span> Creamos un cursos despues de span, luego nos ponemos inico de la palabra del color y CONTROL + SHIFT + FELCHA ADELANTE, copiamos, vamos para atras y pegamos: <span class=\"amarillo\">amarillo</span> <span class=\"rojo\">rojo</span> <span class=\"verde\">verde</span> <span class=\"naranja\">naranja</span> <span class=\"morado\">morado</span> <span class=\"negro\">negro</span> <span class=\"blanco\">blanco</span> MULTICURSOR PARA FORMATO Se crea multicursor con CONTROL + SHIFT + FLECHAS y damos espacios para quedar alineado. De palabras seleccionamos con ALT copiamos, vamos para atras y pegar: <span>amarillo</span> <p>rojo</p> <div-personalizado>verde</div-personalizado> <bold>naranja</bold> <otro-div-complejo>naranja-azul</otro-div-complejo> <!-- Objetivo final --> <span class=\"amarillo\">amarillo</span> <p class=\"rojo\">rojo</p> <div-personalizado class=\"verde\">verde</div-personalizado> <bold class=\"naranja\">naranja</bold> <otro-div-complejo class=\"naranja-azul\">naranja-azul</otro-div-complejo> LOWECASE / UPPERCASE Creamos multicursor y CONTROL + SHIFT + U/L SELECCION VARIAS COSAS A LA VEZ CONTROL + D. DEFINICIONES Y SNIPPETS BUSCAR DEFINICIONES Ctrl + P => luego escribir la @: Ctrl + Shift = O BUSCAR LINEAS CONTROL + P + :n\u00balinea MARKDOWN PREVIEW CONTROL + SHIFT + V CONTROL + K V Ctrl + P : Markdown Open Preview Ctrl + P : Markdown Open Preview to the side REPLACE SYMBOL Para reemplazar la misma palabra en todos los documentos enlazados o llamados se apreta F2. CREAR SNIPPET Es un fragmento de c\u00f3digo ya creado y poder llamarlo. Manage -> usar snippet -> elegir lenguaje. \"Print to console\": { \"prefix\": \"log\", \"body\": [ \"console.log('${1:Hola mundo}');\", \"$2\" ], \"description\": \"Log output to console\" } Lo llamamos con el nombre del 'prefix'. EXTENSIONES PASTE JSON AS CODE TERMINAL TODO FREE BOOKMARKS MATERIAL ICON THEME MATERIAL THEME LIVE SERVER COLOR HIGHLIGHT BRAKET PAIR COLORIZED 2 GIT https://code.visualstudio.com/docs/editor/versioncontrol https://code.visualstudio.com/docs/editor/github DOCKER https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker https://code.visualstudio.com/docs/containers/overview","title":"VISUAL STUDIO CODE"},{"location":"vscode/#visual-studio-code","text":"Partimos del curso gratis con tips de VSCODE","title":"VISUAL STUDIO CODE"},{"location":"vscode/#ediciones-y-tips-basicos","text":"","title":"EDICIONES Y TIPS B\u00c1SICOS"},{"location":"vscode/#seleccionar-algo","text":"Se apreta SHIFT + FLECHAS","title":"SELECCIONAR ALGO"},{"location":"vscode/#quitar-espacios","text":"Se apreta SHIFT + TAB","title":"QUITAR ESPACIOS"},{"location":"vscode/#quitar-barra-lateral","text":"Se apreta CONTROL + b","title":"QUITAR BARRA LATERAL"},{"location":"vscode/#ordenar-lineas","text":"Se apreta ALT + FLECHA ARRIBA/ABAJO en cualquier parte de la frase y movemos para cambiar el orden: <ul> <li>L\u00ednea 1</li> <li>L\u00ednea 2</li> <li>L\u00ednea 3</li> <li>L\u00ednea 4</li> <li>L\u00ednea 5</li> <li>L\u00ednea 6</li> <li>L\u00ednea 7</li> </ul> Para lineas de m\u00e1s de una linea se SELECCIONA TODAS LAS LINEAS y se apreta ALT + FLECHA ARRIBA/ABAJO: <ul> <li> <span>l\u00ednea 1</span> <span>Nada importante 1</span> </li> <li> <span>l\u00ednea 2</span> <span>Nada importante 2</span> </li> <li> <li> <span>l\u00ednea 3</span> <span>Nada importante 3</span> </li> <span>l\u00ednea 4</span> <span>Nada importante 4</span> </li> </ul>","title":"ORDENAR LINEAS"},{"location":"vscode/#comentar-codigo","text":"Se apreta CONTROL + SHIFT + A o CONTROL + SHIFT + / Para trozos en medio de frases solo la opci\u00f3n de CONTROL + SHIFT + A.","title":"COMENTAR C\u00d3DIGO"},{"location":"vscode/#crear-una-ruta-de-archivo","text":"Apretamos CONTROL + CLICK y creamos file de la ruta clicada y se crea todos los direcotorios y ficheros: <script src=\"assets/js/app.js\"></script> Tambi\u00e9n sirve para ir a una funci\u00f3n concreta de un archivo enlazado. O vista previa sin salir del documento con SHIFT + F12.","title":"CREAR UNA RUTA DE ARCHIVO"},{"location":"vscode/#borrar-lineas","text":"Borra una linea CONTROL + SHIFT + K. Borrar todo que se llame igual CONTROL + SHIFT + L y CONTROL + SHIFT + K.","title":"BORRAR LINEAS"},{"location":"vscode/#rehacer-deshacer","text":"CONTROL + Z deshacer. CONTROL + SHIFT + Z rehacer.","title":"REHACER / DESHACER"},{"location":"vscode/#zen-mode","text":"Se apreta CONTROL + K y luego Z.","title":"ZEN MODE"},{"location":"vscode/#navegacion-pestanas","text":"Ctrl + W Cerrar tab Ctrl + K Ctrl + W Cerrar todas Ctrl + Shift + T Reabrir anterior Ctrl + TAB Cambiar de tab","title":"NAVEGACI\u00d3N PESTA\u00d1AS"},{"location":"vscode/#abrir-terminal","text":"Se apreta CONTROL + ` o icono abajo derecha de >. Aqui se puede usar como bash normal, abrir mas terminales, matar terminal, etc.","title":"ABRIR TERMINAL"},{"location":"vscode/#llamar-a-la-paleta-de-busqueda","text":"Se apreta CONTROL + SHIFT + P. Ejemplo de wrap with abreviation y despues como queremos encapsular unas palabras: code, ul>li...","title":"LLAMAR A LA PALETA DE BUSQUEDA"},{"location":"vscode/#configurar-un-shortcut","text":"Manage -> keyboard shorcuts -> buscamos por ejemos wrap y ponemos la combinaci\u00f3n que queremos. Tambien CONTROL + K + CONTROL + S.","title":"CONFIGURAR UN SHORTCUT."},{"location":"vscode/#multicursores-y-edicion-rapida","text":"","title":"MULTICURSORES Y EDICI\u00d3N R\u00c1PIDA"},{"location":"vscode/#clonar-lineas","text":"Shortcut COPY LINE DOWN Y UP.","title":"CLONAR LINEAS"},{"location":"vscode/#crear-multicursor-arribaabajo","text":"Se apreta CONTROL + SHIFT + FLECHAS y escribes una sola vez que sirve para todo.","title":"CREAR MULTICURSOR ARRIBA/ABAJO"},{"location":"vscode/#multicursos-con-copy","text":"Partimos de: <span>amarillo</span> <span>rojo</span> <span>verde</span> <span>naranja</span> <span>morado</span> <span>negro</span> <span>blanco</span> Creamos un cursos despues de span, luego nos ponemos inico de la palabra del color y CONTROL + SHIFT + FELCHA ADELANTE, copiamos, vamos para atras y pegamos: <span class=\"amarillo\">amarillo</span> <span class=\"rojo\">rojo</span> <span class=\"verde\">verde</span> <span class=\"naranja\">naranja</span> <span class=\"morado\">morado</span> <span class=\"negro\">negro</span> <span class=\"blanco\">blanco</span>","title":"MULTICURSOS CON COPY"},{"location":"vscode/#multicursor-para-formato","text":"Se crea multicursor con CONTROL + SHIFT + FLECHAS y damos espacios para quedar alineado. De palabras seleccionamos con ALT copiamos, vamos para atras y pegar: <span>amarillo</span> <p>rojo</p> <div-personalizado>verde</div-personalizado> <bold>naranja</bold> <otro-div-complejo>naranja-azul</otro-div-complejo> <!-- Objetivo final --> <span class=\"amarillo\">amarillo</span> <p class=\"rojo\">rojo</p> <div-personalizado class=\"verde\">verde</div-personalizado> <bold class=\"naranja\">naranja</bold> <otro-div-complejo class=\"naranja-azul\">naranja-azul</otro-div-complejo>","title":"MULTICURSOR PARA FORMATO"},{"location":"vscode/#lowecase-uppercase","text":"Creamos multicursor y CONTROL + SHIFT + U/L","title":"LOWECASE / UPPERCASE"},{"location":"vscode/#seleccion-varias-cosas-a-la-vez","text":"CONTROL + D.","title":"SELECCION VARIAS COSAS A LA VEZ"},{"location":"vscode/#definiciones-y-snippets","text":"","title":"DEFINICIONES Y SNIPPETS"},{"location":"vscode/#buscar-definiciones","text":"Ctrl + P => luego escribir la @: Ctrl + Shift = O","title":"BUSCAR DEFINICIONES"},{"location":"vscode/#buscar-lineas","text":"CONTROL + P + :n\u00balinea","title":"BUSCAR LINEAS"},{"location":"vscode/#markdown-preview","text":"CONTROL + SHIFT + V CONTROL + K V Ctrl + P : Markdown Open Preview Ctrl + P : Markdown Open Preview to the side","title":"MARKDOWN PREVIEW"},{"location":"vscode/#replace-symbol","text":"Para reemplazar la misma palabra en todos los documentos enlazados o llamados se apreta F2.","title":"REPLACE SYMBOL"},{"location":"vscode/#crear-snippet","text":"Es un fragmento de c\u00f3digo ya creado y poder llamarlo. Manage -> usar snippet -> elegir lenguaje. \"Print to console\": { \"prefix\": \"log\", \"body\": [ \"console.log('${1:Hola mundo}');\", \"$2\" ], \"description\": \"Log output to console\" } Lo llamamos con el nombre del 'prefix'.","title":"CREAR SNIPPET"},{"location":"vscode/#extensiones","text":"PASTE JSON AS CODE TERMINAL TODO FREE BOOKMARKS MATERIAL ICON THEME MATERIAL THEME LIVE SERVER COLOR HIGHLIGHT BRAKET PAIR COLORIZED 2 GIT https://code.visualstudio.com/docs/editor/versioncontrol https://code.visualstudio.com/docs/editor/github DOCKER https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker https://code.visualstudio.com/docs/containers/overview","title":"EXTENSIONES"},{"location":"windows/","text":"ADMINISTRAR WINDOWS SERVER 2019 INSTALACI\u00d3N Para probar lo haremos con una maquina virtual. En este caso probaremos con HYPER-V , una herramienta de virtualizaci\u00f3n propia de windows que utiliza un hypervisor. Hyper-V es un programa de virtualizaci\u00f3n de Microsoft basado en un hipervisor para los sistemas de 64 bits\u200b con los procesadores basados en AMD-V o Tecnolog\u00eda de virtualizaci\u00f3n Intel. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Los windows 10 home no lo tienen, no obstante con este fichero lo podremos aplicar: Enable HYPER-V Al reiniciar si no funciona directamente tendremos que hacer las siguientes opciones: Habilitar virtualizaci\u00f3n en la BIOS En la powershell comando bcdedit /set hypervisorlaunchtype auto En la powershell comando dism /online /enable-feature /featurename:Microsoft-Hyper-V -All Iniciamos maquina virtual y configuramos: Idioma/Regi\u00f3n S.O. versi\u00f3n standard evaluation con experiencia de escritorio para que tenga ventanas y no solo consola Instalaci\u00f3n personalizada (si le damos a SHIFT+F10 nos abre una consola de cmd para poder hacer cosas tambi\u00e9n) Varios reinicios Contrase\u00f1a administrador El controlador de dominio es el centro neur\u00e1lgico de un dominio Windows, tal como un servidor Network Information Service (NIS) lo es del servicio de informaci\u00f3n de una red Unix. Los controladores de dominio tienen una serie de responsabilidades, y una de ellas es la autenticaci\u00f3n, que es el proceso de garantizar o denegar a un usuario el acceso a recursos compartidos o a otra m\u00e1quina de la red, normalmente a trav\u00e9s del uso de una contrase\u00f1a. Esto permite validar a los usuarios de una red para ser partes de la plataforma de clientes que recibir\u00e1n los servicios de informaci\u00f3n. Cada controlador de dominio usa un security account manager (SAM), o NTDS en Windows 2003 Server (que es la forma promovida de la SAM, al pasar como controlador de dominio), para mantener una lista de pares de nombre de usuario y contrase\u00f1a. El controlador de dominio entonces crea un repositorio centralizado de contrase\u00f1as, que est\u00e1n enlazados a los nombres de usuarios (una clave por usuario), lo cual es m\u00e1s eficiente que mantener en cada m\u00e1quina cliente centenares de claves para cada recurso de red disponible. En un dominio Windows, cuando un cliente no autorizado solicita un acceso a los recursos compartidos de un servidor, el servidor act\u00faa y pregunta al controlador de dominio si ese usuario est\u00e1 autentificado. Si lo est\u00e1, el servidor establecer\u00e1 una conexi\u00f3n de sesi\u00f3n con los derechos de acceso correspondientes para ese servicio y usuario. Si no lo est\u00e1, la conexi\u00f3n es denegada. Una vez que el controlador de dominio autentifica a un usuario, se devuelve al cliente una ficha especial (token) de autenticaci\u00f3n, de manera que el usuario no necesitar\u00e1 volver a iniciar sesi\u00f3n para acceder a otros recursos en dicho dominio, ya que el usuario se considera autentificado en el dominio. SERVER MANAGER El server manager(administrador del servidor) es el aplicativo para configurar todas las cosas del server. Antes miramos las tarjetas de red con el comando ncpa.cpl . Elegimos la tarjeta - propiedades - ipv4 - propiedades y indicamos la IP que queremos ponerle. Cambiar nombre en servidor local y lo ponemos como un control de dominio(domain controller): En servidor local activamos el escritorio remoto y desactivamos aqui solo(en produccion no) la seguridad de Iexplorer. En servidor local abajo en rendimiento, iniciamos contador y a la dercha configuramos las alertas para 7 dias. En servidor local , parte arriba derecha en administrar le damos a agregar roles y caracteristicas. Seleccionamos el servidor y despues en roles de servidor, seleccionamos servicios de dominio de Active Directory . Siguiente, siguiente e instalamos. En powershell ser\u00eda con el comando install-windowsfeature y para ver los que hay get-windowsfeature Despues de instalar le damos a promover servicio para configurar el AD: Si se trabaja con aparatos mas antiguos, el nivel funcional hay que bajarlos. Siguiente pantalla En Panel - derecha Herramientas - DNS: En DC01 boton derecho ejecutar nslookup(nos sale unkown) Solucinamos dandole a zona inversa crear nueva. Siguiente hasta encontrar el cajon de poner ip, ponemos 192.168.1 Ahora actualizamos el puntero PTR en la directa dando boton derecho a la de la ip y le damos a actualizar y nos sale En boton derecho DC01 propiedades - reenviadores le ponemos los DNS. En revisi\u00f3n hacemos la prueba ESTRUCTURANDO EL AD Representaremos este ejemplo: Vamos al Panel - derecha Herramientas - Usuarios y equipos de Active Directory. Entramos a la organizaci\u00f3n creada de miguel.local y dentro creamos la organizaci\u00f3n global Miguel con boton derecho - nuevo - organizaci\u00f3n. Hemos creado un global porque luego para crear directivas de grupo, afectar\u00e1 a todo lo de abajo y sino tendriamos problemas porque afectaria a cosas que estan en la misma altura que no queremos que tengan esto. Ahora entramos en esta y vamos creando dentro nuevos deparamentos: Si tenemos algun error, al tener la casilla marcada de no poder eliminar. Vamos al menu ver - caracteriticas avanzadas - buscamos el departamento - propiedades - recursos y eliminamos la casilla y luego ya podremos mover o eliminar ese departamento. Ahora entramos en cada departamento y vamos creando usuarios en cada uno con una misma contrase\u00f1a. Ahora dentro de cada departamento , creamos un grupo con el mismo nombre. Esto se hace para poder luego compartir recursos, ya que por unidades organizativas no se puede. El departamento global de Miguel tambien tiene que tener un grupo creado. Porque asi si creamos un recurso o algo global, si se lo damos a esto, se lo aplica a todos los de su union: Ahora en cada departamento, seleccionamos todos los usuarios que hayan, boton derecho asignar un nuevo grupo. Escribimos el nombre del grupo, le damos a comprobar nombres y aceptamos. Si entramos a las propiedades del grupo, veremos que est\u00e1n como miembros. Despues de cada grupo, lo asignamos al grupo general Miguel . GPO Cpanel - Herramientas - Administracion de directivas de grupo. Todas las directivas se guardan en objetos de directivas de grupo. Vemos la que hay y las configuraciones que tienen: Para editar una vamos a boton derecho y editar. Desde aqu\u00ed para modificar algo se busca siguiendo la misma ruta que en la configuraci\u00f3n. Para crear una directiva de grupo(GPO) vamos a objetos y boton derecho crear. Todas las GPO para vincularlas necesitan una unidad organizativa. Para ello, vamos a la unidad de Contabilidad, boton derecho y vincular a la creada. Tambien se puede crear directamente yendo a la unidad y boton derecho crear GPO y vincular: GPO de inicio: sirve para que esa directiva sea comun para todos. Vamos a GPO de inicio y crear. Unas de las m\u00e1s comunes es el fondo de escritorio o que no usen los USB. Para crear una, en contenido boton derecho nuevo. Ahora si creamos una directiva de grupo, le podemos indicar que sea como base la de inicio: Para editar una GPO de inicio, entramos vamos a configuracion boton derecho edicion. Tocamos lo que sea y en la proxima GPO que creemos veremos que tiene estas configuraciones, partir\u00e1 con la reciente modificaci\u00f3n. Siempre parten de como esten en ese momento, los cambios no afectan a todas las que esten asignadas. Se puede crear GPO para la raiz y hereda para todos. No obstante, si queremos que algun departamento no herede, vamos a la unidad organizativa, boton derecho eliminar herencia: No obstante, aunque se quita la herencia. Si las GPO de la raiz le doy boton derecho y exigido. La van a tener si o si: La parte ultima de resultados de directivas de grupo, se puede generar un informe de un usuario/grupo de todas las directivas que tiene. Se puede hacer por consola con la orden gpresult /h file.html RAS(Remote Access Server) Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos acceso remoto que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Una vez instalado vamos a herramientas - enrutamiento y acceso remoto Vamos a boton derecho en DC01 y configurar, despues a tradducion de NAT y elegimos cual es la interfaz a la que conectarse y aceptar. NAT permite que con una solo IP publica puedan varios dispositivos salir a internet por esta IP DHCP Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos DHCP que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Despues de con configuraci\u00f3n posterior, vamos a herramientas - dhcp. En ipv4 creamos ambito nuevo, ponemos la ip del servidor y le damos un rango a partir de la siguiente a esa ip. PRACTICA DC Y 3 SERVERS Nombre servidor: dc01 -Tarjetas de Red propia y privada(ip/dnd 192.168.16.100 Instalar active directory y configurar nuevo bosque Hacer dns inverso de la red 192.168.16 Instalar acceso remoto y enruntamiento, configurar enruntamiento, opci\u00f3n Nat con la Red wan instalar Dhcp y configurar. Creamos \u00e1mbito, rango de ips 1-50 ejemplo. Puerta enlace ser\u00e1 nuestra ip del server abrimos otra m\u00e1quina de Windows y probamos que se pueda conectar x Dhcp del server y pueda navegar. No conectar directo a la Red que sale. crear unidades/usuarios con centro de administraci\u00f3n de AD o en grupo de unidades organizativas o usuarios y equipos AD. Siempre creamos una ra\u00edz. Los usuarios gestionamos su horario de conexi\u00f3n. Podemos hacer uno de modo plantilla y el resto copiar a partir de ah\u00ed. Creamos un grupo tmb de cada cosa. Agregamos cada usuario a su grupo. Y grupo con subgrupos instalamos en servicio de archivos - scsi - desduplicacion de datos. En configuraci\u00f3n luego en vol\u00famenes seleccionamos que unidad queremos hacerlo y su programaci\u00f3n. Esto ahorra mucho espacio y junta fragmentos de mismo archivos. crear una GPO de pol\u00edtica en objetos de administraci\u00f3n de unidades organizativas y editamos. Habilitamos directiva de redes y el ping(firewall permitir excepciones compartir archivos e impresoras entrantes y ponemos la ip de la Red global) y la de (firewallbpermir excepciones icmp). Tmb inicio de sesi\u00f3n interactivo t\u00edtulo mensaje al iniciar y luego el de texto mensaje. Vinculamos esta pol\u00edtica a la unidad general creada para que solo afecte a ellos. instalamos en servicios archivos de almacenamiento - scsi - administrador de recursos del servidor de archivos. Dentro en caracter\u00edsticas a\u00f1adimos copias de seguridad de Windows server. creamos una copia de seguridad - programamos personalizada todo lo que queremos hacer de copia y donde guardarlo. en recursos compartidos - tarea nuevo recurso y seleccionar carpeta a compartir y seleccionar qui\u00e9n puede acceder y sus permisos. Podemos decir que en la carpeta compartida los users no puedan almacenar cierto tipo de archivos. Vamos a herramientas - administrador de recursos compartidos del servidor - filtrado - filtros. Seleccionamos carpeta y ponemos po ejemplo bloquear archivos d audio y v\u00eddeo. Tambi\u00e9n podemos crear cuota para ponerle l\u00edmites a las carpetas abrimos otro Windows y cambiamos nombre del equipo y lueg lo a\u00f1adimos al dominio creado. se puede hacer una prueba de reiniciar remotamente el equipo desde cmd server. de un volumen o disco podemos ir a propiedades y habilitar instant\u00e1neas para poder volver a Estados anteriores. recuperar servidor desde una copia de seguridad. Arrancamos con la iso, reparar equipo, solucionar problemas, recuperar de una imagen del sistema y elegimos la copia. vpn modo sencillo: enrutamienti y acceso remoto. Bot\u00f3n derecho y en mi dc01 y configurar. Seleccionar vpn y elegimos interfaz que da Internet wan. Autom\u00e1ticamente elegimos, no radius. Vamos a otro equipo,configuraci\u00f3n de vpn vpn manual. Lo mismo pero elegimos Nat la interfaz. Propiedades de dc01, enrutador de la y servidor remoto. En ipv4 creamos interfaz /prot\u00f3 igmp(wan proxy y lan enrutador). Interfaz /prot\u00f3 Nat (wan interfaz Internet y habilitar;puertos de puerta enlace y de seguridad 4,ip loop, lan interfaz privada). Puertos/clientes propiedades (enrutador enrutamienti y servidor ipv4. Puertos propiedades activamos las dos opciones. 3 servers; ras, Dhcp, AD. AD en dominio con puerta enlace ras y dns el mismo 192.168.16.200 // 254//200 Ras dis interfaces. 192.168.100. 254 se instala enrutamiento y acceso remoto. Dhcp instala Dhcp puerta enlace ras //251 //254 En el ad se a\u00f1ade d servidor el dhcp Cliente coge IP del dhcp k coge Internet por el ras NOTAS Nat Y Red interna. Dhcp el del otro adaptador Crear equipos en herramientas de directivas de grupo, y en los usuarios creados cuando inicien sesi\u00f3n, en propiedades del sistema, agregar al dominio creado. En Linux en admin - autenticaci\u00f3 - cuentas de usuarios de winbind. Controladores es la IP y dominio con nombre corto Agregar caracter\u00edsticas para recursos compartidos y creamos una carpeta que sea compartida por los usuarios que digamos Si conectamos otro pc cn sistema en la misma Red interna del Dhcp como adaptador, le da la IP el dhcp Comandos cmd.: D: dir hostname sconfig cd c:\\Windows\\ruta shutdown Comandos powershell: dsadd agregar objetos, cuentas... Crear vlans: En Dhcp creamos \u00e1mbitos con diferentes rangos de ups y despu\u00e9s activamos \u00e1mbito. En opciones de servidores se crean los registros que ser\u00e1n comunes para todos los ambitos: se a\u00f1aden registros ips d dns, correo, enrutador Se puede crear superambito para unir zonas de \u00e1mbito como por ejemplo pisos de un sitio. Las reservas sirven para reservar la misma IP al que se conecta En dns se crean las zonas inversas de cada Red a\u00f1adida","title":"Windows"},{"location":"windows/#administrar-windows-server-2019","text":"","title":"ADMINISTRAR WINDOWS SERVER 2019"},{"location":"windows/#instalacion","text":"Para probar lo haremos con una maquina virtual. En este caso probaremos con HYPER-V , una herramienta de virtualizaci\u00f3n propia de windows que utiliza un hypervisor. Hyper-V es un programa de virtualizaci\u00f3n de Microsoft basado en un hipervisor para los sistemas de 64 bits\u200b con los procesadores basados en AMD-V o Tecnolog\u00eda de virtualizaci\u00f3n Intel. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Los windows 10 home no lo tienen, no obstante con este fichero lo podremos aplicar: Enable HYPER-V Al reiniciar si no funciona directamente tendremos que hacer las siguientes opciones: Habilitar virtualizaci\u00f3n en la BIOS En la powershell comando bcdedit /set hypervisorlaunchtype auto En la powershell comando dism /online /enable-feature /featurename:Microsoft-Hyper-V -All Iniciamos maquina virtual y configuramos: Idioma/Regi\u00f3n S.O. versi\u00f3n standard evaluation con experiencia de escritorio para que tenga ventanas y no solo consola Instalaci\u00f3n personalizada (si le damos a SHIFT+F10 nos abre una consola de cmd para poder hacer cosas tambi\u00e9n) Varios reinicios Contrase\u00f1a administrador El controlador de dominio es el centro neur\u00e1lgico de un dominio Windows, tal como un servidor Network Information Service (NIS) lo es del servicio de informaci\u00f3n de una red Unix. Los controladores de dominio tienen una serie de responsabilidades, y una de ellas es la autenticaci\u00f3n, que es el proceso de garantizar o denegar a un usuario el acceso a recursos compartidos o a otra m\u00e1quina de la red, normalmente a trav\u00e9s del uso de una contrase\u00f1a. Esto permite validar a los usuarios de una red para ser partes de la plataforma de clientes que recibir\u00e1n los servicios de informaci\u00f3n. Cada controlador de dominio usa un security account manager (SAM), o NTDS en Windows 2003 Server (que es la forma promovida de la SAM, al pasar como controlador de dominio), para mantener una lista de pares de nombre de usuario y contrase\u00f1a. El controlador de dominio entonces crea un repositorio centralizado de contrase\u00f1as, que est\u00e1n enlazados a los nombres de usuarios (una clave por usuario), lo cual es m\u00e1s eficiente que mantener en cada m\u00e1quina cliente centenares de claves para cada recurso de red disponible. En un dominio Windows, cuando un cliente no autorizado solicita un acceso a los recursos compartidos de un servidor, el servidor act\u00faa y pregunta al controlador de dominio si ese usuario est\u00e1 autentificado. Si lo est\u00e1, el servidor establecer\u00e1 una conexi\u00f3n de sesi\u00f3n con los derechos de acceso correspondientes para ese servicio y usuario. Si no lo est\u00e1, la conexi\u00f3n es denegada. Una vez que el controlador de dominio autentifica a un usuario, se devuelve al cliente una ficha especial (token) de autenticaci\u00f3n, de manera que el usuario no necesitar\u00e1 volver a iniciar sesi\u00f3n para acceder a otros recursos en dicho dominio, ya que el usuario se considera autentificado en el dominio.","title":"INSTALACI\u00d3N"},{"location":"windows/#server-manager","text":"El server manager(administrador del servidor) es el aplicativo para configurar todas las cosas del server. Antes miramos las tarjetas de red con el comando ncpa.cpl . Elegimos la tarjeta - propiedades - ipv4 - propiedades y indicamos la IP que queremos ponerle. Cambiar nombre en servidor local y lo ponemos como un control de dominio(domain controller): En servidor local activamos el escritorio remoto y desactivamos aqui solo(en produccion no) la seguridad de Iexplorer. En servidor local abajo en rendimiento, iniciamos contador y a la dercha configuramos las alertas para 7 dias. En servidor local , parte arriba derecha en administrar le damos a agregar roles y caracteristicas. Seleccionamos el servidor y despues en roles de servidor, seleccionamos servicios de dominio de Active Directory . Siguiente, siguiente e instalamos. En powershell ser\u00eda con el comando install-windowsfeature y para ver los que hay get-windowsfeature Despues de instalar le damos a promover servicio para configurar el AD: Si se trabaja con aparatos mas antiguos, el nivel funcional hay que bajarlos. Siguiente pantalla En Panel - derecha Herramientas - DNS: En DC01 boton derecho ejecutar nslookup(nos sale unkown) Solucinamos dandole a zona inversa crear nueva. Siguiente hasta encontrar el cajon de poner ip, ponemos 192.168.1 Ahora actualizamos el puntero PTR en la directa dando boton derecho a la de la ip y le damos a actualizar y nos sale En boton derecho DC01 propiedades - reenviadores le ponemos los DNS. En revisi\u00f3n hacemos la prueba","title":"SERVER MANAGER"},{"location":"windows/#estructurando-el-ad","text":"Representaremos este ejemplo: Vamos al Panel - derecha Herramientas - Usuarios y equipos de Active Directory. Entramos a la organizaci\u00f3n creada de miguel.local y dentro creamos la organizaci\u00f3n global Miguel con boton derecho - nuevo - organizaci\u00f3n. Hemos creado un global porque luego para crear directivas de grupo, afectar\u00e1 a todo lo de abajo y sino tendriamos problemas porque afectaria a cosas que estan en la misma altura que no queremos que tengan esto. Ahora entramos en esta y vamos creando dentro nuevos deparamentos: Si tenemos algun error, al tener la casilla marcada de no poder eliminar. Vamos al menu ver - caracteriticas avanzadas - buscamos el departamento - propiedades - recursos y eliminamos la casilla y luego ya podremos mover o eliminar ese departamento. Ahora entramos en cada departamento y vamos creando usuarios en cada uno con una misma contrase\u00f1a. Ahora dentro de cada departamento , creamos un grupo con el mismo nombre. Esto se hace para poder luego compartir recursos, ya que por unidades organizativas no se puede. El departamento global de Miguel tambien tiene que tener un grupo creado. Porque asi si creamos un recurso o algo global, si se lo damos a esto, se lo aplica a todos los de su union: Ahora en cada departamento, seleccionamos todos los usuarios que hayan, boton derecho asignar un nuevo grupo. Escribimos el nombre del grupo, le damos a comprobar nombres y aceptamos. Si entramos a las propiedades del grupo, veremos que est\u00e1n como miembros. Despues de cada grupo, lo asignamos al grupo general Miguel .","title":"ESTRUCTURANDO EL AD"},{"location":"windows/#gpo","text":"Cpanel - Herramientas - Administracion de directivas de grupo. Todas las directivas se guardan en objetos de directivas de grupo. Vemos la que hay y las configuraciones que tienen: Para editar una vamos a boton derecho y editar. Desde aqu\u00ed para modificar algo se busca siguiendo la misma ruta que en la configuraci\u00f3n. Para crear una directiva de grupo(GPO) vamos a objetos y boton derecho crear. Todas las GPO para vincularlas necesitan una unidad organizativa. Para ello, vamos a la unidad de Contabilidad, boton derecho y vincular a la creada. Tambien se puede crear directamente yendo a la unidad y boton derecho crear GPO y vincular: GPO de inicio: sirve para que esa directiva sea comun para todos. Vamos a GPO de inicio y crear. Unas de las m\u00e1s comunes es el fondo de escritorio o que no usen los USB. Para crear una, en contenido boton derecho nuevo. Ahora si creamos una directiva de grupo, le podemos indicar que sea como base la de inicio: Para editar una GPO de inicio, entramos vamos a configuracion boton derecho edicion. Tocamos lo que sea y en la proxima GPO que creemos veremos que tiene estas configuraciones, partir\u00e1 con la reciente modificaci\u00f3n. Siempre parten de como esten en ese momento, los cambios no afectan a todas las que esten asignadas. Se puede crear GPO para la raiz y hereda para todos. No obstante, si queremos que algun departamento no herede, vamos a la unidad organizativa, boton derecho eliminar herencia: No obstante, aunque se quita la herencia. Si las GPO de la raiz le doy boton derecho y exigido. La van a tener si o si: La parte ultima de resultados de directivas de grupo, se puede generar un informe de un usuario/grupo de todas las directivas que tiene. Se puede hacer por consola con la orden gpresult /h file.html","title":"GPO"},{"location":"windows/#rasremote-access-server","text":"Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos acceso remoto que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Una vez instalado vamos a herramientas - enrutamiento y acceso remoto Vamos a boton derecho en DC01 y configurar, despues a tradducion de NAT y elegimos cual es la interfaz a la que conectarse y aceptar. NAT permite que con una solo IP publica puedan varios dispositivos salir a internet por esta IP","title":"RAS(Remote Access Server)"},{"location":"windows/#dhcp","text":"Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos DHCP que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Despues de con configuraci\u00f3n posterior, vamos a herramientas - dhcp. En ipv4 creamos ambito nuevo, ponemos la ip del servidor y le damos un rango a partir de la siguiente a esa ip.","title":"DHCP"},{"location":"windows/#practica-dc-y-3-servers","text":"Nombre servidor: dc01 -Tarjetas de Red propia y privada(ip/dnd 192.168.16.100 Instalar active directory y configurar nuevo bosque Hacer dns inverso de la red 192.168.16 Instalar acceso remoto y enruntamiento, configurar enruntamiento, opci\u00f3n Nat con la Red wan instalar Dhcp y configurar. Creamos \u00e1mbito, rango de ips 1-50 ejemplo. Puerta enlace ser\u00e1 nuestra ip del server abrimos otra m\u00e1quina de Windows y probamos que se pueda conectar x Dhcp del server y pueda navegar. No conectar directo a la Red que sale. crear unidades/usuarios con centro de administraci\u00f3n de AD o en grupo de unidades organizativas o usuarios y equipos AD. Siempre creamos una ra\u00edz. Los usuarios gestionamos su horario de conexi\u00f3n. Podemos hacer uno de modo plantilla y el resto copiar a partir de ah\u00ed. Creamos un grupo tmb de cada cosa. Agregamos cada usuario a su grupo. Y grupo con subgrupos instalamos en servicio de archivos - scsi - desduplicacion de datos. En configuraci\u00f3n luego en vol\u00famenes seleccionamos que unidad queremos hacerlo y su programaci\u00f3n. Esto ahorra mucho espacio y junta fragmentos de mismo archivos. crear una GPO de pol\u00edtica en objetos de administraci\u00f3n de unidades organizativas y editamos. Habilitamos directiva de redes y el ping(firewall permitir excepciones compartir archivos e impresoras entrantes y ponemos la ip de la Red global) y la de (firewallbpermir excepciones icmp). Tmb inicio de sesi\u00f3n interactivo t\u00edtulo mensaje al iniciar y luego el de texto mensaje. Vinculamos esta pol\u00edtica a la unidad general creada para que solo afecte a ellos. instalamos en servicios archivos de almacenamiento - scsi - administrador de recursos del servidor de archivos. Dentro en caracter\u00edsticas a\u00f1adimos copias de seguridad de Windows server. creamos una copia de seguridad - programamos personalizada todo lo que queremos hacer de copia y donde guardarlo. en recursos compartidos - tarea nuevo recurso y seleccionar carpeta a compartir y seleccionar qui\u00e9n puede acceder y sus permisos. Podemos decir que en la carpeta compartida los users no puedan almacenar cierto tipo de archivos. Vamos a herramientas - administrador de recursos compartidos del servidor - filtrado - filtros. Seleccionamos carpeta y ponemos po ejemplo bloquear archivos d audio y v\u00eddeo. Tambi\u00e9n podemos crear cuota para ponerle l\u00edmites a las carpetas abrimos otro Windows y cambiamos nombre del equipo y lueg lo a\u00f1adimos al dominio creado. se puede hacer una prueba de reiniciar remotamente el equipo desde cmd server. de un volumen o disco podemos ir a propiedades y habilitar instant\u00e1neas para poder volver a Estados anteriores. recuperar servidor desde una copia de seguridad. Arrancamos con la iso, reparar equipo, solucionar problemas, recuperar de una imagen del sistema y elegimos la copia. vpn modo sencillo: enrutamienti y acceso remoto. Bot\u00f3n derecho y en mi dc01 y configurar. Seleccionar vpn y elegimos interfaz que da Internet wan. Autom\u00e1ticamente elegimos, no radius. Vamos a otro equipo,configuraci\u00f3n de vpn vpn manual. Lo mismo pero elegimos Nat la interfaz. Propiedades de dc01, enrutador de la y servidor remoto. En ipv4 creamos interfaz /prot\u00f3 igmp(wan proxy y lan enrutador). Interfaz /prot\u00f3 Nat (wan interfaz Internet y habilitar;puertos de puerta enlace y de seguridad 4,ip loop, lan interfaz privada). Puertos/clientes propiedades (enrutador enrutamienti y servidor ipv4. Puertos propiedades activamos las dos opciones. 3 servers; ras, Dhcp, AD. AD en dominio con puerta enlace ras y dns el mismo 192.168.16.200 // 254//200 Ras dis interfaces. 192.168.100. 254 se instala enrutamiento y acceso remoto. Dhcp instala Dhcp puerta enlace ras //251 //254 En el ad se a\u00f1ade d servidor el dhcp Cliente coge IP del dhcp k coge Internet por el ras","title":"PRACTICA DC Y 3 SERVERS"},{"location":"windows/#notas","text":"Nat Y Red interna. Dhcp el del otro adaptador Crear equipos en herramientas de directivas de grupo, y en los usuarios creados cuando inicien sesi\u00f3n, en propiedades del sistema, agregar al dominio creado. En Linux en admin - autenticaci\u00f3 - cuentas de usuarios de winbind. Controladores es la IP y dominio con nombre corto Agregar caracter\u00edsticas para recursos compartidos y creamos una carpeta que sea compartida por los usuarios que digamos Si conectamos otro pc cn sistema en la misma Red interna del Dhcp como adaptador, le da la IP el dhcp Comandos cmd.: D: dir hostname sconfig cd c:\\Windows\\ruta shutdown Comandos powershell: dsadd agregar objetos, cuentas... Crear vlans: En Dhcp creamos \u00e1mbitos con diferentes rangos de ups y despu\u00e9s activamos \u00e1mbito. En opciones de servidores se crean los registros que ser\u00e1n comunes para todos los ambitos: se a\u00f1aden registros ips d dns, correo, enrutador Se puede crear superambito para unir zonas de \u00e1mbito como por ejemplo pisos de un sitio. Las reservas sirven para reservar la misma IP al que se conecta En dns se crean las zonas inversas de cada Red a\u00f1adida","title":"NOTAS"}]}