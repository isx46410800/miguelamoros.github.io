{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apuntes Miguel For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"MkDocs"},{"location":"#apuntes-miguel","text":"For full documentation visit mkdocs.org .","title":"Apuntes Miguel"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"ansible/","text":"ANSIBLE DOCUMENTACI\u00d3N Apuntes Ansible Libros Ansible GUIA FUNCIONES ANSIBLE : Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX. Instalaci\u00f3n yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada. Inventarios Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Comando b\u00e1sico ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta Ayuda Ansible ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user Playbook Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula) M\u00f3dulos Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes Variables Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" Condicionales Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\" Bucles Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario Roles Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks... Ansible Galaxy Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker' Resumen Repaso de ansible. Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se conecta por SSH. Inventory Sirve para listar todos los hosts, todas las ips que tenemos que aprovisionar. Con cat /etc/ansible/hosts vemos un ejemplo de los hosts que tenemos que administrar: ## [webservers] - nombre del grupo ## alpha.example.org ## beta.example.org ## 192.168.1.100 ## 192.168.1.110 ## db[01:03].intranet.mydomain.net ## db02.intranet.mydomain.net Probamos conexi\u00f3n con algun hosts poniendo ansible alpha.example.org -m ping : [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m ping localhost | SUCCESS => { \"changed\": false, \"ping\": \"pong\" } Podemos indicarle otro fichero con otros host poniendo la opci\u00f3n -i file_hosts . M\u00f3dulos documentaci\u00f3n m\u00f3dulos Por defecto si no pongo el modulo -m, coge shell como m\u00f3dulo: [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -a 'echo hola miguel' localhost | CHANGED | rc=0 >> hola miguel # [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m shell -a 'uname -a' localhost | CHANGED | rc=0 >> Linux miguel 5.3.11-100.fc29.x86_64 #1 SMP Tue Nov 12 20:41:25 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Instalamos un paquete en la m\u00e1quina remota como superusuario(-b) y preguntando la contrase\u00f1a de root en esa m\u00e1quina(-K): [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -b -K -m dnf -a 'name=vim state=present' BECOME password: localhost | SUCCESS => { \"changed\": false, \"msg\": \"Nothing to do\", \"rc\": 0, \"results\": [] } Playbook Se escribe un yaml y son objetos que se escriben tareas que han de hacer en nuestras m\u00e1quinas remotas: --- - hosts: localhost tasks: - name: instala vim dnf: name=vim state=present become: true - name: saludar shell: echo hola Resultado: [isx46410800@miguel ansible]$ ansible-playbook playbook01.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala vim] ************************************************************************************************** ok: [localhost] TASK [saludar] ****************************************************************************************************** changed: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 otro ejemplo que maneja servicios: - hosts: localhost become: true tasks: - name: instala vim dnf: name=vim state=present - name: saludar shell: echo hola - name: detener apache service: name=httpd state=stopped Usuarios Podemos poner el usuario con la opci\u00f3n -u . No obstante en el fichero de configuraci\u00f3n /etc/ansible/ansible.cfg podemos poner [defaults]remote_users=miguel y entonces cada orden coger\u00e1 como usuario miguel. Podemos cargar otro fichero de conf poniendo ANSIBLE_CONFIG=ruta_file_cfg. Handlers Le pide a ansible que cuando haga una tarea success lo notifique para poder hacer otras cosas. isx46410800@miguel ansible]$ cat playbook02.yaml --- - hosts: localhost become: true tasks: - name: instala apache dnf: name=httpd state=present update_cache=true notify: - \"Reinicia el servidor web\" handlers: - name: reinicia el server apache service: name=httpd state=restarted [isx46410800@miguel ansible]$ ansible-playbook playbook02.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala apache] *********************************************************************************************** ok: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Nos saldr\u00eda una notificaci\u00f3n de un handler al instalar, no sale porque ya estaba instalado. CURSO COMPLETO Environment Vemos un ejemplo de como es un ambiente con Ansible, conectando una m\u00e1quina central con el lenguaje Ansible hacia otros hosts con sistemas operativos y ordenando que tiene que tener cada cosa y como conectarse: Nos conectamos a una instancia ubuntu aws por ssh: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem ubuntu@18.134.164.27 Con el un usuario ubuntu de aws, creamos llaves para conectarnos a ese user: ubuntu@ip-172-31-23-107:~$ sudo su - root@ip-172-31-23-107:~# useradd ansible_user root@ip-172-31-23-107:~# passwd ansibler_user [isx46410800@miguel .ssh]$ ssh-keygen -rw-------. 1 isx46410800 isx46410800 2602 Apr 2 23:22 ansibleuser -rw-r--r--. 1 isx46410800 isx46410800 572 Apr 2 23:22 ansibleuser.pub root@ip-172-31-23-107:/home/ubuntu/.ssh# vi authorized_keys root@ip-172-31-23-107:/home/ubuntu# chown -R ubuntu .ssh/ Ahora podremos conectarnos con la llave privada al usuario ubuntu sin autenticar al tener copiada la llave publica: [isx46410800@miguel .ssh]$ ssh -i ansibleuser ubuntu@18.134.164.27 Nos conectamos a un docker con fedora: [isx46410800@miguel curso_ansible]$ docker run --name container -h container -p 2222:22 --privileged -d isx46410800/ansible:ssh Creamos un usuario y copiamos tambi\u00e9n las llaves al usuario para conectarnos: [root@container docker]# adduser fedora [root@container docker]# passwd fedora [root@container docker]# cd /home/fedora/ [root@container fedora]# mkdir .ssh [root@container fedora]# chmod 700 .ssh [root@container fedora]# vi .ssh/authorized_keys [root@container fedora]# chmod 600 .ssh/authorized_keys [root@container fedora]# chown -R fedora /home/fedora/.ssh Inventory Creamos un primer inventario para conectarnos a la m\u00e1quina de amazon: # conexi\u00f3n a un host remoto, indicando nombre host, ip, llave y usuario al que conectamos ec2 ansible_host=18.134.164.27 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu Probamos con la orden ansible -i inventario nombre_host -atributo opcion del atributo: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } El fichero de configuraci\u00f3n de ansible est\u00e1 en /etc/ansible/ansible.cfg. # este fichero de configuraci\u00f3n se escriben reglas para grupos de hosts o hosts sueltos donde ir\u00e1n a buscar las cosas por defecto a este archivo(/etc/ansible/ansible.cfg) [defaults] INVENTORY=./inventory01 Vemos que conecta igual poniendo el inventario como que no: [isx46410800@miguel curso_ansible]$ ansible ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } M\u00f3dulos Los m\u00f3dulos son la cantidad de opciones que podemos hacer a la hora de conectarnos con las m\u00e1quinas: ping, package, service... Vemos todas con la orden ansible-doc --list : ansible-doc file Hacer un ping: ansible -i inventory01 ec2 -m ping Crear un directorio/file(absent, directory, file, hard, link, touch): ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0775\", \"owner\": \"ubuntu\", \"path\": \"/home/ubuntu/crear_directorio\", \"size\": 4096, \"state\": \"directory\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio/crear_file.txt state=touch' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"dest\": \"/home/ubuntu/crear_directorio/crear_file.txt\", \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 0, \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ crear_file.txt Copiar un fichero: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m copy -a 'src=./ansible.cfg dest=/home/ubuntu/crear_directorio' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"checksum\": \"bf35d403c825217ade9f009d13cbdd6fc0a3078f\", \"dest\": \"/home/ubuntu/crear_directorio/ansible.cfg\", \"gid\": 1000, \"group\": \"ubuntu\", \"md5sum\": \"15b402b635fbd568d10b82d4b67da871\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 208, \"src\": \"/home/ubuntu/.ansible/tmp/ansible-tmp-1617401616.4248621-11162-62473546166429/source\", \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ ansible.cfg crear_file.txt # otro ejemplo de crear un file con contenido y copiarlo ansible ec2 -m copy -a \"content='TopSecret' dest='/opt/data/secret.txt'\" A\u00f1adir una linea a un fichero: [isx46410800@miguel curso_ansible]$ cat file.txt fichero de ejemplo para modulo de a\u00f1adir lineas [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m lineinfile -a 'path=/home/ubuntu/crear_directorio/file.txt line=\"a\u00f1adimos esto al modulo lineinfile\"' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"backup\": \"\", \"changed\": true, \"msg\": \"line added\" } ubuntu@ip-172-31-23-107:~$ cat crear_directorio/file.txt fichero de ejemplo para modulo de a\u00f1adir lineas a\u00f1adimos esto al modulo lineinfile Descargar contenido de un URL y enviarlo en un fichero a un host remoto: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m uri -a 'url=https://api.github.com/users/isx46410800/repos dest=/home/ubuntu/crear_directorio/repos.json' Instalar/borrar un paquete: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' -b [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' --become -b para ser superusuario -k para preguntar passwd si hemos creado un user vamos a la maquina y a\u00f1adimos en la fichero /etc/sudoers: user ALL=(ALL:ALL) NOPASSWD:ALL Encender un servicio: ansible ec2 -m service -a \"name=nginx state=started\" Crear un usuario: ansible ec2 -m user -a \"name=miguel state=present\" Hacer una orden normal de comando: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Playbooks En vez de hacerlo en una linea con comandos, se crea un playbook en formato YAML para hacer ahi el listado de tareas a realizar a los hosts: - name: primer ejemplo playbook hosts: ec2 tasks: # creamos un file con contenido dentro - name: hello file is copied copy: content: \"Hello World\" dest: /home/ubuntu/crear_directorio/hello.txt # creamos un segundo file con contenido dentro - name: hi file is copied copy: content: \"Hi World\" dest: /home/ubuntu/crear_directorio/hi.txt # comprimimos estos dos ficheros - name: hello and hi files compressed archive: path: - /home/ubuntu/crear_directorio/hello.txt - /home/ubuntu/crear_directorio/hi.txt dest: /home/ubuntu/crear_directorio/hh.zip format: zip Lo lanzamos con la orden: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook01.yaml Ejemplo de deploy de una web al host remoto: [isx46410800@miguel curso_ansible]$ cat playbook02-web-static.yaml - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present # encendemos el servicio apache - name: apache running service: name: apache2 state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: /var/www/html state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: /var/www/html #/usr/share/nginx/html Resultado: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook02-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [installing apache server] *************************************************************************************** ok: [ec2] TASK [apache running] ************************************************************************************************* ok: [ec2] TASK [creating var directory] **************************************************************************************** ok: [ec2] TASK [static website is deployed] ************************************************************************************ changed: [ec2] PLAY RECAP *********************************************************************************************************** ec2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Variables Podemos usar variables en el fichero de playbook para no tener que escribir lo mismo: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Tambi\u00e9n podemos poner en la orden de ansible-playbook la opcion --extra-vars e indicar la variable y contenido y piyar\u00eda esa variable como prioridad en vez de la del playbook: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es apache2\" } # [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml --extra-vars webserver=httpd PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es httpd\" } Tabi\u00e9n se puede crear un fichero de variables y llamar a donde est\u00e1n las variables: [isx46410800@miguel curso_ansible]$ cat vars.yaml webserver: apache2 webserver_dir: /var/www/html # - name: ejemplo deploy web static hosts: ec2 become: yes vars_files: - vars-yaml tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" Ejemplo pipeline con diferentes Branchs de git: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html git_branch: ansible-course-index-v2 tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Podemos crear una variable de registro con el contenido de una tarea: - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present register: apache_install_output - name: print register in terminal debug: var: apache_install_output - name: copy the output copy: content: \"{{ apache_install_output }}\" dest: /home/ubuntu/crear_directorio/register.txt Ansible facts: - name: ansible facts hosts: ec2 become: yes #gather_facts: no tasks: # print ansible_facts - name: print ansible_facts debug: var: ansible_facts se puede poner con una variable de gather_facts: no y no saldrian los facts Otras variables como inventory_hostaname, hostvars, group_names, groups... Pr\u00e1ctica DEV y PROD Nos queremos conectar a 3 instancas AWS, una de dev y dos de prod. Creamos nuevo inventario: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod2 ansible_host=35.178.101.37 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu [prod] prod1 prod2 [dev] dev1 Probamos conexi\u00f3n despues de meterle la llave publica a cada uno: [isx46410800@miguel curso_ansible]$ ansible -i inventory02_prod_dev all -m ping Podemos simplicar variables a\u00f1adiendo un grupo de variables: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 prod2 ansible_host=35.178.101.37 [prod] prod1 prod2 [prod:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu git_branch=ansible-course-index-v2 [dev] dev1 [dev:vars] git_branch=ansible-course Tambien se puede crear un directorio host_vars -> dev1 --> vars.yaml con las 3 variables asignadas separados por dos puntos. tambien se puede crear un directorio group_vars -> prod -> vars.yaml con las variables del grupo prod:vars. Luego se borraria porque ya las tenemos ah\u00ed. Resultados: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook09-proyect_dev-prod.yaml PLAY [ejemplo deploy en dev y prod] ********************************************************************************** TASK [Gathering Facts] *********************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [installing apache server] ************************************************************************************** ok: [prod1] ok: [prod2] changed: [dev1] TASK [apache running] ************************************************************************************************ ok: [prod1] ok: [dev1] ok: [prod2] TASK [creating var directory] **************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [static website is deployed] ************************************************************************************ changed: [prod1] changed: [prod2] changed: [dev1] PLAY RECAP *********************************************************************************************************** dev1 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod1 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Vemos la info de variables m\u00e1gicas: - name: play for discover magic variables hosts: dev,prod tasks: - name: print inventory_hostname debug: var: inventory_hostname - name: print hostvars debug: var: hostvars - name: print group_names debug: var: group_names - name: print groups debug: var: groups [isx46410800@miguel curso_ansible]$ ansible-playbook 10-playbook-magic_variables.yaml > magic.tmp Podemos conseguir la info de un host con la orden ansible-inventory: [isx46410800@miguel curso_ansible]$ ansible-inventory --host dev1 { \"ansible_host\": \"35.177.51.40\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"git_branch\": \"ansible-course\" } Ansible Task Control Los LOOPS sirven para ejecutar unas mismas tareas pero en una iterando el item que queremos pasarle como variable. Simplificamos una tares poniendo loop y los items debajo a iterar. Ejemplos: become: yes tasks: - name: \"package is installed\" package: name: \"{\u200c{ item }}\" state: latest loop: - mysql - mongodb-org # become: yes tasks: # .... - name: service is up service: name: \"{\u200c{ item }}\" state: started loop: - mysql - mongod En nuestro ejemplo para hacer iterar el loop para que primero haga deploy de una web y despues de otra: - name: ejemplo deploy web static hosts: dev,prod become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy webs staticas - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/{{ item }}.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html loop: - index - about Utilizamos el WHEN como opci\u00f3n para decir que se haga tal cosa sea igual a esa variable. En este caso las variables no se ponen entre corchetes[]. become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: index static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html - name: about static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/about.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html when: git_branch == 'ansible-course' Resultados: TASK [index static website is deployed] ****************************************************************************** changed: [prod2] changed: [prod1] changed: [dev1] TASK [about static website is deployed] ****************************************************************************** skipping: [prod1] skipping: [prod2] changed: [dev1] Otros Ejemplos comunes del WHEN: when: ansible_distribution == 'Ubuntu' when: app_replicas == 12 when: app_replicas < 12 when: ansible_distribution != 'Centos' when: git_branch is defined when: git_branch is not defined when: ( git_branch in [\"master\", \"development\"] ) when: ( app_replicas == 12 ) and ( ansible_distribution == 'Ubuntu') when: - app_replicas == 12 - ansible_distribution == 'Ubuntu' when: ( app_replicas == 12 ) or ( ansible_distribution == 'Ubuntu' ) Ejemplo de crear usuarios segun si est\u00e1n en una maquina y segun el papel que tengan: [isx46410800@miguel curso_ansible]$ cat users.yaml assignments_users: - name: miguel role: developer - name: isabel role: developer - name: cristina role: ops - name: play create users per role hosts: dev,prod become: yes vars_files: - ./users.yaml tasks: - name: user exists per its role user: name: \"{{ item.name }}\" state: present loop: \"{{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) Lo que hacemos es que si un usuario tiene el rol de develop vaya a las maquinas devs y si es ops que vaya a las de prod. Podemos comprobar los usuarios en cada host con la orden: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Podemos delegar una tarea a otro host, es decir en vez de la m\u00e1quina indicada, que se envie a otra con delegate_to : - name: delegar una tarea a otro host con delegate_to hosts: dev tasks: - name: file copied copy: content: este mensaje de prueba dest: /tmp/message.txt delegate_to: localhost Otro ejemplo de delegar tareas: - name: play to show how to use delegate_to in assignment hosts: dev1 tasks: - name: repos list is downloaded uri: url: https://api.github.com/users/atoumi/repos dest: /tmp/git-repos.json delegate_to: localhost Podemos importar la informaci\u00f3n de tareas con el m\u00f3dulo import_tasks . Se crea un archivo aparte de las tareas que se quieren importar y se a\u00f1aden al playbook principal: - name: play import_tasks hosts: dev1 tasks: - name: import nginx tasks import_tasks: nginx_install.yaml Los handers son avisadores de que haga una cosa o notifique si una tarea ha cambiado, ejemplo: - name: play illustrates the Slide of handlers hosts: somehost tasks: - name: t1 module-a: attr1: val1 - name: t2 module-b: attr1: val1 notify: t3 # run t3 only if t2 CHANGED - name: t4 module-d: attr1: val1 handlers: - name: t3 module-c: attr1: val1 Jinja2 templates DOCUMENTACI\u00d3N JINJA Ejemplo de filtros que ponemos ponerle en las tareas con jinja: # https://jinja.palletsprojects.com/en/2.11.x/templates/#builtin-filters - name: play with jinja2 filters hosts: container gather_facts: no vars: git_username: atoumi git_password: Gfdfd445e git_repos: [\"eks-course\", \"ansible-course\", \"react-csv\"] course_lectures_nb: [6, 6, 10, 8, 9] tasks: - name: j2 filter - capitalize debug: msg: | original : {{ git_username }} with filter: {{ git_username | capitalize }} # require: pip3 install passlib - name: j2 filter - password_hash('sha512') debug: msg: | original : {{ git_password }} with filter: {{ git_password | password_hash('sha512') }} - name: j2 filter - length - nb of repos debug: msg: | original : {{ git_repos }} with filter: {{ git_repos | length }} - name: j2 filter - sum - total nb of lectures debug: msg: | original : {{ course_lectures_nb }} with filter: {{ course_lectures_nb | sum }} - name: j2 filter - max - max nb of lectures in a section debug: msg: | original : {{ course_lectures_nb }} Lista de filtros: ansible jinja Ejemplo de poner filtros creando usuarios, metiendolo en la maquina que le toque segun rol y poniendo su passwd. luego nos conectamos y funciona: where \"vars/31-users.yaml\" content is : assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # - name: play create users per role hosts: dev,prod become: yes vars_files: - vars/31-users.yaml tasks: - name: user exists per its role user: name: \"{\u200c{ item.name }}\" state: present password: \"{\u200c{item.password | password_hash('sha512') }}\" loop: \"{\u200c{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) # ssh ahmed@<ip-server> # .. then put the password (ahmed123) Con el modulo template podemos copiar un fichero que tiene variables del sistemas hacia destino. Si lo hacemos con el modulo copy, se copia literalmente sin sustituir las variables: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb #playbook - name: play to generate SPECS report hosts: dev1 tasks: - name: report is generated template: src: ./file_vars.conf dest: /tmp/specs.conf # ubuntu@ip-172-31-19-134:~$ cat /tmp/specs.conf Distribution : Ubuntu Distribution Release : focal Distribution Version : 20.04 Nbre CPU core : 1 cores Total Memory : 978 mb Se pueden poner tambi\u00e9n sintaxi jinja en el fichero como por ejemplo condicionales: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb # {% if ansible_facts.memtotal_mb < 4000 %} The memory size is small {% else %} The memory size is OK {% endif %} Tambien se puede hacer jinja con loop: #playbook - name: play to generate SPECS report hosts: dev,prod vars: app_pages: - home - login - logout app_users: - name: ahmed role: developer - name: mouath role: developer - name: ali role: ops - name: omar role: ops tasks: - name: report is generated template: src: loop_jinja.conf dest: /tmp/app-report.conf #loop_jinja.conf ==== Print app_pages ===== {% for page in app_pages %} {{ page }}.html is a web page {% endfor %} ==== Print app_users ==== {% for u in app_users %} {{ u.name | capitalize }} is {{ u.role }} {% endfor %} Ejemplo de un /etc/hosts (etc/hosts that includes all hosts where {\u200c{ inventory_hostname }} magic variable is the domain name of the target host) - name: play common /etc/hosts hosts: all become: yes tasks: - name: copy /etc/hosts template: src: hosts.j2 dest: /etc/hosts # 127.0.0.1 localhost # The following lnes are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts {% for host in groups['all'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ host }} {% endfor %} Resumen sintaxi JINJA: Print variable or Expression Print the variable by using the variable name surrounded by double braces. {\u200c{ my_var }} # .i.e: {\u200c{ ansible_distribution }} Filters Variables can be transformed/modified by filters. Think about filter as a function {\u200c{ my_var | my_filter }} # .i.e : {\u200c{ app_title | capitalize }} Read it like my_filter(my_var) If Block {% if CONDITION1 %} blah blah blah {% elif CONDITION2 %} blahelif blahelif blahelif . {% else %} blahelse so far {% endif %} #.i.e {% if git_branch == 'master' %} RELEASE: {\u200c{ app_version }} {% else %} SNAPSHOT: {\u200c{ app_version }}-RC{\u200c{ build_number }} {% endif %} For Loop {% for ELEMENT in ARRAY %} Process {\u200c{ ELEMENT }} {% endfor %} #. i.e: assume that ( app_pages = [\"login.html\", \"index.html\"] ) {% for page in app_pages %} <a href=\"https://example.com/{\u200c{ page }}\">{\u200c{ page }}</a> {% endfor %} Ansible Vault Ansible sirve para desencriptar las passwords que salen en un fichero. Ordenes: ansible-vault create users_password.yaml ansible-vault encrypt users_password.yaml ansible-vault decrypt users_password.yaml ansible-vault edit users_password.yaml ansible-vault show users_password.yaml Partimos del ejemplo: #`ansible-vault create users_password.yaml` assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # playbook - name: play use encrypted vars hosts: localhost vars_files: - users_password.yaml tasks: - name: encrypted variable is printed debug: var: assignment_users Podemos crear en ansible.cfg la variable vault_password_file = file.key con una passwd de vault. Ansible Role & Galaxy Los roles ansibles son la forma en que hacemos esto. Cuando se crea un rol, se descompone el playbook en partes y esas partes se encuentran en una estructura de directorios. Ejemplo de instalar un role: ansible-galaxy install role_file ansible-galaxy install -r ./requirements.yaml ansible-galaxy init my_role #estructura de directorios ansible-galaxy --help Importar role en un playbook: en ansible.cfg: [defaults] roles_path = ./roles ansible-galaxy install -r ./requirements.yaml tasks: - import_role: name: role_file vars: { var1: val1.. } Instalar Jenkkins Primer paso a\u00f1adimos en el ansible.cfg el roles_path, donde se instalaran los roles que queremos instalar: [defaults] INVENTORY=./inventory02_prod_dev roles_path=./roles ansible-galaxy : Instala roles de Ansible Galaxy, una plataforma para el intercambio de roles (recetas) Ansible. Podemos encontrar info de lo que queremos instalar con Ansible en Ansible Galaxy . En este caso buscamos Jenkins y vemos la opci\u00f3n de como descargarlo o si vamos al repo github, vemos un ejemplo de playbook para instalarlo. Primero podemos crear un fichero de requisitos de los paquetes a instalar de jenkins como roles para luego poder instalarlos remotamente: # - src: # name: # version: # roles para instalar jenkins - src: geerlingguy.java name: geerlingguy.java - src: geerlingguy.jenkins name: geerlingguy.jenkins [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - hosts: dev1 become: true vars: #jenkins_hostname: jenkins.example.com java_packages: - openjdk-8-jdk roles: - role: geerlingguy.java - role: geerlingguy.jenkins Instalar Docker A\u00f1adimos a los requisitos: - src: geerlingguy.docker name: geerlingguy.docker Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook poniendo que podamos acceder como usuario el usuario ansible ssh(ubuntu): - hosts: prod1,prod2,dev1 become: yes tasks: - name: docker is installed import_role: name: geerlingguy.docker vars: docker_users: - \"{{ ansible_ssh_user }}\" Instalar Kubernetes A\u00f1adimos a requisitos: - src: geerlingguy.kubernetes name: geerlingguy.kubernetes Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - name: play kubernets is up hosts: cluster become: yes tasks: - name: docker role import_role: name: geerlingguy.docker - name: kubernetes role import_role: name: geerlingguy.kubernetes Hariamos 3 maquinas, 1 master y 2 nodos, los a\u00f1adiriamos al inventario y le pondriamos un grupo [cluster] y en cada host la variable role_kubernetes=node/master Ansible Collection A veces los roles no son suficientes con descargarlos y necesitamos las colecciones, que son un conjunto de playbooks, roles, modulos y plugins. El fichero galaxy.yaml es el unico fichero requerido, aunque hay tambien directorios de roles, plugins, docs, playbooks... Orden: ansible-galaxy collection install file_collection ansible-galaxy install -r ./requirements.yaml ansible-galaxy collection init my_collection_file #estructura de directorios ansible-galaxy --help Donde guardarlos: [defaults] INVENTORY=./inventory02_prod_dev COLLECTIONS_PATHS=./collections A\u00f1adimos a requisitos: - collections: newswangerd.collection_demo Instalamos: [isx46410800@miguel curso_ansible]$ ansible-galaxy collection install -r requirements.yaml Playbook: - name: play usage collections hosts: dev1 collections: - newswangerd.collection_demo tasks: - name: module usage from collection real_facts: name: Abdennour - name: role usage from collection import_role: name: factoid Capstone Project - Put all Together in a Real Project with Go, React and MongoDB Idea: Frotend en una maquina tendremos REACT app, en backend tendremos la app GO y todo est\u00e1ra conectado en una VM con la ddbb mongoDB. Creaci\u00f3n de instancias AWS Creamos dos instancias en Amazon: app y db [isx46410800@miguel project_real]$ cat inventory_project app ansible_host=52.56.149.20 db ansible_host=18.130.63.197 [todo] app db [todo:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu #ansible_ssh_pass=ubuntu2021 [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_proyect Comprobamos que esta conexi\u00f3n funciona: [isx46410800@miguel project_real]$ ansible app,db -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } [isx46410800@miguel project_real]$ ansible todo -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } Instalaci\u00f3n MONGODB Para instalar Mongodb se necesita estos pasos Playbook de tareas para instalarlo e iniciarlo en la instancia de DB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database hosts: db become: yes tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes Comprobamos: isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml PLAY [play Database] ************************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [db] TASK [gnupg is installed] ******************************************************************************************** ok: [db] TASK [mongodb-key is added] ****************************************************************************************** changed: [db] TASK [mongo-db repo is enabled] ************************************************************************************** changed: [db] TASK [mongodb-org is installed] ************************************************************************************** changed: [db] TASK [mongod is enable] ********************************************************************************************** changed: [db] PLAY RECAP *********************************************************************************************************** db : ok=6 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ubuntu@ip-172-31-22-5:~$ sudo systemctl status mongod \u25cf mongod.service - MongoDB Database Server Loaded: loaded (/lib/systemd/system/mongod.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-04-06 12:02:22 UTC; 53s ago Docs: https://docs.mongodb.org/manual Main PID: 13440 (mongod) Memory: 64.2M CGroup: /system.slice/mongod.service \u2514\u250013440 /usr/bin/mongod --config /etc/mongod.conf Apr 06 12:02:22 ip-172-31-22-5 systemd[1]: Started MongoDB Database Server. Configuraci\u00f3n y creaci\u00f3n de superuser de MONGODB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database install mongodb hosts: db become: yes tags: - db-install tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes - name: play Database config and create superuser hosts: db become: yes tags: - db-config-admin tasks: # drop admin user if exists (mongo admin --eval 'db.dropUser(\"superadmin\")') # create admin user (mongo admin --eval 'db.createUser'({ user: \"superadmin\", pwd:})) - name: create admin user command: \"{{ item }}\" loop: - mongo admin --eval 'db.dropUser(\"{{ db_admin_user }}\")' - | mongo admin --eval 'db.createUser( { user: \"{{ db_admin_user }}\", pwd: \"{{ db_admin_pass }}\", roles: [ { role: \"clusterAdmin\", db: \"admin\" }, { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } )' # enable security (/etc/mongod.conf) ---> Restart Mongodb #> security: #> authorization: \"enabled\" - name: security is enabled blockinfile: path: /etc/mongod.conf block: | security: authorization: \"enabled\" state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted las variables hacen referencias al archivo host_vars/db/secret.yaml Creamos un file de contrase\u00f1a para encriptarlo con vault: [isx46410800@miguel project_real]$ mkdir -p host_vars/db # [isx46410800@miguel project_real]$ cat key.txt miguel14031993 # [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt # [isx46410800@miguel project_real]$ ansible-vault create host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r Comprobamos que est\u00e1 todo en orden: ubuntu@ip-172-31-22-5:~$ systemctl status mongod ubuntu@ip-172-31-22-5:~$ cat /etc/mongod.conf ubuntu@ip-172-31-22-5:~$ mongo admin -u superadmin -p PassMy1243r Configuramos ahora el primer usuario mongo : ... - name: play - rest of configuration hosts: db become: yes tags: - db-config tasks: - name: pip3 installed package: name: python3-pip state: latest - name: pip pymongo installed pip: name: pymongo state: latest - name: todo db user exists mongodb_user: login_user: \"{{ db_admin_user }}\" login_password: \"{{ db_admin_pass }}\" database: admin user: \"{{ db_todo_user }}\" password: \"{{ db_todo_pass }}\" state: present roles: - db: \"{{ db_name_todo }}\" role: readWrite [isx46410800@miguel project_real]$ ansible-vault edit host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r db_name_todo: test db_todo_user: todo db_todo_pass: todo [isx46410800@miguel project_real]$ ansible-inventory --host db { \"ansible_host\": \"35.176.225.221\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"db_admin_pass\": \"PassMy1243r\", \"db_admin_user\": \"superadmin\", \"db_name_todo\": \"test\", \"db_todo_pass\": \"todo\", \"db_todo_user\": \"todo\" } Lanzamos solo la tercera parte del playbook a\u00f1adido con la orden: [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config A\u00f1adimos la linea de las interfaces para que reciba desde cualquier ip: - name: db accepts connection from anywhere lineinfile: path: /etc/mongod.conf line: \" bindIp: 0.0.0.0\" regexp: '^(.*)binIp(.*)$' state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config Backend APP Hacemos playbook de instalar goland en el host app: [isx46410800@miguel project_real]$ cat playbooks/backend.yaml - name: play Backend hosts: app become: yes tags: - be-pre-build tasks: - name: go is installed import_role: name: gantsign.golang vars: golang_version: \"1.14\" golang_packages: - github.com/gorilla/mux - go.gomongodb.org/mongo-driver/mongo golang_users: - \"{{ ansible_ssh_user }}\" - name: play Backend hosts: app become: yes tags: - be-build tasks: - name: workspace build exist file: path: /opt/build_dir state: directory - name: git checkout git: repo: https://gitlab.com/isx46410800/curso_ansible.git dest: /opt/build_dir/curso_ansible - name: go build shell: . /etc/profile;go build -o /tmp/todo args: chdir: /opt/build_dir/curso_ansible/server [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt roles_path=./roles [isx46410800@miguel project_real]$ cat requirements.yaml - src: gantsign.golang name: gantsign.golang [isx46410800@miguel project_real]$ ansible-playbook playbooks/backend.yaml --tags be-pre-build *https://github.com/kubernetes-tn/go-to-do-app * *https://github.com/abdennour/ansible-course *","title":"Ansible"},{"location":"ansible/#ansible","text":"DOCUMENTACI\u00d3N Apuntes Ansible Libros Ansible GUIA FUNCIONES ANSIBLE : Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX.","title":"ANSIBLE"},{"location":"ansible/#instalacion","text":"yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada.","title":"Instalaci\u00f3n"},{"location":"ansible/#inventarios","text":"Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Inventarios"},{"location":"ansible/#comando-basico","text":"ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta","title":"Comando b\u00e1sico"},{"location":"ansible/#ayuda-ansible","text":"ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user","title":"Ayuda Ansible"},{"location":"ansible/#playbook","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula)","title":"Playbook"},{"location":"ansible/#modulos","text":"Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes","title":"M\u00f3dulos"},{"location":"ansible/#variables","text":"Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\"","title":"Variables"},{"location":"ansible/#condicionales","text":"Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\"","title":"Condicionales"},{"location":"ansible/#bucles","text":"Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario","title":"Bucles"},{"location":"ansible/#roles","text":"Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks...","title":"Roles"},{"location":"ansible/#ansible-galaxy","text":"Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker'","title":"Ansible Galaxy"},{"location":"ansible/#resumen","text":"Repaso de ansible. Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se conecta por SSH.","title":"Resumen"},{"location":"ansible/#inventory","text":"Sirve para listar todos los hosts, todas las ips que tenemos que aprovisionar. Con cat /etc/ansible/hosts vemos un ejemplo de los hosts que tenemos que administrar: ## [webservers] - nombre del grupo ## alpha.example.org ## beta.example.org ## 192.168.1.100 ## 192.168.1.110 ## db[01:03].intranet.mydomain.net ## db02.intranet.mydomain.net Probamos conexi\u00f3n con algun hosts poniendo ansible alpha.example.org -m ping : [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m ping localhost | SUCCESS => { \"changed\": false, \"ping\": \"pong\" } Podemos indicarle otro fichero con otros host poniendo la opci\u00f3n -i file_hosts .","title":"Inventory"},{"location":"ansible/#modulos_1","text":"documentaci\u00f3n m\u00f3dulos Por defecto si no pongo el modulo -m, coge shell como m\u00f3dulo: [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -a 'echo hola miguel' localhost | CHANGED | rc=0 >> hola miguel # [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -m shell -a 'uname -a' localhost | CHANGED | rc=0 >> Linux miguel 5.3.11-100.fc29.x86_64 #1 SMP Tue Nov 12 20:41:25 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Instalamos un paquete en la m\u00e1quina remota como superusuario(-b) y preguntando la contrase\u00f1a de root en esa m\u00e1quina(-K): [isx46410800@miguel miguelamoros.github.io]$ ansible localhost -b -K -m dnf -a 'name=vim state=present' BECOME password: localhost | SUCCESS => { \"changed\": false, \"msg\": \"Nothing to do\", \"rc\": 0, \"results\": [] }","title":"M\u00f3dulos"},{"location":"ansible/#playbook_1","text":"Se escribe un yaml y son objetos que se escriben tareas que han de hacer en nuestras m\u00e1quinas remotas: --- - hosts: localhost tasks: - name: instala vim dnf: name=vim state=present become: true - name: saludar shell: echo hola Resultado: [isx46410800@miguel ansible]$ ansible-playbook playbook01.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala vim] ************************************************************************************************** ok: [localhost] TASK [saludar] ****************************************************************************************************** changed: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 otro ejemplo que maneja servicios: - hosts: localhost become: true tasks: - name: instala vim dnf: name=vim state=present - name: saludar shell: echo hola - name: detener apache service: name=httpd state=stopped","title":"Playbook"},{"location":"ansible/#usuarios","text":"Podemos poner el usuario con la opci\u00f3n -u . No obstante en el fichero de configuraci\u00f3n /etc/ansible/ansible.cfg podemos poner [defaults]remote_users=miguel y entonces cada orden coger\u00e1 como usuario miguel. Podemos cargar otro fichero de conf poniendo ANSIBLE_CONFIG=ruta_file_cfg.","title":"Usuarios"},{"location":"ansible/#handlers","text":"Le pide a ansible que cuando haga una tarea success lo notifique para poder hacer otras cosas. isx46410800@miguel ansible]$ cat playbook02.yaml --- - hosts: localhost become: true tasks: - name: instala apache dnf: name=httpd state=present update_cache=true notify: - \"Reinicia el servidor web\" handlers: - name: reinicia el server apache service: name=httpd state=restarted [isx46410800@miguel ansible]$ ansible-playbook playbook02.yaml -K BECOME password: [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all' PLAY [localhost] **************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************** ok: [localhost] TASK [instala apache] *********************************************************************************************** ok: [localhost] PLAY RECAP ********************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Nos saldr\u00eda una notificaci\u00f3n de un handler al instalar, no sale porque ya estaba instalado.","title":"Handlers"},{"location":"ansible/#curso-completo","text":"","title":"CURSO COMPLETO"},{"location":"ansible/#environment","text":"Vemos un ejemplo de como es un ambiente con Ansible, conectando una m\u00e1quina central con el lenguaje Ansible hacia otros hosts con sistemas operativos y ordenando que tiene que tener cada cosa y como conectarse: Nos conectamos a una instancia ubuntu aws por ssh: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem ubuntu@18.134.164.27 Con el un usuario ubuntu de aws, creamos llaves para conectarnos a ese user: ubuntu@ip-172-31-23-107:~$ sudo su - root@ip-172-31-23-107:~# useradd ansible_user root@ip-172-31-23-107:~# passwd ansibler_user [isx46410800@miguel .ssh]$ ssh-keygen -rw-------. 1 isx46410800 isx46410800 2602 Apr 2 23:22 ansibleuser -rw-r--r--. 1 isx46410800 isx46410800 572 Apr 2 23:22 ansibleuser.pub root@ip-172-31-23-107:/home/ubuntu/.ssh# vi authorized_keys root@ip-172-31-23-107:/home/ubuntu# chown -R ubuntu .ssh/ Ahora podremos conectarnos con la llave privada al usuario ubuntu sin autenticar al tener copiada la llave publica: [isx46410800@miguel .ssh]$ ssh -i ansibleuser ubuntu@18.134.164.27 Nos conectamos a un docker con fedora: [isx46410800@miguel curso_ansible]$ docker run --name container -h container -p 2222:22 --privileged -d isx46410800/ansible:ssh Creamos un usuario y copiamos tambi\u00e9n las llaves al usuario para conectarnos: [root@container docker]# adduser fedora [root@container docker]# passwd fedora [root@container docker]# cd /home/fedora/ [root@container fedora]# mkdir .ssh [root@container fedora]# chmod 700 .ssh [root@container fedora]# vi .ssh/authorized_keys [root@container fedora]# chmod 600 .ssh/authorized_keys [root@container fedora]# chown -R fedora /home/fedora/.ssh","title":"Environment"},{"location":"ansible/#inventory_1","text":"Creamos un primer inventario para conectarnos a la m\u00e1quina de amazon: # conexi\u00f3n a un host remoto, indicando nombre host, ip, llave y usuario al que conectamos ec2 ansible_host=18.134.164.27 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu Probamos con la orden ansible -i inventario nombre_host -atributo opcion del atributo: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } El fichero de configuraci\u00f3n de ansible est\u00e1 en /etc/ansible/ansible.cfg. # este fichero de configuraci\u00f3n se escriben reglas para grupos de hosts o hosts sueltos donde ir\u00e1n a buscar las cosas por defecto a este archivo(/etc/ansible/ansible.cfg) [defaults] INVENTORY=./inventory01 Vemos que conecta igual poniendo el inventario como que no: [isx46410800@miguel curso_ansible]$ ansible ec2 -m ping ec2 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Inventory"},{"location":"ansible/#modulos_2","text":"Los m\u00f3dulos son la cantidad de opciones que podemos hacer a la hora de conectarnos con las m\u00e1quinas: ping, package, service... Vemos todas con la orden ansible-doc --list : ansible-doc file Hacer un ping: ansible -i inventory01 ec2 -m ping Crear un directorio/file(absent, directory, file, hard, link, touch): ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio state=directory' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0775\", \"owner\": \"ubuntu\", \"path\": \"/home/ubuntu/crear_directorio\", \"size\": 4096, \"state\": \"directory\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m file -a 'path=/home/ubuntu/crear_directorio/crear_file.txt state=touch' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"dest\": \"/home/ubuntu/crear_directorio/crear_file.txt\", \"gid\": 1000, \"group\": \"ubuntu\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 0, \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ crear_file.txt Copiar un fichero: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m copy -a 'src=./ansible.cfg dest=/home/ubuntu/crear_directorio' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": true, \"checksum\": \"bf35d403c825217ade9f009d13cbdd6fc0a3078f\", \"dest\": \"/home/ubuntu/crear_directorio/ansible.cfg\", \"gid\": 1000, \"group\": \"ubuntu\", \"md5sum\": \"15b402b635fbd568d10b82d4b67da871\", \"mode\": \"0664\", \"owner\": \"ubuntu\", \"size\": 208, \"src\": \"/home/ubuntu/.ansible/tmp/ansible-tmp-1617401616.4248621-11162-62473546166429/source\", \"state\": \"file\", \"uid\": 1000 } ubuntu@ip-172-31-23-107:~$ ls crear_directorio/ ansible.cfg crear_file.txt # otro ejemplo de crear un file con contenido y copiarlo ansible ec2 -m copy -a \"content='TopSecret' dest='/opt/data/secret.txt'\" A\u00f1adir una linea a un fichero: [isx46410800@miguel curso_ansible]$ cat file.txt fichero de ejemplo para modulo de a\u00f1adir lineas [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m lineinfile -a 'path=/home/ubuntu/crear_directorio/file.txt line=\"a\u00f1adimos esto al modulo lineinfile\"' ec2 | CHANGED => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"backup\": \"\", \"changed\": true, \"msg\": \"line added\" } ubuntu@ip-172-31-23-107:~$ cat crear_directorio/file.txt fichero de ejemplo para modulo de a\u00f1adir lineas a\u00f1adimos esto al modulo lineinfile Descargar contenido de un URL y enviarlo en un fichero a un host remoto: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m uri -a 'url=https://api.github.com/users/isx46410800/repos dest=/home/ubuntu/crear_directorio/repos.json' Instalar/borrar un paquete: [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' -b [isx46410800@miguel curso_ansible]$ ansible -i inventory01 ec2 -m package -a 'name=vim state=present' --become -b para ser superusuario -k para preguntar passwd si hemos creado un user vamos a la maquina y a\u00f1adimos en la fichero /etc/sudoers: user ALL=(ALL:ALL) NOPASSWD:ALL Encender un servicio: ansible ec2 -m service -a \"name=nginx state=started\" Crear un usuario: ansible ec2 -m user -a \"name=miguel state=present\" Hacer una orden normal de comando: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd'","title":"M\u00f3dulos"},{"location":"ansible/#playbooks","text":"En vez de hacerlo en una linea con comandos, se crea un playbook en formato YAML para hacer ahi el listado de tareas a realizar a los hosts: - name: primer ejemplo playbook hosts: ec2 tasks: # creamos un file con contenido dentro - name: hello file is copied copy: content: \"Hello World\" dest: /home/ubuntu/crear_directorio/hello.txt # creamos un segundo file con contenido dentro - name: hi file is copied copy: content: \"Hi World\" dest: /home/ubuntu/crear_directorio/hi.txt # comprimimos estos dos ficheros - name: hello and hi files compressed archive: path: - /home/ubuntu/crear_directorio/hello.txt - /home/ubuntu/crear_directorio/hi.txt dest: /home/ubuntu/crear_directorio/hh.zip format: zip Lo lanzamos con la orden: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook01.yaml Ejemplo de deploy de una web al host remoto: [isx46410800@miguel curso_ansible]$ cat playbook02-web-static.yaml - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present # encendemos el servicio apache - name: apache running service: name: apache2 state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: /var/www/html state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: /var/www/html #/usr/share/nginx/html Resultado: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook02-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [installing apache server] *************************************************************************************** ok: [ec2] TASK [apache running] ************************************************************************************************* ok: [ec2] TASK [creating var directory] **************************************************************************************** ok: [ec2] TASK [static website is deployed] ************************************************************************************ changed: [ec2] PLAY RECAP *********************************************************************************************************** ec2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0","title":"Playbooks"},{"location":"ansible/#variables_1","text":"Podemos usar variables en el fichero de playbook para no tener que escribir lo mismo: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/ansible-course/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Tambi\u00e9n podemos poner en la orden de ansible-playbook la opcion --extra-vars e indicar la variable y contenido y piyar\u00eda esa variable como prioridad en vez de la del playbook: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es apache2\" } # [isx46410800@miguel curso_ansible]$ ansible-playbook playbook04-debug-vars-web-static.yaml --extra-vars webserver=httpd PLAY [ejemplo deploy web static] ************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [ec2] TASK [debug] ********************************************************************************************************* ok: [ec2] => { \"msg\": \"El valor del servidor es httpd\" } Tabi\u00e9n se puede crear un fichero de variables y llamar a donde est\u00e1n las variables: [isx46410800@miguel curso_ansible]$ cat vars.yaml webserver: apache2 webserver_dir: /var/www/html # - name: ejemplo deploy web static hosts: ec2 become: yes vars_files: - vars-yaml tasks: # debug sirve para mostrar mensajes - name: debug debug: msg: \"El valor del servidor es {{ webserver }}\" Ejemplo pipeline con diferentes Branchs de git: - name: ejemplo deploy web static hosts: ec2 become: yes vars: webserver: apache2 webserver_dir: /var/www/html git_branch: ansible-course-index-v2 tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html Podemos crear una variable de registro con el contenido de una tarea: - name: ejemplo deploy web static hosts: ec2 become: yes tasks: # instalamos apache - name: installing apache server package: name: apache2 state: present register: apache_install_output - name: print register in terminal debug: var: apache_install_output - name: copy the output copy: content: \"{{ apache_install_output }}\" dest: /home/ubuntu/crear_directorio/register.txt Ansible facts: - name: ansible facts hosts: ec2 become: yes #gather_facts: no tasks: # print ansible_facts - name: print ansible_facts debug: var: ansible_facts se puede poner con una variable de gather_facts: no y no saldrian los facts Otras variables como inventory_hostaname, hostvars, group_names, groups...","title":"Variables"},{"location":"ansible/#practica-dev-y-prod","text":"Nos queremos conectar a 3 instancas AWS, una de dev y dos de prod. Creamos nuevo inventario: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod2 ansible_host=35.178.101.37 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu [prod] prod1 prod2 [dev] dev1 Probamos conexi\u00f3n despues de meterle la llave publica a cada uno: [isx46410800@miguel curso_ansible]$ ansible -i inventory02_prod_dev all -m ping Podemos simplicar variables a\u00f1adiendo un grupo de variables: ### conexion para una maquina de desarrollo y dos de produccion dev1 ansible_host=35.177.51.40 ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu prod1 ansible_host=3.8.212.118 prod2 ansible_host=35.178.101.37 [prod] prod1 prod2 [prod:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu git_branch=ansible-course-index-v2 [dev] dev1 [dev:vars] git_branch=ansible-course Tambien se puede crear un directorio host_vars -> dev1 --> vars.yaml con las 3 variables asignadas separados por dos puntos. tambien se puede crear un directorio group_vars -> prod -> vars.yaml con las variables del grupo prod:vars. Luego se borraria porque ya las tenemos ah\u00ed. Resultados: [isx46410800@miguel curso_ansible]$ ansible-playbook playbook09-proyect_dev-prod.yaml PLAY [ejemplo deploy en dev y prod] ********************************************************************************** TASK [Gathering Facts] *********************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [installing apache server] ************************************************************************************** ok: [prod1] ok: [prod2] changed: [dev1] TASK [apache running] ************************************************************************************************ ok: [prod1] ok: [dev1] ok: [prod2] TASK [creating var directory] **************************************************************************************** ok: [prod1] ok: [dev1] ok: [prod2] TASK [static website is deployed] ************************************************************************************ changed: [prod1] changed: [prod2] changed: [dev1] PLAY RECAP *********************************************************************************************************** dev1 : ok=5 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod1 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 prod2 : ok=5 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Vemos la info de variables m\u00e1gicas: - name: play for discover magic variables hosts: dev,prod tasks: - name: print inventory_hostname debug: var: inventory_hostname - name: print hostvars debug: var: hostvars - name: print group_names debug: var: group_names - name: print groups debug: var: groups [isx46410800@miguel curso_ansible]$ ansible-playbook 10-playbook-magic_variables.yaml > magic.tmp Podemos conseguir la info de un host con la orden ansible-inventory: [isx46410800@miguel curso_ansible]$ ansible-inventory --host dev1 { \"ansible_host\": \"35.177.51.40\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"git_branch\": \"ansible-course\" }","title":"Pr\u00e1ctica DEV y PROD"},{"location":"ansible/#ansible-task-control","text":"Los LOOPS sirven para ejecutar unas mismas tareas pero en una iterando el item que queremos pasarle como variable. Simplificamos una tares poniendo loop y los items debajo a iterar. Ejemplos: become: yes tasks: - name: \"package is installed\" package: name: \"{\u200c{ item }}\" state: latest loop: - mysql - mongodb-org # become: yes tasks: # .... - name: service is up service: name: \"{\u200c{ item }}\" state: started loop: - mysql - mongod En nuestro ejemplo para hacer iterar el loop para que primero haga deploy de una web y despues de otra: - name: ejemplo deploy web static hosts: dev,prod become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy webs staticas - name: static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/{{ item }}.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html loop: - index - about Utilizamos el WHEN como opci\u00f3n para decir que se haga tal cosa sea igual a esa variable. En este caso las variables no se ponen entre corchetes[]. become: yes vars: webserver: apache2 webserver_dir: /var/www/html tasks: # instalamos apache - name: installing apache server package: name: \"{{ webserver }}\" state: present # encendemos el servicio apache - name: apache running service: name: \"{{ webserver }}\" state: started # creamos el directorio de var para meter la web de index.html - name: creating var directory file: path: \"{{ webserver_dir }}\" state: directory # deploy web statica - name: index static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/index.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html - name: about static website is deployed uri: url: https://raw.githubusercontent.com/abdennour/example-static-website/{{ git_branch }}/about.html dest: \"{{ webserver_dir }}\" #/usr/share/nginx/html when: git_branch == 'ansible-course' Resultados: TASK [index static website is deployed] ****************************************************************************** changed: [prod2] changed: [prod1] changed: [dev1] TASK [about static website is deployed] ****************************************************************************** skipping: [prod1] skipping: [prod2] changed: [dev1] Otros Ejemplos comunes del WHEN: when: ansible_distribution == 'Ubuntu' when: app_replicas == 12 when: app_replicas < 12 when: ansible_distribution != 'Centos' when: git_branch is defined when: git_branch is not defined when: ( git_branch in [\"master\", \"development\"] ) when: ( app_replicas == 12 ) and ( ansible_distribution == 'Ubuntu') when: - app_replicas == 12 - ansible_distribution == 'Ubuntu' when: ( app_replicas == 12 ) or ( ansible_distribution == 'Ubuntu' ) Ejemplo de crear usuarios segun si est\u00e1n en una maquina y segun el papel que tengan: [isx46410800@miguel curso_ansible]$ cat users.yaml assignments_users: - name: miguel role: developer - name: isabel role: developer - name: cristina role: ops - name: play create users per role hosts: dev,prod become: yes vars_files: - ./users.yaml tasks: - name: user exists per its role user: name: \"{{ item.name }}\" state: present loop: \"{{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) Lo que hacemos es que si un usuario tiene el rol de develop vaya a las maquinas devs y si es ops que vaya a las de prod. Podemos comprobar los usuarios en cada host con la orden: [isx46410800@miguel curso_ansible]$ ansible dev,prod -m command -a 'cat /etc/passwd' Podemos delegar una tarea a otro host, es decir en vez de la m\u00e1quina indicada, que se envie a otra con delegate_to : - name: delegar una tarea a otro host con delegate_to hosts: dev tasks: - name: file copied copy: content: este mensaje de prueba dest: /tmp/message.txt delegate_to: localhost Otro ejemplo de delegar tareas: - name: play to show how to use delegate_to in assignment hosts: dev1 tasks: - name: repos list is downloaded uri: url: https://api.github.com/users/atoumi/repos dest: /tmp/git-repos.json delegate_to: localhost Podemos importar la informaci\u00f3n de tareas con el m\u00f3dulo import_tasks . Se crea un archivo aparte de las tareas que se quieren importar y se a\u00f1aden al playbook principal: - name: play import_tasks hosts: dev1 tasks: - name: import nginx tasks import_tasks: nginx_install.yaml Los handers son avisadores de que haga una cosa o notifique si una tarea ha cambiado, ejemplo: - name: play illustrates the Slide of handlers hosts: somehost tasks: - name: t1 module-a: attr1: val1 - name: t2 module-b: attr1: val1 notify: t3 # run t3 only if t2 CHANGED - name: t4 module-d: attr1: val1 handlers: - name: t3 module-c: attr1: val1","title":"Ansible Task Control"},{"location":"ansible/#jinja2-templates","text":"DOCUMENTACI\u00d3N JINJA Ejemplo de filtros que ponemos ponerle en las tareas con jinja: # https://jinja.palletsprojects.com/en/2.11.x/templates/#builtin-filters - name: play with jinja2 filters hosts: container gather_facts: no vars: git_username: atoumi git_password: Gfdfd445e git_repos: [\"eks-course\", \"ansible-course\", \"react-csv\"] course_lectures_nb: [6, 6, 10, 8, 9] tasks: - name: j2 filter - capitalize debug: msg: | original : {{ git_username }} with filter: {{ git_username | capitalize }} # require: pip3 install passlib - name: j2 filter - password_hash('sha512') debug: msg: | original : {{ git_password }} with filter: {{ git_password | password_hash('sha512') }} - name: j2 filter - length - nb of repos debug: msg: | original : {{ git_repos }} with filter: {{ git_repos | length }} - name: j2 filter - sum - total nb of lectures debug: msg: | original : {{ course_lectures_nb }} with filter: {{ course_lectures_nb | sum }} - name: j2 filter - max - max nb of lectures in a section debug: msg: | original : {{ course_lectures_nb }} Lista de filtros: ansible jinja Ejemplo de poner filtros creando usuarios, metiendolo en la maquina que le toque segun rol y poniendo su passwd. luego nos conectamos y funciona: where \"vars/31-users.yaml\" content is : assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # - name: play create users per role hosts: dev,prod become: yes vars_files: - vars/31-users.yaml tasks: - name: user exists per its role user: name: \"{\u200c{ item.name }}\" state: present password: \"{\u200c{item.password | password_hash('sha512') }}\" loop: \"{\u200c{ assignment_users }}\" when: (item.role == 'developer' and 'dev' in group_names) or (item.role == 'ops' and 'prod' in group_names) # ssh ahmed@<ip-server> # .. then put the password (ahmed123) Con el modulo template podemos copiar un fichero que tiene variables del sistemas hacia destino. Si lo hacemos con el modulo copy, se copia literalmente sin sustituir las variables: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb #playbook - name: play to generate SPECS report hosts: dev1 tasks: - name: report is generated template: src: ./file_vars.conf dest: /tmp/specs.conf # ubuntu@ip-172-31-19-134:~$ cat /tmp/specs.conf Distribution : Ubuntu Distribution Release : focal Distribution Version : 20.04 Nbre CPU core : 1 cores Total Memory : 978 mb Se pueden poner tambi\u00e9n sintaxi jinja en el fichero como por ejemplo condicionales: #fichero file.conf Distribution : {{ ansible_facts.distribution }} Distribution Release : {{ ansible_facts.distribution_release }} Distribution Version : {{ ansible_facts.distribution_version }} Nbre CPU core : {{ ansible_facts.processor_cores }} cores Total Memory : {{ ansible_facts.memtotal_mb }} mb # {% if ansible_facts.memtotal_mb < 4000 %} The memory size is small {% else %} The memory size is OK {% endif %} Tambien se puede hacer jinja con loop: #playbook - name: play to generate SPECS report hosts: dev,prod vars: app_pages: - home - login - logout app_users: - name: ahmed role: developer - name: mouath role: developer - name: ali role: ops - name: omar role: ops tasks: - name: report is generated template: src: loop_jinja.conf dest: /tmp/app-report.conf #loop_jinja.conf ==== Print app_pages ===== {% for page in app_pages %} {{ page }}.html is a web page {% endfor %} ==== Print app_users ==== {% for u in app_users %} {{ u.name | capitalize }} is {{ u.role }} {% endfor %} Ejemplo de un /etc/hosts (etc/hosts that includes all hosts where {\u200c{ inventory_hostname }} magic variable is the domain name of the target host) - name: play common /etc/hosts hosts: all become: yes tasks: - name: copy /etc/hosts template: src: hosts.j2 dest: /etc/hosts # 127.0.0.1 localhost # The following lnes are desirable for IPv6 capable hosts ::1 ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters ff02::3 ip6-allhosts {% for host in groups['all'] %} {{ hostvars[host]['ansible_default_ipv4']['address'] }} {{ host }} {% endfor %} Resumen sintaxi JINJA: Print variable or Expression Print the variable by using the variable name surrounded by double braces. {\u200c{ my_var }} # .i.e: {\u200c{ ansible_distribution }} Filters Variables can be transformed/modified by filters. Think about filter as a function {\u200c{ my_var | my_filter }} # .i.e : {\u200c{ app_title | capitalize }} Read it like my_filter(my_var) If Block {% if CONDITION1 %} blah blah blah {% elif CONDITION2 %} blahelif blahelif blahelif . {% else %} blahelse so far {% endif %} #.i.e {% if git_branch == 'master' %} RELEASE: {\u200c{ app_version }} {% else %} SNAPSHOT: {\u200c{ app_version }}-RC{\u200c{ build_number }} {% endif %} For Loop {% for ELEMENT in ARRAY %} Process {\u200c{ ELEMENT }} {% endfor %} #. i.e: assume that ( app_pages = [\"login.html\", \"index.html\"] ) {% for page in app_pages %} <a href=\"https://example.com/{\u200c{ page }}\">{\u200c{ page }}</a> {% endfor %}","title":"Jinja2 templates"},{"location":"ansible/#ansible-vault","text":"Ansible sirve para desencriptar las passwords que salen en un fichero. Ordenes: ansible-vault create users_password.yaml ansible-vault encrypt users_password.yaml ansible-vault decrypt users_password.yaml ansible-vault edit users_password.yaml ansible-vault show users_password.yaml Partimos del ejemplo: #`ansible-vault create users_password.yaml` assignment_users: - name: ahmed password: ahmed123 role: developer - name: mouath password: mouath123 role: developer - name: ali password: aliali123 role: ops # playbook - name: play use encrypted vars hosts: localhost vars_files: - users_password.yaml tasks: - name: encrypted variable is printed debug: var: assignment_users Podemos crear en ansible.cfg la variable vault_password_file = file.key con una passwd de vault.","title":"Ansible Vault"},{"location":"ansible/#ansible-role-galaxy","text":"Los roles ansibles son la forma en que hacemos esto. Cuando se crea un rol, se descompone el playbook en partes y esas partes se encuentran en una estructura de directorios. Ejemplo de instalar un role: ansible-galaxy install role_file ansible-galaxy install -r ./requirements.yaml ansible-galaxy init my_role #estructura de directorios ansible-galaxy --help Importar role en un playbook: en ansible.cfg: [defaults] roles_path = ./roles ansible-galaxy install -r ./requirements.yaml tasks: - import_role: name: role_file vars: { var1: val1.. }","title":"Ansible Role &amp; Galaxy"},{"location":"ansible/#instalar-jenkkins","text":"Primer paso a\u00f1adimos en el ansible.cfg el roles_path, donde se instalaran los roles que queremos instalar: [defaults] INVENTORY=./inventory02_prod_dev roles_path=./roles ansible-galaxy : Instala roles de Ansible Galaxy, una plataforma para el intercambio de roles (recetas) Ansible. Podemos encontrar info de lo que queremos instalar con Ansible en Ansible Galaxy . En este caso buscamos Jenkins y vemos la opci\u00f3n de como descargarlo o si vamos al repo github, vemos un ejemplo de playbook para instalarlo. Primero podemos crear un fichero de requisitos de los paquetes a instalar de jenkins como roles para luego poder instalarlos remotamente: # - src: # name: # version: # roles para instalar jenkins - src: geerlingguy.java name: geerlingguy.java - src: geerlingguy.jenkins name: geerlingguy.jenkins [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - hosts: dev1 become: true vars: #jenkins_hostname: jenkins.example.com java_packages: - openjdk-8-jdk roles: - role: geerlingguy.java - role: geerlingguy.jenkins","title":"Instalar Jenkkins"},{"location":"ansible/#instalar-docker","text":"A\u00f1adimos a los requisitos: - src: geerlingguy.docker name: geerlingguy.docker Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook poniendo que podamos acceder como usuario el usuario ansible ssh(ubuntu): - hosts: prod1,prod2,dev1 become: yes tasks: - name: docker is installed import_role: name: geerlingguy.docker vars: docker_users: - \"{{ ansible_ssh_user }}\"","title":"Instalar Docker"},{"location":"ansible/#instalar-kubernetes","text":"A\u00f1adimos a requisitos: - src: geerlingguy.kubernetes name: geerlingguy.kubernetes Descargamos roles: [isx46410800@miguel curso_ansible]$ ansible-galaxy install -r requirements.yaml Playbook: - name: play kubernets is up hosts: cluster become: yes tasks: - name: docker role import_role: name: geerlingguy.docker - name: kubernetes role import_role: name: geerlingguy.kubernetes Hariamos 3 maquinas, 1 master y 2 nodos, los a\u00f1adiriamos al inventario y le pondriamos un grupo [cluster] y en cada host la variable role_kubernetes=node/master","title":"Instalar Kubernetes"},{"location":"ansible/#ansible-collection","text":"A veces los roles no son suficientes con descargarlos y necesitamos las colecciones, que son un conjunto de playbooks, roles, modulos y plugins. El fichero galaxy.yaml es el unico fichero requerido, aunque hay tambien directorios de roles, plugins, docs, playbooks... Orden: ansible-galaxy collection install file_collection ansible-galaxy install -r ./requirements.yaml ansible-galaxy collection init my_collection_file #estructura de directorios ansible-galaxy --help Donde guardarlos: [defaults] INVENTORY=./inventory02_prod_dev COLLECTIONS_PATHS=./collections A\u00f1adimos a requisitos: - collections: newswangerd.collection_demo Instalamos: [isx46410800@miguel curso_ansible]$ ansible-galaxy collection install -r requirements.yaml Playbook: - name: play usage collections hosts: dev1 collections: - newswangerd.collection_demo tasks: - name: module usage from collection real_facts: name: Abdennour - name: role usage from collection import_role: name: factoid","title":"Ansible Collection"},{"location":"ansible/#capstone-project-put-all-together-in-a-real-project-with-go-react-and-mongodb","text":"Idea: Frotend en una maquina tendremos REACT app, en backend tendremos la app GO y todo est\u00e1ra conectado en una VM con la ddbb mongoDB.","title":"Capstone Project - Put all Together in a Real Project with Go, React and MongoDB"},{"location":"ansible/#creacion-de-instancias-aws","text":"Creamos dos instancias en Amazon: app y db [isx46410800@miguel project_real]$ cat inventory_project app ansible_host=52.56.149.20 db ansible_host=18.130.63.197 [todo] app db [todo:vars] ansible_private_key_file=~/.ssh/ansible_user ansible_ssh_user=ubuntu #ansible_ssh_pass=ubuntu2021 [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_proyect Comprobamos que esta conexi\u00f3n funciona: [isx46410800@miguel project_real]$ ansible app,db -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } [isx46410800@miguel project_real]$ ansible todo -m ping db | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" } app | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python3\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Creaci\u00f3n de instancias AWS"},{"location":"ansible/#instalacion-mongodb","text":"Para instalar Mongodb se necesita estos pasos Playbook de tareas para instalarlo e iniciarlo en la instancia de DB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database hosts: db become: yes tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes Comprobamos: isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml PLAY [play Database] ************************************************************************************************* TASK [Gathering Facts] *********************************************************************************************** ok: [db] TASK [gnupg is installed] ******************************************************************************************** ok: [db] TASK [mongodb-key is added] ****************************************************************************************** changed: [db] TASK [mongo-db repo is enabled] ************************************************************************************** changed: [db] TASK [mongodb-org is installed] ************************************************************************************** changed: [db] TASK [mongod is enable] ********************************************************************************************** changed: [db] PLAY RECAP *********************************************************************************************************** db : ok=6 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ubuntu@ip-172-31-22-5:~$ sudo systemctl status mongod \u25cf mongod.service - MongoDB Database Server Loaded: loaded (/lib/systemd/system/mongod.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-04-06 12:02:22 UTC; 53s ago Docs: https://docs.mongodb.org/manual Main PID: 13440 (mongod) Memory: 64.2M CGroup: /system.slice/mongod.service \u2514\u250013440 /usr/bin/mongod --config /etc/mongod.conf Apr 06 12:02:22 ip-172-31-22-5 systemd[1]: Started MongoDB Database Server. Configuraci\u00f3n y creaci\u00f3n de superuser de MONGODB: [isx46410800@miguel project_real]$ cat playbooks/database.yaml - name: play Database install mongodb hosts: db become: yes tags: - db-install tasks: # sudo apt-get install gnupg - name: gnupg is installed package: name: gnupg state: present #wget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add - - name: mongodb-key is added apt_key: url: https://www.mongodb.org/static/pgp/server-4.4.asc #echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list - name: mongo-db repo is enabled apt_repository: repo: deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.4 multiverse state: present filename: mongodb # sudo apt-get install mongodb-org - name: mongodb-org is installed package: name: mongodb-org state: present #sudo systemctl enable mongod #sudo systemctl start mongod - name: mongod is enable service: name: mongod state: started enabled: yes - name: play Database config and create superuser hosts: db become: yes tags: - db-config-admin tasks: # drop admin user if exists (mongo admin --eval 'db.dropUser(\"superadmin\")') # create admin user (mongo admin --eval 'db.createUser'({ user: \"superadmin\", pwd:})) - name: create admin user command: \"{{ item }}\" loop: - mongo admin --eval 'db.dropUser(\"{{ db_admin_user }}\")' - | mongo admin --eval 'db.createUser( { user: \"{{ db_admin_user }}\", pwd: \"{{ db_admin_pass }}\", roles: [ { role: \"clusterAdmin\", db: \"admin\" }, { role: \"userAdminAnyDatabase\", db: \"admin\" } ] } )' # enable security (/etc/mongod.conf) ---> Restart Mongodb #> security: #> authorization: \"enabled\" - name: security is enabled blockinfile: path: /etc/mongod.conf block: | security: authorization: \"enabled\" state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted las variables hacen referencias al archivo host_vars/db/secret.yaml Creamos un file de contrase\u00f1a para encriptarlo con vault: [isx46410800@miguel project_real]$ mkdir -p host_vars/db # [isx46410800@miguel project_real]$ cat key.txt miguel14031993 # [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt # [isx46410800@miguel project_real]$ ansible-vault create host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r Comprobamos que est\u00e1 todo en orden: ubuntu@ip-172-31-22-5:~$ systemctl status mongod ubuntu@ip-172-31-22-5:~$ cat /etc/mongod.conf ubuntu@ip-172-31-22-5:~$ mongo admin -u superadmin -p PassMy1243r Configuramos ahora el primer usuario mongo : ... - name: play - rest of configuration hosts: db become: yes tags: - db-config tasks: - name: pip3 installed package: name: python3-pip state: latest - name: pip pymongo installed pip: name: pymongo state: latest - name: todo db user exists mongodb_user: login_user: \"{{ db_admin_user }}\" login_password: \"{{ db_admin_pass }}\" database: admin user: \"{{ db_todo_user }}\" password: \"{{ db_todo_pass }}\" state: present roles: - db: \"{{ db_name_todo }}\" role: readWrite [isx46410800@miguel project_real]$ ansible-vault edit host_vars/db/secret.yaml db_admin_user: superadmin db_admin_pass: PassMy1243r db_name_todo: test db_todo_user: todo db_todo_pass: todo [isx46410800@miguel project_real]$ ansible-inventory --host db { \"ansible_host\": \"35.176.225.221\", \"ansible_private_key_file\": \"~/.ssh/ansible_user\", \"ansible_ssh_user\": \"ubuntu\", \"db_admin_pass\": \"PassMy1243r\", \"db_admin_user\": \"superadmin\", \"db_name_todo\": \"test\", \"db_todo_pass\": \"todo\", \"db_todo_user\": \"todo\" } Lanzamos solo la tercera parte del playbook a\u00f1adido con la orden: [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config A\u00f1adimos la linea de las interfaces para que reciba desde cualquier ip: - name: db accepts connection from anywhere lineinfile: path: /etc/mongod.conf line: \" bindIp: 0.0.0.0\" regexp: '^(.*)binIp(.*)$' state: present notify: mongodb is restarted handlers: - name: mongodb is restarted service: name: mongod state: restarted [isx46410800@miguel project_real]$ ansible-playbook playbooks/database.yaml --tags db-config","title":"Instalaci\u00f3n MONGODB"},{"location":"ansible/#backend-app","text":"Hacemos playbook de instalar goland en el host app: [isx46410800@miguel project_real]$ cat playbooks/backend.yaml - name: play Backend hosts: app become: yes tags: - be-pre-build tasks: - name: go is installed import_role: name: gantsign.golang vars: golang_version: \"1.14\" golang_packages: - github.com/gorilla/mux - go.gomongodb.org/mongo-driver/mongo golang_users: - \"{{ ansible_ssh_user }}\" - name: play Backend hosts: app become: yes tags: - be-build tasks: - name: workspace build exist file: path: /opt/build_dir state: directory - name: git checkout git: repo: https://gitlab.com/isx46410800/curso_ansible.git dest: /opt/build_dir/curso_ansible - name: go build shell: . /etc/profile;go build -o /tmp/todo args: chdir: /opt/build_dir/curso_ansible/server [isx46410800@miguel project_real]$ cat ansible.cfg [defaults] INVENTORY=./inventory_project vault_password_file=./key.txt roles_path=./roles [isx46410800@miguel project_real]$ cat requirements.yaml - src: gantsign.golang name: gantsign.golang [isx46410800@miguel project_real]$ ansible-playbook playbooks/backend.yaml --tags be-pre-build *https://github.com/kubernetes-tn/go-to-do-app * *https://github.com/abdennour/ansible-course *","title":"Backend APP"},{"location":"aws/","text":"AWS GU\u00cdA AWS APUNTES DE AWS S3 (Simple Storage) En esta parte podemos crear los llamados cajones/cestas en la nube para guardar ciertos tipo de datos en nuestro AWS. Se llaman BUCKETS. En estos BUCKETS podemos meter archivos, especificar distintas medidas de almacenamiento, podemos indicarle el versioning de lo que tenemos dentro a lo actualizado, logging si se necesita loguearse para poder acceder al bucket y podemos gestionar otro tipo de medidas y seguridad para poder acceder al bucket y conectarse a el. Otro tipo de uso de los BUCKETS son el uso de almacenar paginas web estaticas. IAM - SECURITY En esta secci\u00f3n podemos crear: Usuarios Grupos Politicas de grupo Roles EC2 (Elastic Compute Services) Para conectarnos a una instancia podemos verlo en las opciones de CONNECT y podemos hacerlo desde linux o windows. Para windows generamos la clave privada con PuttyGen y luego la a\u00f1adimos a Putty normal para conectarse. Para conectanos a una instancia windows a la hora de conectar nos descargamos un archivo desktop remote file, descargamos el documento PEM, y lo iniciamos a\u00f1adiendo la clave al programa y tendremos nuestra conexion a una instancia windows. De una instancia ya creada y configurada podemos crear una IMAGEN AMI del estado de esa instancia. As\u00ed luego podemos crear una nueva instancia a partir de esta imagen AMI. En los pasos de configuraci\u00f3n de la instancia tambi\u00e9n se puede pasar script para por ejemplo instalar paquetes ya antes de arrancar la instancia. Los EBS son los volumenes elasticos que se pueden asignar a las instancias para que se vayan redimensionando a medida que se van llenando o vaciando. Podemos crear los EBS en volumenes o en la hora de crear la instancia en el apartado de a\u00f1adir Storage y a\u00f1adir un disco EBS. Si lo creamos aparte, luego lo vamos a acciones de instancia y ATTACH storage, seleccionamos el volumen EBS y luego lo formateamos con mkfs -t ext4 y lo montamos en un directorio. Si creamos algo dentro de el, si luego lo volvemos a separar y este volumen lo juntamos a otra instancia, tendr\u00e1 el contenido de la otra. Se pueden crear SNAPSHOOTS de los volumenes EBS y asignarlo a otras instancias con ese contenido original. AWS CLI Existe una herramienta de consola para conectarse a AWS. Instalamos con pip install aws cli Usamos con comandos aws + comando + opciones DOCUMENTACION AWS CLI En aws creamos un usuario awscli y ponemos su password roles administration access etc. Nos apuntamos las credenciales para poder conectarnos desde la herramienta de consola. Vamos a aws configure y metemos las credenciales que nos va pidiendo. Con aws s3 ls vemos todo lo que tenemos. Con aws ec2 ?/help vemos las opciones que podemos hacer en EC2. Conectamos por ejemplo con aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t2.micro --key-name MyKeyPair --security-group-ids sg-903004f8 --subnet-id subnet-6e7f829e EFS Amazon Elastic File System: Sistema de archivos el\u00e1stico, simple, sin servidor y con posibilidad de establecer y olvidar. Amazon Elastic File System (Amazon EFS) crece y se reduce autom\u00e1ticamente a medida que se agregan y eliminan archivos sin necesidad de administraci\u00f3n o aprovisionamiento. Sistema de archivos el\u00e1stico, simple, sin servidor y con posibilidad de establecer y olvidar Cree y configure sistemas de archivos compartidos de forma sencilla y r\u00e1pida para los servicios inform\u00e1ticos de AWS, sin necesidad de aprovisionamiento, implementaci\u00f3n, parches ni mantenimiento. Escale su sistema de archivos autom\u00e1ticamente a medida que se agregan y eliminan archivos. Ampl\u00ede el rendimiento a niveles m\u00e1s altos cuando sea necesario. Pague solo por el almacenamiento que utilice y reduzca los costos hasta en un 92 % al trasladar autom\u00e1ticamente los archivos a los que se accede con poca frecuencia. Acceda de forma segura y fiable a los archivos con un sistema de archivos completamente administrado y dise\u00f1ado para una alta disponibilidad y una durabilidad del 99,999999999 % (11 9). Para crearlo vamos a EFS y creamos un sistema de archivos elastico. AWS STORAGE GATEWAY AWS Storage Gateway es un servicio de almacenamiento h\u00edbrido que permite que las aplicaciones que se encuentran en sitio puedan utilizar almacenamiento pr\u00e1cticamente ilimitado de la nube de AWS, de manera transparente y sin contratiempos. Storage Gateway no requiere cambios en sus aplicaciones y se integra f\u00e1cilmente ya que utiliza protocolos de comunicaci\u00f3n y almacenamiento est\u00e1ndar como HTTPS (HTTP sobre SSL/TLS) para comunicaci\u00f3n con la nube, as\u00ed como SMB (Server Message Block) y NFS (Network File System) para el acceso a los datos almacenados. Mediante Storage Gateway, usted puede ampliar su capacidad local de almacenamiento mientras reduce y simplifica la infraestructura de su centro de datos o de sus oficinas remotas. Las aplicaciones se conectan al servicio de Storage Gateway a trav\u00e9s de una m\u00e1quina virtual o dispositivo de hardware que se instala en sitio y que utiliza protocolos est\u00e1ndar como NFS, SMB e iSCSI. Este dispositivo local se conecta al servicio de Storage Gateway que a su vez se conecta con los servicios de almacenamiento de AWS tales como Amazon S3, Amazon S3 Glacier, Amazon S3 Glacier Deep Archive y Amazon EBS y de esta manera se provee almacenamiento para sus aplicaciones y usuarios que se encuentran en sitio. Este dispositivo tambi\u00e9n se utiliza como cach\u00e9 local para proveer un acceso de baja latencia a los datos m\u00e1s activos. Como en AWS nuestra prioridad es la seguridad, la conexi\u00f3n al servicio de AWS Storage Gateway se hace por medio de un canal seguro utilizando HTTPS. CREACION DE AWS STORAGE Las clases de almacenamiento de AMAZON S3 GLACIER se crearon espec\u00edficamente para el archivo de datos y le ofrecen el mayor rendimiento, la mayor flexibilidad de recuperaci\u00f3n y el menor costo de almacenamiento de archivos en la nube. Todas las clases de almacenamiento S3 Glacier ofrecen escalabilidad pr\u00e1cticamente ilimitada y est\u00e1n dise\u00f1adas para lograr un 99,999999999 % (11 nueves) de durabilidad de datos. Las clases de almacenamiento S3 Glacier ofrecen opciones para acceder m\u00e1s r\u00e1pidamente a los datos de archivos y el menor costo en almacenamiento de archivos en la nube. Amazon Glacier es un servicio de archivo de datos especialmente dise\u00f1ado para las copias de seguridad o para aquellos datos que no se necesiten acceder constantemente o inmediatamente. El archivo de datos en Amazon Glacier significa que aunque puede almacenar sus datos a un costo extremadamente bajo (incluso en comparaci\u00f3n con Amazon S3), no puede recuperar sus datos inmediatamente cuando lo desee. Los datos almacenados en Amazon Glacier tardan varias horas en recuperarse, por lo que es ideal para archivar. Hay tres opciones para recuperar datos con diferentes tiempos de acceso y costo: recuperaciones aceleradas, est\u00e1ndar y masivas, de la siguiente manera: \u2013 Las recuperaciones aceleradas generalmente est\u00e1n disponibles dentro de 1 a 5 minutos. \u2013 Las recuperaciones est\u00e1ndar generalmente se completan dentro de 3 a 5 horas. \u2013 Las recuperaciones a granel generalmente se completan dentro de 5 a 12 horas. (AWS) Los datos se almacenan en Amazon Glacier en \u00abarchivos\u00bb. Un archivo puede ser cualquier informaci\u00f3n, como una foto, video o documento. Puede cargar un solo archivo como un archivo o agregar m\u00faltiples archivos en un archivo TAR o ZIP y cargarlo como un archivo. Un solo archivo puede ser tan grande como 40 terabytes. Puede almacenar una cantidad ilimitada de archivos y una cantidad ilimitada de datos en Amazon Glacier. A cada archivo se le asigna una ID de archivo \u00fanica en el momento de la creaci\u00f3n, y el contenido del archivo es inmutable, lo que significa que despu\u00e9s de que se crea un archivo, no se puede actualizar. AWS BBDD Existen varios tipos: DynamoBD: Document and key-value store. RDS: SQL database engines. ELASTICACHE: In-memory cach\u00e9. REDSHIFT: data warehouse. Amazon RDS proporciona una selecci\u00f3n de tipos de instancias optimizadas para diferentes casos de uso de bases de datos relacionales. Los tipos de instancia abarcan varias combinaciones de capacidad de CPU, memoria, almacenamiento y redes. Le proporcionan flexibilidad para elegir la combinaci\u00f3n de recursos adecuada para sus bases de datos. Cada tipo de instancia incluye varios tama\u00f1os de instancia, lo que le permite escalar su base de datos seg\u00fan los requisitos de la carga de trabajo de destino. Vamos a RDS y seleccionamos el tipo de bbdd que queremos crear. A partir de ahi creamos para que es el uso, usuario, password, base de datos, tama\u00f1o, backup, seguridad, logs, etc. Luego por ejemplo podemos conectarnos desde una instancia al nuestra bbdd como por ejemplo con mysql -h endpoint_de_la_bbdd -u xxx -p y desde ahi podemos crear tablas etc. Tambien podemos crear snapshoots y de ahi crear otra modificada etc. AMAZON AURORA es una base de datos relacional compatible con MySQL y PostgreSQL creada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos empresariales tradicionales con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Amazon Aurora es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos de MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos de PostgreSQL est\u00e1ndar. Ofrece la seguridad, disponibilidad y fiabilidad de las bases de datos de nivel comercial por una d\u00e9cima parte del costo. Amazon Aurora est\u00e1 completamente administrada por Amazon Relational Database Service (RDS), que automatiza las tareas administrativas demandantes como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la aplicaci\u00f3n de parches y las copias de seguridad. Amazon Aurora ofrece un sistema de almacenamiento distribuido, tolerante a errores y de recuperaci\u00f3n autom\u00e1tica que ajusta su escala verticalmente de forma autom\u00e1tica hasta 128 TB por instancia de base de datos. Amazon Aurora suministra alto rendimiento y disponibilidad con hasta 15 r\u00e9plicas de lectura de baja latencia, recuperaci\u00f3n a un momento dado, generaci\u00f3n de copias de seguridad continua en Amazon S3 y replicaci\u00f3n en tres zonas de disponibilidad. DISE\u00d1O DE ARQUITECTURAS Arquitectura dise\u00f1o web: LoadBalancer: 3 tipos: basico, aplication y autoscaling. El basico se puede ir a loadbalancer y crear, meter las istancias que pertenecen y el puerto y security groups. El aplication, se crea igual pero se suele crear un TargetGroup que diga que comprueba el index.html por el puerto 80. Un vez creado se va a editar y a\u00f1adir instancias y se le indica que target group se le indica para que compruebe su health check de que esa parte funciona. El AutoScaling se puede crear un conjunto de instancias con la misma configuracion e ir indicando que aumente de instancia o disminuya segun criterios como el uso de cpu etc. VPC Es crear una red privada y a partir de ahi montar todo tu insfraestructura. Si por ejemplo ponemos la red 10.0.0.0/16, no se pueden usar: .1/16 es router .2/16 es para dnd server .3/16 es para uso futuro de aws .255/16 es broadcast .0/16 es la red Vamos a VPC creamos una red con subnet y sus rangos. Despues creamos instancias y le asignamos esta redes. Tambien podemos crear Elastic IP y asignar IP fijas publicas a estas instancias para esta red. Se puede crear en modo de privada y publica subredes, segun la estructura que queramos. Se puede crear Internet gateway y hacer atach con el VPC que sirve para conectar esta red a Internet. El egress-only sirve para IPv6. Tambien se pueden crear ACL en el PVC. Listas de control de acceso (ACL) de red: las ACL de red act\u00faan como firewall para las subredes asociadas y controlan el tr\u00e1fico entrante y saliente en el \u00e1mbito de la subred. Grupos de seguridad: los grupos de seguridad act\u00faan como firewall para las instancias Amazon EC2 asociadas, al controlar el tr\u00e1fico entrante y saliente en el nivel de la instancia. Cuando lanza una instancia, puede asociarla a uno o varios grupos de seguridad que haya creado. Cada instancia de su VPC podr\u00eda pertenecer a un conjunto distinto de grupos de seguridad. Si no especifica ning\u00fan grupo de seguridad al lanzar una instancia, esta se asocia autom\u00e1ticamente al grupo de seguridad predeterminado de la VPC. Connection peering para crear conexion entre diferentes VPC. Para hacer conexiones VPN se necesita tambien un Virtual Private Gateway para luego poder crear estos tuneles de la red de casa a la nube de amazon. SERVICES AMAZON AWS Simple Notification Service (SNS): servicios de notificaciones de suscripciones, email etc. AWS Simple Queue Service (SQS): servicio de cola de mensajes. AWS CloudWatch: herramienta para gestionar las metricas de nuestras maquinas de aws. AWS CloudTrail: sirve para guardar datos de eventos de nuestras maquinas en un bucket s3. AWS Route53: herramienta que sirve para crear, registrar y gestionar dominios. AWS CloudFormation: crear todo mediante codigo json. AWS Developer tools: herramientas parecidad a git como CodeCommit y luego hay CodeStar que es muy parecido a crear un pod de openshift, y herramientas para test, dev y deploy. LAMBDA AWS Lambda es un servicio inform\u00e1tico sin servidor y basado en eventos que le permite ejecutar c\u00f3digo para pr\u00e1cticamente cualquier tipo de aplicaci\u00f3n o servicio backend sin necesidad de aprovisionar o administrar servidores. Puede activar Lambda desde m\u00e1s de 200 servicios de AWS y aplicaciones de software como servicio (SaaS), y solo paga por lo que utiliza. Ejecute el c\u00f3digo sin aprovisionar ni administrar la infraestructura. Simplemente escriba y cargue el c\u00f3digo como un archivo .zip o una imagen de contenedor. Responda autom\u00e1ticamente a las solicitudes de ejecuci\u00f3n de c\u00f3digo a cualquier escala, desde una docena de eventos al d\u00eda hasta cientos de miles por segundo. Ahorre costos al pagar solamente por el tiempo de inform\u00e1tica que utiliza, por milisegundo, en lugar de aprovisionar la infraestructura por adelantado para la capacidad m\u00e1xima. Optimice el tiempo de ejecuci\u00f3n del c\u00f3digo y el rendimiento con el tama\u00f1o adecuado de la memoria de las funciones. Responda a la alta demanda en milisegundos de dos d\u00edgitos con simultaneidad aprovisionada. IAC CLOUDFORMATION IAC (Infrastructure as Code) AWS CloudFormation le ofrece una forma sencilla de modelar un conjunto de recursos relacionados de AWS y de terceros, aprovisionarlos de manera r\u00e1pida y consistente y administrarlos a lo largo de sus ciclos de vida tratando la infraestructura como un c\u00f3digo. La plantilla de CloudFormation describe los recursos que desea y sus dependencias para que los pueda lanzar y configurar juntos como una pila. Puede usar la plantilla para crear, actualizar y eliminar toda una pila como una \u00fanica unidad, tantas veces como lo necesite, en lugar de administrar los recursos de manera individual. Puede administrar y aprovisionar pilas en varias cuentas y regiones de AWS. web (https://aws.amazon.com/es/cloudformation/) Documentaci\u00f3n (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) Sintaxis simple: --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro Sintaxis compleja: --- Parameters: SecurityGroupDescription: Description: Security-Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup # an elastic Ip for our instance MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 # our second EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Red SecurityGroupDescription SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 10.10.0.1/32 Create STACK Vamos a consola , cloudformation, crear stack, crear template, subir fichero y ponemos nuestra instancia simple de c\u00f3digo en el fichero y veremos como crea una instancia del tipo que se le indica: --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro Para actualizar la que tenemos vamos a crear stack, replace current template, subimos fichero y veremos en los eventos como se actualiza: --- Parameters: SecurityGroupDescription: Description: Security-Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup # an elastic Ip for our instance MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 # our second EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Red SecurityGroupDescription SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 10.10.0.1/32 Despues se puede borrar con Delete en los eventos. Par\u00e1metros Podemos indicar una serie de parametros a la hora de crear nuestra IAC: Ejemplo 1: Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro. Ejemplo2: Parameters: DBPort: Default: 3306 Description: TCP/IP port for the database Type: Number MinValue: 1150 MaxValue: 65535 DBPwd: NoEcho: true Description: The database admin account password Type: String MinLength: 1 MaxLength: 41 AllowedPattern: ^[a-zA-Z0-9]*$ Recursos En esta secci\u00f3n declaramos qu\u00e9 recursos queremos crear en nuestro stack Ejemplo: Resources: MyInstance: Type: \"AWS::EC2::Instance\" Properties: UserData: \"Fn::Base64\": !Sub | Queue=${MyQueue} AvailabilityZone: \"us-east-1a\" ImageId: \"ami-0ff8a91507f77f867\" MyQueue: Type: \"AWS::SQS::Queue\" Properties: {} Todos los tipos de recursos Ejemplo: Type: AWS::EC2::Instance Properties: AdditionalInfo: String Affinity: String AvailabilityZone: String BlockDeviceMappings: - BlockDeviceMapping CpuOptions: CpuOptions CreditSpecification: CreditSpecification DisableApiTermination: Boolean EbsOptimized: Boolean ElasticGpuSpecifications: - ElasticGpuSpecification ElasticInferenceAccelerators: - ElasticInferenceAccelerator EnclaveOptions: EnclaveOptions HibernationOptions: HibernationOptions HostId: String HostResourceGroupArn: String IamInstanceProfile: String ImageId: String InstanceInitiatedShutdownBehavior: String InstanceType: String Ipv6AddressCount: Integer Ipv6Addresses: - InstanceIpv6Address KernelId: String KeyName: String LaunchTemplate: LaunchTemplateSpecification LicenseSpecifications: - LicenseSpecification Monitoring: Boolean NetworkInterfaces: - NetworkInterface PlacementGroupName: String PrivateIpAddress: String RamdiskId: String SecurityGroupIds: - String SecurityGroups: - String SourceDestCheck: Boolean SsmAssociations: - SsmAssociation SubnetId: String Tags: - Tag Tenancy: String UserData: String Volumes: - Volume Mapping Mapea asignando una clave a un valor Ejemplo: AWSTemplateFormatVersion: \"2010-09-09\" Mappings: RegionMap: us-east-1: HVM64: ami-0ff8a91507f77f867 HVMG2: ami-0a584ac55a7631c0c us-west-1: HVM64: ami-0bdb828fd58c52235 HVMG2: ami-066ee5fd4a9ef77f1 eu-west-1: HVM64: ami-047bb4163c506cd98 HVMG2: ami-0a7c483d527806435 ap-northeast-1: HVM64: ami-06cd52961ce9f0d85 HVMG2: ami-053cdd503598e4a9d ap-southeast-1: HVM64: ami-08569b978cc4dfa10 HVMG2: ami-0be9df32ae9f92309 Resources: myEC2Instance: Type: \"AWS::EC2::Instance\" Properties: ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", HVM64] InstanceType: m1.small Outputs Los outputs nos salidas de mensajes. Intentar no poner informaci\u00f3n sensible. Ejemplo: Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance Conditions Los conditions nos indican que hace algo si se cumple tal condici\u00f3n. Ejemplo: AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: CreateProdResources: !Equals - !Ref EnvType - prod Resources: EC2Instance: Type: 'AWS::EC2::Instance' Properties: ImageId: ami-0ff8a91507f77f867 MountPoint: Type: 'AWS::EC2::VolumeAttachment' Condition: CreateProdResources Properties: InstanceId: !Ref EC2Instance VolumeId: !Ref NewVolume Device: /dev/sdh NewVolume: Type: 'AWS::EC2::Volume' Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt - EC2Instance - AvailabilityZone Functions Hay diferentes funciones internas en la creaci\u00f3n de un template IAC User data Podemos crear plantillas escribiendo ordenes de linux con user data Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-009d6802948d06e52 InstanceType: t2.micro KeyName: !Ref SSHKey SecurityGroups: - !Ref SSHSecurityGroup UserData: Fn::Base64: | #!/bin/bash -xe yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"Hello World from user data\" > /var/www/html/index.html # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 Creamos una stack con el template, ponemos el parametro creado y vemos que sea crea tanto por la ip:80 como por consola. Cfn-init Vemos la docu de cfn init Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-009d6802948d06e52 InstanceType: t2.micro KeyName: !Ref SSHKey SecurityGroups: - !Ref SSHSecurityGroup UserData: Fn::Base64: !Sub | #!/bin/bash -xe # get the latest cloudformation package yum update -y aws-cfn-bootstrap # start cfn-init /opt/aws/bin/cfn-init -s ${AWS::StackId} -r MyInstance --region ${AWS::Region} || error_exit \"Failed to run cfn-init\" Metadata: Comment: Install a simple Apache HTTP page AWS::CloudFormation::Init: config: package: yum: httpd:[] files: \"/var/www/html/index.html\" content: | <h1>Hello word, using cfn-init</h1> mode: '000644' commands: hello: command: \"echo 'hello world'\" services: sysvinit: httpd: enabled: 'true' ensureRunning: 'true' # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 Cfn-signal Docu de cfn-signal Ejemplo: AWSTemplateFormatVersion: 2010-09-09 Description: Simple EC2 instance Resources: MyInstance: Type: AWS::EC2::Instance Metadata: 'AWS::CloudFormation::Init': config: files: /tmp/test.txt: content: Hello world! mode: '000755' owner: root group: root Properties: ImageId: ami-a4c7edb2 InstanceType: t2.micro UserData: !Base64 'Fn::Join': - '' - - | #!/bin/bash -x - | # Install the files and packages from the metadata - '/opt/aws/bin/cfn-init -v ' - ' --stack ' - !Ref 'AWS::StackName' - ' --resource MyInstance ' - ' --region ' - !Ref 'AWS::Region' - |+ - | # Signal the status from cfn-init - '/opt/aws/bin/cfn-signal -e $? ' - ' --stack ' - !Ref 'AWS::StackName' - ' --resource MyInstance ' - ' --region ' - !Ref 'AWS::Region' - |+ CreationPolicy: ResourceSignal: Timeout: PT5M Rollback Cuando creamos una stack podemos indicar si queremos un rollback para poder volver a un estado anterior. Cfn nested stacks Docu de stack anidadas entre s\u00ed cfn nested Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: TemplateURL: https://s3.amazonaws.com/cloudformation-templates-us-east-1/LAMP_Single-Instance.template Parameters: KeyName: !Ref SSHKey DBName: \"mydb\" DBUser: \"user\" DBPassword: \"pass\" DBRootPassword: \"passroot\" InstanceType: t2.micro SSHLocation: \"0.0.0.0/0\" Outputs: StackRef: value: !Ref myStack OutputFromNestedStack: value: !GetAtt mystack.Outputs.WebsiteURL Change sets DOCU Sirve para actualizar o subir otra cosa de una stack ya creada y podremos ver el proceso de si se har\u00eda bien o no. Si nos interesa, le damos a EXECUTE y empieza a cambiarse, sino DELETE. Depends on DOCU De crear una instancia que depende de otra creada. Ejemplo: AWSTemplateFormatVersion: '2010-09-09' Mappings: RegionMap: us-east-1: AMI: ami-0ff8a91507f77f867 us-west-1: AMI: ami-0bdb828fd58c52235 eu-west-1: AMI: ami-047bb4163c506cd98 ap-northeast-1: AMI: ami-06cd52961ce9f0d85 ap-southeast-1: AMI: ami-08569b978cc4dfa10 Resources: Ec2Instance: Type: AWS::EC2::Instance Properties: ImageId: Fn::FindInMap: - RegionMap - Ref: AWS::Region - AMI DependsOn: myDB myDB: Type: AWS::RDS::DBInstance Properties: AllocatedStorage: '5' DBInstanceClass: db.t2.small Engine: MySQL EngineVersion: '5.5' MasterUsername: MyName MasterUserPassword: MyPassword Detect drift stack Sirve para hacer marcas y ver donde se cambi\u00f3 si actualizas la template o modificas algo de la template DOCU Vamos a stack actions - detect drift una vez ya hemos metido nuestra template creada. Podemos ver en view drift y si modificamos cosas como el security groups veremos que la marca cambia.","title":"AWS"},{"location":"aws/#aws","text":"","title":"AWS"},{"location":"aws/#guia-aws","text":"APUNTES DE AWS","title":"GU\u00cdA AWS"},{"location":"aws/#s3-simple-storage","text":"En esta parte podemos crear los llamados cajones/cestas en la nube para guardar ciertos tipo de datos en nuestro AWS. Se llaman BUCKETS. En estos BUCKETS podemos meter archivos, especificar distintas medidas de almacenamiento, podemos indicarle el versioning de lo que tenemos dentro a lo actualizado, logging si se necesita loguearse para poder acceder al bucket y podemos gestionar otro tipo de medidas y seguridad para poder acceder al bucket y conectarse a el. Otro tipo de uso de los BUCKETS son el uso de almacenar paginas web estaticas.","title":"S3 (Simple Storage)"},{"location":"aws/#iam-security","text":"En esta secci\u00f3n podemos crear: Usuarios Grupos Politicas de grupo Roles","title":"IAM - SECURITY"},{"location":"aws/#ec2-elastic-compute-services","text":"Para conectarnos a una instancia podemos verlo en las opciones de CONNECT y podemos hacerlo desde linux o windows. Para windows generamos la clave privada con PuttyGen y luego la a\u00f1adimos a Putty normal para conectarse. Para conectanos a una instancia windows a la hora de conectar nos descargamos un archivo desktop remote file, descargamos el documento PEM, y lo iniciamos a\u00f1adiendo la clave al programa y tendremos nuestra conexion a una instancia windows. De una instancia ya creada y configurada podemos crear una IMAGEN AMI del estado de esa instancia. As\u00ed luego podemos crear una nueva instancia a partir de esta imagen AMI. En los pasos de configuraci\u00f3n de la instancia tambi\u00e9n se puede pasar script para por ejemplo instalar paquetes ya antes de arrancar la instancia. Los EBS son los volumenes elasticos que se pueden asignar a las instancias para que se vayan redimensionando a medida que se van llenando o vaciando. Podemos crear los EBS en volumenes o en la hora de crear la instancia en el apartado de a\u00f1adir Storage y a\u00f1adir un disco EBS. Si lo creamos aparte, luego lo vamos a acciones de instancia y ATTACH storage, seleccionamos el volumen EBS y luego lo formateamos con mkfs -t ext4 y lo montamos en un directorio. Si creamos algo dentro de el, si luego lo volvemos a separar y este volumen lo juntamos a otra instancia, tendr\u00e1 el contenido de la otra. Se pueden crear SNAPSHOOTS de los volumenes EBS y asignarlo a otras instancias con ese contenido original.","title":"EC2 (Elastic Compute Services)"},{"location":"aws/#aws-cli","text":"Existe una herramienta de consola para conectarse a AWS. Instalamos con pip install aws cli Usamos con comandos aws + comando + opciones DOCUMENTACION AWS CLI En aws creamos un usuario awscli y ponemos su password roles administration access etc. Nos apuntamos las credenciales para poder conectarnos desde la herramienta de consola. Vamos a aws configure y metemos las credenciales que nos va pidiendo. Con aws s3 ls vemos todo lo que tenemos. Con aws ec2 ?/help vemos las opciones que podemos hacer en EC2. Conectamos por ejemplo con aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t2.micro --key-name MyKeyPair --security-group-ids sg-903004f8 --subnet-id subnet-6e7f829e","title":"AWS CLI"},{"location":"aws/#efs","text":"Amazon Elastic File System: Sistema de archivos el\u00e1stico, simple, sin servidor y con posibilidad de establecer y olvidar. Amazon Elastic File System (Amazon EFS) crece y se reduce autom\u00e1ticamente a medida que se agregan y eliminan archivos sin necesidad de administraci\u00f3n o aprovisionamiento. Sistema de archivos el\u00e1stico, simple, sin servidor y con posibilidad de establecer y olvidar Cree y configure sistemas de archivos compartidos de forma sencilla y r\u00e1pida para los servicios inform\u00e1ticos de AWS, sin necesidad de aprovisionamiento, implementaci\u00f3n, parches ni mantenimiento. Escale su sistema de archivos autom\u00e1ticamente a medida que se agregan y eliminan archivos. Ampl\u00ede el rendimiento a niveles m\u00e1s altos cuando sea necesario. Pague solo por el almacenamiento que utilice y reduzca los costos hasta en un 92 % al trasladar autom\u00e1ticamente los archivos a los que se accede con poca frecuencia. Acceda de forma segura y fiable a los archivos con un sistema de archivos completamente administrado y dise\u00f1ado para una alta disponibilidad y una durabilidad del 99,999999999 % (11 9). Para crearlo vamos a EFS y creamos un sistema de archivos elastico.","title":"EFS"},{"location":"aws/#aws-storage-gateway","text":"AWS Storage Gateway es un servicio de almacenamiento h\u00edbrido que permite que las aplicaciones que se encuentran en sitio puedan utilizar almacenamiento pr\u00e1cticamente ilimitado de la nube de AWS, de manera transparente y sin contratiempos. Storage Gateway no requiere cambios en sus aplicaciones y se integra f\u00e1cilmente ya que utiliza protocolos de comunicaci\u00f3n y almacenamiento est\u00e1ndar como HTTPS (HTTP sobre SSL/TLS) para comunicaci\u00f3n con la nube, as\u00ed como SMB (Server Message Block) y NFS (Network File System) para el acceso a los datos almacenados. Mediante Storage Gateway, usted puede ampliar su capacidad local de almacenamiento mientras reduce y simplifica la infraestructura de su centro de datos o de sus oficinas remotas. Las aplicaciones se conectan al servicio de Storage Gateway a trav\u00e9s de una m\u00e1quina virtual o dispositivo de hardware que se instala en sitio y que utiliza protocolos est\u00e1ndar como NFS, SMB e iSCSI. Este dispositivo local se conecta al servicio de Storage Gateway que a su vez se conecta con los servicios de almacenamiento de AWS tales como Amazon S3, Amazon S3 Glacier, Amazon S3 Glacier Deep Archive y Amazon EBS y de esta manera se provee almacenamiento para sus aplicaciones y usuarios que se encuentran en sitio. Este dispositivo tambi\u00e9n se utiliza como cach\u00e9 local para proveer un acceso de baja latencia a los datos m\u00e1s activos. Como en AWS nuestra prioridad es la seguridad, la conexi\u00f3n al servicio de AWS Storage Gateway se hace por medio de un canal seguro utilizando HTTPS. CREACION DE AWS STORAGE Las clases de almacenamiento de AMAZON S3 GLACIER se crearon espec\u00edficamente para el archivo de datos y le ofrecen el mayor rendimiento, la mayor flexibilidad de recuperaci\u00f3n y el menor costo de almacenamiento de archivos en la nube. Todas las clases de almacenamiento S3 Glacier ofrecen escalabilidad pr\u00e1cticamente ilimitada y est\u00e1n dise\u00f1adas para lograr un 99,999999999 % (11 nueves) de durabilidad de datos. Las clases de almacenamiento S3 Glacier ofrecen opciones para acceder m\u00e1s r\u00e1pidamente a los datos de archivos y el menor costo en almacenamiento de archivos en la nube. Amazon Glacier es un servicio de archivo de datos especialmente dise\u00f1ado para las copias de seguridad o para aquellos datos que no se necesiten acceder constantemente o inmediatamente. El archivo de datos en Amazon Glacier significa que aunque puede almacenar sus datos a un costo extremadamente bajo (incluso en comparaci\u00f3n con Amazon S3), no puede recuperar sus datos inmediatamente cuando lo desee. Los datos almacenados en Amazon Glacier tardan varias horas en recuperarse, por lo que es ideal para archivar. Hay tres opciones para recuperar datos con diferentes tiempos de acceso y costo: recuperaciones aceleradas, est\u00e1ndar y masivas, de la siguiente manera: \u2013 Las recuperaciones aceleradas generalmente est\u00e1n disponibles dentro de 1 a 5 minutos. \u2013 Las recuperaciones est\u00e1ndar generalmente se completan dentro de 3 a 5 horas. \u2013 Las recuperaciones a granel generalmente se completan dentro de 5 a 12 horas. (AWS) Los datos se almacenan en Amazon Glacier en \u00abarchivos\u00bb. Un archivo puede ser cualquier informaci\u00f3n, como una foto, video o documento. Puede cargar un solo archivo como un archivo o agregar m\u00faltiples archivos en un archivo TAR o ZIP y cargarlo como un archivo. Un solo archivo puede ser tan grande como 40 terabytes. Puede almacenar una cantidad ilimitada de archivos y una cantidad ilimitada de datos en Amazon Glacier. A cada archivo se le asigna una ID de archivo \u00fanica en el momento de la creaci\u00f3n, y el contenido del archivo es inmutable, lo que significa que despu\u00e9s de que se crea un archivo, no se puede actualizar.","title":"AWS STORAGE GATEWAY"},{"location":"aws/#aws-bbdd","text":"Existen varios tipos: DynamoBD: Document and key-value store. RDS: SQL database engines. ELASTICACHE: In-memory cach\u00e9. REDSHIFT: data warehouse. Amazon RDS proporciona una selecci\u00f3n de tipos de instancias optimizadas para diferentes casos de uso de bases de datos relacionales. Los tipos de instancia abarcan varias combinaciones de capacidad de CPU, memoria, almacenamiento y redes. Le proporcionan flexibilidad para elegir la combinaci\u00f3n de recursos adecuada para sus bases de datos. Cada tipo de instancia incluye varios tama\u00f1os de instancia, lo que le permite escalar su base de datos seg\u00fan los requisitos de la carga de trabajo de destino. Vamos a RDS y seleccionamos el tipo de bbdd que queremos crear. A partir de ahi creamos para que es el uso, usuario, password, base de datos, tama\u00f1o, backup, seguridad, logs, etc. Luego por ejemplo podemos conectarnos desde una instancia al nuestra bbdd como por ejemplo con mysql -h endpoint_de_la_bbdd -u xxx -p y desde ahi podemos crear tablas etc. Tambien podemos crear snapshoots y de ahi crear otra modificada etc. AMAZON AURORA es una base de datos relacional compatible con MySQL y PostgreSQL creada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos empresariales tradicionales con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Amazon Aurora es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos de MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos de PostgreSQL est\u00e1ndar. Ofrece la seguridad, disponibilidad y fiabilidad de las bases de datos de nivel comercial por una d\u00e9cima parte del costo. Amazon Aurora est\u00e1 completamente administrada por Amazon Relational Database Service (RDS), que automatiza las tareas administrativas demandantes como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la aplicaci\u00f3n de parches y las copias de seguridad. Amazon Aurora ofrece un sistema de almacenamiento distribuido, tolerante a errores y de recuperaci\u00f3n autom\u00e1tica que ajusta su escala verticalmente de forma autom\u00e1tica hasta 128 TB por instancia de base de datos. Amazon Aurora suministra alto rendimiento y disponibilidad con hasta 15 r\u00e9plicas de lectura de baja latencia, recuperaci\u00f3n a un momento dado, generaci\u00f3n de copias de seguridad continua en Amazon S3 y replicaci\u00f3n en tres zonas de disponibilidad.","title":"AWS BBDD"},{"location":"aws/#diseno-de-arquitecturas","text":"Arquitectura dise\u00f1o web: LoadBalancer: 3 tipos: basico, aplication y autoscaling. El basico se puede ir a loadbalancer y crear, meter las istancias que pertenecen y el puerto y security groups. El aplication, se crea igual pero se suele crear un TargetGroup que diga que comprueba el index.html por el puerto 80. Un vez creado se va a editar y a\u00f1adir instancias y se le indica que target group se le indica para que compruebe su health check de que esa parte funciona. El AutoScaling se puede crear un conjunto de instancias con la misma configuracion e ir indicando que aumente de instancia o disminuya segun criterios como el uso de cpu etc.","title":"DISE\u00d1O DE ARQUITECTURAS"},{"location":"aws/#vpc","text":"Es crear una red privada y a partir de ahi montar todo tu insfraestructura. Si por ejemplo ponemos la red 10.0.0.0/16, no se pueden usar: .1/16 es router .2/16 es para dnd server .3/16 es para uso futuro de aws .255/16 es broadcast .0/16 es la red Vamos a VPC creamos una red con subnet y sus rangos. Despues creamos instancias y le asignamos esta redes. Tambien podemos crear Elastic IP y asignar IP fijas publicas a estas instancias para esta red. Se puede crear en modo de privada y publica subredes, segun la estructura que queramos. Se puede crear Internet gateway y hacer atach con el VPC que sirve para conectar esta red a Internet. El egress-only sirve para IPv6. Tambien se pueden crear ACL en el PVC. Listas de control de acceso (ACL) de red: las ACL de red act\u00faan como firewall para las subredes asociadas y controlan el tr\u00e1fico entrante y saliente en el \u00e1mbito de la subred. Grupos de seguridad: los grupos de seguridad act\u00faan como firewall para las instancias Amazon EC2 asociadas, al controlar el tr\u00e1fico entrante y saliente en el nivel de la instancia. Cuando lanza una instancia, puede asociarla a uno o varios grupos de seguridad que haya creado. Cada instancia de su VPC podr\u00eda pertenecer a un conjunto distinto de grupos de seguridad. Si no especifica ning\u00fan grupo de seguridad al lanzar una instancia, esta se asocia autom\u00e1ticamente al grupo de seguridad predeterminado de la VPC. Connection peering para crear conexion entre diferentes VPC. Para hacer conexiones VPN se necesita tambien un Virtual Private Gateway para luego poder crear estos tuneles de la red de casa a la nube de amazon.","title":"VPC"},{"location":"aws/#services-amazon","text":"AWS Simple Notification Service (SNS): servicios de notificaciones de suscripciones, email etc. AWS Simple Queue Service (SQS): servicio de cola de mensajes. AWS CloudWatch: herramienta para gestionar las metricas de nuestras maquinas de aws. AWS CloudTrail: sirve para guardar datos de eventos de nuestras maquinas en un bucket s3. AWS Route53: herramienta que sirve para crear, registrar y gestionar dominios. AWS CloudFormation: crear todo mediante codigo json. AWS Developer tools: herramientas parecidad a git como CodeCommit y luego hay CodeStar que es muy parecido a crear un pod de openshift, y herramientas para test, dev y deploy.","title":"SERVICES AMAZON"},{"location":"aws/#lambda","text":"AWS Lambda es un servicio inform\u00e1tico sin servidor y basado en eventos que le permite ejecutar c\u00f3digo para pr\u00e1cticamente cualquier tipo de aplicaci\u00f3n o servicio backend sin necesidad de aprovisionar o administrar servidores. Puede activar Lambda desde m\u00e1s de 200 servicios de AWS y aplicaciones de software como servicio (SaaS), y solo paga por lo que utiliza. Ejecute el c\u00f3digo sin aprovisionar ni administrar la infraestructura. Simplemente escriba y cargue el c\u00f3digo como un archivo .zip o una imagen de contenedor. Responda autom\u00e1ticamente a las solicitudes de ejecuci\u00f3n de c\u00f3digo a cualquier escala, desde una docena de eventos al d\u00eda hasta cientos de miles por segundo. Ahorre costos al pagar solamente por el tiempo de inform\u00e1tica que utiliza, por milisegundo, en lugar de aprovisionar la infraestructura por adelantado para la capacidad m\u00e1xima. Optimice el tiempo de ejecuci\u00f3n del c\u00f3digo y el rendimiento con el tama\u00f1o adecuado de la memoria de las funciones. Responda a la alta demanda en milisegundos de dos d\u00edgitos con simultaneidad aprovisionada.","title":"LAMBDA"},{"location":"aws/#iac-cloudformation","text":"IAC (Infrastructure as Code) AWS CloudFormation le ofrece una forma sencilla de modelar un conjunto de recursos relacionados de AWS y de terceros, aprovisionarlos de manera r\u00e1pida y consistente y administrarlos a lo largo de sus ciclos de vida tratando la infraestructura como un c\u00f3digo. La plantilla de CloudFormation describe los recursos que desea y sus dependencias para que los pueda lanzar y configurar juntos como una pila. Puede usar la plantilla para crear, actualizar y eliminar toda una pila como una \u00fanica unidad, tantas veces como lo necesite, en lugar de administrar los recursos de manera individual. Puede administrar y aprovisionar pilas en varias cuentas y regiones de AWS. web (https://aws.amazon.com/es/cloudformation/) Documentaci\u00f3n (https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) Sintaxis simple: --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro Sintaxis compleja: --- Parameters: SecurityGroupDescription: Description: Security-Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup # an elastic Ip for our instance MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 # our second EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Red SecurityGroupDescription SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 10.10.0.1/32","title":"IAC CLOUDFORMATION"},{"location":"aws/#create-stack","text":"Vamos a consola , cloudformation, crear stack, crear template, subir fichero y ponemos nuestra instancia simple de c\u00f3digo en el fichero y veremos como crea una instancia del tipo que se le indica: --- Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro Para actualizar la que tenemos vamos a crear stack, replace current template, subimos fichero y veremos en los eventos como se actualiza: --- Parameters: SecurityGroupDescription: Description: Security-Group Description Type: String Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-123456789 InstanceType: t2.micro SecurityGroups: - !Ref SSHSecurityGroup - !Ref ServerSecurityGroup # an elastic Ip for our instance MyEIP: Type: AWS::EC2::EIP Properties: InstanceId: !Ref MyInstance # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 # our second EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: !Red SecurityGroupDescription SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 10.10.0.1/32 Despues se puede borrar con Delete en los eventos.","title":"Create STACK"},{"location":"aws/#parametros","text":"Podemos indicar una serie de parametros a la hora de crear nuestra IAC: Ejemplo 1: Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro. Ejemplo2: Parameters: DBPort: Default: 3306 Description: TCP/IP port for the database Type: Number MinValue: 1150 MaxValue: 65535 DBPwd: NoEcho: true Description: The database admin account password Type: String MinLength: 1 MaxLength: 41 AllowedPattern: ^[a-zA-Z0-9]*$","title":"Par\u00e1metros"},{"location":"aws/#recursos","text":"En esta secci\u00f3n declaramos qu\u00e9 recursos queremos crear en nuestro stack Ejemplo: Resources: MyInstance: Type: \"AWS::EC2::Instance\" Properties: UserData: \"Fn::Base64\": !Sub | Queue=${MyQueue} AvailabilityZone: \"us-east-1a\" ImageId: \"ami-0ff8a91507f77f867\" MyQueue: Type: \"AWS::SQS::Queue\" Properties: {} Todos los tipos de recursos Ejemplo: Type: AWS::EC2::Instance Properties: AdditionalInfo: String Affinity: String AvailabilityZone: String BlockDeviceMappings: - BlockDeviceMapping CpuOptions: CpuOptions CreditSpecification: CreditSpecification DisableApiTermination: Boolean EbsOptimized: Boolean ElasticGpuSpecifications: - ElasticGpuSpecification ElasticInferenceAccelerators: - ElasticInferenceAccelerator EnclaveOptions: EnclaveOptions HibernationOptions: HibernationOptions HostId: String HostResourceGroupArn: String IamInstanceProfile: String ImageId: String InstanceInitiatedShutdownBehavior: String InstanceType: String Ipv6AddressCount: Integer Ipv6Addresses: - InstanceIpv6Address KernelId: String KeyName: String LaunchTemplate: LaunchTemplateSpecification LicenseSpecifications: - LicenseSpecification Monitoring: Boolean NetworkInterfaces: - NetworkInterface PlacementGroupName: String PrivateIpAddress: String RamdiskId: String SecurityGroupIds: - String SecurityGroups: - String SourceDestCheck: Boolean SsmAssociations: - SsmAssociation SubnetId: String Tags: - Tag Tenancy: String UserData: String Volumes: - Volume","title":"Recursos"},{"location":"aws/#mapping","text":"Mapea asignando una clave a un valor Ejemplo: AWSTemplateFormatVersion: \"2010-09-09\" Mappings: RegionMap: us-east-1: HVM64: ami-0ff8a91507f77f867 HVMG2: ami-0a584ac55a7631c0c us-west-1: HVM64: ami-0bdb828fd58c52235 HVMG2: ami-066ee5fd4a9ef77f1 eu-west-1: HVM64: ami-047bb4163c506cd98 HVMG2: ami-0a7c483d527806435 ap-northeast-1: HVM64: ami-06cd52961ce9f0d85 HVMG2: ami-053cdd503598e4a9d ap-southeast-1: HVM64: ami-08569b978cc4dfa10 HVMG2: ami-0be9df32ae9f92309 Resources: myEC2Instance: Type: \"AWS::EC2::Instance\" Properties: ImageId: !FindInMap [RegionMap, !Ref \"AWS::Region\", HVM64] InstanceType: m1.small","title":"Mapping"},{"location":"aws/#outputs","text":"Los outputs nos salidas de mensajes. Intentar no poner informaci\u00f3n sensible. Ejemplo: Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance Outputs: BackupLoadBalancerDNSName: Description: The DNSName of the backup load balancer Value: !GetAtt BackupLoadBalancer.DNSName Condition: CreateProdResources InstanceID: Description: The Instance ID Value: !Ref EC2Instance","title":"Outputs"},{"location":"aws/#conditions","text":"Los conditions nos indican que hace algo si se cumple tal condici\u00f3n. Ejemplo: AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: CreateProdResources: !Equals - !Ref EnvType - prod Resources: EC2Instance: Type: 'AWS::EC2::Instance' Properties: ImageId: ami-0ff8a91507f77f867 MountPoint: Type: 'AWS::EC2::VolumeAttachment' Condition: CreateProdResources Properties: InstanceId: !Ref EC2Instance VolumeId: !Ref NewVolume Device: /dev/sdh NewVolume: Type: 'AWS::EC2::Volume' Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt - EC2Instance - AvailabilityZone","title":"Conditions"},{"location":"aws/#functions","text":"Hay diferentes funciones internas en la creaci\u00f3n de un template IAC","title":"Functions"},{"location":"aws/#user-data","text":"Podemos crear plantillas escribiendo ordenes de linux con user data Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-009d6802948d06e52 InstanceType: t2.micro KeyName: !Ref SSHKey SecurityGroups: - !Ref SSHSecurityGroup UserData: Fn::Base64: | #!/bin/bash -xe yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"Hello World from user data\" > /var/www/html/index.html # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 Creamos una stack con el template, ponemos el parametro creado y vemos que sea crea tanto por la ip:80 como por consola.","title":"User data"},{"location":"aws/#cfn-init","text":"Vemos la docu de cfn init Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-009d6802948d06e52 InstanceType: t2.micro KeyName: !Ref SSHKey SecurityGroups: - !Ref SSHSecurityGroup UserData: Fn::Base64: !Sub | #!/bin/bash -xe # get the latest cloudformation package yum update -y aws-cfn-bootstrap # start cfn-init /opt/aws/bin/cfn-init -s ${AWS::StackId} -r MyInstance --region ${AWS::Region} || error_exit \"Failed to run cfn-init\" Metadata: Comment: Install a simple Apache HTTP page AWS::CloudFormation::Init: config: package: yum: httpd:[] files: \"/var/www/html/index.html\" content: | <h1>Hello word, using cfn-init</h1> mode: '000644' commands: hello: command: \"echo 'hello world'\" services: sysvinit: httpd: enabled: 'true' ensureRunning: 'true' # our EC2 security group SSHSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Enable port 22 SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80","title":"Cfn-init"},{"location":"aws/#cfn-signal","text":"Docu de cfn-signal Ejemplo: AWSTemplateFormatVersion: 2010-09-09 Description: Simple EC2 instance Resources: MyInstance: Type: AWS::EC2::Instance Metadata: 'AWS::CloudFormation::Init': config: files: /tmp/test.txt: content: Hello world! mode: '000755' owner: root group: root Properties: ImageId: ami-a4c7edb2 InstanceType: t2.micro UserData: !Base64 'Fn::Join': - '' - - | #!/bin/bash -x - | # Install the files and packages from the metadata - '/opt/aws/bin/cfn-init -v ' - ' --stack ' - !Ref 'AWS::StackName' - ' --resource MyInstance ' - ' --region ' - !Ref 'AWS::Region' - |+ - | # Signal the status from cfn-init - '/opt/aws/bin/cfn-signal -e $? ' - ' --stack ' - !Ref 'AWS::StackName' - ' --resource MyInstance ' - ' --region ' - !Ref 'AWS::Region' - |+ CreationPolicy: ResourceSignal: Timeout: PT5M","title":"Cfn-signal"},{"location":"aws/#rollback","text":"Cuando creamos una stack podemos indicar si queremos un rollback para poder volver a un estado anterior.","title":"Rollback"},{"location":"aws/#cfn-nested-stacks","text":"Docu de stack anidadas entre s\u00ed cfn nested Ejemplo: --- Parameters: SSHKey: Description: Name of an existing EC2 keypair to enable ssh access to the instance Type: AWS::EC2::KeyPair::KeyName Resources: MyInstance: Type: AWS::EC2::Instance Properties: TemplateURL: https://s3.amazonaws.com/cloudformation-templates-us-east-1/LAMP_Single-Instance.template Parameters: KeyName: !Ref SSHKey DBName: \"mydb\" DBUser: \"user\" DBPassword: \"pass\" DBRootPassword: \"passroot\" InstanceType: t2.micro SSHLocation: \"0.0.0.0/0\" Outputs: StackRef: value: !Ref myStack OutputFromNestedStack: value: !GetAtt mystack.Outputs.WebsiteURL","title":"Cfn nested stacks"},{"location":"aws/#change-sets","text":"DOCU Sirve para actualizar o subir otra cosa de una stack ya creada y podremos ver el proceso de si se har\u00eda bien o no. Si nos interesa, le damos a EXECUTE y empieza a cambiarse, sino DELETE.","title":"Change sets"},{"location":"aws/#depends-on","text":"DOCU De crear una instancia que depende de otra creada. Ejemplo: AWSTemplateFormatVersion: '2010-09-09' Mappings: RegionMap: us-east-1: AMI: ami-0ff8a91507f77f867 us-west-1: AMI: ami-0bdb828fd58c52235 eu-west-1: AMI: ami-047bb4163c506cd98 ap-northeast-1: AMI: ami-06cd52961ce9f0d85 ap-southeast-1: AMI: ami-08569b978cc4dfa10 Resources: Ec2Instance: Type: AWS::EC2::Instance Properties: ImageId: Fn::FindInMap: - RegionMap - Ref: AWS::Region - AMI DependsOn: myDB myDB: Type: AWS::RDS::DBInstance Properties: AllocatedStorage: '5' DBInstanceClass: db.t2.small Engine: MySQL EngineVersion: '5.5' MasterUsername: MyName MasterUserPassword: MyPassword","title":"Depends on"},{"location":"aws/#detect-drift-stack","text":"Sirve para hacer marcas y ver donde se cambi\u00f3 si actualizas la template o modificas algo de la template DOCU Vamos a stack actions - detect drift una vez ya hemos metido nuestra template creada. Podemos ver en view drift y si modificamos cosas como el security groups veremos que la marca cambia.","title":"Detect drift stack"},{"location":"azure/","text":"AZURE QUE ES Microsoft Azure es un servicio de Inform\u00e1tica en la Nube creado por Microsoft para construir, testear, desplegar y gestionar aplicaciones y servicios a trav\u00e9s de centros de datos gestionados por Microsoft. Microsoft Azure, como otros proveedores de nube, nos permite alquilar recursos como espacio de almacenamiento o ciclos de CPU en equipos f\u00edsicos que no debo administrar. Solo se paga por lo que usa (o al menos se mide). Los servicios inform\u00e1ticos ofrecidos suelen variar en funci\u00f3n de cada proveedor. Normalmente estos servicios incluyen: Potencia de proceso: por ejemplo, aplicaciones web o servidores Linux. Almacenamiento: por ejemplo, archivos y bases de datos. Redes: por ejemplo, conexiones seguras entre el proveedor de nube y la empresa. Potencia de proceso Cuando hacemos virtualmente cualquier acci\u00f3n en internet, como pagar una factura online, leer un peri\u00f3dico y enviar un correo electr\u00f3nico, estamos interactuando con servidores de nube que procesan cada solicitud y devuelven una respuesta. Todo esto requiere de c\u00f3mputo. Maquina virtual Contenedores Serverless o informatica sin servidor Almacenamiento La mayor\u00eda de las aplicaciones leen y escriben datos. Y en este sentido, el tipo de datos y c\u00f3mo se almacenan puede ser diferente seg\u00fan el tipo de aplicaci\u00f3n, la necesidad y velocidad requerida. Los proveedores de nube suelen ofrecer soluciones de almacenamiento para m\u00e1quinas virtuales, aplicaciones web, bases de datos, archivos de datos y anal\u00edtica. Por ejemplo, si quiere almacenar texto o un clip de pel\u00edcula, podr\u00eda usar un archivo en disco. Si tuviera un conjunto de relaciones (por ejemplo, una libreta de direcciones), podr\u00eda decidirse por un enfoque m\u00e1s estructurado, como usar una base de datos. La ventaja de utilizar almacenamiento basado en nube, es que no debemos preocuparnos por el escalado. Si se necesita m\u00e1s espacio, se puede agregar pagando un poco m\u00e1s de precio, e inclusive si las necesidades de almacenamiento bajan, tambi\u00e9n bajar\u00e1 el precio asociado. Redes En todos estos casos, las redes cobran una importancia vital. Los proveedores de nube suelen tener servicios de redes que nos permiten: + Crear y configurar Redes Virtuales. + Crear y conectar de extremo a extremo redes en la nube con una infraestructura local (conocidas como site-to-site, y point-to-site). + Parametrizar reglas de acceso a recursos. + Monitorear tr\u00e1fico de redes. + Aplicar reglas, restricciones y protecciones a las comunicaciones. CREAR CUENTA Web azure A trav\u00e9s de azure.com: es la forma m\u00e1s r\u00e1pida y f\u00e1cil que tienen las organizaciones de todos los tama\u00f1os para empezar a usar Azure. Puede administrar las implementaciones y el uso de Azure, como as\u00ed tambi\u00e9n obtener una factura mensual de Microsoft por los servicios usados. Con la ayuda de un Partner de Microsoft. Es un modelo para obtener facturaci\u00f3n local en tu pa\u00eds. De esta manera, Azure se brindar\u00e1 como servicio administrado a trav\u00e9s de un partner, qui\u00e9n te proporcionar\u00e1 el acceso y la facturaci\u00f3n, junto con un soporte t\u00e9cnico b\u00e1sico. A trav\u00e9s de un representante directo de Microsoft, opci\u00f3n pensada para organizaciones de gran tama\u00f1o o clientes que ya trabajan con la marca. A diferencia de azure.com (que requiere tarjeta de cr\u00e9dito), esto habilitar\u00e1 un tipo de contrato especial con varias ventajas al momento de necesitar varias suscripciones. Los servicios de Azure est\u00e1n disponibles a trav\u00e9s de Centros de Datos gestionados por Microsoft. Los mismos est\u00e1n conformados por edificios. xisten +60 regiones anunciadas en todo el mundo, y muchas que est\u00e1n anunciadas como adicionales futuras. Esto representa una presencia f\u00edsica en 140 pa\u00edses. En el mapa podr\u00e1s ver la ubicaci\u00f3n de los centros de datos, a excepci\u00f3n de 3 correspondientes a gobierno por lo cual su ubicaci\u00f3n es secreta. SLA significa en ingl\u00e9s \u201cservice level agreement\u201d, y en espa\u00f1ol \u201cacuerdo de nivel de servicio\u201d. Es un acuerdo escrito entre un proveedor de servicio y su cliente con objeto de fijar el nivel acordado para la calidad de dicho servicio. Este nivel puede ser un porcentaje que representa la disponibilidad m\u00ednima PRINCIPIO 5-3-2 La inform\u00e1tica en la nube es un metodo de gestion de recursos de IT donde los usuarios acceden a los recursos virtuales de computo, red y almacenamiento que estan disponibles online. Estos recursos se pueden aprovionar de manera instantanea y elastica. Se compone de: 5 Caracteristicas 3 metodos de entrega 2 modelos de implementacion Caracteristicas Autoservicio y bajo demanda: un consumidor puede provisionarse de caracteristicas como tiempo de uso, almacenamiento, memoria... Acceso amplio y ubicuo: los recursos pueden ser accecidos desde cualquier lugar y cualquier dispositivo. Ubicacion transparente y agrupacion de recursos: suelen estar en diferentes localizaciones sobre distintos recursos fisicos o virtuales que son dinamicamente asignados. Elasticidad rapida (estirarse y contraerse): pueden aumentar en epocas de mucha carga asi como reducirlo cuando no se use. Servicio medido (e incluso pago por uso): recursos y capacidades segun lo que necesitas. Metodos de entrega IaaS(Infraestructura como Servicio): Cliente tiene capacidad de utilizar almacenamiento, red, recursos sofware, SO, app. No tiene el control sobre la infraestructura pero sino tiene el control del resto apartir del SO. COntrol limitado sobre red como el firewall Ejemplo serian las maquinas virtuales PaaS(Plataforma como Servicio): Podemos desplegar apps propias o de terceros Control sobre las apps y la configuracion de ellas Ejemplo seria servicios hosting Saas(Software como Servicio): Capacidad de usar aplicaciones en una infraestructura de nube que cumple con las 5 caracteristicas No tenemos control sobre ningun componente, solo lo usamos. Ejemplo seria Office 365, exchange, gmail, yahoo, google apps. Modelos de implementaci\u00f3n Nube Privada: en mi propio centro de datos Nube P\u00fablica: en azure o otros proveedores de servicios Puede haber la mezcla con Nube Hibrida. MAQUINAS VIRTUALES Tipo de recurso escalable por Azure Se tiene control total sobre la configuraci\u00f3n y se puede instalar de todo No es necesario comprar hardware fisico para escalar o ampliar Azure tiene servicios para supervisar, proteger y administrar las actualizaciones y revisiones del sistema operativo Soy responsable de: Mantener el SO y sus actualizaciones Trabar sobre la performance Monitorear el espacio de disco usado Componentes: Disco virtual: el disco es el que tendr\u00e1, por ejemplo, el sistema operativo instalado. Gracias al disco virtual puedo iniciar el equipo y guardar informaci\u00f3n en forma persistente Placa de red virtual: al igual que en un equipo f\u00edsico, es la que me facilitar\u00e1 la conexi\u00f3n con una o m\u00e1s redes. Direcciones IP: gracias a la cual podr\u00e9 conectarme al equipo virtual. Estas direcciones IP pueden ser privadas y p\u00fablicas. Grupos de seguridad de red: que nos ayudar\u00e1n a definir desde qu\u00e9 origenes me puedo conectar, y hacia qu\u00e9 destinos puedo acceder, teniendo en cuenta protocolos, puertos, etc. Los Network Security Groups son una manera \u00e1gil de gestionar los permisos de red, para una o m\u00e1s m\u00e1quinas. Configurar: puedo el nombre de la MV, el SO y el tama\u00f1o. Tiene al menos dos discos, uno para el SO y otro temporal para la memoria virtual. Spot Virtual: herramienta que lo que no se use se vaya ahi para ahorrar. CREAR MV Assignment Tasks A Ingresar al Portal de Azure. Crear un \"Grupo de Recursos\" [Resource Group] con el nombre \"azf-vms-1\" Completed on 4 noviembre, 2020 7:01 pm B Dentro del Resource Group, seleccionar la opci\u00f3n \"Crear recurso\" [Create resources]. Seleccionar el grupo \"C\u00f3mputo\" [Compute] y de la lista \"M\u00e1quina Virtual\" [Virtual Machine]. Completed on 4 noviembre, 2020 7:01 pm C En el asistente de creaci\u00f3n, validar que la suscripci\u00f3n seleccionada sea la correcta (probablemente sea \"FREE TRIAL\") y el Resource Group seleccionado es el correcto: \"azf-vms-1\". Completed on 4 noviembre, 2020 7:01 pm D Ingresar un nombre para la m\u00e1quina virtual, por ejemplo \"azf-vm-windows-2019\". Completed on 4 noviembre, 2020 7:01 pm E Seleccionar una regi\u00f3n de Azure. Por ejemplo \"Este de Estados Unidos\" [East US], una imagen \"WIndows Server 2019 Datacenter\" y un tama\u00f1o de m\u00e1quina virtual (explorar todas las im\u00e1genes y elegir un tama\u00f1o como \"B1ms\" (es un equipo barato para este ejercicio). Completed on 4 noviembre, 2020 7:01 pm F Ingresar un usuario [Username] y una contrase\u00f1a dos veces [Password]. \u00a1No olvidarlas! Completed on 4 noviembre, 2020 7:01 pm G Seleccionar el puerto de entrada [Inbound port] \"RDP (3389)\" para poder ingresar luego al equipo. Completed on 4 noviembre, 2020 7:02 pm H Ir al paso siguiente: \"Discos\" [Disks]. Ingresar un Disco de Datos adicional [Create new disk] del menor tama\u00f1o posible. Completed on 4 noviembre, 2020 7:06 pm I Ir al siguiente paso \"Redes\" [Networking]. Crear una nueva red con el nombre \"azf-vnet-1\" con el espacio de direcciones \"10.0.0.0/16\" y crear una subnet con el nombre \"Sub1\" y el espacio de direcciones \"10.0.0.0/24\". Validar que una vez creada la red, est\u00e9 seleccionada en \"Virtual Network\" y \"Subnet\" en el asistente del equipo virtual. Completed on 4 noviembre, 2020 7:11 pm J Crear una IP p\u00fablica con el nombre \"azf-ip-1\", el \"SKU Basic\" y asignaci\u00f3n \"Static\". Completed on 4 noviembre, 2020 7:11 pm K Seleccionar el grupo de seguridad de red [NIC network security group] en \"Basic\". Completed on 4 noviembre, 2020 7:11 pm L Validar que los puertos habilitados son solo \"RDP (3389)\". Completed on 4 noviembre, 2020 7:11 pm M Ir al siguiente paso \"Administraci\u00f3n\" [Management]. Completed on 4 noviembre, 2020 7:13 pm N Seleccionar en diagn\u00f3stico de booteo [Boot diagnostics] en \"On\". Esto requerir\u00e1 crear una cuenta de almacenamiento. Completed on 4 noviembre, 2020 7:16 pm O En el campo \"Cuenta de Almacenamiento de Diagn\u00f3stico\" [Diagnostics storage account] crear una nueva cuenta de almacenamiento con el nombre \"azfstorageXXXX\" donde XXXX es un n\u00famero aleatorio generado por ti (dado que los nombres de cuentas de almacenamiento deben ser \u00fanicos en todo Azure). El tipo de cuenta debe ser \"Storage (general purpose v1) y \"Locally-redundant storage (LRS). Completed on 4 noviembre, 2020 7:17 pm P Habilitar el apagado autom\u00e1tico [Auto-shutdown] y elegir un horario de apagado para tu zona geogr\u00e1fica. Completed on 4 noviembre, 2020 7:17 pm Q No modificar el resto de las opciones e ir al siguiente paso \"Avanzado\" [Advanced]. Completed on 4 noviembre, 2020 7:18 pm R No modificar ninguna opci\u00f3n e ir al siguiente paso \"Etiquetas\" [Tags]. Completed on 4 noviembre, 2020 7:18 pm S Ir al \u00faltimo paso \"Revisi\u00f3n y Creaci\u00f3n\" [Review + create]. Cuando pase todas las validaciones, revisar el resumen de opciones seleccionadas que coincidan con lo solicitado y crear la m\u00e1quina. Completed on 4 noviembre, 2020 7:19 pm T Cuando finalice la creaci\u00f3n, ir al equipo virtual y seleccionar \"Conectar\" [Connect] y elegir \"RDP\". Se descargar\u00e1 un archivo, y desde el cliente de Escritorio Remoto de tu computadora conectarse. U Ingresar el nombre de usuario y contrase\u00f1a que ingresamos en pasos anteriores, y comprobar que nos podemos conectar al equipo. V Ir al Grupo de Recursos [Resource Group] que hemos creado y comprobar que todos los recursos (m\u00e1quina virtual, discos, placas de red) est\u00e1n creados. HERRAMIENTAS AZURE Azure Portal para interactuar con Azure a trav\u00e9s de una interfaz gr\u00e1fica de usuario (GUI). Azure PowerShell y la interfaz de la l\u00ednea de comandos de Azure (CLI) para las interacciones con Azure de l\u00ednea de comandos y basadas en automatizaci\u00f3n. Azure Cloud Shell para una interfaz de l\u00ednea de comandos basada en web. Azure Mobile App para supervisar y administrar los recursos desde el dispositivo m\u00f3vil. AZURE POWERSHELL Instamos la herramienta AzurePowershell , apartir de la versi\u00f3n 7 es multiplataforma. linux # Register the Microsoft signature key sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # Register the Microsoft RedHat repository curl https://packages.microsoft.com/config/rhel/7/prod.repo | sudo tee /etc/yum.repos.d/microsoft.repo # Update the list of products sudo dnf check-update # Install a system component sudo dnf install compat-openssl10 # Install PowerShell sudo dnf install -y powershell # Start PowerShell pwsh Instalamos el modulo de AZ powershell: pwsh Install-Module -Name Az Nos conectamos con nuestra cuenta yendo al link que nos indica: PS /home/isx46410800/Documents> Connect-AzAccount Orden de listar los Resources Groups Get-AzResourceGroup : PS /home/isx46410800/Documents> Get-AzResourceGroup ResourceGroupName : NetworkWatcherRG Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/NetworkWatcherRG # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 Crear un Resource Group por comando New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" : PS /home/isx46410800/Documents> New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" ResourceGroupName : azf-rgexmaple-rg Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-rgexmaple-rg # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 azf-rgexmaple-rg eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 AZURE CLI Documentacion CLI Instalaci\u00f3n linux: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # sudo sh -c 'echo -e \"[azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/azure-cli.repo' # sudo yum install azure-cli Iniciamos sesion haciendo login: az login Listamos los resource groups con az group list --output table : Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded Creamos un resource group con az group create --location \"eastus\" --name \"azf-cli-rg\" : { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cli-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cli-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded AZURE CLOUD SHELL Es un shell a trav\u00e9s del navegador desde cualquier sistema operativo. Necesitamos crear un storage y file share para utilizarlo. Ahora dentro podemos usar las mismas ordenes que cli por ejemplo y ver/crear resource groups como ejemplo: miguel@Azure:~$ az group create --location \"eastus\" --name \"azf-cloudshell-example-rg\" { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cloudshell-example-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cloudshell-example-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } miguel@Azure:~$ az group list --output table Name Location Status ------------------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded azf-cloudshell-rg eastus Succeeded azf-cloudshell-example-rg eastus Succeeded Podemos verlo en pantalla completa con www.shell.azure.com EJEMPLO DE CREAR MV POR SCRIPT Crear una MV con CLI a trav\u00e9s de cloud shell: # create a resource group az group create --name azf-crear-mv-cli --location eastus # Create a virtual network net and subnet. az network vnet create --resource-group azf-cloudshell-example-rg --name azf-vnet-1 --address-prefix 10.0.0.0/16 --subnet-name sub1 --subnet-prefix 10.0.0.0/24 # Create a public IP address. az network public-ip create --name myPublicCliIP --resource-group azf-cloudshell-example-rg --sku Basic --allocation-method Static # create vm customized az vm create --subscription \"Evaluaci\u00f3n gratuita\" -g azf-cloudshell-example-rg --name vm-cli-example --image win2019datacenter --location eastus --size Standard_B1ms --admin-username vmadmin --admin-password mi--GU--el14 --vnet-name azf-vnet-1 --subnet sub1 # add new optional disk az vm disk attach -g azf-cloudshell-example-rg --vm-name vm-cli-example --name myDataDisk --new --size-gb 4 # input port to connect VM az vm open-port --port 3389 --resource-group azf-cloudshell-example-rg --name vm-cli-example # habilitar el auto shutdown az vm auto-shutdown -g azf-cloudshell-example-rg -n vm-cli-example --time 0200 # borrar recurso az group delete --name azf-cloudshell-example-rg AZURE APP SERVICE Azure App Service es un servicio de alojamiento web totalmente administrado que permite crear aplicaciones web, back-ends m\u00f3viles y API RESTful. Desde sitios web peque\u00f1os hasta aplicaciones web con una escala global, existen opciones de precios y rendimiento que se adaptan a todas las necesidades. Azure App Service permite desarrollar software en el lenguaje preferido, ya sea. NET, .NET Core, Java, Ruby, Node.js, PHP o Python. Las aplicaciones se ejecutan y escalan f\u00e1cilmente en los entornos basados tanto en Windows como en Linux. Azure App Service no solo agrega a la aplicaci\u00f3n la funcionalidad de Microsoft Azure, como la seguridad, el equilibrio de carga, el escalado autom\u00e1tico y la administraci\u00f3n automatizada. Tambi\u00e9n puede sacar partido de las funcionalidades de DevOps, por ejemplo, la implementaci\u00f3n continua desde Azure DevOps, GitHub, Docker Hub y otros or\u00edgenes, la administraci\u00f3n de paquetes, entornos de ensayo, dominio personalizado y certificados TLS/SSL. Con Azure App Service se paga por los recursos de proceso de Azure que se utilizan. Los recursos de proceso que usa se determinan mediante el plan de App Service en el que ejecuta las aplicaciones, y determina el costo del servicio. Crear una Web App Home - Crear recurso - Web App Configuraci\u00f3n inicial: A Crear una nueva Web App (elegir del men\u00fa destacados seg\u00fan indica el video). Completed on 6 noviembre, 2020 11:55 pm B Elegir la suscripci\u00f3n, crear un resource group desde el mismo asistente, y completar un nombre: por ejemplo \"azf-webapp-codexxx\" donde xxx son 3 n\u00fameros aleatorios, tanto para la web app como para el resource group. No olvides este RG que lo utilizaremos m\u00e1s adelante. Completed on 6 noviembre, 2020 11:55 pm C Seleccionar la forma de publicaci\u00f3n [Publish] \"Code\". Elegir el Runtime stack .NET Core 3.1 (LTS) y la plataforma Windows. Completed on 6 noviembre, 2020 11:55 pm D Elegir la regi\u00f3n que Azure propone. Se puede seguir el del ejemplo: Central US. Completed on 6 noviembre, 2020 11:55 pm E Crear un Windows Plan con el nombre: \"azf-webapp-code-plan\" y el tama\u00f1o Standard S1 (no te preocupes por el costo, en breve lo eliminaremos). F En las opciones de monitoreo, NO habilitaremos Application Insights. Completed on 6 noviembre, 2020 11:56 pm G No completaremos nada en los tags. H Revisaremos y crearemos la Web App. I Comprobar que desde la URL en la p\u00e1gina de Overview que la web app funciona correctamente. Veremos una URL donde tenemos nuestra web app Services y Plans El plan es el hardware del que voy a disponer. El service es la app o conjunto de apps dentro del plan. En App Service, cada aplicaci\u00f3n se ejecuta en un Plan de App Service. En forma b\u00e1sica, un App Service Plan define un conjunto de recursos de proceso para que una aplicaci\u00f3n web se ejecute. Estos recursos de proceso son an\u00e1logos a la granja de servidores de un hospedaje web convencional. Cuando se crea un plan de App Service en una regi\u00f3n determinada (por ejemplo, Oeste de Europa), se crea un conjunto de recursos de proceso para ese plan en dicha regi\u00f3n. Todas las aplicaciones que coloque en este plan de App Service se ejecutan en estos recursos de proceso seg\u00fan lo definido por el plan de App Service. Cada plan de App Service define: Regi\u00f3n (oeste de EE. UU., este de EE. UU., etc.). N\u00famero de instancias posibles de VM (si, por detr\u00e1s hay VMs). Tama\u00f1o de las instancias de VM (peque\u00f1o, mediano, grande). Plan de tarifa (Gratis, Compartido, B\u00e1sico, Est\u00e1ndar, Premium, PremiumV2 y Aislado). Si bien inicialmente puede resultar algo confuso, cuando en Azure veamos el t\u00e9rmino \u201cApp Service\u201d nos referimos a las Web Apps, API Apps & Mobile Apps. Cuando veamos el t\u00e9rmino \u201cApp Service Plans\u201d nos referimos expl\u00edcitamente a los planes. La relaci\u00f3n entre un App Service Plan un un App Service es una relaci\u00f3n 1:muchos :-). Una App Service Plan puede contener muchos App Services, mientras que un App Service s\u00f3lo puede estar dentro de un App Service Plan. En todos los casos, cuando contratamos un App Service Plan \u201cBasic\u201d, \u201cStandard\u201d, \u201cPremium\u201d e \u201cIsolado\u201d, s\u00f3lo pagamos por el tama\u00f1o del plan que contratemos (scale up) y la cantidad de instancias con las que escalemos (scale out, predeterminadamente configurada en 1): Aunque tengamos muchos App Services (Web Apps, API Apps & Mobile Apps) dentro del mismo plan, s\u00f3lo pagaremos por el tama\u00f1o y cantidad de instancias del App Service Plan, y no por cada una de las Apps que tenga dentro. Por supuesto, como el App Service Plan determina un hardware asociado, tendremos un l\u00edmite. Si ponemos muchos sitios web dentro que consumen muchos recursos y/o tienen demasiado tr\u00e1fico, nuestro App Service Plan comenzar\u00e1 a arrojar errores por no disponibilidad de servicio. Este punto no es menor, y debe ser planeado en el dise\u00f1o de la soluci\u00f3n completa de Azure. Crear una Web App Container la creaci\u00f3n de una Web App for Containers. Esto significa, que nuestra Web App estar\u00e1 en un contenedor de ejemplo en Microsoft Azure. Recordemos que Azure App Service permite generar diversas aplicaciones: Web App. API App. Mobile App. En escencia, son similares, si bien encontraremos en cada una de ellas particularidades propias. La m\u00e1s simple es una Web App, y es justamente en este ejercicio la que generaremos. A diferencia del anterior donde tambi\u00e9n generamos una Web App, esta vez lo haremos utilizando Docker. Configuraci\u00f3n inicial: A Crear un nuevo recurso de tipo \"Web App\" que esta en la lista de populares. B Crear un grupo de recursos seg\u00fan el gusto que tengas. Si elijes un nuevo grupo de recursos (diferente al ejercicio de creaci\u00f3n de Web App Simple) recuerda BORRAR este nuevo grupo de recursos al finalizar el ejercicio. En caso que sea el mismo, puedes dejarlo dado que m\u00e1s adelante borraremos todo. C Elegiremos el tipo de publicaci\u00f3n \"Docker Container\" y el sistema operativo \"Linux\". D Generaremos un nuevo App Service Plan y un tama\u00f1o de tipo \"S1\". E En la configuarci\u00f3n de \"Docker\" elegiremos \"Single Container\". F Con respecto al origen de la instancia elegiremos \"Quickstart\". La opci\u00f3n \"sample\" ser\u00e1 \"NGINX\". G No debemos habilitar nada de monitoreo, dado que no est\u00e1 soportado con contenedores. H Revisar y crear el recurso. I Comprobar por la URL del Web App que todo est\u00e1 funcionando como esperamos. Crear API App Pasos: A Ingresar al Resource Group que hemos generado en la \"Creaci\u00f3n de una Web App Simple\". Deber\u00eda tener el formato \"azf-webapp-codexxx\" con 3 n\u00fameros elegidos por ti. B Creamos un nuevo recurso y buscamos en la barra de b\u00fasqueda \"API App\". C Ingresaremos un nombre para la app, respetando el formato \"azf-apiapp-codexxx\" ingresando los 3 mismos n\u00fameros aleatorios puestos para el resource group y la web app anterior. D El resource group ya deber\u00eda estar seleccionado (es el que ya existe). E En la opci\u00f3n de \"App Service Plan\" debemos elegir el que ya existe porque lo generamos antes. Su nombre deber\u00eda ser \"azf-webapp-code-plan\". F Le damos un clic en \"Create\". G Una vez que se genere, vamos a comprobar que el sitio funciona bien. H Comprobar que en el listado de Apps en el \"App Service Plan\" ahora tenemos m\u00e1s items. En el Deployment center o en el centro de despliegue podemos configurar archivos o repositorios de donde coger nuestro c\u00f3digo fuente para la aplicaci\u00f3n web. https://docs.microsoft.com/es-mx/learn/modules/create-publish-webapp-app-service-vs-code/3-exercise-create-web-application-vs-code?pivots=pythonflask ALMACENAMIENTO AZURE La plataforma de Azure Storage es la soluci\u00f3n de almacenamiento en la nube de Microsoft para los escenarios modernos de almacenamiento de datos. Los servicios principales de almacenamiento ofrecen un almac\u00e9n de objetos escalable de forma masiva para objetos de datos, un almacenamiento en disco para m\u00e1quinas virtuales (VM) de Azure, un servicio de sistema de archivos para la nube, un almac\u00e9n de mensajes para mensajer\u00eda confiable y un almac\u00e9n NoSQL. Los servicios principales y considerados primitivos de Azure Storage son: Blobs de Azure [Azure Blobs]: con opciones de blobs en bloques, anexos y p\u00e1ginas. Archivos de Azure [Azure Files]: recursos compartidos para uso local y en la nube. Colas de Azure [Azure Queues]: almac\u00e9n de mensajer\u00eda simple. Tablas de Azure [Azure Tables]: almac\u00e9n NoSQL sin esquema para datos estructurados. Adem\u00e1s, consideraremos el servicio de Azure Disks, un servicio totalmente administrado para los discos de nuestras m\u00e1quinas virtuales: Discos de Azure [Azure Disks]: conocidos como \u201cdiscos administrados\u201d. Azure disks Los discos administrados de Azure son vol\u00famenes de almacenamiento de nivel de bloque administrados por Azure y utilizados con Azure Virtual Machines. Los discos administrados son como un disco f\u00edsico en un servidor local, pero virtualizados. Con los discos administrados, todo lo que deber\u00e1s hacer es especificar el tama\u00f1o del disco, el tipo de disco y aprovisionar el disco. Una vez que aprovisiona el disco, Azure se encarga del resto. Los tipos de discos disponibles son ultra, unidades de estado s\u00f3lido (SSD) premium, SSD est\u00e1ndar y unidades de disco duro est\u00e1ndar (HDD). Blob storage Azure Blob Storage es la soluci\u00f3n de almacenamiento de objetos de Microsoft para la nube. Blob Storage est\u00e1 optimizado para el almacenamiento de cantidades masivas de datos no estructurados. En la primera lecci\u00f3n de esta secci\u00f3n, ya hemos aprendido sobre los diversos tipos de datos, entre ellos los no estructurados. Blob Storage est\u00e1 dise\u00f1ado para: Servicio de im\u00e1genes o documentos directamente a un explorador. Almacenamiento de archivos para acceso distribuido. Streaming de audio y v\u00eddeo. Escribir en archivos de registro. Almacenamiento de datos para copia de seguridad y restauraci\u00f3n, recuperaci\u00f3n ante desastres y archivado. Almacenamiento de datos para el an\u00e1lisis en local o en un servicio hospedado de Azure. Azure files Archivos de Azure (Azure Files) ofrece recursos compartidos de archivos en la nube totalmente administrados, a los que se puede acceder mediante el protocolo SMB (Bloque de mensajes del servidor) est\u00e1ndar. Los recursos compartidos de Azure Files se pueden montar simult\u00e1neamente en implementaciones de Windows, Linux y macOS en la nube o locales. Adem\u00e1s, los recursos compartidos de archivos de Azure Files se pueden almacenar en la cach\u00e9 de los servidores de Windows Server con Azure File Sync, lo que permite un acceso r\u00e1pido all\u00ed donde se utilizan los datos. Se crea un file share dentro de mi cuenta de storage azure. Se sube archivos y nos podemos conectar al file share a trav\u00e9s de un script que te da en las opciones de azure. Ahora ya tendemos el recursos de red la nueva unidad compartida y se actualiza todo. Tambien podemos hacer un azure file sync que es como un server en la nube que se actualiza en mi ordenador, azure y nube. Nos tendremos que descargar el file sync de azure despues de crear un server y un group sync y funcionar\u00e1 como lo anterior. Colas de azure Azure Queue Storage es un servicio para almacenar grandes cantidades de mensajes, a los que se puede acceder desde cualquier lugar del mundo a trav\u00e9s de llamadas autenticadas mediante HTTP o HTTPS. Un mensaje de la cola puede llegar a tener hasta 64 KB. Una cola puede contener millones de mensajes, hasta el l\u00edmite de capacidad total de una cuenta de almacenamiento. Las colas se utilizan normalmente para crear un trabajo pendiente del trabajo que se va a procesar de forma asincr\u00f3nica. Azure Storage Explorer Azure Storage Explorer es una herramienta gratuita para administrar f\u00e1cilmente sus recursos de almacenamiento en la nube de Azure en cualquier parte, desde Windows, macOS o Linux. Permite cargar, descargar y administrar blobs, archivos, colas y tablas de Azure, as\u00ed como entidades de Azure Cosmos DB y Azure Data Lake Storage. Permite acceder f\u00e1cilmente a los discos de las m\u00e1quinas virtuales y trabajar con Azure Resource Manager o con cuentas de almacenamiento cl\u00e1sicas. Asimismo, administrar y configurar reglas de uso compartido de recursos entre or\u00edgenes. AZURE FUNCTIONS Azure Functions permite ejecutar peque\u00f1os fragmentos de c\u00f3digo (denominados \u201cfunciones\u201d) sin preocuparse por el resto de la infraestructura de la aplicaci\u00f3n. Es ideal para tareas espec\u00edficas, dado que simplifica la necesidad de generar c\u00f3digo reduci\u00e9ndolo s\u00f3lo a la parte \u201cl\u00f3gica\u201d que necesitamos. Imaginemos que necesitamos generar una tarea donde, cada vez que se escribe un nuevo archivo en un Storage Account de Azure, se dispare una funci\u00f3n que guarde en una base de datos registro de dicho archivo. Si pensamos en hacer esto desde cero, probablemente nos requiera: Generar un proyecto con un IDE y un Framework / Lenguaje espec\u00edfico. Construir un servicio y alojarlo en alg\u00fan lugar. Resolver otros aspectos propios del proyecto, donde alojarlo, c\u00f3mo hacer que se dispare cuando se graba un archivo nuevo en el Storage Account, etc. Los desencadenadores son lo que provocan que una funci\u00f3n se ejecute. Un desencadenador define c\u00f3mo se invoca una funci\u00f3n y cada funci\u00f3n debe tener exactamente un desencadenador. Los desencadenadores tienen datos asociados, que a menudo son la carga de la funci\u00f3n. El enlace a una funci\u00f3n es una manera de conectar otro recurso a la funci\u00f3n mediante declaraci\u00f3n. Los enlaces pueden estar conectados como enlaces de entrada, enlaces de salida o ambos. Los datos de los enlaces se proporcionan a la funci\u00f3n como par\u00e1metros. AZURE BASES DE DATOS Microsoft SQL es un motor de base de datos utilizado en forma global y conocida por casi todos. Vamos a conocer las opciones que tenemos de Microsoft SQL en Azure, ya sea como IaaS, PaaS e inclusive Serverless. SQL Server en Azure Virtual Machines nos permite usar versiones completas de SQL Server en la nube sin tener que administrar todo el hardware local. SQL Server en Azure Virtual Machines tambi\u00e9n simplifica los costos de licencia cuando se paga por uso. La galer\u00eda de im\u00e1genes de m\u00e1quina virtual le permite crear una m\u00e1quina virtual con SQL Server con la versi\u00f3n, la edici\u00f3n y el sistema operativo correctos. Esto hace que las m\u00e1quinas virtuales sean una buena opci\u00f3n para muchas cargas de trabajo de SQL Server diferentes. El recurso M\u00e1quinas virtuales SQL es un servicio de administraci\u00f3n independiente de la m\u00e1quina virtual y, de hecho, aparece como un m\u00f3dulo distinto. Cuando Azure detecta que un motor SQL Server est\u00e1 instalado dentro de una m\u00e1quina virtual, habilita el punto de administraci\u00f3n con opciones espec\u00edficas que el administrador de VMs deber\u00e1 comenzar a gestionar. Azure SQL Database es un motor de base de datos de tipo plataforma como servicio (PaaS) totalmente administrado por Microsoft, que se encarga de la mayor\u00eda de las funciones de administraci\u00f3n de bases de datos. \u00bfQu\u00e9 tipo de funciones de administraci\u00f3n est\u00e1n a cargo de Microsoft? Por ejemplo: actualizar el motor, aplicar revisiones, crear copias de seguridad, supervisar sin intervenci\u00f3n del usuario. Azure SQL Database se ejecuta siempre en la \u00faltima versi\u00f3n estable del motor de base de datos de SQL Server y en un sistema operativo revisado con el 99,99 % de disponibilidad. Las capacidades de PaaS que est\u00e1n integradas en Azure SQL Database permiten centrarse en las actividades de administraci\u00f3n y optimizaci\u00f3n de bases de datos espec\u00edficas del dominio que son cr\u00edticas para el negocio, y no en mantener la infraestructura. Se puede descargar el software de SSMS y conectar desde el pc a azure y gestionar las bbdd. Se pueden crear: Single databases(siempre ha de haber un server database) Elastic Pool databses(recursos compartidos y dentro varias ddbb) Managed instances(se crea en la nube las ddbb) Managed instances: Es un servicio de base de datos en la nube inteligente y escalable que combina la mayor compatibilidad con el motor de base de datos de SQL Server en una plataforma como servicio totalmente administrada por Microsoft. Tiene casi un 100 % de compatibilidad con el motor de base de datos m\u00e1s reciente de SQL Server (Enterprise Edition). Url para migrar bbdd a azure COSMOS DB Azure Cosmos DB es un servicio de base de datos con varios modelos distribuido de forma global de Microsoft. En esta lecci\u00f3n vamos a repasar los conceptos de las bases de datos no-sql vs las relacionales, para poder ir entendiendo un poco m\u00e1s de qu\u00e9 se trata este servicio de base de datos de Microsoft. Azure Cosmos DB es un servicio de base de datos multimodelo distribuido y con escalado horizontal. Al ser multimodelo, admite de forma nativa modelos de datos de documentos, pares clave-valor, grafos y en columnas. Con respecto a la administraci\u00f3n e interfaz de comunicaci\u00f3n, Azure Cosmos DB permite acceder a sus datos con diferentes APIs: como SQL (documentos), MongoDB (documentos), Azure Table Storage (clave-valor), Gremlin (grafos) y Cassandra (en columnas). A nivel de funcionalidad, Azure Cosmos DB indexa datos autom\u00e1ticamente sin que haya que ocuparse de la administraci\u00f3n de esquemas ni de \u00edndices. Creaci\u00f3n En un resource group a\u00f1adimos un cosmosDB. Para crear una cuenta de Azure Cosmos DB gratuita por 30 dias link Una vez creado el cosmosDB vamos a Data Explorer y creamos una db y despues un container dentro de esta db. Por ejemplo creamos un container con nombre personas y el key es /dni. Dentro de personas vamos creando nuevos items: { \"dni\": \"46410800C\", \"nombre\": \"Miguel\", \"apellidos\": \"Amor\u00f3s Moret\" } Tambien podemos hacer query: SELECT c.nombre, c.apellidos FROM c where c.dni = \"46410800C\" BALANCEADORES AZURE En Microsoft Azure tenemos diversas opciones de balanceadores, tambi\u00e9n conocidos como equilibradores de carga. El t\u00e9rmino equilibrio de carga hace referencia a la distribuci\u00f3n de cargas de trabajo entre varios recursos de proceso. El equilibrio de carga busca optimizar el uso de recursos, maximizar el rendimiento, minimizar el tiempo de respuesta y evitar la sobrecarga de un solo recurso. Tambi\u00e9n puede mejorar la disponibilidad al compartir una carga de trabajo entre recursos de proceso redundantes. Global frente a regional + Los servicios de equilibrio de carga globales distribuyen el tr\u00e1fico en servidores de back-end regionales, nubes o servicios locales h\u00edbridos. Estos servicios enrutan el tr\u00e1fico del usuario final al servidor de back-end disponible m\u00e1s cercano. Tambi\u00e9n reaccionan a los cambios en la confiabilidad o el rendimiento del servicio, con el fin de maximizar la disponibilidad y el rendimiento. Puede pensar en ellos como sistemas que equilibran la carga entre los stamp, puntos de conexi\u00f3n o unidades de escalado de la aplicaci\u00f3n hospedados en diferentes regiones o zonas geogr\u00e1ficas. Los servicios de equilibrio de carga regionales distribuyen el tr\u00e1fico de las redes virtuales entre las m\u00e1quinas virtuales (VM) o puntos de conexi\u00f3n de servicio zonales y con redundancia de zona de una regi\u00f3n. Puede pensarlos como sistemas que equilibran la carga entre m\u00e1quinas virtuales, contenedores o cl\u00fasteres dentro de una regi\u00f3n en una red virtual. HTTP(S) frente a no HTTP(S) + Los servicios de equilibrio de cargas HTTP(S) son equilibradores de carga de capa 7 que solo aceptan el tr\u00e1fico HTTP(S). Est\u00e1n dise\u00f1ados para las aplicaciones web u otros puntos de conexi\u00f3n HTTP(S). Incluyen caracter\u00edsticas, como la descarga de SSL, el firewall de aplicaciones web, el equilibrio de carga basado en rutas de acceso y la afinidad de sesi\u00f3n. Los servicios de equilibrio de carga que no son HTTP/S pueden controlar el tr\u00e1fico que no es de HTTP(S) y se recomiendan para las cargas de trabajo que no son web. Servicios de Balanceo en Azure Load Balancer + Proporciona un servicio de equilibrio de carga de capa 4 con latencia baja y rendimiento alto (entrante y saliente) para todos los protocolos UDP y TCP. Se dise\u00f1\u00f3 para administrar millones de solicitudes por segundo, a la vez que garantiza que la soluci\u00f3n tiene una alta disponibilidad. Azure Load Balancer tiene redundancia de zona, lo que garantiza una alta disponibilidad en las instancias de Availability Zones. Traffic Manager + Es un equilibrador de carga de tr\u00e1fico basado en DNS que le permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Dado que Traffic Manager es un servicio de equilibrio de carga basado en DNS, solo equilibra la carga en el nivel del dominio. Por ese motivo, no puede conmutar por error tan r\u00e1pidamente como con Front Door, debido a los desaf\u00edos comunes relacionados con el almacenamiento en cach\u00e9 de DNS y a los sistemas que no respetan los TTL de DNS. Application Gateway + Proporciona un controlador de entrega de aplicaciones (ADC) como servicio, que ofrece diversas funcionalidades de equilibrio de carga de capa 7. \u00daselo para optimizar la productividad de las granjas de servidores web al traspasar la carga de la terminaci\u00f3n SSL con mayor actividad de la CPU a la puerta de enlace. Front Door + Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Ejercicio balanceador Vamos a realizar todas las configuraciones necesarias para poder, luego, trabajar sobre un Azure Load Balancer. Estas configuraciones las consideramos pre-requisitos y contemplan: Ejecuci\u00f3n en un modelo de Infraestructura como C\u00f3digo de la creaci\u00f3n de m\u00e1quinas virtuales y otros componentes requeridos para el ejercicio. Son un total de 3 equipos virtuales que utilizan 1 VCore cada uno, todos utilizando un Availability Set (Conjunto de Disponibilidad) con 2 dominios de falla y 5 dominios de actualizaci\u00f3n. Instalaci\u00f3n y configuraci\u00f3n de Internet Information Services (IIS) con una p\u00e1gina web simple. Creaci\u00f3n y asignaci\u00f3n de un Network Security Group (NSG) \u00fanico para los 3 equipos virtuales. Creamos un template y hacemos deploy del siguiente codigo Nos conectamos remotamente al server1 e instalamos el rol IIS de server web. Modificamos el index.html y ponemos que saludamos desde server 1. Ahora en el resource group creamoe un network security group. Dentro creamos un inbound de http por el puerto 80. Ahora de cada network interface las asignamos a este creado y borramos la del template. Ahora si entramos a la Ip de la primera VM por el puerto 80 desde el navegador normal, vemos su index.html. Despues de cada interfaz de red, modificamos su ip configuration y le ponemos ip statica y ip publica desasociada. Despues creamos un recurso nuevo. un load balancer publico, basic y con una ip static. Despues entramos y creamos un backend en nuestra loadblancer con nuestras 3 virtual machines. Creamos un healthprobe para http. A\u00f1adimos un load rule. Ahora si ponemos en el navegador la ip del balanceador, nos ir\u00e1 responiendo dinamicamente, el servidor de respuesta. Azure Trafic Manager Azure Traffic Manager es un balanceador de tr\u00e1fico basado en DNS que permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Trabaja a nivel capa 7 de OSI, aunque s\u00f3lo a nivel DNS. Traffic Manager usa DNS para dirigir las solicitudes del cliente al punto de conexi\u00f3n de servicio m\u00e1s adecuado en funci\u00f3n de un m\u00e9todo de enrutamiento del tr\u00e1fico y el mantenimiento de los puntos de conexi\u00f3n. Un punto de conexi\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Es fundamental entender que Traffic Manager funciona a nivel de DNS. Traffic Manager usa DNS para dirigir a los clientes a puntos de conexi\u00f3n espec\u00edficos del servicio basados en las reglas del m\u00e9todo de enrutamiento de tr\u00e1fico. Los clientes se conectan directamenteal punto de conexi\u00f3n seleccionado. Traffic Manager no es un proxy ni una puerta de enlace. Traffic Manager no ve el tr\u00e1fico que circula entre el cliente y el servicio. EJERCICIO: Vamos a completar los pre-requisitos necesarios para poder llevar adelante el ejercicio de Azure Traffic Manager. Estos pre-requisitos son: 3 App Service Plans con sistema operativo Linux. 3 App Service (Web Apps) con runtime PHP 7.3. 1 VM con Windows creada en Estados Unidos (si no estas en USA) o en Brasil (si est\u00e1s en USA). Publicaci\u00f3n de la soluci\u00f3n PHP (index.php) en cada sitio web (Brasil, Estados Unidos y Asia). La deber\u00e1s descargar desde GitHub. # A Crear un Resource Group para Traffic Manager. Completed on 12 noviembre, 2020 8:04 pm B Crear Sitio Web en Brasil Sur con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). C Crear Sitio Web en Estados Unidos Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). D Crear Sitio Web en Asia Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). E Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Sur de Brasil. F Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Estados Unidos. G Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Asia. H Crear la M\u00e1quina Virtual con Windows en Estados Unidos \u00f3 Brasil, seg\u00fan corresponda a tu ubicaci\u00f3n. Abrimos el ftp de cada uno y copiamos el index.php del github. Ahora creamos un Traffic Manager profile en nuestro grupo de recurso. Creamos 3 endpoints en mi traffic manager y mapeado por geolocalizaci\u00f3n. Segun petici\u00f3n donde estemos nos contesta una u otra Azure Aplication Gateway Azure Application Gateway es un equilibrador de carga de tr\u00e1fico web que permite administrar el tr\u00e1fico a las aplicaciones web. Los equilibradores de carga tradicionales operan en la capa de transporte (OSI capa 4: TCP y UDP) y enrutan el tr\u00e1fico en funci\u00f3n de la direcci\u00f3n IP y puerto de origen a una direcci\u00f3n IP y puerto de destino. Application Gateway puede tomar decisiones de enrutamiento basadas en atributos adicionales de una solicitud HTTP, por ejemplo los encabezados de host o la ruta de acceso del URI. A Paso inicial: confirmar que complet\u00e9 todos los requisitos (sitios web pre-requisitos de la lecci\u00f3n de Traffic Manager). B Crear un Resource Group para Application Gateway. C Crear un Application Gateway (SKU Standard_v2, Zonas 1 2 y 3 del Este de Estados Unidos, y escalado manual con 2 instancias iniciales). D Crear la Virtual Network con las opciones predeterminadas. E Crear un Front-End con una direcci\u00f3n de Front-End p\u00fablica. F Crear un Back-End Pool con los 3 sitios web configurados. G Crear un Routing Rule con Listener Basico. H Crear un HTTP Settings seg\u00fan indicaciones del Video. I Corregir HTTP Settings para evitar error en el estado de salud de los Websites de App Service. J Probar acceso por IP del Front-End (p\u00fablica) para validar que podamos ingresar al sitio web. Hacer F5 para validar que se cambia la p\u00e1gina de inicio seg\u00fan zona. Azure Front Door (AFD) Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Azure Front Door permite definir, administrar y supervisar el enrutamiento global para el tr\u00e1fico web mediante la optimizaci\u00f3n para obtener el mejor rendimiento y la conmutaci\u00f3n por error global r\u00e1pida para alta disponibilidad. Front Door funciona en la capa 7 o la capa HTTP/HTTPS, y usa el protocolo de difusi\u00f3n por proximidad con divisi\u00f3n TCP y la red global de Microsoft para mejorar la conectividad global. Por tanto, seg\u00fan la selecci\u00f3n del m\u00e9todo de enrutamiento en la configuraci\u00f3n, puede asegurarse de que Front Door enruta las solicitudes de cliente al back-end de aplicaci\u00f3n m\u00e1s r\u00e1pido y disponible. Un back-end de aplicaci\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Front Door proporciona una serie de m\u00e9todos de enrutamiento del tr\u00e1fico y opciones de seguimiento de estado del back-end para satisfacer las distintas necesidades de las aplicaciones y los modelos de conmutaci\u00f3n autom\u00e1tica por error. Al igual que Traffic Manager, Front Door es resistente a errores, incluidos los que afectan a una regi\u00f3n completa de Azure. CONTENEDORES los contenedores como unidades de despliegue. Los contenedores ofrecen las ventajas del aislamiento, la portabilidad, la agilidad, la escalabilidad y el control a lo largo de todo el flujo de trabajo del ciclo de vida de la aplicaci\u00f3n. La ventaja m\u00e1s importante es el aislamiento del entorno que se proporciona entre el desarrollo y las operaciones. Azure Container Registry es un servicio privado administrado del Registro de Docker que usa Docker Registry 2.0, que es de c\u00f3digo abierto. Cree y mantenga los registros de Azure Container para almacenar y administrar las im\u00e1genes privadas del contenedor Docker y los artefactos relacionados. EJERCICIO APP SERVICE Azure Container Instances es un servicio de Microsoft Azure que permite en forma r\u00e1pida y sencilla ejecutar un contenedor en Azure, sin tener que administrar ninguna m\u00e1quina virtual y sin necesidad de adoptar un servicio de nivel superior. EJERCICIO Kubernetes es una plataforma de r\u00e1pida evoluci\u00f3n que administra aplicaciones basadas en contenedores y sus componentes de red y almacenamiento asociados. El foco est\u00e1 en las cargas de trabajo de la aplicaci\u00f3n, no en los componentes de infraestructura subyacente. Kubernetes proporciona un enfoque declarativo en las implementaciones, respaldado por un s\u00f3lido conjunto de API para las operaciones de administraci\u00f3n. Azure Kubernetes Service (AKS) proporciona un servicio de Kubernetes administrado que reduce la complejidad de las principales tareas de administraci\u00f3n e implementaci\u00f3n, incluida la coordinaci\u00f3n de actualizaciones. El plano de control de AKS es administrado por la plataforma de Azure, y solo paga por los nodos de AKS que ejecutan sus aplicaciones. AKS se ha dise\u00f1ado sobre el motor de c\u00f3digo abierto de Azure Kubernetes Service (aks-engine). Los nodos del plano de control proporcionan los servicios centrales de Kubernetes y la orquestaci\u00f3n de las cargas de trabajo de las aplicaciones. Los nodos ejecutan las cargas de trabajo de la aplicaci\u00f3n. LOGIC APPS PowerAutomate, antes conocido como Flow, es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. Vamos a conocer algunos detalles del servicio para luego compararlo con Logic App (el servicio en el que realmente queremos hacer doble clic). Es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. El principal usuario objetivo es el de negocio, \u201cCitizen Developer\u201d / Integrador, es decir aquel usuario que crea nuevas aplicaciones comerciales para el consumo de otros utilizando entornos de desarrollo y tiempo de ejecuci\u00f3n autorizados por la TI corporativa. Su foco es brindar una experiencia para integraciones simples con aplicaciones y servicios. Logic Apps es un servicio PaaS (Plataforma como Servicio) para automatizar flujos de trabajo sobre m\u00faltiples aplicaciones SaaS y servicios IaaS, simplificando la complejidad requerida para la integraci\u00f3n empresarial. En si mismo, extiendo las capacidades de Power Automate. El principal usuario objetivo son Desarrolladores y IT Pros, es decir usuarios de sistemas que tienen conocimiento mucho m\u00e1s avanzado que un \u201cCitizen Developer\u201d. Su foco es brindar una experiencia para integraciones complejas y avanzadas, comparativamente con Power Automate. EJERCICIO En este tutorial se muestra c\u00f3mo utilizar Azure Functions con Logic Apps y Cognitive Services en Azure para ejecutar el an\u00e1lisis de opiniones de entradas de Twitter. Una funci\u00f3n desencadenada por HTTP clasifica los tweets en verdes, amarillos o rojos, en funci\u00f3n de la puntuaci\u00f3n de la opini\u00f3n. Se env\u00eda un correo electr\u00f3nico cuando se detecta una opini\u00f3n deficiente. IA, ML Y DL El aprendizaje profundo (deep learning), es un subconjunto del aprendizaje autom\u00e1tico basado en redes neuronales artificiales que permiten a un equipo entrenarse a s\u00ed mismo. En este caso, el proceso de aprendizaje se llama profundo porque la estructura de redes neuronales artificiales se compone de varias capas de entrada, salida y ocultas. Cada capa contiene unidades que transforman los datos de entrada en informaci\u00f3n que la capa siguiente puede usar para realizar una tarea de predicci\u00f3n determinada. Gracias a esta estructura, un equipo puede aprender a trav\u00e9s de su propio procesamiento de datos. El aprendizaje autom\u00e1tico (machine learning) es un subconjunto de la inteligencia artificial que incluye t\u00e9cnicas (como el aprendizaje profundo) que permiten a los equipos mejorar en las tareas con la experiencia. En este caso, el proceso de aprendizaje se basa en los pasos siguientes: Alimente un algoritmo con datos proporcion\u00e1ndole m\u00e1s informaci\u00f3n (por ejemplo, realizando la extracci\u00f3n de caracter\u00edsticas). Utilice estos datos para entrenar un modelo. Pruebe e implemente el modelo. Consuma el modelo implementado para realizar una tarea de predicci\u00f3n automatizada concreta. La inteligencia artificial (IA artificial intelligence) es una t\u00e9cnica que permite a los equipos imitar la inteligencia humana. Incluye el aprendizaje autom\u00e1tico. Es importante conocer la relaci\u00f3n entre aprendizaje autom\u00e1tico, aprendizaje profundo e inteligencia artificial: El aprendizaje autom\u00e1tico es una forma de lograr inteligencia artificial, lo que significa que, mediante el uso de t\u00e9cnicas de aprendizaje autom\u00e1tico y aprendizaje profundo, se pueden crear sistemas inform\u00e1ticos y aplicaciones que puedan realizar tareas asociadas normalmente a la inteligencia humana, como la percepci\u00f3n visual, el reconocimiento de voz, la toma de decisiones y la traducci\u00f3n de un idioma a otro. AZURE COGNITIVE SERVICES Azure Cognitive Services son servicios en la nube con API REST y SDK de biblioteca cliente que ayudan a los desarrolladores a compilar aplicaciones inteligentes cognitivas sin tener inteligencia artificial (IA) directa ni aptitudes o conocimientos sobre ciencia de datos. Azure Cognitive Services permiten a los desarrolladores agregar f\u00e1cilmente caracter\u00edsticas cognitivas en sus aplicaciones. El objetivo de Azure Cognitive Services es ayudar a los desarrolladores a crear aplicaciones que puedan ver, o\u00edr, hablar, comprender e incluso empezar a razonar. Cognitive Services proporciona funciones de aprendizaje autom\u00e1tico para solucionar problemas generales, como el an\u00e1lisis de texto para la opini\u00f3n emocional o el an\u00e1lisis de im\u00e1genes para reconocer objetos o caras. No es necesario tener conocimientos de aprendizaje autom\u00e1tico ni ciencia de datos para usar estos servicios, como tampoco conocer de programaci\u00f3n. Spatial Anchors es un servicio multiplataforma para desarrolladores que le permite crear experiencias de realidad mixta mediante objetos cuya ubicaci\u00f3n persiste en todos los dispositivos a lo largo del tiempo. Estas aplicaciones pueden admitir Microsoft HoloLens, dispositivos iOS compatibles con ARKit y dispositivos Android compatibles con ARCore. Azure Spatial Anchors permite a los desarrolladores trabajar con plataformas de realidad mixta para percibir el espacio, designar puntos precisos de inter\u00e9s y volver a recuperar esos puntos de inter\u00e9s desde los dispositivos compatibles. Estos puntos de inter\u00e9s precisos se conocen como delimitadores espaciales. BIG DATA Big Data (conocido tambi\u00e9n como Macrodatos) es un t\u00e9rmino que describe el gran volumen de datos, tanto estructurados como no estructurados, que inundan los negocios cada d\u00eda. Pero no es la cantidad de datos lo que es importante. Lo que importa con el Big Data es lo que las organizaciones hacen con los datos. Big Data se puede analizar para obtener ideas que conduzcan a mejores decisiones y movimientos de negocios estrat\u00e9gicos. Cuando hablamos de Big Data nos referimos a conjuntos de datos o combinaciones de conjuntos de datos cuyo tama\u00f1o (volumen), complejidad (variabilidad) y velocidad de crecimiento (velocidad) dificultan su captura, gesti\u00f3n, procesamiento o an\u00e1lisis mediante tecnolog\u00edas y herramientas convencionales, tales como bases de datos relacionales y estad\u00edsticas convencionales o paquetes de visualizaci\u00f3n, dentro del tiempo necesario para que sean \u00fatiles. Las Cargas de Trabajo Tradicionales (RDBMS) utilizan procesamiento de transacciones en l\u00ednea (OLTP) y procesamiento anal\u00edtico en l\u00ednea (OLAP). En los sistemas OLTP, los datos suelen ser relacionales con un esquema predefinido y un conjunto de restricciones para mantener la integridad referencial. A menudo, se pueden consolidar datos de varios or\u00edgenes de la organizaci\u00f3n en un almacenamiento de datos, utilizando un proceso ETL para mover y transformar los datos de origen. Una arquitectura de Big Data (Macrodatos) est\u00e1 dise\u00f1ada para controlar la ingesta, el procesamiento y el an\u00e1lisis de datos que son demasiado grandes o complejos para los sistemas de bases de datos tradicionales. Los datos se pueden procesar por lotes o en tiempo real. Las soluciones de macrodatos suelen implicar una gran cantidad de datos no relacionales, tales como datos de clave y valor, documentos JSON o datos de series temporales. Los sistemas RDBMS tradicionales no suelen ser adecuados para almacenar este tipo de datos. El t\u00e9rmino NoSQL hace referencia a la familia de bases de datos que est\u00e1n dise\u00f1adas para contener datos no relacionales. El t\u00e9rmino no es totalmente exacto, porque muchos almacenes de datos no relacionales admiten consultas compatibles con SQL. El t\u00e9rmino NoSQL significa \u201cno solo SQL\u201d. Conceptos Un data warehouse es un repositorio unificado y estructurado para todos los datos que recogen los diversos sistemas de una empresa. El repositorio puede ser f\u00edsico o l\u00f3gico y hace hincapi\u00e9 en la captura de datos de diversas fuentes sobre todo para fines anal\u00edticos y de acceso. Un data lake es un repositorio de almacenamiento que contienen una gran cantidad de datos en bruto, estructurado, semi-estructurados & no estructurados, y que se mantienen all\u00ed hasta que sea necesario. A diferencia de un data warehouse jer\u00e1rquico que almacena datos en ficheros o carpetas, un data lake utiliza una arquitectura plana para almacenar los datos. Existen algunas diferencias clave entre Data Lake y Data Warehouse: Datos: Un data warehouse s\u00f3lo almacena datos que han sido modelados o estructurados, mientras que un Data Lake no hace acepci\u00f3n de datos. Lo almacena todo, estructurado, semiestructurado y no estructurado. Procesamiento: Antes de que una empresa pueda cargar datos en un data warehouse, primero debe darles forma y estructura, es decir, los datos deben ser modelados. Eso se llama schema-on-write. Con un data lake, s\u00f3lo se cargan los datos sin procesar, tal y como est\u00e1n, y cuando est\u00e9 listo para usar los datos, es cuando se le da forma y estructura. Eso se llama schema-on-read. Dos enfoques muy diferentes. Almacenamiento: Una de las principales caracter\u00edsticas de las tecnolog\u00edas de big data, como Hadoop, es que el coste de almacenamiento de datos es relativamente bajo en comparaci\u00f3n con el de un data warehouse. Hay dos razones principales para esto: en primer lugar, Hadoop es software de c\u00f3digo abierto, por lo que la concesi\u00f3n de licencias y el soporte de la comunidad es gratuito. Y segundo, Hadoop est\u00e1 dise\u00f1ado para ser instalado en hardware de bajo coste. Agilidad: Un almac\u00e9n de datos es un repositorio altamente estructurado, por definici\u00f3n. No es t\u00e9cnicamente dif\u00edcil cambiar la estructura, pero puede tomar mucho tiempo dado todos los procesos de negocio que est\u00e1n vinculados a ella. Un data lake, por otro lado, carece de la estructura de un data warehouse, lo que da a los desarrolladores y a los cient\u00edficos de datos la capacidad de configurar y reconfigurar f\u00e1cilmente y en tiempo real sus modelos, consultas y aplicaciones. Seguridad: La tecnolog\u00eda del data warehouse existe desde hace d\u00e9cadas, mientras que la tecnolog\u00eda de big data (la base de un Data Lake) es relativamente nueva. Por lo tanto, la capacidad de asegurar datos en un data warehouse es mucho m\u00e1s madura que asegurar datos en un data lake. Cabe se\u00f1alar, sin embargo, que se est\u00e1 realizando un importante esfuerzo en materia de seguridad en la actualidad en la industria de Big Data. Azure Data Factory es la plataforma que resuelve estos escenarios de datos. Se trata de un servicio de integraci\u00f3n de datos y ETL basado en la nube que le permite crear flujos de trabajo orientados a datos a fin de coordinar el movimiento y la transformaci\u00f3n de datos a escala. Con Azure Data Factory, puede crear y programar flujos de trabajo basados en datos (llamados canalizaciones) que pueden ingerir datos de distintos almacenes de datos. Puede crear procesos ETL complejos que transformen datos visualmente con flujos de datos o mediante servicios de proceso como Azure HDInsight Hadoop, Azure Databricks y Azure SQL Database. Azure Databricks es una plataforma de an\u00e1lisis basada en Apache Spark optimizada para la plataforma de servicios en la nube de Microsoft Azure. Dise\u00f1ada por los fundadores de Apache Spark, Databricks est\u00e1 integrado con Azure para proporcionar una configuraci\u00f3n con un solo clic, flujos de trabajo optimizados y un \u00e1rea de trabajo interactiva que permite la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas empresariales. Y lo mejor: no requiere administraci\u00f3n :-). Apache Spark es un framework de programaci\u00f3n para procesamiento de datos distribuidos dise\u00f1ado para ser r\u00e1pido y de prop\u00f3sito general. Como su propio nombre indica, ha sido desarrollada en el marco del proyecto Apache, lo que garantiza su licencia Open Source. Azure Synapse es un servicio de an\u00e1lisis que engloba el almacenamiento de datos empresariales y el an\u00e1lisis de macrodatos. Le ofrece la libertad de consultar los datos como prefiera, ya sea a petici\u00f3n sin servidor o con recursos aprovisionados, a escala. Azure Synapse re\u00fane estos dos mundos con una experiencia unificada para ingerir, preparar, administrar y servir datos para las necesidades inmediatas de inteligencia empresarial y aprendizaje autom\u00e1tico. MENSAJERIA Azure ofrece varios servicios que le ayudan en la entrega de mensajes de evento en una soluci\u00f3n. Estos servicios son los siguientes: - Event Grid - Event Hubs - Service Bus Evento: es una notificaci\u00f3n ligera de una condici\u00f3n o un cambio de estado. El publicador del evento no tiene ninguna expectativa sobre c\u00f3mo se trata el evento. El consumidor del evento decide qu\u00e9 hacer con la notificaci\u00f3n. Los eventos pueden ser unidades discretas o parte de una serie. Mensaje: son datos sin procesar producidos por un servicio que se consumen o almacenan en otro lugar. El mensaje contiene los datos que desencaden\u00f3 la canalizaci\u00f3n del mensaje. El publicador del mensaje tiene una expectativa sobre la forma en que el consumidor trata el mensaje. Existe un contrato entre ambas partes. Por ejemplo, el publicador env\u00eda un mensaje con los datos sin procesar, espera que el consumidor cree un archivo a partir de esos datos y env\u00eda una respuesta cuando el trabajo finaliza. Microsoft Azure Service Bus es un agente de mensajes de integraci\u00f3n empresarial completamente administrado. Service Bus puede desacoplar aplicaciones y servicios. Service Bus ofrece una plataforma confiable y segura para la transferencia asincr\u00f3nica de datos y estado. Azure Event Hubs es una plataforma de streaming de macrodatos y un servicio de ingesta de eventos. Puede recibir y procesar millones de eventos por segundo. Los datos enviados a un centro de eventos se pueden transformar y almacenar con cualquier proveedor de an\u00e1lisis en tiempo real o adaptadores de procesamiento por lotes y almacenamiento. Azure IoT Hub es la puerta de enlace en la nube que conecta dispositivos IoT para recopilar los datos y dirigir las perspectivas y automatizaci\u00f3n empresariales. Adem\u00e1s, IoT Hub incluye caracter\u00edsticas que enriquecen la relaci\u00f3n entre los dispositivos y los sistemas back-end. Las capacidades de comunicaci\u00f3n bidireccional implican que al tiempo que se reciben datos de los dispositivos, tambi\u00e9n es posible devolver comandos y directivas a los dispositivos. Azure Event Grid permite crear f\u00e1cilmente aplicaciones con arquitecturas basadas en eventos. Event Grid est\u00e1 dise\u00f1ado para eventos, no datos. Cuando un Event Grid es informado de un evento, luego toma acciones determinadas. AZURE ACTIVE DIRECTORY Azure Active Directory (Azure AD) es un servicio de administraci\u00f3n de identidades y acceso basado en la nube de Microsoft que ayuda a los empleados a iniciar sesi\u00f3n y acceder a recursos en: Recursos externos, como Microsoft 365, Azure Portal y miles de otras aplicaciones SaaS. Recursos internos, como las aplicaciones de la red corporativa y la intranet, junto con todas las aplicaciones en la nube desarrolladas por su propia organizaci\u00f3n. Los siguientes perfiles usan Azure AD: Administradores de TI. Si es administrador de TI, puede usar Azure AD para controlar el acceso a sus aplicaciones y a los recursos de est\u00e1s en funci\u00f3n de los requisitos de su empresa. Por ejemplo, puede usar Azure AD para requerir autenticaci\u00f3n multifactor al acceder a recursos importantes de la organizaci\u00f3n. Adem\u00e1s, puede usar Azure AD para automatizar el aprovisionamiento de usuarios entre la instancia existente de Windows Server AD y las aplicaciones en la nube, incluida Microsoft 365. Por \u00faltimo, Azure AD proporciona eficaces herramientas que ayudan a proteger autom\u00e1ticamente las identidades y credenciales de los usuarios y a cumplir los requisitos de gobernanza de acceso. Desarrolladores de aplicaciones. Como desarrollador de aplicaciones, puede usar Azure AD como enfoque basado en est\u00e1ndares para agregar el inicio de sesi\u00f3n \u00fanico (SSO) a cualquier aplicaci\u00f3n, lo que le permite trabajar con las credenciales existentes de un usuario. Azure AD tambi\u00e9n proporciona varias API que pueden ayudarle a crear experiencias de aplicaci\u00f3n personalizadas que usen los datos existentes de la organizaci\u00f3n. Suscriptores de Microsoft 365, Office 365, Azure o Dynamics CRM Online. Los suscriptores usan Azure AD. Cada inquilino de Microsoft 365, Office 365, Azure y Dynamics CRM Online es autom\u00e1ticamente un inquilino de Azure AD. Puede empezar de inmediato a administrar el acceso a las aplicaciones en la nube integradas. Entidades: Cloud Only: Esta opci\u00f3n de identidad es la que, por defecto, se habilita cuando creamos una cuenta en Office 365 o cuando creamos un directorio de Active Directory en Azure nuevo. En este caso, la contrase\u00f1a es gestionada por Azure Active Directory y no tiene relaci\u00f3n absoluta con nuestra infraestructura on-promises. De hecho, tampoco nuestro UPN de usuario tiene relaci\u00f3n con nuestra infraestructura on-promises, y de hecho (finalmente) debemos dar de alta los usuarios uno por uno. Sincronizada: Esta opci\u00f3n es la primera que, de alguna forma, relaciona nuestro servicio de directorio local (Active Directory Domain Services u otro LDAP) con el servicio de directorio en la nube (Azure Active Directory). Federada: La tercera y \u00faltima opci\u00f3n agrega la posibilidad de que el inicio de sesi\u00f3n se efectivice en nuestra infraestructura local. Azure AD Connect: El uso de esta caracter\u00edstica es gratis y est\u00e1 incluido en su suscripci\u00f3n de Azure. Es el componente que nos permite sincronizar identidades, o conectar nuestro servicio de directorio de Azure AD con otros LDAPs propios. Esta herramienta nos permitir\u00e1 sincronizar uno o muchos LDAPs con Azure AD, pero no un LDAP con muchos Azure ADs. Este item es importante de tener en cuenta cuando estemos planificando la arquitectura de soluci\u00f3n. DEVOPS DevOps es \u201cla union de personas, procesos, y productos para permitir la entrega continua de valor a sus usuarios finales\u201d. DevOps es una pr\u00e1ctica de ingenier\u00eda de software que tiene como objetivo unificar el desarrollo de software (Dev) y la operaci\u00f3n del software (Ops). La principal caracter\u00edstica del movimiento DevOps es defender en\u00e9rgicamente la automatizaci\u00f3n y el monitoreo en todos los pasos de la construcci\u00f3n del software, desde la integraci\u00f3n, las pruebas, la liberaci\u00f3n hasta la implementaci\u00f3n y la administraci\u00f3n de la infraestructura. DevOps apunta a ciclos de desarrollo m\u00e1s cortos, mayor frecuencia de implementaci\u00f3n, lanzamientos m\u00e1s confiables, en estrecha alineaci\u00f3n con los objetivos comerciales. Cuando hablamos de DevOps podemos hablar de 3 etapas, que no necesariamente se dan en cascada. Cuando trabajamos DevOps en las organizaciones debemos pensar en: Personas, Procesos y Productos. Cuando hablamos de personas, tenemos que entender que los equipos de trabajo est\u00e1n conformados por Personas: estas tienen emociones, que pueden manejar mejor o peor dependiendo de las situaciones personales y laborales que atraviesen. Entender esto y darle una mirada humana nos facilitar\u00e1 entender que DEBEMOS trabajar con las personas, desarrollar caracter\u00edsticas deseables que lleven a que sean \u201cEasy to work with\u201d, es decir una persona \u201ccon la que es f\u00e1cil y queremos trabajar\". Existen 4 grandes procesos, en alto nivel, que tenemos que tener en cuenta. Estos procesos, por supuesto, pueden darse c\u00edclicamente y no necesariamente en cascada: Planificar: crear un backlog, equipos interdisciplinarios, planificar testing, etc. Desarrollar + Probar: ya conozco que debo hacer, tengo mi backlog al menos en alto nivel, y podemos avanzar en la construcci\u00f3n. Liberar: lo que planifiqu\u00e9 y desarroll\u00e9 + prob\u00e9, lo debo liberar a ambientes. Desarrollo, Testing, Producci\u00f3n, etc. Monitorear + Aprender: un punto clave, es que el trabajo no termin\u00f3 con la liberaci\u00f3n. Reci\u00e9n comienza aqu\u00ed el aprendizaje basado en un monitoreo proactivo para volver a arrancar esta rueda: planificar, desarrollar + probar, y liberar, para volver a aprender. Desarrollo \u00e1gil: pr\u00e1cticas y marcos de trabajo \u00e1giles para el desarrollo. Integraci\u00f3n Continua: en todo momento, integrar nuestro c\u00f3digo con pruebas y chequeos de salud. Entrega / Despliegue Continuo: hacia ambientes, ya sea desarrollo, testing, pre producci\u00f3n o producci\u00f3n, acompa\u00f1ado de una robusta gesti\u00f3n de releases. AZURE DEVOPS Azure DevOps proporciona servicios de desarrollador para ayudar a los equipos a planificar el trabajo, colaborar en el desarrollo de c\u00f3digo y crear e implementar aplicaciones. Los desarrolladores pueden trabajar en la nube con Azure DevOps Services o localmente con Azure DevOps Server. Azure DevOps Server se llamaba anteriormente Visual Studio Team Foundation Server (TFS). Azure Repos proporciona repositorios de Git o Team Foundation Version Control (TFVC) para el control de c\u00f3digo fuente de su c\u00f3digo. Azure Pipelines proporciona servicios de creaci\u00f3n y lanzamiento para admitir la integraci\u00f3n y entrega continuas de sus aplicaciones. Azure Boards ofrece un conjunto de herramientas \u00e1giles para respaldar la planificaci\u00f3n y el seguimiento del trabajo, los defectos de c\u00f3digo y los problemas con los m\u00e9todos Kanban y Scrum. Azure Test Plans proporciona varias herramientas para probar sus aplicaciones, incluidas pruebas manuales / exploratorias y pruebas continuas. Azure Artifacts permite a los equipos compartir paquetes Maven, npm y NuGet de fuentes p\u00fablicas y privadas e integrar el uso compartido de paquetes en sus canalizaciones de CI / CD. Azure DevOps es gratuito hasta 5 usuarios por organizaci\u00f3n. Una organizaci\u00f3n de Azure DevOps permite tener, dentor, una colecci\u00f3n de proyectos con diversos repositorios. La creaci\u00f3n de repositorios tambi\u00e9n es gratuita, e inclusive la utilizaci\u00f3n de Pipelines hosteados por Microsoft para proyectos privados de hasta 1800 minutos por mes tambi\u00e9n es gratuito. En caso de proyectos p\u00fablicos, no hay l\u00edmite de minutos para Pipelines. Precios Vamos a devops starter para crear nuestro pipeline. SEGURIDAD AZURE Azure Security Center es un sistema unificado de administraci\u00f3n de seguridad de la infraestructura que fortalece la posici\u00f3n de seguridad de los centros de datos y proporciona una protecci\u00f3n contra amenazas avanzada de todas las cargas de trabajo h\u00edbridas que se encuentran en la nube, ya sea que est\u00e9n en Azure o no, as\u00ed como tambi\u00e9n en el entorno local. Proteger los recursos es un esfuerzo conjunto entre el proveedor de nube, Azure y usted, el cliente. Cuando migra a la nube, debe asegurarse de que las cargas de trabajo est\u00e9n seguras y, al mismo tiempo, cuando se migra a IaaS (infraestructura como servicio), el cliente tiene una responsabilidad mayor que cuando se encontraba en PaaS (plataforma como servicio) y SaaS (software como servicio). Azure Security Center le brinda las herramientas necesarias para fortalecer la red, proteger los servicios y garantizar que tiene la mejor posici\u00f3n de seguridad. Azure Security Center admite m\u00e1quinas virtuales y servidores en diferentes tipos de entornos h\u00edbridos: Solo Azure Azure y entorno local Azure y otras nubes Azure, otras nubes y entorno local Azure Information Protection (AIP) es una soluci\u00f3n basada en la nube que permite a las organizaciones clasificar y proteger documentos y correos electr\u00f3nicos mediante etiquetas. Se pueden aplicar etiquetas: Autom\u00e1ticamente por los administradores mediante reglas y condiciones. Manualmente por los usuarios. Mediante una combinaci\u00f3n en la que los administradores definen las recomendaciones que se muestran a los usuarios. Las etiquetas pueden clasificar y, opcionalmente, proteger el documento, con lo que puede hacer lo siguiente: Seguir y controlar el modo en que se usa el contenido. Analizar los flujos de datos para obtener una visi\u00f3n general de su negocio : detectar comportamientos de riesgo y tomar medidas correctivas. Hacer un seguimiento del acceso a los documentos e impedir la p\u00e9rdida de datos o su uso indebido. Use Azure Information Protection para aplicar la clasificaci\u00f3n a etiquetas en documentos y correos electr\u00f3nicos. Estas son algunas de las funciones de etiquetar contenido: Clasificaci\u00f3n que se puede detectar independientemente de d\u00f3nde se almacenen los datos o con qui\u00e9n se compartan. Distintivos visuales, como encabezados, pies de p\u00e1gina o marcas de agua. Metadatos que se agregan a los archivos y encabezados de correo electr\u00f3nico en texto no cifrado. Los metadatos de texto no cifrado garantizan que otros servicios puedan identificar la clasificaci\u00f3n y tomar las medidas adecuadas. Microsoft Azure Sentinel es una soluci\u00f3n de administraci\u00f3n de eventos de informaci\u00f3n de seguridad (SIEM) y respuesta automatizada de orquestaci\u00f3n de seguridad (SOAR) que es escalable y nativa de la nube. Azure Sentinel ofrece an\u00e1lisis de seguridad inteligente e inteligencia frente a amenazas en toda la empresa, de forma que proporciona una \u00fanica soluci\u00f3n para la detecci\u00f3n de alertas, la visibilidad de amenazas, la b\u00fasqueda proactiva y la respuesta a amenazas. Azure Sentinel permite obtener una vista general de toda la empresa, lo que suaviza la tensi\u00f3n de ataques cada vez m\u00e1s sofisticados, vol\u00famenes de alertas cada vez mayores y plazos de resoluci\u00f3n largos. Recopile datos a escala de nube de todos los usuarios, dispositivos, aplicaciones y de toda la infraestructura, tanto en el entorno local como en diversas nubes. Detecte amenazas que antes no se detectaban y reduzca los falsos positivos mediante el an\u00e1lisis y la inteligencia de amenazas sin precedentes de Microsoft. Investigue amenazas con inteligencia artificial y busque actividades sospechosas a escala, aprovechando el trabajo de ciberseguridad que ha llevado a cabo Microsoft durante d\u00e9cadas. Responda a los incidentes con rapidez con la orquestaci\u00f3n y la automatizaci\u00f3n de tareas comunes integradas.","title":"Azure"},{"location":"azure/#azure","text":"","title":"AZURE"},{"location":"azure/#que-es","text":"Microsoft Azure es un servicio de Inform\u00e1tica en la Nube creado por Microsoft para construir, testear, desplegar y gestionar aplicaciones y servicios a trav\u00e9s de centros de datos gestionados por Microsoft. Microsoft Azure, como otros proveedores de nube, nos permite alquilar recursos como espacio de almacenamiento o ciclos de CPU en equipos f\u00edsicos que no debo administrar. Solo se paga por lo que usa (o al menos se mide). Los servicios inform\u00e1ticos ofrecidos suelen variar en funci\u00f3n de cada proveedor. Normalmente estos servicios incluyen: Potencia de proceso: por ejemplo, aplicaciones web o servidores Linux. Almacenamiento: por ejemplo, archivos y bases de datos. Redes: por ejemplo, conexiones seguras entre el proveedor de nube y la empresa.","title":"QUE ES"},{"location":"azure/#potencia-de-proceso","text":"Cuando hacemos virtualmente cualquier acci\u00f3n en internet, como pagar una factura online, leer un peri\u00f3dico y enviar un correo electr\u00f3nico, estamos interactuando con servidores de nube que procesan cada solicitud y devuelven una respuesta. Todo esto requiere de c\u00f3mputo. Maquina virtual Contenedores Serverless o informatica sin servidor","title":"Potencia de proceso"},{"location":"azure/#almacenamiento","text":"La mayor\u00eda de las aplicaciones leen y escriben datos. Y en este sentido, el tipo de datos y c\u00f3mo se almacenan puede ser diferente seg\u00fan el tipo de aplicaci\u00f3n, la necesidad y velocidad requerida. Los proveedores de nube suelen ofrecer soluciones de almacenamiento para m\u00e1quinas virtuales, aplicaciones web, bases de datos, archivos de datos y anal\u00edtica. Por ejemplo, si quiere almacenar texto o un clip de pel\u00edcula, podr\u00eda usar un archivo en disco. Si tuviera un conjunto de relaciones (por ejemplo, una libreta de direcciones), podr\u00eda decidirse por un enfoque m\u00e1s estructurado, como usar una base de datos. La ventaja de utilizar almacenamiento basado en nube, es que no debemos preocuparnos por el escalado. Si se necesita m\u00e1s espacio, se puede agregar pagando un poco m\u00e1s de precio, e inclusive si las necesidades de almacenamiento bajan, tambi\u00e9n bajar\u00e1 el precio asociado.","title":"Almacenamiento"},{"location":"azure/#redes","text":"En todos estos casos, las redes cobran una importancia vital. Los proveedores de nube suelen tener servicios de redes que nos permiten: + Crear y configurar Redes Virtuales. + Crear y conectar de extremo a extremo redes en la nube con una infraestructura local (conocidas como site-to-site, y point-to-site). + Parametrizar reglas de acceso a recursos. + Monitorear tr\u00e1fico de redes. + Aplicar reglas, restricciones y protecciones a las comunicaciones.","title":"Redes"},{"location":"azure/#crear-cuenta","text":"Web azure A trav\u00e9s de azure.com: es la forma m\u00e1s r\u00e1pida y f\u00e1cil que tienen las organizaciones de todos los tama\u00f1os para empezar a usar Azure. Puede administrar las implementaciones y el uso de Azure, como as\u00ed tambi\u00e9n obtener una factura mensual de Microsoft por los servicios usados. Con la ayuda de un Partner de Microsoft. Es un modelo para obtener facturaci\u00f3n local en tu pa\u00eds. De esta manera, Azure se brindar\u00e1 como servicio administrado a trav\u00e9s de un partner, qui\u00e9n te proporcionar\u00e1 el acceso y la facturaci\u00f3n, junto con un soporte t\u00e9cnico b\u00e1sico. A trav\u00e9s de un representante directo de Microsoft, opci\u00f3n pensada para organizaciones de gran tama\u00f1o o clientes que ya trabajan con la marca. A diferencia de azure.com (que requiere tarjeta de cr\u00e9dito), esto habilitar\u00e1 un tipo de contrato especial con varias ventajas al momento de necesitar varias suscripciones. Los servicios de Azure est\u00e1n disponibles a trav\u00e9s de Centros de Datos gestionados por Microsoft. Los mismos est\u00e1n conformados por edificios. xisten +60 regiones anunciadas en todo el mundo, y muchas que est\u00e1n anunciadas como adicionales futuras. Esto representa una presencia f\u00edsica en 140 pa\u00edses. En el mapa podr\u00e1s ver la ubicaci\u00f3n de los centros de datos, a excepci\u00f3n de 3 correspondientes a gobierno por lo cual su ubicaci\u00f3n es secreta. SLA significa en ingl\u00e9s \u201cservice level agreement\u201d, y en espa\u00f1ol \u201cacuerdo de nivel de servicio\u201d. Es un acuerdo escrito entre un proveedor de servicio y su cliente con objeto de fijar el nivel acordado para la calidad de dicho servicio. Este nivel puede ser un porcentaje que representa la disponibilidad m\u00ednima","title":"CREAR CUENTA"},{"location":"azure/#principio-5-3-2","text":"La inform\u00e1tica en la nube es un metodo de gestion de recursos de IT donde los usuarios acceden a los recursos virtuales de computo, red y almacenamiento que estan disponibles online. Estos recursos se pueden aprovionar de manera instantanea y elastica. Se compone de: 5 Caracteristicas 3 metodos de entrega 2 modelos de implementacion","title":"PRINCIPIO 5-3-2"},{"location":"azure/#caracteristicas","text":"Autoservicio y bajo demanda: un consumidor puede provisionarse de caracteristicas como tiempo de uso, almacenamiento, memoria... Acceso amplio y ubicuo: los recursos pueden ser accecidos desde cualquier lugar y cualquier dispositivo. Ubicacion transparente y agrupacion de recursos: suelen estar en diferentes localizaciones sobre distintos recursos fisicos o virtuales que son dinamicamente asignados. Elasticidad rapida (estirarse y contraerse): pueden aumentar en epocas de mucha carga asi como reducirlo cuando no se use. Servicio medido (e incluso pago por uso): recursos y capacidades segun lo que necesitas.","title":"Caracteristicas"},{"location":"azure/#metodos-de-entrega","text":"IaaS(Infraestructura como Servicio): Cliente tiene capacidad de utilizar almacenamiento, red, recursos sofware, SO, app. No tiene el control sobre la infraestructura pero sino tiene el control del resto apartir del SO. COntrol limitado sobre red como el firewall Ejemplo serian las maquinas virtuales PaaS(Plataforma como Servicio): Podemos desplegar apps propias o de terceros Control sobre las apps y la configuracion de ellas Ejemplo seria servicios hosting Saas(Software como Servicio): Capacidad de usar aplicaciones en una infraestructura de nube que cumple con las 5 caracteristicas No tenemos control sobre ningun componente, solo lo usamos. Ejemplo seria Office 365, exchange, gmail, yahoo, google apps.","title":"Metodos de entrega"},{"location":"azure/#modelos-de-implementacion","text":"Nube Privada: en mi propio centro de datos Nube P\u00fablica: en azure o otros proveedores de servicios Puede haber la mezcla con Nube Hibrida.","title":"Modelos de implementaci\u00f3n"},{"location":"azure/#maquinas-virtuales","text":"Tipo de recurso escalable por Azure Se tiene control total sobre la configuraci\u00f3n y se puede instalar de todo No es necesario comprar hardware fisico para escalar o ampliar Azure tiene servicios para supervisar, proteger y administrar las actualizaciones y revisiones del sistema operativo Soy responsable de: Mantener el SO y sus actualizaciones Trabar sobre la performance Monitorear el espacio de disco usado Componentes: Disco virtual: el disco es el que tendr\u00e1, por ejemplo, el sistema operativo instalado. Gracias al disco virtual puedo iniciar el equipo y guardar informaci\u00f3n en forma persistente Placa de red virtual: al igual que en un equipo f\u00edsico, es la que me facilitar\u00e1 la conexi\u00f3n con una o m\u00e1s redes. Direcciones IP: gracias a la cual podr\u00e9 conectarme al equipo virtual. Estas direcciones IP pueden ser privadas y p\u00fablicas. Grupos de seguridad de red: que nos ayudar\u00e1n a definir desde qu\u00e9 origenes me puedo conectar, y hacia qu\u00e9 destinos puedo acceder, teniendo en cuenta protocolos, puertos, etc. Los Network Security Groups son una manera \u00e1gil de gestionar los permisos de red, para una o m\u00e1s m\u00e1quinas. Configurar: puedo el nombre de la MV, el SO y el tama\u00f1o. Tiene al menos dos discos, uno para el SO y otro temporal para la memoria virtual. Spot Virtual: herramienta que lo que no se use se vaya ahi para ahorrar.","title":"MAQUINAS VIRTUALES"},{"location":"azure/#crear-mv","text":"Assignment Tasks A Ingresar al Portal de Azure. Crear un \"Grupo de Recursos\" [Resource Group] con el nombre \"azf-vms-1\" Completed on 4 noviembre, 2020 7:01 pm B Dentro del Resource Group, seleccionar la opci\u00f3n \"Crear recurso\" [Create resources]. Seleccionar el grupo \"C\u00f3mputo\" [Compute] y de la lista \"M\u00e1quina Virtual\" [Virtual Machine]. Completed on 4 noviembre, 2020 7:01 pm C En el asistente de creaci\u00f3n, validar que la suscripci\u00f3n seleccionada sea la correcta (probablemente sea \"FREE TRIAL\") y el Resource Group seleccionado es el correcto: \"azf-vms-1\". Completed on 4 noviembre, 2020 7:01 pm D Ingresar un nombre para la m\u00e1quina virtual, por ejemplo \"azf-vm-windows-2019\". Completed on 4 noviembre, 2020 7:01 pm E Seleccionar una regi\u00f3n de Azure. Por ejemplo \"Este de Estados Unidos\" [East US], una imagen \"WIndows Server 2019 Datacenter\" y un tama\u00f1o de m\u00e1quina virtual (explorar todas las im\u00e1genes y elegir un tama\u00f1o como \"B1ms\" (es un equipo barato para este ejercicio). Completed on 4 noviembre, 2020 7:01 pm F Ingresar un usuario [Username] y una contrase\u00f1a dos veces [Password]. \u00a1No olvidarlas! Completed on 4 noviembre, 2020 7:01 pm G Seleccionar el puerto de entrada [Inbound port] \"RDP (3389)\" para poder ingresar luego al equipo. Completed on 4 noviembre, 2020 7:02 pm H Ir al paso siguiente: \"Discos\" [Disks]. Ingresar un Disco de Datos adicional [Create new disk] del menor tama\u00f1o posible. Completed on 4 noviembre, 2020 7:06 pm I Ir al siguiente paso \"Redes\" [Networking]. Crear una nueva red con el nombre \"azf-vnet-1\" con el espacio de direcciones \"10.0.0.0/16\" y crear una subnet con el nombre \"Sub1\" y el espacio de direcciones \"10.0.0.0/24\". Validar que una vez creada la red, est\u00e9 seleccionada en \"Virtual Network\" y \"Subnet\" en el asistente del equipo virtual. Completed on 4 noviembre, 2020 7:11 pm J Crear una IP p\u00fablica con el nombre \"azf-ip-1\", el \"SKU Basic\" y asignaci\u00f3n \"Static\". Completed on 4 noviembre, 2020 7:11 pm K Seleccionar el grupo de seguridad de red [NIC network security group] en \"Basic\". Completed on 4 noviembre, 2020 7:11 pm L Validar que los puertos habilitados son solo \"RDP (3389)\". Completed on 4 noviembre, 2020 7:11 pm M Ir al siguiente paso \"Administraci\u00f3n\" [Management]. Completed on 4 noviembre, 2020 7:13 pm N Seleccionar en diagn\u00f3stico de booteo [Boot diagnostics] en \"On\". Esto requerir\u00e1 crear una cuenta de almacenamiento. Completed on 4 noviembre, 2020 7:16 pm O En el campo \"Cuenta de Almacenamiento de Diagn\u00f3stico\" [Diagnostics storage account] crear una nueva cuenta de almacenamiento con el nombre \"azfstorageXXXX\" donde XXXX es un n\u00famero aleatorio generado por ti (dado que los nombres de cuentas de almacenamiento deben ser \u00fanicos en todo Azure). El tipo de cuenta debe ser \"Storage (general purpose v1) y \"Locally-redundant storage (LRS). Completed on 4 noviembre, 2020 7:17 pm P Habilitar el apagado autom\u00e1tico [Auto-shutdown] y elegir un horario de apagado para tu zona geogr\u00e1fica. Completed on 4 noviembre, 2020 7:17 pm Q No modificar el resto de las opciones e ir al siguiente paso \"Avanzado\" [Advanced]. Completed on 4 noviembre, 2020 7:18 pm R No modificar ninguna opci\u00f3n e ir al siguiente paso \"Etiquetas\" [Tags]. Completed on 4 noviembre, 2020 7:18 pm S Ir al \u00faltimo paso \"Revisi\u00f3n y Creaci\u00f3n\" [Review + create]. Cuando pase todas las validaciones, revisar el resumen de opciones seleccionadas que coincidan con lo solicitado y crear la m\u00e1quina. Completed on 4 noviembre, 2020 7:19 pm T Cuando finalice la creaci\u00f3n, ir al equipo virtual y seleccionar \"Conectar\" [Connect] y elegir \"RDP\". Se descargar\u00e1 un archivo, y desde el cliente de Escritorio Remoto de tu computadora conectarse. U Ingresar el nombre de usuario y contrase\u00f1a que ingresamos en pasos anteriores, y comprobar que nos podemos conectar al equipo. V Ir al Grupo de Recursos [Resource Group] que hemos creado y comprobar que todos los recursos (m\u00e1quina virtual, discos, placas de red) est\u00e1n creados.","title":"CREAR MV"},{"location":"azure/#herramientas-azure","text":"Azure Portal para interactuar con Azure a trav\u00e9s de una interfaz gr\u00e1fica de usuario (GUI). Azure PowerShell y la interfaz de la l\u00ednea de comandos de Azure (CLI) para las interacciones con Azure de l\u00ednea de comandos y basadas en automatizaci\u00f3n. Azure Cloud Shell para una interfaz de l\u00ednea de comandos basada en web. Azure Mobile App para supervisar y administrar los recursos desde el dispositivo m\u00f3vil.","title":"HERRAMIENTAS AZURE"},{"location":"azure/#azure-powershell","text":"Instamos la herramienta AzurePowershell , apartir de la versi\u00f3n 7 es multiplataforma. linux # Register the Microsoft signature key sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # Register the Microsoft RedHat repository curl https://packages.microsoft.com/config/rhel/7/prod.repo | sudo tee /etc/yum.repos.d/microsoft.repo # Update the list of products sudo dnf check-update # Install a system component sudo dnf install compat-openssl10 # Install PowerShell sudo dnf install -y powershell # Start PowerShell pwsh Instalamos el modulo de AZ powershell: pwsh Install-Module -Name Az Nos conectamos con nuestra cuenta yendo al link que nos indica: PS /home/isx46410800/Documents> Connect-AzAccount Orden de listar los Resources Groups Get-AzResourceGroup : PS /home/isx46410800/Documents> Get-AzResourceGroup ResourceGroupName : NetworkWatcherRG Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/NetworkWatcherRG # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 Crear un Resource Group por comando New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" : PS /home/isx46410800/Documents> New-AzResourceGroup -Name \"azf-rgexmaple-rg\" -Location \"EastUs\" ResourceGroupName : azf-rgexmaple-rg Location : eastus ProvisioningState : Succeeded Tags : ResourceId : /subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-rgexmaple-rg # PS /home/isx46410800/Documents> Get-AzResourceGroup | Format-Table -AutoSize ResourceGroupName Location ProvisioningState Tags TagsTable ResourceId ----------------- -------- ----------------- ---- --------- ---------- NetworkWatcherRG eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026 azf-rgexmaple-rg eastus Succeeded /subscriptions/6ff39ded-c781-4b20-9f5b-079a\u2026","title":"AZURE POWERSHELL"},{"location":"azure/#azure-cli","text":"Documentacion CLI Instalaci\u00f3n linux: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc # sudo sh -c 'echo -e \"[azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/azure-cli.repo' # sudo yum install azure-cli Iniciamos sesion haciendo login: az login Listamos los resource groups con az group list --output table : Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded Creamos un resource group con az group create --location \"eastus\" --name \"azf-cli-rg\" : { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cli-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cli-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } Name Location Status ---------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded","title":"AZURE CLI"},{"location":"azure/#azure-cloud-shell","text":"Es un shell a trav\u00e9s del navegador desde cualquier sistema operativo. Necesitamos crear un storage y file share para utilizarlo. Ahora dentro podemos usar las mismas ordenes que cli por ejemplo y ver/crear resource groups como ejemplo: miguel@Azure:~$ az group create --location \"eastus\" --name \"azf-cloudshell-example-rg\" { \"id\": \"/subscriptions/6ff39ded-c781-4b20-9f5b-079a7ef44081/resourceGroups/azf-cloudshell-example-rg\", \"location\": \"eastus\", \"managedBy\": null, \"name\": \"azf-cloudshell-example-rg\", \"properties\": { \"provisioningState\": \"Succeeded\" }, \"tags\": null, \"type\": \"Microsoft.Resources/resourceGroups\" } miguel@Azure:~$ az group list --output table Name Location Status ------------------------- ---------- --------- NetworkWatcherRG eastus Succeeded azf-rgexmaple-rg eastus Succeeded azf-cli-rg eastus Succeeded azf-cloudshell-rg eastus Succeeded azf-cloudshell-example-rg eastus Succeeded Podemos verlo en pantalla completa con www.shell.azure.com","title":"AZURE CLOUD SHELL"},{"location":"azure/#ejemplo-de-crear-mv-por-script","text":"Crear una MV con CLI a trav\u00e9s de cloud shell: # create a resource group az group create --name azf-crear-mv-cli --location eastus # Create a virtual network net and subnet. az network vnet create --resource-group azf-cloudshell-example-rg --name azf-vnet-1 --address-prefix 10.0.0.0/16 --subnet-name sub1 --subnet-prefix 10.0.0.0/24 # Create a public IP address. az network public-ip create --name myPublicCliIP --resource-group azf-cloudshell-example-rg --sku Basic --allocation-method Static # create vm customized az vm create --subscription \"Evaluaci\u00f3n gratuita\" -g azf-cloudshell-example-rg --name vm-cli-example --image win2019datacenter --location eastus --size Standard_B1ms --admin-username vmadmin --admin-password mi--GU--el14 --vnet-name azf-vnet-1 --subnet sub1 # add new optional disk az vm disk attach -g azf-cloudshell-example-rg --vm-name vm-cli-example --name myDataDisk --new --size-gb 4 # input port to connect VM az vm open-port --port 3389 --resource-group azf-cloudshell-example-rg --name vm-cli-example # habilitar el auto shutdown az vm auto-shutdown -g azf-cloudshell-example-rg -n vm-cli-example --time 0200 # borrar recurso az group delete --name azf-cloudshell-example-rg","title":"EJEMPLO DE CREAR MV POR SCRIPT"},{"location":"azure/#azure-app-service","text":"Azure App Service es un servicio de alojamiento web totalmente administrado que permite crear aplicaciones web, back-ends m\u00f3viles y API RESTful. Desde sitios web peque\u00f1os hasta aplicaciones web con una escala global, existen opciones de precios y rendimiento que se adaptan a todas las necesidades. Azure App Service permite desarrollar software en el lenguaje preferido, ya sea. NET, .NET Core, Java, Ruby, Node.js, PHP o Python. Las aplicaciones se ejecutan y escalan f\u00e1cilmente en los entornos basados tanto en Windows como en Linux. Azure App Service no solo agrega a la aplicaci\u00f3n la funcionalidad de Microsoft Azure, como la seguridad, el equilibrio de carga, el escalado autom\u00e1tico y la administraci\u00f3n automatizada. Tambi\u00e9n puede sacar partido de las funcionalidades de DevOps, por ejemplo, la implementaci\u00f3n continua desde Azure DevOps, GitHub, Docker Hub y otros or\u00edgenes, la administraci\u00f3n de paquetes, entornos de ensayo, dominio personalizado y certificados TLS/SSL. Con Azure App Service se paga por los recursos de proceso de Azure que se utilizan. Los recursos de proceso que usa se determinan mediante el plan de App Service en el que ejecuta las aplicaciones, y determina el costo del servicio.","title":"AZURE APP SERVICE"},{"location":"azure/#crear-una-web-app","text":"Home - Crear recurso - Web App Configuraci\u00f3n inicial: A Crear una nueva Web App (elegir del men\u00fa destacados seg\u00fan indica el video). Completed on 6 noviembre, 2020 11:55 pm B Elegir la suscripci\u00f3n, crear un resource group desde el mismo asistente, y completar un nombre: por ejemplo \"azf-webapp-codexxx\" donde xxx son 3 n\u00fameros aleatorios, tanto para la web app como para el resource group. No olvides este RG que lo utilizaremos m\u00e1s adelante. Completed on 6 noviembre, 2020 11:55 pm C Seleccionar la forma de publicaci\u00f3n [Publish] \"Code\". Elegir el Runtime stack .NET Core 3.1 (LTS) y la plataforma Windows. Completed on 6 noviembre, 2020 11:55 pm D Elegir la regi\u00f3n que Azure propone. Se puede seguir el del ejemplo: Central US. Completed on 6 noviembre, 2020 11:55 pm E Crear un Windows Plan con el nombre: \"azf-webapp-code-plan\" y el tama\u00f1o Standard S1 (no te preocupes por el costo, en breve lo eliminaremos). F En las opciones de monitoreo, NO habilitaremos Application Insights. Completed on 6 noviembre, 2020 11:56 pm G No completaremos nada en los tags. H Revisaremos y crearemos la Web App. I Comprobar que desde la URL en la p\u00e1gina de Overview que la web app funciona correctamente. Veremos una URL donde tenemos nuestra web app","title":"Crear una Web App"},{"location":"azure/#services-y-plans","text":"El plan es el hardware del que voy a disponer. El service es la app o conjunto de apps dentro del plan. En App Service, cada aplicaci\u00f3n se ejecuta en un Plan de App Service. En forma b\u00e1sica, un App Service Plan define un conjunto de recursos de proceso para que una aplicaci\u00f3n web se ejecute. Estos recursos de proceso son an\u00e1logos a la granja de servidores de un hospedaje web convencional. Cuando se crea un plan de App Service en una regi\u00f3n determinada (por ejemplo, Oeste de Europa), se crea un conjunto de recursos de proceso para ese plan en dicha regi\u00f3n. Todas las aplicaciones que coloque en este plan de App Service se ejecutan en estos recursos de proceso seg\u00fan lo definido por el plan de App Service. Cada plan de App Service define: Regi\u00f3n (oeste de EE. UU., este de EE. UU., etc.). N\u00famero de instancias posibles de VM (si, por detr\u00e1s hay VMs). Tama\u00f1o de las instancias de VM (peque\u00f1o, mediano, grande). Plan de tarifa (Gratis, Compartido, B\u00e1sico, Est\u00e1ndar, Premium, PremiumV2 y Aislado). Si bien inicialmente puede resultar algo confuso, cuando en Azure veamos el t\u00e9rmino \u201cApp Service\u201d nos referimos a las Web Apps, API Apps & Mobile Apps. Cuando veamos el t\u00e9rmino \u201cApp Service Plans\u201d nos referimos expl\u00edcitamente a los planes. La relaci\u00f3n entre un App Service Plan un un App Service es una relaci\u00f3n 1:muchos :-). Una App Service Plan puede contener muchos App Services, mientras que un App Service s\u00f3lo puede estar dentro de un App Service Plan. En todos los casos, cuando contratamos un App Service Plan \u201cBasic\u201d, \u201cStandard\u201d, \u201cPremium\u201d e \u201cIsolado\u201d, s\u00f3lo pagamos por el tama\u00f1o del plan que contratemos (scale up) y la cantidad de instancias con las que escalemos (scale out, predeterminadamente configurada en 1): Aunque tengamos muchos App Services (Web Apps, API Apps & Mobile Apps) dentro del mismo plan, s\u00f3lo pagaremos por el tama\u00f1o y cantidad de instancias del App Service Plan, y no por cada una de las Apps que tenga dentro. Por supuesto, como el App Service Plan determina un hardware asociado, tendremos un l\u00edmite. Si ponemos muchos sitios web dentro que consumen muchos recursos y/o tienen demasiado tr\u00e1fico, nuestro App Service Plan comenzar\u00e1 a arrojar errores por no disponibilidad de servicio. Este punto no es menor, y debe ser planeado en el dise\u00f1o de la soluci\u00f3n completa de Azure.","title":"Services y Plans"},{"location":"azure/#crear-una-web-app-container","text":"la creaci\u00f3n de una Web App for Containers. Esto significa, que nuestra Web App estar\u00e1 en un contenedor de ejemplo en Microsoft Azure. Recordemos que Azure App Service permite generar diversas aplicaciones: Web App. API App. Mobile App. En escencia, son similares, si bien encontraremos en cada una de ellas particularidades propias. La m\u00e1s simple es una Web App, y es justamente en este ejercicio la que generaremos. A diferencia del anterior donde tambi\u00e9n generamos una Web App, esta vez lo haremos utilizando Docker. Configuraci\u00f3n inicial: A Crear un nuevo recurso de tipo \"Web App\" que esta en la lista de populares. B Crear un grupo de recursos seg\u00fan el gusto que tengas. Si elijes un nuevo grupo de recursos (diferente al ejercicio de creaci\u00f3n de Web App Simple) recuerda BORRAR este nuevo grupo de recursos al finalizar el ejercicio. En caso que sea el mismo, puedes dejarlo dado que m\u00e1s adelante borraremos todo. C Elegiremos el tipo de publicaci\u00f3n \"Docker Container\" y el sistema operativo \"Linux\". D Generaremos un nuevo App Service Plan y un tama\u00f1o de tipo \"S1\". E En la configuarci\u00f3n de \"Docker\" elegiremos \"Single Container\". F Con respecto al origen de la instancia elegiremos \"Quickstart\". La opci\u00f3n \"sample\" ser\u00e1 \"NGINX\". G No debemos habilitar nada de monitoreo, dado que no est\u00e1 soportado con contenedores. H Revisar y crear el recurso. I Comprobar por la URL del Web App que todo est\u00e1 funcionando como esperamos.","title":"Crear una Web App Container"},{"location":"azure/#crear-api-app","text":"Pasos: A Ingresar al Resource Group que hemos generado en la \"Creaci\u00f3n de una Web App Simple\". Deber\u00eda tener el formato \"azf-webapp-codexxx\" con 3 n\u00fameros elegidos por ti. B Creamos un nuevo recurso y buscamos en la barra de b\u00fasqueda \"API App\". C Ingresaremos un nombre para la app, respetando el formato \"azf-apiapp-codexxx\" ingresando los 3 mismos n\u00fameros aleatorios puestos para el resource group y la web app anterior. D El resource group ya deber\u00eda estar seleccionado (es el que ya existe). E En la opci\u00f3n de \"App Service Plan\" debemos elegir el que ya existe porque lo generamos antes. Su nombre deber\u00eda ser \"azf-webapp-code-plan\". F Le damos un clic en \"Create\". G Una vez que se genere, vamos a comprobar que el sitio funciona bien. H Comprobar que en el listado de Apps en el \"App Service Plan\" ahora tenemos m\u00e1s items. En el Deployment center o en el centro de despliegue podemos configurar archivos o repositorios de donde coger nuestro c\u00f3digo fuente para la aplicaci\u00f3n web. https://docs.microsoft.com/es-mx/learn/modules/create-publish-webapp-app-service-vs-code/3-exercise-create-web-application-vs-code?pivots=pythonflask","title":"Crear API App"},{"location":"azure/#almacenamiento-azure","text":"La plataforma de Azure Storage es la soluci\u00f3n de almacenamiento en la nube de Microsoft para los escenarios modernos de almacenamiento de datos. Los servicios principales de almacenamiento ofrecen un almac\u00e9n de objetos escalable de forma masiva para objetos de datos, un almacenamiento en disco para m\u00e1quinas virtuales (VM) de Azure, un servicio de sistema de archivos para la nube, un almac\u00e9n de mensajes para mensajer\u00eda confiable y un almac\u00e9n NoSQL. Los servicios principales y considerados primitivos de Azure Storage son: Blobs de Azure [Azure Blobs]: con opciones de blobs en bloques, anexos y p\u00e1ginas. Archivos de Azure [Azure Files]: recursos compartidos para uso local y en la nube. Colas de Azure [Azure Queues]: almac\u00e9n de mensajer\u00eda simple. Tablas de Azure [Azure Tables]: almac\u00e9n NoSQL sin esquema para datos estructurados. Adem\u00e1s, consideraremos el servicio de Azure Disks, un servicio totalmente administrado para los discos de nuestras m\u00e1quinas virtuales: Discos de Azure [Azure Disks]: conocidos como \u201cdiscos administrados\u201d.","title":"ALMACENAMIENTO AZURE"},{"location":"azure/#azure-disks","text":"Los discos administrados de Azure son vol\u00famenes de almacenamiento de nivel de bloque administrados por Azure y utilizados con Azure Virtual Machines. Los discos administrados son como un disco f\u00edsico en un servidor local, pero virtualizados. Con los discos administrados, todo lo que deber\u00e1s hacer es especificar el tama\u00f1o del disco, el tipo de disco y aprovisionar el disco. Una vez que aprovisiona el disco, Azure se encarga del resto. Los tipos de discos disponibles son ultra, unidades de estado s\u00f3lido (SSD) premium, SSD est\u00e1ndar y unidades de disco duro est\u00e1ndar (HDD).","title":"Azure disks"},{"location":"azure/#blob-storage","text":"Azure Blob Storage es la soluci\u00f3n de almacenamiento de objetos de Microsoft para la nube. Blob Storage est\u00e1 optimizado para el almacenamiento de cantidades masivas de datos no estructurados. En la primera lecci\u00f3n de esta secci\u00f3n, ya hemos aprendido sobre los diversos tipos de datos, entre ellos los no estructurados. Blob Storage est\u00e1 dise\u00f1ado para: Servicio de im\u00e1genes o documentos directamente a un explorador. Almacenamiento de archivos para acceso distribuido. Streaming de audio y v\u00eddeo. Escribir en archivos de registro. Almacenamiento de datos para copia de seguridad y restauraci\u00f3n, recuperaci\u00f3n ante desastres y archivado. Almacenamiento de datos para el an\u00e1lisis en local o en un servicio hospedado de Azure.","title":"Blob storage"},{"location":"azure/#azure-files","text":"Archivos de Azure (Azure Files) ofrece recursos compartidos de archivos en la nube totalmente administrados, a los que se puede acceder mediante el protocolo SMB (Bloque de mensajes del servidor) est\u00e1ndar. Los recursos compartidos de Azure Files se pueden montar simult\u00e1neamente en implementaciones de Windows, Linux y macOS en la nube o locales. Adem\u00e1s, los recursos compartidos de archivos de Azure Files se pueden almacenar en la cach\u00e9 de los servidores de Windows Server con Azure File Sync, lo que permite un acceso r\u00e1pido all\u00ed donde se utilizan los datos. Se crea un file share dentro de mi cuenta de storage azure. Se sube archivos y nos podemos conectar al file share a trav\u00e9s de un script que te da en las opciones de azure. Ahora ya tendemos el recursos de red la nueva unidad compartida y se actualiza todo. Tambien podemos hacer un azure file sync que es como un server en la nube que se actualiza en mi ordenador, azure y nube. Nos tendremos que descargar el file sync de azure despues de crear un server y un group sync y funcionar\u00e1 como lo anterior.","title":"Azure files"},{"location":"azure/#colas-de-azure","text":"Azure Queue Storage es un servicio para almacenar grandes cantidades de mensajes, a los que se puede acceder desde cualquier lugar del mundo a trav\u00e9s de llamadas autenticadas mediante HTTP o HTTPS. Un mensaje de la cola puede llegar a tener hasta 64 KB. Una cola puede contener millones de mensajes, hasta el l\u00edmite de capacidad total de una cuenta de almacenamiento. Las colas se utilizan normalmente para crear un trabajo pendiente del trabajo que se va a procesar de forma asincr\u00f3nica.","title":"Colas de azure"},{"location":"azure/#azure-storage-explorer","text":"Azure Storage Explorer es una herramienta gratuita para administrar f\u00e1cilmente sus recursos de almacenamiento en la nube de Azure en cualquier parte, desde Windows, macOS o Linux. Permite cargar, descargar y administrar blobs, archivos, colas y tablas de Azure, as\u00ed como entidades de Azure Cosmos DB y Azure Data Lake Storage. Permite acceder f\u00e1cilmente a los discos de las m\u00e1quinas virtuales y trabajar con Azure Resource Manager o con cuentas de almacenamiento cl\u00e1sicas. Asimismo, administrar y configurar reglas de uso compartido de recursos entre or\u00edgenes.","title":"Azure Storage Explorer"},{"location":"azure/#azure-functions","text":"Azure Functions permite ejecutar peque\u00f1os fragmentos de c\u00f3digo (denominados \u201cfunciones\u201d) sin preocuparse por el resto de la infraestructura de la aplicaci\u00f3n. Es ideal para tareas espec\u00edficas, dado que simplifica la necesidad de generar c\u00f3digo reduci\u00e9ndolo s\u00f3lo a la parte \u201cl\u00f3gica\u201d que necesitamos. Imaginemos que necesitamos generar una tarea donde, cada vez que se escribe un nuevo archivo en un Storage Account de Azure, se dispare una funci\u00f3n que guarde en una base de datos registro de dicho archivo. Si pensamos en hacer esto desde cero, probablemente nos requiera: Generar un proyecto con un IDE y un Framework / Lenguaje espec\u00edfico. Construir un servicio y alojarlo en alg\u00fan lugar. Resolver otros aspectos propios del proyecto, donde alojarlo, c\u00f3mo hacer que se dispare cuando se graba un archivo nuevo en el Storage Account, etc. Los desencadenadores son lo que provocan que una funci\u00f3n se ejecute. Un desencadenador define c\u00f3mo se invoca una funci\u00f3n y cada funci\u00f3n debe tener exactamente un desencadenador. Los desencadenadores tienen datos asociados, que a menudo son la carga de la funci\u00f3n. El enlace a una funci\u00f3n es una manera de conectar otro recurso a la funci\u00f3n mediante declaraci\u00f3n. Los enlaces pueden estar conectados como enlaces de entrada, enlaces de salida o ambos. Los datos de los enlaces se proporcionan a la funci\u00f3n como par\u00e1metros.","title":"AZURE FUNCTIONS"},{"location":"azure/#azure-bases-de-datos","text":"Microsoft SQL es un motor de base de datos utilizado en forma global y conocida por casi todos. Vamos a conocer las opciones que tenemos de Microsoft SQL en Azure, ya sea como IaaS, PaaS e inclusive Serverless. SQL Server en Azure Virtual Machines nos permite usar versiones completas de SQL Server en la nube sin tener que administrar todo el hardware local. SQL Server en Azure Virtual Machines tambi\u00e9n simplifica los costos de licencia cuando se paga por uso. La galer\u00eda de im\u00e1genes de m\u00e1quina virtual le permite crear una m\u00e1quina virtual con SQL Server con la versi\u00f3n, la edici\u00f3n y el sistema operativo correctos. Esto hace que las m\u00e1quinas virtuales sean una buena opci\u00f3n para muchas cargas de trabajo de SQL Server diferentes. El recurso M\u00e1quinas virtuales SQL es un servicio de administraci\u00f3n independiente de la m\u00e1quina virtual y, de hecho, aparece como un m\u00f3dulo distinto. Cuando Azure detecta que un motor SQL Server est\u00e1 instalado dentro de una m\u00e1quina virtual, habilita el punto de administraci\u00f3n con opciones espec\u00edficas que el administrador de VMs deber\u00e1 comenzar a gestionar. Azure SQL Database es un motor de base de datos de tipo plataforma como servicio (PaaS) totalmente administrado por Microsoft, que se encarga de la mayor\u00eda de las funciones de administraci\u00f3n de bases de datos. \u00bfQu\u00e9 tipo de funciones de administraci\u00f3n est\u00e1n a cargo de Microsoft? Por ejemplo: actualizar el motor, aplicar revisiones, crear copias de seguridad, supervisar sin intervenci\u00f3n del usuario. Azure SQL Database se ejecuta siempre en la \u00faltima versi\u00f3n estable del motor de base de datos de SQL Server y en un sistema operativo revisado con el 99,99 % de disponibilidad. Las capacidades de PaaS que est\u00e1n integradas en Azure SQL Database permiten centrarse en las actividades de administraci\u00f3n y optimizaci\u00f3n de bases de datos espec\u00edficas del dominio que son cr\u00edticas para el negocio, y no en mantener la infraestructura. Se puede descargar el software de SSMS y conectar desde el pc a azure y gestionar las bbdd. Se pueden crear: Single databases(siempre ha de haber un server database) Elastic Pool databses(recursos compartidos y dentro varias ddbb) Managed instances(se crea en la nube las ddbb) Managed instances: Es un servicio de base de datos en la nube inteligente y escalable que combina la mayor compatibilidad con el motor de base de datos de SQL Server en una plataforma como servicio totalmente administrada por Microsoft. Tiene casi un 100 % de compatibilidad con el motor de base de datos m\u00e1s reciente de SQL Server (Enterprise Edition). Url para migrar bbdd a azure","title":"AZURE BASES DE DATOS"},{"location":"azure/#cosmos-db","text":"Azure Cosmos DB es un servicio de base de datos con varios modelos distribuido de forma global de Microsoft. En esta lecci\u00f3n vamos a repasar los conceptos de las bases de datos no-sql vs las relacionales, para poder ir entendiendo un poco m\u00e1s de qu\u00e9 se trata este servicio de base de datos de Microsoft. Azure Cosmos DB es un servicio de base de datos multimodelo distribuido y con escalado horizontal. Al ser multimodelo, admite de forma nativa modelos de datos de documentos, pares clave-valor, grafos y en columnas. Con respecto a la administraci\u00f3n e interfaz de comunicaci\u00f3n, Azure Cosmos DB permite acceder a sus datos con diferentes APIs: como SQL (documentos), MongoDB (documentos), Azure Table Storage (clave-valor), Gremlin (grafos) y Cassandra (en columnas). A nivel de funcionalidad, Azure Cosmos DB indexa datos autom\u00e1ticamente sin que haya que ocuparse de la administraci\u00f3n de esquemas ni de \u00edndices.","title":"COSMOS DB"},{"location":"azure/#creacion","text":"En un resource group a\u00f1adimos un cosmosDB. Para crear una cuenta de Azure Cosmos DB gratuita por 30 dias link Una vez creado el cosmosDB vamos a Data Explorer y creamos una db y despues un container dentro de esta db. Por ejemplo creamos un container con nombre personas y el key es /dni. Dentro de personas vamos creando nuevos items: { \"dni\": \"46410800C\", \"nombre\": \"Miguel\", \"apellidos\": \"Amor\u00f3s Moret\" } Tambien podemos hacer query: SELECT c.nombre, c.apellidos FROM c where c.dni = \"46410800C\"","title":"Creaci\u00f3n"},{"location":"azure/#balanceadores-azure","text":"En Microsoft Azure tenemos diversas opciones de balanceadores, tambi\u00e9n conocidos como equilibradores de carga. El t\u00e9rmino equilibrio de carga hace referencia a la distribuci\u00f3n de cargas de trabajo entre varios recursos de proceso. El equilibrio de carga busca optimizar el uso de recursos, maximizar el rendimiento, minimizar el tiempo de respuesta y evitar la sobrecarga de un solo recurso. Tambi\u00e9n puede mejorar la disponibilidad al compartir una carga de trabajo entre recursos de proceso redundantes. Global frente a regional + Los servicios de equilibrio de carga globales distribuyen el tr\u00e1fico en servidores de back-end regionales, nubes o servicios locales h\u00edbridos. Estos servicios enrutan el tr\u00e1fico del usuario final al servidor de back-end disponible m\u00e1s cercano. Tambi\u00e9n reaccionan a los cambios en la confiabilidad o el rendimiento del servicio, con el fin de maximizar la disponibilidad y el rendimiento. Puede pensar en ellos como sistemas que equilibran la carga entre los stamp, puntos de conexi\u00f3n o unidades de escalado de la aplicaci\u00f3n hospedados en diferentes regiones o zonas geogr\u00e1ficas. Los servicios de equilibrio de carga regionales distribuyen el tr\u00e1fico de las redes virtuales entre las m\u00e1quinas virtuales (VM) o puntos de conexi\u00f3n de servicio zonales y con redundancia de zona de una regi\u00f3n. Puede pensarlos como sistemas que equilibran la carga entre m\u00e1quinas virtuales, contenedores o cl\u00fasteres dentro de una regi\u00f3n en una red virtual. HTTP(S) frente a no HTTP(S) + Los servicios de equilibrio de cargas HTTP(S) son equilibradores de carga de capa 7 que solo aceptan el tr\u00e1fico HTTP(S). Est\u00e1n dise\u00f1ados para las aplicaciones web u otros puntos de conexi\u00f3n HTTP(S). Incluyen caracter\u00edsticas, como la descarga de SSL, el firewall de aplicaciones web, el equilibrio de carga basado en rutas de acceso y la afinidad de sesi\u00f3n. Los servicios de equilibrio de carga que no son HTTP/S pueden controlar el tr\u00e1fico que no es de HTTP(S) y se recomiendan para las cargas de trabajo que no son web. Servicios de Balanceo en Azure Load Balancer + Proporciona un servicio de equilibrio de carga de capa 4 con latencia baja y rendimiento alto (entrante y saliente) para todos los protocolos UDP y TCP. Se dise\u00f1\u00f3 para administrar millones de solicitudes por segundo, a la vez que garantiza que la soluci\u00f3n tiene una alta disponibilidad. Azure Load Balancer tiene redundancia de zona, lo que garantiza una alta disponibilidad en las instancias de Availability Zones. Traffic Manager + Es un equilibrador de carga de tr\u00e1fico basado en DNS que le permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Dado que Traffic Manager es un servicio de equilibrio de carga basado en DNS, solo equilibra la carga en el nivel del dominio. Por ese motivo, no puede conmutar por error tan r\u00e1pidamente como con Front Door, debido a los desaf\u00edos comunes relacionados con el almacenamiento en cach\u00e9 de DNS y a los sistemas que no respetan los TTL de DNS. Application Gateway + Proporciona un controlador de entrega de aplicaciones (ADC) como servicio, que ofrece diversas funcionalidades de equilibrio de carga de capa 7. \u00daselo para optimizar la productividad de las granjas de servidores web al traspasar la carga de la terminaci\u00f3n SSL con mayor actividad de la CPU a la puerta de enlace. Front Door + Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones.","title":"BALANCEADORES AZURE"},{"location":"azure/#ejercicio-balanceador","text":"Vamos a realizar todas las configuraciones necesarias para poder, luego, trabajar sobre un Azure Load Balancer. Estas configuraciones las consideramos pre-requisitos y contemplan: Ejecuci\u00f3n en un modelo de Infraestructura como C\u00f3digo de la creaci\u00f3n de m\u00e1quinas virtuales y otros componentes requeridos para el ejercicio. Son un total de 3 equipos virtuales que utilizan 1 VCore cada uno, todos utilizando un Availability Set (Conjunto de Disponibilidad) con 2 dominios de falla y 5 dominios de actualizaci\u00f3n. Instalaci\u00f3n y configuraci\u00f3n de Internet Information Services (IIS) con una p\u00e1gina web simple. Creaci\u00f3n y asignaci\u00f3n de un Network Security Group (NSG) \u00fanico para los 3 equipos virtuales. Creamos un template y hacemos deploy del siguiente codigo Nos conectamos remotamente al server1 e instalamos el rol IIS de server web. Modificamos el index.html y ponemos que saludamos desde server 1. Ahora en el resource group creamoe un network security group. Dentro creamos un inbound de http por el puerto 80. Ahora de cada network interface las asignamos a este creado y borramos la del template. Ahora si entramos a la Ip de la primera VM por el puerto 80 desde el navegador normal, vemos su index.html. Despues de cada interfaz de red, modificamos su ip configuration y le ponemos ip statica y ip publica desasociada. Despues creamos un recurso nuevo. un load balancer publico, basic y con una ip static. Despues entramos y creamos un backend en nuestra loadblancer con nuestras 3 virtual machines. Creamos un healthprobe para http. A\u00f1adimos un load rule. Ahora si ponemos en el navegador la ip del balanceador, nos ir\u00e1 responiendo dinamicamente, el servidor de respuesta.","title":"Ejercicio balanceador"},{"location":"azure/#azure-trafic-manager","text":"Azure Traffic Manager es un balanceador de tr\u00e1fico basado en DNS que permite distribuir el tr\u00e1fico de forma \u00f3ptima a servicios de regiones de Azure globales, al tiempo que proporciona una alta disponibilidad y capacidad de respuesta. Trabaja a nivel capa 7 de OSI, aunque s\u00f3lo a nivel DNS. Traffic Manager usa DNS para dirigir las solicitudes del cliente al punto de conexi\u00f3n de servicio m\u00e1s adecuado en funci\u00f3n de un m\u00e9todo de enrutamiento del tr\u00e1fico y el mantenimiento de los puntos de conexi\u00f3n. Un punto de conexi\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Es fundamental entender que Traffic Manager funciona a nivel de DNS. Traffic Manager usa DNS para dirigir a los clientes a puntos de conexi\u00f3n espec\u00edficos del servicio basados en las reglas del m\u00e9todo de enrutamiento de tr\u00e1fico. Los clientes se conectan directamenteal punto de conexi\u00f3n seleccionado. Traffic Manager no es un proxy ni una puerta de enlace. Traffic Manager no ve el tr\u00e1fico que circula entre el cliente y el servicio. EJERCICIO: Vamos a completar los pre-requisitos necesarios para poder llevar adelante el ejercicio de Azure Traffic Manager. Estos pre-requisitos son: 3 App Service Plans con sistema operativo Linux. 3 App Service (Web Apps) con runtime PHP 7.3. 1 VM con Windows creada en Estados Unidos (si no estas en USA) o en Brasil (si est\u00e1s en USA). Publicaci\u00f3n de la soluci\u00f3n PHP (index.php) en cada sitio web (Brasil, Estados Unidos y Asia). La deber\u00e1s descargar desde GitHub. # A Crear un Resource Group para Traffic Manager. Completed on 12 noviembre, 2020 8:04 pm B Crear Sitio Web en Brasil Sur con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). C Crear Sitio Web en Estados Unidos Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). D Crear Sitio Web en Asia Este con su App Service Plan (Linux + PHP \u00f3 .NET) con nombre \u00fanico (te sugerimos n\u00fameros aleatorios al final). E Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Sur de Brasil. F Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Estados Unidos. G Subir el contenido (PHP \u00f3 .NET) al Sitio Web del Este de Asia. H Crear la M\u00e1quina Virtual con Windows en Estados Unidos \u00f3 Brasil, seg\u00fan corresponda a tu ubicaci\u00f3n. Abrimos el ftp de cada uno y copiamos el index.php del github. Ahora creamos un Traffic Manager profile en nuestro grupo de recurso. Creamos 3 endpoints en mi traffic manager y mapeado por geolocalizaci\u00f3n. Segun petici\u00f3n donde estemos nos contesta una u otra","title":"Azure Trafic Manager"},{"location":"azure/#azure-aplication-gateway","text":"Azure Application Gateway es un equilibrador de carga de tr\u00e1fico web que permite administrar el tr\u00e1fico a las aplicaciones web. Los equilibradores de carga tradicionales operan en la capa de transporte (OSI capa 4: TCP y UDP) y enrutan el tr\u00e1fico en funci\u00f3n de la direcci\u00f3n IP y puerto de origen a una direcci\u00f3n IP y puerto de destino. Application Gateway puede tomar decisiones de enrutamiento basadas en atributos adicionales de una solicitud HTTP, por ejemplo los encabezados de host o la ruta de acceso del URI. A Paso inicial: confirmar que complet\u00e9 todos los requisitos (sitios web pre-requisitos de la lecci\u00f3n de Traffic Manager). B Crear un Resource Group para Application Gateway. C Crear un Application Gateway (SKU Standard_v2, Zonas 1 2 y 3 del Este de Estados Unidos, y escalado manual con 2 instancias iniciales). D Crear la Virtual Network con las opciones predeterminadas. E Crear un Front-End con una direcci\u00f3n de Front-End p\u00fablica. F Crear un Back-End Pool con los 3 sitios web configurados. G Crear un Routing Rule con Listener Basico. H Crear un HTTP Settings seg\u00fan indicaciones del Video. I Corregir HTTP Settings para evitar error en el estado de salud de los Websites de App Service. J Probar acceso por IP del Front-End (p\u00fablica) para validar que podamos ingresar al sitio web. Hacer F5 para validar que se cambia la p\u00e1gina de inicio seg\u00fan zona.","title":"Azure Aplication Gateway"},{"location":"azure/#azure-front-door-afd","text":"Es una red de entrega de aplicaciones que proporciona equilibrio de carga global y un servicio de aceleraci\u00f3n de sitios para las aplicaciones web. Ofrece funcionalidades de capa 7 para la aplicaci\u00f3n, como la descarga SSL, el enrutamiento basado en rutas, la conmutaci\u00f3n por error r\u00e1pida y el almacenamiento en cach\u00e9, entre otros, para mejorar el rendimiento y la alta disponibilidad de las aplicaciones. Azure Front Door permite definir, administrar y supervisar el enrutamiento global para el tr\u00e1fico web mediante la optimizaci\u00f3n para obtener el mejor rendimiento y la conmutaci\u00f3n por error global r\u00e1pida para alta disponibilidad. Front Door funciona en la capa 7 o la capa HTTP/HTTPS, y usa el protocolo de difusi\u00f3n por proximidad con divisi\u00f3n TCP y la red global de Microsoft para mejorar la conectividad global. Por tanto, seg\u00fan la selecci\u00f3n del m\u00e9todo de enrutamiento en la configuraci\u00f3n, puede asegurarse de que Front Door enruta las solicitudes de cliente al back-end de aplicaci\u00f3n m\u00e1s r\u00e1pido y disponible. Un back-end de aplicaci\u00f3n es cualquier servicio accesible desde Internet hospedado dentro o fuera de Azure. Front Door proporciona una serie de m\u00e9todos de enrutamiento del tr\u00e1fico y opciones de seguimiento de estado del back-end para satisfacer las distintas necesidades de las aplicaciones y los modelos de conmutaci\u00f3n autom\u00e1tica por error. Al igual que Traffic Manager, Front Door es resistente a errores, incluidos los que afectan a una regi\u00f3n completa de Azure.","title":"Azure Front Door (AFD)"},{"location":"azure/#contenedores","text":"los contenedores como unidades de despliegue. Los contenedores ofrecen las ventajas del aislamiento, la portabilidad, la agilidad, la escalabilidad y el control a lo largo de todo el flujo de trabajo del ciclo de vida de la aplicaci\u00f3n. La ventaja m\u00e1s importante es el aislamiento del entorno que se proporciona entre el desarrollo y las operaciones. Azure Container Registry es un servicio privado administrado del Registro de Docker que usa Docker Registry 2.0, que es de c\u00f3digo abierto. Cree y mantenga los registros de Azure Container para almacenar y administrar las im\u00e1genes privadas del contenedor Docker y los artefactos relacionados. EJERCICIO APP SERVICE Azure Container Instances es un servicio de Microsoft Azure que permite en forma r\u00e1pida y sencilla ejecutar un contenedor en Azure, sin tener que administrar ninguna m\u00e1quina virtual y sin necesidad de adoptar un servicio de nivel superior. EJERCICIO Kubernetes es una plataforma de r\u00e1pida evoluci\u00f3n que administra aplicaciones basadas en contenedores y sus componentes de red y almacenamiento asociados. El foco est\u00e1 en las cargas de trabajo de la aplicaci\u00f3n, no en los componentes de infraestructura subyacente. Kubernetes proporciona un enfoque declarativo en las implementaciones, respaldado por un s\u00f3lido conjunto de API para las operaciones de administraci\u00f3n. Azure Kubernetes Service (AKS) proporciona un servicio de Kubernetes administrado que reduce la complejidad de las principales tareas de administraci\u00f3n e implementaci\u00f3n, incluida la coordinaci\u00f3n de actualizaciones. El plano de control de AKS es administrado por la plataforma de Azure, y solo paga por los nodos de AKS que ejecutan sus aplicaciones. AKS se ha dise\u00f1ado sobre el motor de c\u00f3digo abierto de Azure Kubernetes Service (aks-engine). Los nodos del plano de control proporcionan los servicios centrales de Kubernetes y la orquestaci\u00f3n de las cargas de trabajo de las aplicaciones. Los nodos ejecutan las cargas de trabajo de la aplicaci\u00f3n.","title":"CONTENEDORES"},{"location":"azure/#logic-apps","text":"PowerAutomate, antes conocido como Flow, es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. Vamos a conocer algunos detalles del servicio para luego compararlo con Logic App (el servicio en el que realmente queremos hacer doble clic). Es un servicio SaaS (Software como Servicio) para automatizar flujos de trabajo utilizando m\u00faltiples conectores a aplicaciones y servicios SaaS. El principal usuario objetivo es el de negocio, \u201cCitizen Developer\u201d / Integrador, es decir aquel usuario que crea nuevas aplicaciones comerciales para el consumo de otros utilizando entornos de desarrollo y tiempo de ejecuci\u00f3n autorizados por la TI corporativa. Su foco es brindar una experiencia para integraciones simples con aplicaciones y servicios. Logic Apps es un servicio PaaS (Plataforma como Servicio) para automatizar flujos de trabajo sobre m\u00faltiples aplicaciones SaaS y servicios IaaS, simplificando la complejidad requerida para la integraci\u00f3n empresarial. En si mismo, extiendo las capacidades de Power Automate. El principal usuario objetivo son Desarrolladores y IT Pros, es decir usuarios de sistemas que tienen conocimiento mucho m\u00e1s avanzado que un \u201cCitizen Developer\u201d. Su foco es brindar una experiencia para integraciones complejas y avanzadas, comparativamente con Power Automate. EJERCICIO En este tutorial se muestra c\u00f3mo utilizar Azure Functions con Logic Apps y Cognitive Services en Azure para ejecutar el an\u00e1lisis de opiniones de entradas de Twitter. Una funci\u00f3n desencadenada por HTTP clasifica los tweets en verdes, amarillos o rojos, en funci\u00f3n de la puntuaci\u00f3n de la opini\u00f3n. Se env\u00eda un correo electr\u00f3nico cuando se detecta una opini\u00f3n deficiente.","title":"LOGIC APPS"},{"location":"azure/#ia-ml-y-dl","text":"El aprendizaje profundo (deep learning), es un subconjunto del aprendizaje autom\u00e1tico basado en redes neuronales artificiales que permiten a un equipo entrenarse a s\u00ed mismo. En este caso, el proceso de aprendizaje se llama profundo porque la estructura de redes neuronales artificiales se compone de varias capas de entrada, salida y ocultas. Cada capa contiene unidades que transforman los datos de entrada en informaci\u00f3n que la capa siguiente puede usar para realizar una tarea de predicci\u00f3n determinada. Gracias a esta estructura, un equipo puede aprender a trav\u00e9s de su propio procesamiento de datos. El aprendizaje autom\u00e1tico (machine learning) es un subconjunto de la inteligencia artificial que incluye t\u00e9cnicas (como el aprendizaje profundo) que permiten a los equipos mejorar en las tareas con la experiencia. En este caso, el proceso de aprendizaje se basa en los pasos siguientes: Alimente un algoritmo con datos proporcion\u00e1ndole m\u00e1s informaci\u00f3n (por ejemplo, realizando la extracci\u00f3n de caracter\u00edsticas). Utilice estos datos para entrenar un modelo. Pruebe e implemente el modelo. Consuma el modelo implementado para realizar una tarea de predicci\u00f3n automatizada concreta. La inteligencia artificial (IA artificial intelligence) es una t\u00e9cnica que permite a los equipos imitar la inteligencia humana. Incluye el aprendizaje autom\u00e1tico. Es importante conocer la relaci\u00f3n entre aprendizaje autom\u00e1tico, aprendizaje profundo e inteligencia artificial: El aprendizaje autom\u00e1tico es una forma de lograr inteligencia artificial, lo que significa que, mediante el uso de t\u00e9cnicas de aprendizaje autom\u00e1tico y aprendizaje profundo, se pueden crear sistemas inform\u00e1ticos y aplicaciones que puedan realizar tareas asociadas normalmente a la inteligencia humana, como la percepci\u00f3n visual, el reconocimiento de voz, la toma de decisiones y la traducci\u00f3n de un idioma a otro.","title":"IA, ML Y DL"},{"location":"azure/#azure-cognitive-services","text":"Azure Cognitive Services son servicios en la nube con API REST y SDK de biblioteca cliente que ayudan a los desarrolladores a compilar aplicaciones inteligentes cognitivas sin tener inteligencia artificial (IA) directa ni aptitudes o conocimientos sobre ciencia de datos. Azure Cognitive Services permiten a los desarrolladores agregar f\u00e1cilmente caracter\u00edsticas cognitivas en sus aplicaciones. El objetivo de Azure Cognitive Services es ayudar a los desarrolladores a crear aplicaciones que puedan ver, o\u00edr, hablar, comprender e incluso empezar a razonar. Cognitive Services proporciona funciones de aprendizaje autom\u00e1tico para solucionar problemas generales, como el an\u00e1lisis de texto para la opini\u00f3n emocional o el an\u00e1lisis de im\u00e1genes para reconocer objetos o caras. No es necesario tener conocimientos de aprendizaje autom\u00e1tico ni ciencia de datos para usar estos servicios, como tampoco conocer de programaci\u00f3n. Spatial Anchors es un servicio multiplataforma para desarrolladores que le permite crear experiencias de realidad mixta mediante objetos cuya ubicaci\u00f3n persiste en todos los dispositivos a lo largo del tiempo. Estas aplicaciones pueden admitir Microsoft HoloLens, dispositivos iOS compatibles con ARKit y dispositivos Android compatibles con ARCore. Azure Spatial Anchors permite a los desarrolladores trabajar con plataformas de realidad mixta para percibir el espacio, designar puntos precisos de inter\u00e9s y volver a recuperar esos puntos de inter\u00e9s desde los dispositivos compatibles. Estos puntos de inter\u00e9s precisos se conocen como delimitadores espaciales.","title":"AZURE COGNITIVE SERVICES"},{"location":"azure/#big-data","text":"Big Data (conocido tambi\u00e9n como Macrodatos) es un t\u00e9rmino que describe el gran volumen de datos, tanto estructurados como no estructurados, que inundan los negocios cada d\u00eda. Pero no es la cantidad de datos lo que es importante. Lo que importa con el Big Data es lo que las organizaciones hacen con los datos. Big Data se puede analizar para obtener ideas que conduzcan a mejores decisiones y movimientos de negocios estrat\u00e9gicos. Cuando hablamos de Big Data nos referimos a conjuntos de datos o combinaciones de conjuntos de datos cuyo tama\u00f1o (volumen), complejidad (variabilidad) y velocidad de crecimiento (velocidad) dificultan su captura, gesti\u00f3n, procesamiento o an\u00e1lisis mediante tecnolog\u00edas y herramientas convencionales, tales como bases de datos relacionales y estad\u00edsticas convencionales o paquetes de visualizaci\u00f3n, dentro del tiempo necesario para que sean \u00fatiles. Las Cargas de Trabajo Tradicionales (RDBMS) utilizan procesamiento de transacciones en l\u00ednea (OLTP) y procesamiento anal\u00edtico en l\u00ednea (OLAP). En los sistemas OLTP, los datos suelen ser relacionales con un esquema predefinido y un conjunto de restricciones para mantener la integridad referencial. A menudo, se pueden consolidar datos de varios or\u00edgenes de la organizaci\u00f3n en un almacenamiento de datos, utilizando un proceso ETL para mover y transformar los datos de origen. Una arquitectura de Big Data (Macrodatos) est\u00e1 dise\u00f1ada para controlar la ingesta, el procesamiento y el an\u00e1lisis de datos que son demasiado grandes o complejos para los sistemas de bases de datos tradicionales. Los datos se pueden procesar por lotes o en tiempo real. Las soluciones de macrodatos suelen implicar una gran cantidad de datos no relacionales, tales como datos de clave y valor, documentos JSON o datos de series temporales. Los sistemas RDBMS tradicionales no suelen ser adecuados para almacenar este tipo de datos. El t\u00e9rmino NoSQL hace referencia a la familia de bases de datos que est\u00e1n dise\u00f1adas para contener datos no relacionales. El t\u00e9rmino no es totalmente exacto, porque muchos almacenes de datos no relacionales admiten consultas compatibles con SQL. El t\u00e9rmino NoSQL significa \u201cno solo SQL\u201d.","title":"BIG DATA"},{"location":"azure/#conceptos","text":"Un data warehouse es un repositorio unificado y estructurado para todos los datos que recogen los diversos sistemas de una empresa. El repositorio puede ser f\u00edsico o l\u00f3gico y hace hincapi\u00e9 en la captura de datos de diversas fuentes sobre todo para fines anal\u00edticos y de acceso. Un data lake es un repositorio de almacenamiento que contienen una gran cantidad de datos en bruto, estructurado, semi-estructurados & no estructurados, y que se mantienen all\u00ed hasta que sea necesario. A diferencia de un data warehouse jer\u00e1rquico que almacena datos en ficheros o carpetas, un data lake utiliza una arquitectura plana para almacenar los datos. Existen algunas diferencias clave entre Data Lake y Data Warehouse: Datos: Un data warehouse s\u00f3lo almacena datos que han sido modelados o estructurados, mientras que un Data Lake no hace acepci\u00f3n de datos. Lo almacena todo, estructurado, semiestructurado y no estructurado. Procesamiento: Antes de que una empresa pueda cargar datos en un data warehouse, primero debe darles forma y estructura, es decir, los datos deben ser modelados. Eso se llama schema-on-write. Con un data lake, s\u00f3lo se cargan los datos sin procesar, tal y como est\u00e1n, y cuando est\u00e9 listo para usar los datos, es cuando se le da forma y estructura. Eso se llama schema-on-read. Dos enfoques muy diferentes. Almacenamiento: Una de las principales caracter\u00edsticas de las tecnolog\u00edas de big data, como Hadoop, es que el coste de almacenamiento de datos es relativamente bajo en comparaci\u00f3n con el de un data warehouse. Hay dos razones principales para esto: en primer lugar, Hadoop es software de c\u00f3digo abierto, por lo que la concesi\u00f3n de licencias y el soporte de la comunidad es gratuito. Y segundo, Hadoop est\u00e1 dise\u00f1ado para ser instalado en hardware de bajo coste. Agilidad: Un almac\u00e9n de datos es un repositorio altamente estructurado, por definici\u00f3n. No es t\u00e9cnicamente dif\u00edcil cambiar la estructura, pero puede tomar mucho tiempo dado todos los procesos de negocio que est\u00e1n vinculados a ella. Un data lake, por otro lado, carece de la estructura de un data warehouse, lo que da a los desarrolladores y a los cient\u00edficos de datos la capacidad de configurar y reconfigurar f\u00e1cilmente y en tiempo real sus modelos, consultas y aplicaciones. Seguridad: La tecnolog\u00eda del data warehouse existe desde hace d\u00e9cadas, mientras que la tecnolog\u00eda de big data (la base de un Data Lake) es relativamente nueva. Por lo tanto, la capacidad de asegurar datos en un data warehouse es mucho m\u00e1s madura que asegurar datos en un data lake. Cabe se\u00f1alar, sin embargo, que se est\u00e1 realizando un importante esfuerzo en materia de seguridad en la actualidad en la industria de Big Data. Azure Data Factory es la plataforma que resuelve estos escenarios de datos. Se trata de un servicio de integraci\u00f3n de datos y ETL basado en la nube que le permite crear flujos de trabajo orientados a datos a fin de coordinar el movimiento y la transformaci\u00f3n de datos a escala. Con Azure Data Factory, puede crear y programar flujos de trabajo basados en datos (llamados canalizaciones) que pueden ingerir datos de distintos almacenes de datos. Puede crear procesos ETL complejos que transformen datos visualmente con flujos de datos o mediante servicios de proceso como Azure HDInsight Hadoop, Azure Databricks y Azure SQL Database. Azure Databricks es una plataforma de an\u00e1lisis basada en Apache Spark optimizada para la plataforma de servicios en la nube de Microsoft Azure. Dise\u00f1ada por los fundadores de Apache Spark, Databricks est\u00e1 integrado con Azure para proporcionar una configuraci\u00f3n con un solo clic, flujos de trabajo optimizados y un \u00e1rea de trabajo interactiva que permite la colaboraci\u00f3n entre cient\u00edficos de datos, ingenieros de datos y analistas empresariales. Y lo mejor: no requiere administraci\u00f3n :-). Apache Spark es un framework de programaci\u00f3n para procesamiento de datos distribuidos dise\u00f1ado para ser r\u00e1pido y de prop\u00f3sito general. Como su propio nombre indica, ha sido desarrollada en el marco del proyecto Apache, lo que garantiza su licencia Open Source. Azure Synapse es un servicio de an\u00e1lisis que engloba el almacenamiento de datos empresariales y el an\u00e1lisis de macrodatos. Le ofrece la libertad de consultar los datos como prefiera, ya sea a petici\u00f3n sin servidor o con recursos aprovisionados, a escala. Azure Synapse re\u00fane estos dos mundos con una experiencia unificada para ingerir, preparar, administrar y servir datos para las necesidades inmediatas de inteligencia empresarial y aprendizaje autom\u00e1tico.","title":"Conceptos"},{"location":"azure/#mensajeria","text":"Azure ofrece varios servicios que le ayudan en la entrega de mensajes de evento en una soluci\u00f3n. Estos servicios son los siguientes: - Event Grid - Event Hubs - Service Bus Evento: es una notificaci\u00f3n ligera de una condici\u00f3n o un cambio de estado. El publicador del evento no tiene ninguna expectativa sobre c\u00f3mo se trata el evento. El consumidor del evento decide qu\u00e9 hacer con la notificaci\u00f3n. Los eventos pueden ser unidades discretas o parte de una serie. Mensaje: son datos sin procesar producidos por un servicio que se consumen o almacenan en otro lugar. El mensaje contiene los datos que desencaden\u00f3 la canalizaci\u00f3n del mensaje. El publicador del mensaje tiene una expectativa sobre la forma en que el consumidor trata el mensaje. Existe un contrato entre ambas partes. Por ejemplo, el publicador env\u00eda un mensaje con los datos sin procesar, espera que el consumidor cree un archivo a partir de esos datos y env\u00eda una respuesta cuando el trabajo finaliza. Microsoft Azure Service Bus es un agente de mensajes de integraci\u00f3n empresarial completamente administrado. Service Bus puede desacoplar aplicaciones y servicios. Service Bus ofrece una plataforma confiable y segura para la transferencia asincr\u00f3nica de datos y estado. Azure Event Hubs es una plataforma de streaming de macrodatos y un servicio de ingesta de eventos. Puede recibir y procesar millones de eventos por segundo. Los datos enviados a un centro de eventos se pueden transformar y almacenar con cualquier proveedor de an\u00e1lisis en tiempo real o adaptadores de procesamiento por lotes y almacenamiento. Azure IoT Hub es la puerta de enlace en la nube que conecta dispositivos IoT para recopilar los datos y dirigir las perspectivas y automatizaci\u00f3n empresariales. Adem\u00e1s, IoT Hub incluye caracter\u00edsticas que enriquecen la relaci\u00f3n entre los dispositivos y los sistemas back-end. Las capacidades de comunicaci\u00f3n bidireccional implican que al tiempo que se reciben datos de los dispositivos, tambi\u00e9n es posible devolver comandos y directivas a los dispositivos. Azure Event Grid permite crear f\u00e1cilmente aplicaciones con arquitecturas basadas en eventos. Event Grid est\u00e1 dise\u00f1ado para eventos, no datos. Cuando un Event Grid es informado de un evento, luego toma acciones determinadas.","title":"MENSAJERIA"},{"location":"azure/#azure-active-directory","text":"Azure Active Directory (Azure AD) es un servicio de administraci\u00f3n de identidades y acceso basado en la nube de Microsoft que ayuda a los empleados a iniciar sesi\u00f3n y acceder a recursos en: Recursos externos, como Microsoft 365, Azure Portal y miles de otras aplicaciones SaaS. Recursos internos, como las aplicaciones de la red corporativa y la intranet, junto con todas las aplicaciones en la nube desarrolladas por su propia organizaci\u00f3n. Los siguientes perfiles usan Azure AD: Administradores de TI. Si es administrador de TI, puede usar Azure AD para controlar el acceso a sus aplicaciones y a los recursos de est\u00e1s en funci\u00f3n de los requisitos de su empresa. Por ejemplo, puede usar Azure AD para requerir autenticaci\u00f3n multifactor al acceder a recursos importantes de la organizaci\u00f3n. Adem\u00e1s, puede usar Azure AD para automatizar el aprovisionamiento de usuarios entre la instancia existente de Windows Server AD y las aplicaciones en la nube, incluida Microsoft 365. Por \u00faltimo, Azure AD proporciona eficaces herramientas que ayudan a proteger autom\u00e1ticamente las identidades y credenciales de los usuarios y a cumplir los requisitos de gobernanza de acceso. Desarrolladores de aplicaciones. Como desarrollador de aplicaciones, puede usar Azure AD como enfoque basado en est\u00e1ndares para agregar el inicio de sesi\u00f3n \u00fanico (SSO) a cualquier aplicaci\u00f3n, lo que le permite trabajar con las credenciales existentes de un usuario. Azure AD tambi\u00e9n proporciona varias API que pueden ayudarle a crear experiencias de aplicaci\u00f3n personalizadas que usen los datos existentes de la organizaci\u00f3n. Suscriptores de Microsoft 365, Office 365, Azure o Dynamics CRM Online. Los suscriptores usan Azure AD. Cada inquilino de Microsoft 365, Office 365, Azure y Dynamics CRM Online es autom\u00e1ticamente un inquilino de Azure AD. Puede empezar de inmediato a administrar el acceso a las aplicaciones en la nube integradas. Entidades: Cloud Only: Esta opci\u00f3n de identidad es la que, por defecto, se habilita cuando creamos una cuenta en Office 365 o cuando creamos un directorio de Active Directory en Azure nuevo. En este caso, la contrase\u00f1a es gestionada por Azure Active Directory y no tiene relaci\u00f3n absoluta con nuestra infraestructura on-promises. De hecho, tampoco nuestro UPN de usuario tiene relaci\u00f3n con nuestra infraestructura on-promises, y de hecho (finalmente) debemos dar de alta los usuarios uno por uno. Sincronizada: Esta opci\u00f3n es la primera que, de alguna forma, relaciona nuestro servicio de directorio local (Active Directory Domain Services u otro LDAP) con el servicio de directorio en la nube (Azure Active Directory). Federada: La tercera y \u00faltima opci\u00f3n agrega la posibilidad de que el inicio de sesi\u00f3n se efectivice en nuestra infraestructura local. Azure AD Connect: El uso de esta caracter\u00edstica es gratis y est\u00e1 incluido en su suscripci\u00f3n de Azure. Es el componente que nos permite sincronizar identidades, o conectar nuestro servicio de directorio de Azure AD con otros LDAPs propios. Esta herramienta nos permitir\u00e1 sincronizar uno o muchos LDAPs con Azure AD, pero no un LDAP con muchos Azure ADs. Este item es importante de tener en cuenta cuando estemos planificando la arquitectura de soluci\u00f3n.","title":"AZURE ACTIVE DIRECTORY"},{"location":"azure/#devops","text":"DevOps es \u201cla union de personas, procesos, y productos para permitir la entrega continua de valor a sus usuarios finales\u201d. DevOps es una pr\u00e1ctica de ingenier\u00eda de software que tiene como objetivo unificar el desarrollo de software (Dev) y la operaci\u00f3n del software (Ops). La principal caracter\u00edstica del movimiento DevOps es defender en\u00e9rgicamente la automatizaci\u00f3n y el monitoreo en todos los pasos de la construcci\u00f3n del software, desde la integraci\u00f3n, las pruebas, la liberaci\u00f3n hasta la implementaci\u00f3n y la administraci\u00f3n de la infraestructura. DevOps apunta a ciclos de desarrollo m\u00e1s cortos, mayor frecuencia de implementaci\u00f3n, lanzamientos m\u00e1s confiables, en estrecha alineaci\u00f3n con los objetivos comerciales. Cuando hablamos de DevOps podemos hablar de 3 etapas, que no necesariamente se dan en cascada. Cuando trabajamos DevOps en las organizaciones debemos pensar en: Personas, Procesos y Productos. Cuando hablamos de personas, tenemos que entender que los equipos de trabajo est\u00e1n conformados por Personas: estas tienen emociones, que pueden manejar mejor o peor dependiendo de las situaciones personales y laborales que atraviesen. Entender esto y darle una mirada humana nos facilitar\u00e1 entender que DEBEMOS trabajar con las personas, desarrollar caracter\u00edsticas deseables que lleven a que sean \u201cEasy to work with\u201d, es decir una persona \u201ccon la que es f\u00e1cil y queremos trabajar\". Existen 4 grandes procesos, en alto nivel, que tenemos que tener en cuenta. Estos procesos, por supuesto, pueden darse c\u00edclicamente y no necesariamente en cascada: Planificar: crear un backlog, equipos interdisciplinarios, planificar testing, etc. Desarrollar + Probar: ya conozco que debo hacer, tengo mi backlog al menos en alto nivel, y podemos avanzar en la construcci\u00f3n. Liberar: lo que planifiqu\u00e9 y desarroll\u00e9 + prob\u00e9, lo debo liberar a ambientes. Desarrollo, Testing, Producci\u00f3n, etc. Monitorear + Aprender: un punto clave, es que el trabajo no termin\u00f3 con la liberaci\u00f3n. Reci\u00e9n comienza aqu\u00ed el aprendizaje basado en un monitoreo proactivo para volver a arrancar esta rueda: planificar, desarrollar + probar, y liberar, para volver a aprender. Desarrollo \u00e1gil: pr\u00e1cticas y marcos de trabajo \u00e1giles para el desarrollo. Integraci\u00f3n Continua: en todo momento, integrar nuestro c\u00f3digo con pruebas y chequeos de salud. Entrega / Despliegue Continuo: hacia ambientes, ya sea desarrollo, testing, pre producci\u00f3n o producci\u00f3n, acompa\u00f1ado de una robusta gesti\u00f3n de releases.","title":"DEVOPS"},{"location":"azure/#azure-devops","text":"Azure DevOps proporciona servicios de desarrollador para ayudar a los equipos a planificar el trabajo, colaborar en el desarrollo de c\u00f3digo y crear e implementar aplicaciones. Los desarrolladores pueden trabajar en la nube con Azure DevOps Services o localmente con Azure DevOps Server. Azure DevOps Server se llamaba anteriormente Visual Studio Team Foundation Server (TFS). Azure Repos proporciona repositorios de Git o Team Foundation Version Control (TFVC) para el control de c\u00f3digo fuente de su c\u00f3digo. Azure Pipelines proporciona servicios de creaci\u00f3n y lanzamiento para admitir la integraci\u00f3n y entrega continuas de sus aplicaciones. Azure Boards ofrece un conjunto de herramientas \u00e1giles para respaldar la planificaci\u00f3n y el seguimiento del trabajo, los defectos de c\u00f3digo y los problemas con los m\u00e9todos Kanban y Scrum. Azure Test Plans proporciona varias herramientas para probar sus aplicaciones, incluidas pruebas manuales / exploratorias y pruebas continuas. Azure Artifacts permite a los equipos compartir paquetes Maven, npm y NuGet de fuentes p\u00fablicas y privadas e integrar el uso compartido de paquetes en sus canalizaciones de CI / CD. Azure DevOps es gratuito hasta 5 usuarios por organizaci\u00f3n. Una organizaci\u00f3n de Azure DevOps permite tener, dentor, una colecci\u00f3n de proyectos con diversos repositorios. La creaci\u00f3n de repositorios tambi\u00e9n es gratuita, e inclusive la utilizaci\u00f3n de Pipelines hosteados por Microsoft para proyectos privados de hasta 1800 minutos por mes tambi\u00e9n es gratuito. En caso de proyectos p\u00fablicos, no hay l\u00edmite de minutos para Pipelines. Precios Vamos a devops starter para crear nuestro pipeline.","title":"AZURE DEVOPS"},{"location":"azure/#seguridad-azure","text":"Azure Security Center es un sistema unificado de administraci\u00f3n de seguridad de la infraestructura que fortalece la posici\u00f3n de seguridad de los centros de datos y proporciona una protecci\u00f3n contra amenazas avanzada de todas las cargas de trabajo h\u00edbridas que se encuentran en la nube, ya sea que est\u00e9n en Azure o no, as\u00ed como tambi\u00e9n en el entorno local. Proteger los recursos es un esfuerzo conjunto entre el proveedor de nube, Azure y usted, el cliente. Cuando migra a la nube, debe asegurarse de que las cargas de trabajo est\u00e9n seguras y, al mismo tiempo, cuando se migra a IaaS (infraestructura como servicio), el cliente tiene una responsabilidad mayor que cuando se encontraba en PaaS (plataforma como servicio) y SaaS (software como servicio). Azure Security Center le brinda las herramientas necesarias para fortalecer la red, proteger los servicios y garantizar que tiene la mejor posici\u00f3n de seguridad. Azure Security Center admite m\u00e1quinas virtuales y servidores en diferentes tipos de entornos h\u00edbridos: Solo Azure Azure y entorno local Azure y otras nubes Azure, otras nubes y entorno local Azure Information Protection (AIP) es una soluci\u00f3n basada en la nube que permite a las organizaciones clasificar y proteger documentos y correos electr\u00f3nicos mediante etiquetas. Se pueden aplicar etiquetas: Autom\u00e1ticamente por los administradores mediante reglas y condiciones. Manualmente por los usuarios. Mediante una combinaci\u00f3n en la que los administradores definen las recomendaciones que se muestran a los usuarios. Las etiquetas pueden clasificar y, opcionalmente, proteger el documento, con lo que puede hacer lo siguiente: Seguir y controlar el modo en que se usa el contenido. Analizar los flujos de datos para obtener una visi\u00f3n general de su negocio : detectar comportamientos de riesgo y tomar medidas correctivas. Hacer un seguimiento del acceso a los documentos e impedir la p\u00e9rdida de datos o su uso indebido. Use Azure Information Protection para aplicar la clasificaci\u00f3n a etiquetas en documentos y correos electr\u00f3nicos. Estas son algunas de las funciones de etiquetar contenido: Clasificaci\u00f3n que se puede detectar independientemente de d\u00f3nde se almacenen los datos o con qui\u00e9n se compartan. Distintivos visuales, como encabezados, pies de p\u00e1gina o marcas de agua. Metadatos que se agregan a los archivos y encabezados de correo electr\u00f3nico en texto no cifrado. Los metadatos de texto no cifrado garantizan que otros servicios puedan identificar la clasificaci\u00f3n y tomar las medidas adecuadas. Microsoft Azure Sentinel es una soluci\u00f3n de administraci\u00f3n de eventos de informaci\u00f3n de seguridad (SIEM) y respuesta automatizada de orquestaci\u00f3n de seguridad (SOAR) que es escalable y nativa de la nube. Azure Sentinel ofrece an\u00e1lisis de seguridad inteligente e inteligencia frente a amenazas en toda la empresa, de forma que proporciona una \u00fanica soluci\u00f3n para la detecci\u00f3n de alertas, la visibilidad de amenazas, la b\u00fasqueda proactiva y la respuesta a amenazas. Azure Sentinel permite obtener una vista general de toda la empresa, lo que suaviza la tensi\u00f3n de ataques cada vez m\u00e1s sofisticados, vol\u00famenes de alertas cada vez mayores y plazos de resoluci\u00f3n largos. Recopile datos a escala de nube de todos los usuarios, dispositivos, aplicaciones y de toda la infraestructura, tanto en el entorno local como en diversas nubes. Detecte amenazas que antes no se detectaban y reduzca los falsos positivos mediante el an\u00e1lisis y la inteligencia de amenazas sin precedentes de Microsoft. Investigue amenazas con inteligencia artificial y busque actividades sospechosas a escala, aprovechando el trabajo de ciberseguridad que ha llevado a cabo Microsoft durante d\u00e9cadas. Responda a los incidentes con rapidez con la orquestaci\u00f3n y la automatizaci\u00f3n de tareas comunes integradas.","title":"SEGURIDAD AZURE"},{"location":"backup/","text":"HACER BACKUPS tar -cvf backup$(date +%Y%m%d).tar /home Herramienta fwbackups: dnf install fwbackups Otras herramientas","title":"Backup"},{"location":"backup/#hacer-backups","text":"tar -cvf backup$(date +%Y%m%d).tar /home Herramienta fwbackups: dnf install fwbackups Otras herramientas","title":"HACER BACKUPS"},{"location":"bash_scripting/","text":"Comandos para Bash Scripting #! /bin/bash validar argumentos # indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0 validar help #help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi comparadores -lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi conficional if edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi bucle for count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0 bucle while cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0 bucle esac for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0 leer por entrada standard count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0 separar opciones con shift opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0 mirar si es directorio, regular file... #files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0 Mayusculas , copiar , buscar , ordenar tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null) Funciones function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"BashScripting"},{"location":"bash_scripting/#comandos-para-bash-scripting","text":"#! /bin/bash","title":"Comandos para Bash Scripting"},{"location":"bash_scripting/#validar-argumentos","text":"# indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0","title":"validar argumentos"},{"location":"bash_scripting/#validar-help","text":"#help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi","title":"validar help"},{"location":"bash_scripting/#comparadores","text":"-lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi","title":"comparadores"},{"location":"bash_scripting/#conficional-if","text":"edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi","title":"conficional if"},{"location":"bash_scripting/#bucle-for","text":"count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0","title":"bucle for"},{"location":"bash_scripting/#bucle-while","text":"cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0","title":"bucle while"},{"location":"bash_scripting/#bucle-esac","text":"for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0","title":"bucle esac"},{"location":"bash_scripting/#leer-por-entrada-standard","text":"count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0","title":"leer por entrada standard"},{"location":"bash_scripting/#separar-opciones-con-shift","text":"opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0","title":"separar opciones con shift"},{"location":"bash_scripting/#mirar-si-es-directorio-regular-file","text":"#files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0","title":"mirar si es directorio, regular file..."},{"location":"bash_scripting/#mayusculas-copiar-buscar-ordenar","text":"tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null)","title":"Mayusculas , copiar , buscar , ordenar"},{"location":"bash_scripting/#funciones","text":"function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"Funciones"},{"location":"bbdd/","text":"BASES DE DATOS POSTGRESQL Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); `` PHPPGADMIN phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin MARIADB Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql MONGODB Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"Base de datos"},{"location":"bbdd/#bases-de-datos","text":"","title":"BASES DE DATOS"},{"location":"bbdd/#postgresql","text":"Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); ``","title":"POSTGRESQL"},{"location":"bbdd/#phppgadmin","text":"phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin","title":"PHPPGADMIN"},{"location":"bbdd/#mariadb","text":"Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql","title":"MARIADB"},{"location":"bbdd/#mongodb","text":"Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"MONGODB"},{"location":"devops/","text":"CURSO DEVOPS INTEGRADO QU\u00c9 ES DEVOPS La idea es una aplicaci\u00f3n hecha en angular con base de datos postgres y servidor web nginx. La cuesti\u00f3n es tener 5 nodos: uno de test, dos de preproducci\u00f3n y dos de producci\u00f3n. De manera m\u00e1s robusta ser\u00eda tener en cada nodo la app, el server web y la bbdd pero actualmente solo necesitamos tener en cada nodo Docker Engine instalado para una mayor facilidad y rapidez. INSTALACI\u00d3N Instalamos Docker engine y docker compose en nuestra m\u00e1quina. Comprovamos con --version. *NOTA WINDOWS+ Podemos instalar docker engine en windows . Instalamos docker desktop . Activamos virtualizaci\u00f3n en la bios y activamos WSL2 con Enable-WindowsOptionalFeature -Online -FeatureName $(\"VirtualMachinePlatform\", \"Microsoft-Windows-Subsystem-Linux\") Cogemos del repositorio de dockerhub del profe la imagen a descargar que contiene la app de facturaci\u00f3n en angular a utilizar en este curso: docker pull sotobotero/udemy-devops:0.0.1 Iniciamos el contenedor de la app mapeando puertos web y para puerto gr\u00e1fico: docker run -p 80:80 -p 8080:8080 --name billingapp sotobotero/udemy-devops:0.0.1 Comprobamos que funcionan en www.localhost:80 y www.localhost:8080/swagger-ui/index.html. DOCKER-COMPOSE V1 SCRATCH Creamos un docker-compose de prueba con dos nombres diferentes para ver como arranca de manera directa o con un nombre diferente: version: '3.1' services: db: container_name: postgres image: postgres restart: always environment: ports: - 5432:5432 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres adminer: container_name: adminer image: adminer restart: always depends_on: - db ports: - 9090:8080 Descargamos con docker-compose pull // docker-compose -f fichero.yml pull e iniciamos con docker-compose -f fichero.yml up -d Ahora podemos entrar de manera grafica a postgres con admirer en www.localhost:9090 e introducimos las credenciales y funcionando. Ahora vamos a crear from scratch un Dockerfile para crear la app de facturaci\u00f3n: #Creamos nuestra app de facturaci\u00f3n #Partimos de imagen base FROM nginx:alpine #Instalamos java RUN apk -U add openjdk8 && rm -rf /var/cache/apk/*; RUN apk add ttf-dejavu #Instalamos java microservicios y variables ENV JAVA_OPTS=\"\" ARG JAR_FILE ADD ${JAR_FILE} app.jar #Instalamos la app en nginx server y creamos un volumen para info y conf VOLUME /tmp RUN rm -rf /usr/share/nginx/html/* COPY nginx.conf /etc/nginx/nginx.conf COPY dist/billingApp /usr/share/nginx/html COPY appshell.sh appshell.sh #mapeamos puertos EXPOSE 80 8080 ENTRYPOINT [\"sh\", \"/appshell.sh\"] Lo construimos con docker build -t facturacionapp:prod --no-cache --build-arg JAR_FILE=target/*.jar . Lo iniciamos con los puertos EXPOSE docker run -p 80:80 -p 8080:8080 --name billingapp facturacionapp:prod Luego podemos subir nuestra version a Dockerhub con docker tag facturacionapp:prod isx46410800/facturacionapp:1.0 y la subimos docker push isx46410800/facturacionapp:1.0 V2 SERVICES Siempre es local-host:container . Ahora creamos nuestra app de facturaci\u00f3n pero servicio a servicio separado en el docker-compose. Por un lado nos descargamos una imagen de nginx, por otro lado java y despues construimos manualmente con Dockerfile nuestra app angular y nuestra bbdd de postgresql: version: '3.1' services: #database engine service postgres_db: container_name: postgres image: postgres:latest restart: always environment: ports: - 5432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database admin service adminer: container_name: adminer image: adminer restart: always depends_on: - postgres_db ports: - 9090:8080 #Billin app backend service billingapp-back: build: context: ./java args: - JAR_FILE=*.jar container_name: billingApp-back environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db ports: - 8080:8080 #Billin app frontend service billingapp-front: build: context: ./angular container_name: billingApp-front depends_on: - billingapp-back ports: - 80:80 Construir las imagenes definidas en la orquestaci\u00f3n: docker-compose -f stack-billing.yml build Inicializar los contenedores de los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d No se ha especificado, pero todo esto se crea en una misma red virtual para que se puedan comunicar entre los diferentes servicios. Resumen de comandos en DOCKER-COMPOSE: Eliminar todos los contenedores detenidos: `docker system prune` Eliminar todas las im\u00e1genes: docker rmi $(docker images -a -q) Listar los volumenes: docker volume ls Eliminar todos los volumenes: docker volume prune Construir las imagenes definidas en la orquestaci\u00f3n: docker-compose -f stack-billing.yml build Inicializar los contenedores de los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d Detener todos los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml stop Escalar un servicio al iniciar la orquestaci\u00f3n: docker-compose -f stack-billing.yml up --scale billingapp-front=3 -d --force-recreate Detener todos los contenedores: docker stop $(docker ps -a -q) Listar las redes virtuales: docker network ls Eliminar las redes virtuales: docker network prune Reconstruir las imagenes: docker-compose -f stack-billing.yml build --no-cache Reconstruir los contenedores d ela orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d --force-recreate Al crear los volumenes veremos que en las rutas indicadas tendremos los registros de la bbdd que vamos rellenando aunque detengamos y borremos nuestro docker-compose. Podemos escalar servicios con este comando: docker-compose -f stack-billing.yml up --scale billingapp-front=3 -d --force-recreated Tener cuidado que no este puesto el container name y poner un rango de puertos(80-83:80) en este caso en el front que escalamos. La otra forma de escalar seria a\u00f1adir directamente en el docker-compose.yaml: #Billin app frontend service billingapp-front: build: context: ./angular deploy: replicas: 3 resources: limits: cpus: \"0.10\" memory: 250M reservations: cpus: \"0.1\" memory: 120M #container_name: billingApp-front depends_on: - billingapp-back ports: - 80-83:80 Con docker stats vemos las estadisticas que por ejemplo delimitamos en el fichero docker-compose.yml. V3 SERVICES TEST/PROD Ahora veremos como montar de nuevo nuestra infraestructura de app, bbdd, server web y java, primero deploy en test y luego en producci\u00f3n: version: '3.1' services: #database engine service postgres_db_prod: container_name: postgres_prod image: postgres:latest restart: always networks: - env_prod environment: ports: - 5432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data_prod:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database engine service postgres_db_prep: container_name: postgres_prep image: postgres:latest restart: always networks: - env_prep environment: ports: - 4432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data_prep:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database admin service #Use for All enviroments adminer: container_name: adminer image: adminer restart: always networks: - env_prod - env_prep depends_on: - postgres_db_prod - postgres_db_prep ports: - 9090:8080 #ENV_PROD #Billin app backend service billingapp-back-prod: build: context: ./java args: - JAR_FILE=billing-0.0.3-SNAPSHOT.jar networks: - env_prod container_name: billingApp-back-prod environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db_prod ports: - 8080:8080 #Billin app frontend service billingapp-front_prod: build: context: ./angular networks: - env_prod deploy: replicas: 2 resources: limits: cpus: \"0.15\" memory: 250M #recusos dedicados, mantiene los recursos disponibles del host para el contenedor reservations: cpus: 0.1 memory: 128M #container_name: billingApp-front depends_on: - billingapp-back-prod #rango de puertos para escalar ports: - 8081-8082:80 #ENV_PREP #Billin app backend service billingapp-back-prep: build: context: ./java args: - JAR_FILE=billing-0.0.2-SNAPSHOT.jar networks: - env_prep container_name: billingApp-back-prep environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db_prep ports: - 7080:7080 #Billin app frontend service billingapp-front-prep: build: context: ./angular networks: - env_prep deploy: replicas: 2 resources: limits: cpus: \"0.15\" memory: 250M #recusos dedicados, mantiene los recursos disponibles del host para el contenedor reservations: cpus: 0.1 memory: 128M #container_name: billingApp-front depends_on: - billingapp-back-prep #rango de puertos para escalar ports: - 7081-7082:81 networks: env_prod: driver: bridge #activate ipv6 driver_opts: com.docker.network.enable_ipv6: \"true\" #IP Adress Manager ipam: driver: default config: - subnet: 172.16.232.0/24 gateway: 172.16.232.1 - subnet: \"2001:3974:3979::/64\" gateway: \"2001:3974:3979::1\" env_prep: driver: bridge #activate ipv6 driver_opts: com.docker.network.enable_ipv6: \"true\" #IP Adress Manager ipam: driver: default config: - subnet: 172.16.235.0/24 gateway: 172.16.235.1 - subnet: \"2001:3984:3989::/64\" gateway: \"2001:3984:3989::1\" Se especificas las networks que creamos y copiamos un clon de servicio de prod y pre teniendo en cuenta cambiar el path de la bbdd para que no se conflicten. docker-compose -f stack-billing.yml build --no-cache docker-compose -f stack-billing.yml up -d --force-recreate KUBERNETTES INSTALACI\u00d3N La arquitectura consta de un cluster donde hay nodos que trabajan(master y workers), donde dentro estan los pods. En este cluster tenemos Control manager, etcd(bbdd) y scheduler. Tenemos un Apiserver que es quien nos provee la interaccion con el cluster a traves de los comandos kubectl o manera grafica con el kubernetes dashboard . Normalmente se usa la infraestructura de amazon, azure o google. Minikube simula la infraestrura de un cluster de kubernetes y contiene todos los componentes necesarios en un solo nodo. Vemos si nuestro Linux tiene virtualizaci\u00f3n: grep -E --color 'vmx|svm' /proc/cpuinfo Descargar kubectl y hacerlo ejecutable: curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.20.0/bin/linux/amd64/kubectl && chmod +x kubectl Crear el directorio para kubectl: sudo mv ./kubectl /usr/local/bin/kubectl Verificar version: kubectl version --client Descargar minnukube y hacerlo ejecutable: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube Crear el directorio para minikube: sudo mkdir -p /usr/local/bin/ Lanzar el ejecutable: sudo install minikube /usr/local/bin/ Comandos para operar minikube, lo lanzamos con minikube start : minikube start minikube status minikube stop Por defecto cuando hacemos start se ha de indicar cual es el hipervisor a trabajar, por defecto coge docker, pero se le puede poner hyperV, virtualbox,etc. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una capa de software para realizar una virtualizaci\u00f3n de hardware que permite utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Comprobamos que est\u00e1 encendido y el contenedor docker que se crea: [miguel@fedora Downloads]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured [miguel@fedora Downloads]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82f6891e5f44 gcr.io/k8s-minikube/kicbase:v0.0.28 \"/usr/local/bin/entr\u2026\" About a minute ago Up About a minute 127.0.0.1:49157->22/tcp, 127.0.0.1:49156->2376/tcp, 127.0.0.1:49155->5000/tcp, 127.0.0.1:49154->8443/tcp, 127.0.0.1:49153->32443/tcp minikube Lanzar el dashboard grafico donde nos manda a una url para manejarlo: minikube dashboard Eliminar el cluster de minikube: minikube delete Inicar un nuevo cluster de minikube usando el controlador hypervisor de virtualbox: minikube start --driver=virtualbox PODS Vamos a crear dentro de un cluster una app para que el cliente desde fuera pueda acceder desde el navegador. Para ello se crea un POD a trav\u00e9s de la imagen de la app, con volumenes persistentes y este pod tiene una IP no visible. Para que sea visible desde fuera, se crea un servicio para poder acceder a el desde una petici\u00f3n de fuera. Creamos un pod desde nuestra imagen de un repositorio dockerhub: kubectl run kbillingapp --image=sotobotero/udemy-devops:0.0.1 --port=80 80 Lo vemos con kubetctl get pods y kubectl describe pod name_pod . El pod tiene un IP no visible desde fuera y para que se pueda acceder se ha de exponer esta ip como si hicieramos un servicio: kubectl expose pod kbillingapp --type=LoadBalancer --port=8080 --target-port=80 [miguel@fedora Downloads]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kbillingapp LoadBalancer 10.105.34.39 <pending> 8080:30424/TCP 9s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 25m Al ser minikube y no tener mas nodos trabajando (vida real no pasaria) tenemos que pasar otro comando para obtener esa ip externa y acceder: [miguel@fedora Downloads]$ minikube service kbillingapp |-----------|-------------|-------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|---------------------------| | default | kbillingapp | 8080 | http://192.168.49.2:30424 | |-----------|-------------|-------------|---------------------------| VARIABLES Y SECRETS En este caso vamos a tener dos pods: uno con una bbdd postgres que contiene volumenes persistentes y otro el pgadmin grafico. Esto tendr\u00e1 un servicio para poder acceder desde fuera de manera grafica. Para ello, vamos a crear en un SECRET fichero yaml, las credenciales de entrada para postgresql para que esten encriptadas y no sean visibles, lo hacemos en base64 con el comando echo -n \"palabra\" | base64 y para descodificarlo echo \"xxxx\" | base64 -d : #object that store enviroments variables that could be have sensitive data like a password apiVersion: v1 kind: Secret metadata: name: postgres-secret labels: app: postgres #meant that we can use arbitrary key pair values type: Opaque data: POSTGRES_DB: cG9zdGdyZXM= POSTGRES_USER: cG9zdGdyZXM= POSTGRES_PASSWORD: cXdlcnR5 Fichero secret-dev.yaml Creamos otro secret para el PGADMIN grafico: #object that store enviroments variables that could be have sensitive data like a password apiVersion: v1 kind: Secret metadata: name: pgadmin-secret labels: app: postgres #meant that we can use arbitrary key pair values type: Opaque data: PGADMIN_DEFAULT_EMAIL: YWRtaW5AYWRtaW4uY29t PGADMIN_DEFAULT_PASSWORD: cXdlcnR5 PGADMIN_PORT: ODA= Fichero secret-pgadmin.yaml VOLUMES Y CONFIGMAPS Ahora creamos un volumen para la data persistente y un reclamo de espacio para ello. Tambi\u00e9n creamos un configmap para la configuraci\u00f3n de que lance un script a la hora de crear todo: #persistence volumen (PV) is a piece of storage that have idependent lifecycle from pods #thees preserve data throug restartin, rescheduling and even deleting pods #PersistenceVolumeCalin is a request for storage by the user that can be fulfilled by teh PV kind: PersistentVolume #version of ApiServer on control panel node (/api/v1) check using kubectl api-versions apiVersion: v1 metadata: name: postgres-volume labels: #it is aplugin suport many luke amazon EBS azure disk etc. local = local storage devices mounted on nodes. type: local app: postgres spec: storageClassName: manual capacity: storage: 5Gi #many pods on shcheduled on differents nodes can read and write accessModes: - ReadWriteMany #path on cluster's node hostPath: path: \"/mnt/data/\" #it is a reques of resource (persistence volume) from a pod by example, teh pod claim by a storage throug PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-claim labels: app: postgres spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 2Gi apiVersion: v1 kind: ConfigMap metadata: name: postgres-init-script-configmap data: initdb.sh: |- #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL CREATE USER billingapp WITH PASSWORD 'qwerty'; CREATE DATABASE billingapp_db; GRANT ALL PRIVILEGES ON DATABASE billingapp_db TO billingapp; EOSQL DEPLOYS Y SERVICES Ahora definimos el deloy completo de la base de datos que contiene un fichero de variables de entorno, un volumen persistente, el reclamo de espacio del volumen y el configmap con el script de iniciaci\u00f3n. apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment labels: app: postgres spec: #Pods number replicates replicas: 1 #Define how the Deployment identify the pods that it could manage selector: matchLabels: app: postgres #pod template specification template: metadata: #define teh labels for all pods labels: app: postgres spec: containers: - name: postgres image: postgres:latest imagePullPolicy: IfNotPresent #open the port to allow send and receive traffic in teh container ports: - containerPort: 5432 #read envars from secret file envFrom: - secretRef: name: postgres-secret volumeMounts: #This is the path in the container on which the mounting will take place. - mountPath: /var/lib/postgresql/data name: postgredb - mountPath: /docker-entrypoint-initdb.d name : init-script volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-claim - name: init-script configMap: name: postgres-init-script-configmap Ahora hacemos el deploy del pgadmin gr\u00e1fico para acceder a la bbdd graficamente con el puerto y fichero de varibles de entorno: apiVersion: apps/v1 kind: Deployment metadata: name: pgadmin-deployment spec: selector: matchLabels: app: pgadmin replicas: 1 template: metadata: labels: app: pgadmin spec: containers: - name: pgadmin4 image: dpage/pgadmin4 envFrom: - secretRef: name: pgadmin-secret ports: - containerPort: 80 name: pgadminport Ahora tenemos que crear los servicios de ambos pods para poder acceder desde fuera en internet a trav\u00e9s de un puerto publico y est\u00e1n en nodo port porque solo tenemos un nodo, sino seria por loadbalancer por diferentes ip y su carga: kind: Service apiVersion: v1 metadata: name: postgres-service labels: app: postgres spec: ports: - name: postgres port: 5432 nodePort : 30432 #type: LoadBalancer type: NodePort selector: app: postgres apiVersion: v1 kind: Service metadata: name: pgadmin-service labels: app: pgadmin spec: selector: app: pgadmin type: NodePort ports: - port: 80 nodePort: 30200 Para crearlo ahora en kubernetes usamos kubectl apply -f fichero.yaml . Primero los secrets, despues el configmap, despues los volumenes, despues los deploys y si est\u00e1 todo OK, por ultimo los servicios. Comprobamos todo con kubectl get all . Con todo lanzado ahora nos podemos conectar desde fuera por la ip de cada uno, no obstante, al ser nuestro un minikube, debemos usar la ip de este y el puerto de cada cosa. Para ver la ip usamos minikube ip . Ahora tenemos la ip y usamos el puerto de postgres y el puerto de pgadmin para conectarse a cada cosa. Dentro de pgadmin nos podemos conectar a la bbdd de postgres con la ip del pod o la ip del cluster y su puerto correspondente. Si no queremos usar el pgadmin como grafico de bbdd, podemos usar otro motor y nos conectamos al servicio postgres desde fuera y lo tendremos igual. Podemos eliminar todo lo realizado con kubectl delete -f ./ . ORQUESTACI\u00d3N REAL Ahora a\u00f1adiremos el servicio backend y frontend de nuestra aplicaci\u00f3n. El frotend est\u00e1 basado en un server nginx y escrita en angular por lo que al usuario que hace la petici\u00f3n le daremos respuesta html. El backend est\u00e1 escrito en springboot. En este caso tambi\u00e9n se pone una ip fija al servicio postgres ya que la bbdd no ser\u00eda eficiente que vaya cambiando la ip y no encontrar\u00eda la info nuestro backend para poder dar respuesta a las peticiones de los usuarios en el frontend a trav\u00e9s del navegador. kind: Service apiVersion: v1 metadata: name: postgres-service labels: app: postgres spec: clusterIP: 10.96.1.2 ipFamilies: - IPv4 ports: - name: postgres port: 5432 nodePort : 30432 #type: LoadBalancer type: NodePort selector: app: postgres Para que el engine apunte al registro de imagenes en local usamos: minikube docker-env [miguel@fedora deploy_app]$ minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.49.2:2376\" export DOCKER_CERT_PATH=\"/home/miguel/.minikube/certs\" export MINIKUBE_ACTIVE_DOCKERD=\"minikube\" # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) eval $(minikube docker-env) Ahora ya estamos listo para poder crear las imagenes personalizadas y se puedan usar en minikube. Esto solo valdr\u00e1 en esta terminal. Creamos la imagen de java jar: docker build -t billingapp-back:0.0.4 --no-cache --build-arg JAR_FILE=/*.jar . Creamos la imagen de angular: docker build -t billingapp-front:0.0.4 --no-cache . Una vez creadas las dos imagenes, creamos los dos deployments y los dos servicios de la aplicaci\u00f3n a partir de estas imagenes y le ponemos las replicas que necesitamos: apiVersion: apps/v1 kind: Deployment metadata: name: billing-app-back-deployment spec: selector: matchLabels: app: billing-app-back replicas: 3 template: metadata: labels: app: billing-app-back spec: containers: - name: billing-app-back image: billingapp-back:0.0.4 ports: - containerPort: 7080 name: billingappbport apiVersion: apps/v1 kind: Deployment metadata: name: billing-app-front-deployment spec: selector: matchLabels: app: billing-app-front replicas: 2 template: metadata: labels: app: billing-app-front spec: containers: - name: billing-app-front image: billingapp-front:0.0.4 ports: - containerPort: 80 name: billingappfport kind: Service apiVersion: v1 metadata: name: billing-app-back-service labels: app: billing-app-back spec: ports: - name: billing-app-back port: 7080 nodePort : 30780 #type: LoadBalancer type: NodePort selector: app: billing-app-back apiVersion: v1 kind: Service metadata: name: billing-app-front-service labels: app: billing-app-front spec: selector: app: billing-app-front type: NodePort ports: - port: 80 nodePort: 30100 Ahora con todos los servicios metidos, con minikube ip podemos ir con esta ip y el puerto de cada cosa para ver que todo funciona. INtroducir registros y ver como se guardan en la base de datos. GIT Es un sistema de gesti\u00f3n de versiones distribuido y opensource. Hay dos maneras de trabajo: trunked based es la manera m\u00e1s basica de dos ramas y merge; y gitflow que consiste en trabajar en 5 ramas: master, develop, features(donde se va poniendo el codigo nuevo), releases(donde se prueba develop unido de features en entorno de pruebas) y hotfix(rama que sale de urgencia de master para arreglar fallos urgentes de master en producci\u00f3n). Comandos b\u00e1sicos: git init git commit -m \"\" git push -u origin master git branch git checkout git branch nueva_rama git checkout -b nueva_rama git pull origin master git push origin features/ma-1.0 git merge master Para usar el tipo GITFLOW se instala: sudo dnf copr enable elegos/gitflow dnf install git-flow Resumen: En este art\u00edculo, hemos explicado el flujo de trabajo de Gitflow. Gitflow es uno de los muchos estilos de flujos de trabajo de Git que pod\u00e9is utilizar tu equipo y t\u00fa. El flujo general de Gitflow es el siguiente: Se crea una rama develop a partir de main. Se crea una rama release a partir de la develop. Se crean ramas feature a partir de la develop. Cuando se termina una rama feature, se fusiona en la rama develop. Cuando la rama release est\u00e1 lista, se fusiona en las ramas develop y main. Si se detecta un problema en main, se crea una rama hotfix a partir de main. Una vez terminada la rama hotfix, esta se fusiona tanto en develop como en main. Con comandos git flow: # inicio repo git flow git flow init # Creaci\u00f3n de una rama de funci\u00f3n git flow feature start feature_branch # Finalizaci\u00f3n de una rama de funci\u00f3n git flow feature finish feature_branch # Ramas de publicaci\u00f3n $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # Para finalizar una rama release, utiliza los siguientes m\u00e9todos: git flow release finish '0.1.0' # Ramas de correcci\u00f3n git flow hotfix start hotfix_branch # una rama hotfix se fusiona tanto en main como en develop. git flow hotfix finish hotfix_branch Con comandos git: # inicio repo git flow git init # rama developer git branch develop git push -u origin develop # Creaci\u00f3n de una rama de funci\u00f3n git checkout develop git checkout -b feature_branch # Finalizaci\u00f3n de una rama de funci\u00f3n git checkout develop git merge feature_branch # Ramas de publicaci\u00f3n git checkout develop git checkout -b release/0.1.0 Switched to a new branch 'release/0.1.0' # Para finalizar una rama release, utiliza los siguientes m\u00e9todos: git checkout main git merge release/0.1.0 # Ramas de correcci\u00f3n git checkout main git checkout -b hotfix_branch # una rama hotfix se fusiona tanto en main como en develop. git checkout main git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch Ejemplo: A continuaci\u00f3n, se incluye un ejemplo completo que demuestra un flujo de ramas de funci\u00f3n. Vamos a suponer que tenemos una configuraci\u00f3n del repositorio con una rama main: git checkout main git checkout -b develop git checkout -b feature_branch # work happens on feature branch git checkout develop git merge feature_branch git checkout main git merge develop git branch -d feature_branch Adem\u00e1s del flujo de feature y release, aqu\u00ed tenemos un ejemplo de hotfix: git checkout main git checkout -b hotfix_branch # work is done commits are added to the hotfix_branch git checkout develop git merge hotfix_branch git checkout main git merge hotfix_branch JENKINS Usaremos una imagen jenkins con la capa blueocean y dentro contiene maven. Maven es una herramienta de software para la gesti\u00f3n y construcci\u00f3n de proyectos Java creada por Jason van Zyl, de Sonatype, en 2002. Es similar en funcionalidad a Apache Ant, pero tiene un modelo de configuraci\u00f3n de construcci\u00f3n m\u00e1s simple, basado en un formato XML: FROM jenkinsci/blueocean USER root RUN apk update && apk add wget RUN wget --no-verbose -O /tmp/apache-maven-3.6.3-bin.tar.gz https://downloads.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz RUN tar xzf /tmp/apache-maven-3.6.3-bin.tar.gz -C /opt/ RUN ln -s /opt/apache-maven-3.6.3 /opt/maven RUN ln -s /opt/maven/bin/mvn /usr/local/bin RUN rm /tmp/apache-maven-3.6.3-bin.tar.gz RUN chown jenkins:jenkins /opt/maven; ENV MAVEN_HOME=/opt/mvn USER jenkins Construimos la imagen docker build -t jenkins/blueocean --no-cache . y el contenedor docker run --name jenkinsblue -p 8080:8080 -p 50000:50000 jenkins/blueocean . Entramos a localhost:8080 y pegamos el hash que nos dio al construir el container o lo vemos en el fichero que nos indica y seguimos los pasos de instalaci\u00f3n. PIPELINES 1 tutorial Hacemos una peque\u00f1a prueba de construir con nuestro gitlab el codigo de billing/pom.xml, a\u00f1adiendo nuestra credenciales en un proyecto de top-level maven. Probamos la construcci\u00f3n y nos salga SUCCESS. 2 Ahora vamos a probar de a\u00f1adir un WEBHOOK con git para que cuando se haga un cambio en el codigo, automaticamente se lo comunique a jenkins y pueda lanzar un nuevo build. Para ello, usaremos la herramienta NGROK . ngrok es un ejecutable \u00fanico y sin dependencias que podemos descargar para Windows, Linux, macOS y FreeBSD, y que con una simple instrucci\u00f3n nos permite exponer hacia el exterior cualquier servicio web local que tengamos en nuestro ordenador, en cualquier puerto. Seguimos las instrucciones de registro y luego lanzamos el ngrok para que nos de la url a redirigir entre git y jenkins con ngrok http puerto_jenkins(8080) . Vamos a Github donde el codigo de nuestro repo, vamos a settings y a\u00f1adimos webhook con la url indicada en ngrok https://64c1-90-168-170-12.ngrok.io/github-webhook/ en json y que haga push en cada cosa que pase. Veremos en ngrok en la consola que nos sale STATUS 200 OK. Ahora haremos una nueva rama git branch feature/addtest master para hacer cambios y subirlo a git esta nueva rama y cuando detecte este cambio, se lanzar\u00e1 un nuevo webhook que lo podremos comprobar en la consola de ngrok y en github para ver que s\u00ed funciona y ahora podamos construir un nuevo pipeline automatico al hacer cambios en el repo. Session Status online Account isx46410800 (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding http://64c1-90-168-170-12.ngrok.io -> http://localhost:8080 Forwarding https://64c1-90-168-170-12.ngrok.io -> http://localhost:8080 Connections ttl opn rt1 rt5 p50 p90 4 0 0.00 0.00 5.19 6.19 HTTP Requests ------------- POST /github-webhook/ 200 OK POST /github-webhook/ 200 OK Ahora administraremos jenkins y agregaremos al sistema nuestro servidor github con un secret token que obtendremos de settings - developer options - token de github. Lo a\u00f1adimos y creamos un nuevo pipeline - github project - repo git con credenciales de usuario - proyecto maven y ponemos clean install y el pom.xml. Ahora hacemos algun cambio en la rama de test y lo subimos y veremos como se lanza correctamente. Una vez que est\u00e1 SUCCESS y estamos satisfechos , hacemos en github un PULL REQUEST(es hacer como un merge a la rama master para integrar lo nuevo) y vemos en settings si est\u00e1 activado lo de eliminar automaticamente al hacer un pull request. Hacemos merge y ya en nuestro local de consola podemos eliminar tambien esta rama. 3 En estos pasos hacemos un actualizaci\u00f3n del codigo de billing a\u00f1adiendo nuevos test de pruebas. Despues configuramos el build indicando github project, repo git, credenciales, la nueva rama feature/addtest como rama a construir, en comportamientos especiales indicamos jenkins y un email, quitamos el webhook automatico y en el build dejamos el clean install en un projecto maven y a\u00f1adimos con comando shell: git branch, git checkout origin/master y git merge de la nueva rama. Despues en acciones de despues del build hacemos push solo si est\u00e1 todo SUCCESS e indicamos las ramas de master y origin como si fuese el merge. Acabaremos teniendo en nuestra repositorio github solo una rama y el contenido unido automatizado todo. plugin git jenkins usado para acciones de postbuild,etc. 4 A\u00f1adiendo SLACK a la integraci\u00f3n del proceso automatizado. Creamos un canal normal y luego vamos a apliaciones y buscamos jenkins y lo integramos al canal que hemos creado antes para vincularlo. A continuaci\u00f3n, te indica los pasos para integrarlo en el servidor jenkins. Despues en jenkins vamos a plugins e instalamos slack notification . Despues una vez reiniciado, vamos a configurar - sistema - slack y metemos los datos que nos indica el tutorial de integraci\u00f3n. Probamos conexion y SUCCESS/SAVE. Ahora configuramos el projecto y al final del todo indicamos en postbuild las notificaciones de slack. Hacemos algun cambio en el projecto y vemos el resultado en slack. SONARQUBE INSTALACI\u00d3N Y CONFIGURACI\u00d3N SonarQube es una plataforma para evaluar c\u00f3digo fuente. Es software libre y usa diversas herramientas de an\u00e1lisis est\u00e1tico de c\u00f3digo fuente como Checkstyle, PMD o FindBugs para obtener m\u00e9tricas que pueden ayudar a mejorar la calidad del c\u00f3digo de un programa. Integracion con jenkins Lo vamos a instalar en docker: docker pull sonarqube docker run --name sonarqube -p 9000:9000 -p 9002:9002 sonarqube -d Ahora creamos una red virtual para jenkins y sonarqube se puedan comunicar: [miguel@fedora ~]$ docker network create jenkins_sonarqube 0fb0f16f8d5b6db1c9416c0bcbb0b703696acfc41f90b4b84a9c113952a047f9 [miguel@fedora ~]$ docker network connect jenkins_sonarqube sonarqube [miguel@fedora ~]$ docker network connect jenkins_sonarqube jenkinsblue Podemos comprobar las redes con docker container inspect container_name Nos conectamos a sonarqube con localhost:9000 Configuramos sonarqube. Vamos a Administration - security - users - creamos token para jenkins ccca6584e7016f16486388e025e1ce0251c22d90 Ahora vamos a Jenkins, instalamos el plugin de Sonarqube scanner, vamos despues a manage jenkins - sistema - sonarqube y a\u00f1adimos el token como credenciales de sonarqube(nombre container o ip publica si es remoto). Despues volvemos a Manage jenkins - global tools y en Sonarqube scanner instalamos la ultima version. Ahora vamos a configurar nuestro ultimo pipeline de webhook con slack y a\u00f1adimos un nuevo paso que pondremos primero en BUILD: nuevo execute con sonarqube, le ponemos un nombre, le ponemos de argumento -X para debugging y despues ponemos las siguientes variables para indicarle donde tiene que analizar el c\u00f3digo: sonar.projectKey=sonarqube sonar.sources=billing/src/main/java sonar.java.binaries=billing/target/classes Nombre proyecto, codigo fuente y binarios. Veremos ahora un nuevo icono y menu de SUCCESS de Sonarqube. ANALISIS DE RESULTADOS GUIA Sonarqube nos analiza las lineas de codigo y podemos encontrar: bugs, code smell(mejoras de codigo),etc. Podemos ir a ISSUES y analizar cada linea y ver donde se puede mejorar, asignar a gente, comentarlo, marcar que ya est\u00e1 corregido, ver que lineas afectan, etc. DOCKER Ahora vamos a montar automaticamente los container con docker. Instalamos el plugin docker build and publish en jenkins. Creamos puente de conexion entre mi maquina y docker engine ip route show default | awk '/default/ {print $3}' En /lib/systemd/system/docker.service a\u00f1adimos ExecStart=/usr/bin/dockerd -H fd:// -H=tcp://0.0.0.0:2375 Reiniciamos ambos servicios y comprobamos que tengamos los containers encendidos systemctl daemon-reload y service restart docker . Comprobamos que podemos entrar a la API de docker engine curl http://localhost:2375/images/json En configurar nuestro pipeline de webhook. Ahora hemos cambiado los Dockerfiles y unos ficheros para poner otro puerto y hemos modificado el pipeline para construir la imagen automatizada y subida a dockerhub. KUBERNETES Ahora nos conectamos al contenedor de jenkins para instalar la herramienta kubect de administraci\u00f3n de kubernetes: docker exec -it --user=root jenkinsblue /bin/bash Descargar kubectl y hacerlo ejecutable: curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.20.0/bin/linux/amd64/kubectl && chmod +x kubectl Crear el directorio para kubectl: sudo mv ./kubectl /usr/local/bin/kubectl Verificar version: kubectl version --client Ahora salimos y conectamos jenkins a la red de minikube: docker network connect minikube jenkinsblue Ahora vamos a jenkins e instalamos el plugin de kubernetes. Ahora con los ficheros personalizados para Jenkins, creamos un service account para crear la conexion de jenkins con kubernetes. Despues vemos cual es nuestra url y como obtener el token para poder acceder despues al deploy de kubernetes. kubectl apply -f jenkins-account.yaml #this file define a service account for kubernetesplugins on jenkins --- apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: default --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: jenkins namespace: default rules: - apiGroups: [\"\"] resources: [\"pods\",\"services\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"apps\"] resources: [\"deployments\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/exec\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/log\"] verbs: [\"get\",\"list\",\"watch\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: jenkins namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkins subjects: - kind: ServiceAccount name: jenkins --- # Allows jenkins to create persistent volumes # This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace. kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: jenkins-crb subjects: - kind: ServiceAccount namespace: default name: jenkins roleRef: kind: ClusterRole name: jenkinsclusterrole apiGroup: rbac.authorization.k8s.io --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # \"namespace\" omitted since ClusterRoles are not namespaced name: jenkinsclusterrole rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] Obtenemos la url y el token: ver la configuracion de minikube kubectl config view consultar los server account kubectl --namespace default get serviceaccount vel el detalle del server account kubectl --namespace default get serviceaccount jenkins -o yaml obtener el token del server account kubectl describe secrets/jenkins-token-rk2mg Despues vamos a sistema - cloud. Abrimos en pesta\u00f1a nueva, indicamos kubernetes y con los comandos de antes indicamos la url y el contenido del certificado de autoridad. Indicamos las credenciales y test conection. Con esto ya confirmamos que tenemos conexion de jenkins con la api del cluster de kubernetes. pipeline { agent any stages { stage('clone repository') { steps { sh '''java -version mvn --version git --version''' } } stage('Deploy billing App') { steps { withCredentials(bindings: [ string(credentialsId: 'kubernete-jenkis-server-account', variable: 'api_token') ]) { sh 'kubectl --token $api_token --server https://192.168.49.2:8443 --insecure-skip-tls-verify=true apply -f deployment-billing-app-back-jenkins.yaml ' } } } } } Despues lo comprobamos el deploy en el kubernetes dashboard y probarlo en la ip de minikube con el puerto indicado en el yaml. Podemos unir los dos pipelines indicando en el de webhook accion de despues ejecutar con otros proyectos y le ponemos el nombre del deploy_kubernetes y har\u00eda los dos deploy seguidos de pipelines. PROMETEUS Prometheus es un software especializado como sistema de monitorizaci\u00f3n y alertas escrito en el lenguaje de programaci\u00f3n Go. Todos los datos y m\u00e9tricas se almacenan en la base de datos como series temporales (junto al instante de tiempo en el que el valor se ha registrado). Tambi\u00e9n es posible a\u00f1adir etiquetas de tipo clave-valor junto a estas m\u00e9tricas. Prometheus tiene 3 componentes principales. Servidor: Almacena los datos de las m\u00e9tricas. Librer\u00eda Cliente: Se usa para calcular y exponer las m\u00e9tricas al cliente. Gestor de alertas: Genera alertas basadas en reglas. M\u00e9tricas en Prometheus: El tipo de m\u00e9trica contador es un valor que solo se puede incrementar o bien resetear. Se puede usar para contar el n\u00famero de peticiones o de errores en una aplicaci\u00f3n, que son m\u00e9tricas que nunca se reducen. Gauge o medidor es un valor num\u00e9rico que puede incrementarse o decrementar. Un ejemplo puede ser el n\u00famero de servidores en un sistema distribuido. El histograma representa los valores en agrupaciones predefinidas y acumuladas en el tiempo. Por ejemplo, se puede usar un histograma para medir los tiempos de respuesta de nuestra aplicaci\u00f3n en los intervalos de tiempo establecidos para cada petici\u00f3n de un cliente. Comandos seccion: 1- Namespaces: Clusters virtuales en un mismo cluster fisico (separacion logic ade clusters) https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2-verificar namespaces kubectl get namespace 3- Crear el namespace si no existe kubectl create namespace monitoring 4-Crear role de monitorizacion Referencia Authoriation https://kubernetes.io/docs/reference/access-authn-authz/rbac/ kubectl apply -f moniring-role.yaml 5- Crear fichero de configuracion para externalizar la configuracion de prometheus(independiente del ciclo de vida del contenedor) kubectl apply -f configmap-prometheus.yaml 6- Crear el contenedor y el servicio de prometheus (el contenedor) kubectl apply -f deployment-prometheus.yaml 7- verificar los pods del namespace monitoring kubectl get all --namespace=monitoring Vamos a minikube ip para saber la ip y el puerto 30000 para ver que tenemos ya prometheus en marcha. Dentro de prometheus, en status-target vemos los jobs creados en los jobs del yaml. Despues creamos servicios de metricas y alertas. GRAFANA Grafana es el componente que nos permitir\u00e1 crear dashboards para comprender lo que est\u00e1 pasando en nuestro sistema monitorizado. Se integra perfectamente con el software de Prometheus que, aunque ya nos proporciona estos gr\u00e1ficos en su propia interfaz gr\u00e1fica, est\u00e1n pensados como un mecanismo de test o debug. apiVersion: apps/v1 kind: Deployment metadata: name: grafana namespace: monitoring spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: name: grafana labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:latest ports: - name: grafana containerPort: 3000 resources: limits: memory: \"1Gi\" cpu: \"1000m\" requests: memory: 500M cpu: \"500m\" volumeMounts: - mountPath: /var/lib/grafana name: grafana-storage - mountPath: /etc/grafana/provisioning/datasources name: grafana-datasources readOnly: false volumes: - name: grafana-storage emptyDir: {} - name: grafana-datasources configMap: defaultMode: 420 name: grafana-datasources --- apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring annotations: prometheus.io/scrape: 'true' prometheus.io/port: '3000' spec: selector: app: grafana type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 32000 ############################################### #grafana public templates #https://grafana.com/grafana/dashboards?dataSource #https://grafana.com/grafana/dashboards/6417 #https://grafana.com/grafana/dashboards/8588 apiVersion: v1 kind: ConfigMap metadata: name: grafana-datasources namespace: monitoring #This data source is fro prometheus, You can add more datasources here adding more .yml files data: prometheus.yaml: |- { \"apiVersion\": 1, \"datasources\": [ { \"access\":\"proxy\", \"editable\": true, \"name\": \"prometheus\", \"orgId\": 1, \"type\": \"prometheus\", \"url\": \"http://prometheus-service.monitoring.svc:8280\", \"version\": 1 } ] }","title":"Devops"},{"location":"devops/#curso-devops-integrado","text":"QU\u00c9 ES DEVOPS La idea es una aplicaci\u00f3n hecha en angular con base de datos postgres y servidor web nginx. La cuesti\u00f3n es tener 5 nodos: uno de test, dos de preproducci\u00f3n y dos de producci\u00f3n. De manera m\u00e1s robusta ser\u00eda tener en cada nodo la app, el server web y la bbdd pero actualmente solo necesitamos tener en cada nodo Docker Engine instalado para una mayor facilidad y rapidez.","title":"CURSO DEVOPS INTEGRADO"},{"location":"devops/#instalacion","text":"Instalamos Docker engine y docker compose en nuestra m\u00e1quina. Comprovamos con --version. *NOTA WINDOWS+ Podemos instalar docker engine en windows . Instalamos docker desktop . Activamos virtualizaci\u00f3n en la bios y activamos WSL2 con Enable-WindowsOptionalFeature -Online -FeatureName $(\"VirtualMachinePlatform\", \"Microsoft-Windows-Subsystem-Linux\") Cogemos del repositorio de dockerhub del profe la imagen a descargar que contiene la app de facturaci\u00f3n en angular a utilizar en este curso: docker pull sotobotero/udemy-devops:0.0.1 Iniciamos el contenedor de la app mapeando puertos web y para puerto gr\u00e1fico: docker run -p 80:80 -p 8080:8080 --name billingapp sotobotero/udemy-devops:0.0.1 Comprobamos que funcionan en www.localhost:80 y www.localhost:8080/swagger-ui/index.html.","title":"INSTALACI\u00d3N"},{"location":"devops/#docker-compose","text":"","title":"DOCKER-COMPOSE"},{"location":"devops/#v1-scratch","text":"Creamos un docker-compose de prueba con dos nombres diferentes para ver como arranca de manera directa o con un nombre diferente: version: '3.1' services: db: container_name: postgres image: postgres restart: always environment: ports: - 5432:5432 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres adminer: container_name: adminer image: adminer restart: always depends_on: - db ports: - 9090:8080 Descargamos con docker-compose pull // docker-compose -f fichero.yml pull e iniciamos con docker-compose -f fichero.yml up -d Ahora podemos entrar de manera grafica a postgres con admirer en www.localhost:9090 e introducimos las credenciales y funcionando. Ahora vamos a crear from scratch un Dockerfile para crear la app de facturaci\u00f3n: #Creamos nuestra app de facturaci\u00f3n #Partimos de imagen base FROM nginx:alpine #Instalamos java RUN apk -U add openjdk8 && rm -rf /var/cache/apk/*; RUN apk add ttf-dejavu #Instalamos java microservicios y variables ENV JAVA_OPTS=\"\" ARG JAR_FILE ADD ${JAR_FILE} app.jar #Instalamos la app en nginx server y creamos un volumen para info y conf VOLUME /tmp RUN rm -rf /usr/share/nginx/html/* COPY nginx.conf /etc/nginx/nginx.conf COPY dist/billingApp /usr/share/nginx/html COPY appshell.sh appshell.sh #mapeamos puertos EXPOSE 80 8080 ENTRYPOINT [\"sh\", \"/appshell.sh\"] Lo construimos con docker build -t facturacionapp:prod --no-cache --build-arg JAR_FILE=target/*.jar . Lo iniciamos con los puertos EXPOSE docker run -p 80:80 -p 8080:8080 --name billingapp facturacionapp:prod Luego podemos subir nuestra version a Dockerhub con docker tag facturacionapp:prod isx46410800/facturacionapp:1.0 y la subimos docker push isx46410800/facturacionapp:1.0","title":"V1 SCRATCH"},{"location":"devops/#v2-services","text":"Siempre es local-host:container . Ahora creamos nuestra app de facturaci\u00f3n pero servicio a servicio separado en el docker-compose. Por un lado nos descargamos una imagen de nginx, por otro lado java y despues construimos manualmente con Dockerfile nuestra app angular y nuestra bbdd de postgresql: version: '3.1' services: #database engine service postgres_db: container_name: postgres image: postgres:latest restart: always environment: ports: - 5432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database admin service adminer: container_name: adminer image: adminer restart: always depends_on: - postgres_db ports: - 9090:8080 #Billin app backend service billingapp-back: build: context: ./java args: - JAR_FILE=*.jar container_name: billingApp-back environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db ports: - 8080:8080 #Billin app frontend service billingapp-front: build: context: ./angular container_name: billingApp-front depends_on: - billingapp-back ports: - 80:80 Construir las imagenes definidas en la orquestaci\u00f3n: docker-compose -f stack-billing.yml build Inicializar los contenedores de los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d No se ha especificado, pero todo esto se crea en una misma red virtual para que se puedan comunicar entre los diferentes servicios. Resumen de comandos en DOCKER-COMPOSE: Eliminar todos los contenedores detenidos: `docker system prune` Eliminar todas las im\u00e1genes: docker rmi $(docker images -a -q) Listar los volumenes: docker volume ls Eliminar todos los volumenes: docker volume prune Construir las imagenes definidas en la orquestaci\u00f3n: docker-compose -f stack-billing.yml build Inicializar los contenedores de los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d Detener todos los servicios de la orquestaci\u00f3n: docker-compose -f stack-billing.yml stop Escalar un servicio al iniciar la orquestaci\u00f3n: docker-compose -f stack-billing.yml up --scale billingapp-front=3 -d --force-recreate Detener todos los contenedores: docker stop $(docker ps -a -q) Listar las redes virtuales: docker network ls Eliminar las redes virtuales: docker network prune Reconstruir las imagenes: docker-compose -f stack-billing.yml build --no-cache Reconstruir los contenedores d ela orquestaci\u00f3n: docker-compose -f stack-billing.yml up -d --force-recreate Al crear los volumenes veremos que en las rutas indicadas tendremos los registros de la bbdd que vamos rellenando aunque detengamos y borremos nuestro docker-compose. Podemos escalar servicios con este comando: docker-compose -f stack-billing.yml up --scale billingapp-front=3 -d --force-recreated Tener cuidado que no este puesto el container name y poner un rango de puertos(80-83:80) en este caso en el front que escalamos. La otra forma de escalar seria a\u00f1adir directamente en el docker-compose.yaml: #Billin app frontend service billingapp-front: build: context: ./angular deploy: replicas: 3 resources: limits: cpus: \"0.10\" memory: 250M reservations: cpus: \"0.1\" memory: 120M #container_name: billingApp-front depends_on: - billingapp-back ports: - 80-83:80 Con docker stats vemos las estadisticas que por ejemplo delimitamos en el fichero docker-compose.yml.","title":"V2 SERVICES"},{"location":"devops/#v3-services-testprod","text":"Ahora veremos como montar de nuevo nuestra infraestructura de app, bbdd, server web y java, primero deploy en test y luego en producci\u00f3n: version: '3.1' services: #database engine service postgres_db_prod: container_name: postgres_prod image: postgres:latest restart: always networks: - env_prod environment: ports: - 5432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data_prod:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database engine service postgres_db_prep: container_name: postgres_prep image: postgres:latest restart: always networks: - env_prep environment: ports: - 4432:5432 volumes: #allow *.sql, *.sql.gz, or *.sh and is execute only if data directory is empty - ./dbfiles:/docker-entrypoint-initdb.d - /var/lib/postgres_data_prep:/var/lib/postgresql/data environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: qwerty POSTGRES_DB: postgres #database admin service #Use for All enviroments adminer: container_name: adminer image: adminer restart: always networks: - env_prod - env_prep depends_on: - postgres_db_prod - postgres_db_prep ports: - 9090:8080 #ENV_PROD #Billin app backend service billingapp-back-prod: build: context: ./java args: - JAR_FILE=billing-0.0.3-SNAPSHOT.jar networks: - env_prod container_name: billingApp-back-prod environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db_prod ports: - 8080:8080 #Billin app frontend service billingapp-front_prod: build: context: ./angular networks: - env_prod deploy: replicas: 2 resources: limits: cpus: \"0.15\" memory: 250M #recusos dedicados, mantiene los recursos disponibles del host para el contenedor reservations: cpus: 0.1 memory: 128M #container_name: billingApp-front depends_on: - billingapp-back-prod #rango de puertos para escalar ports: - 8081-8082:80 #ENV_PREP #Billin app backend service billingapp-back-prep: build: context: ./java args: - JAR_FILE=billing-0.0.2-SNAPSHOT.jar networks: - env_prep container_name: billingApp-back-prep environment: - JAVA_OPTS= -Xms256M -Xmx256M depends_on: - postgres_db_prep ports: - 7080:7080 #Billin app frontend service billingapp-front-prep: build: context: ./angular networks: - env_prep deploy: replicas: 2 resources: limits: cpus: \"0.15\" memory: 250M #recusos dedicados, mantiene los recursos disponibles del host para el contenedor reservations: cpus: 0.1 memory: 128M #container_name: billingApp-front depends_on: - billingapp-back-prep #rango de puertos para escalar ports: - 7081-7082:81 networks: env_prod: driver: bridge #activate ipv6 driver_opts: com.docker.network.enable_ipv6: \"true\" #IP Adress Manager ipam: driver: default config: - subnet: 172.16.232.0/24 gateway: 172.16.232.1 - subnet: \"2001:3974:3979::/64\" gateway: \"2001:3974:3979::1\" env_prep: driver: bridge #activate ipv6 driver_opts: com.docker.network.enable_ipv6: \"true\" #IP Adress Manager ipam: driver: default config: - subnet: 172.16.235.0/24 gateway: 172.16.235.1 - subnet: \"2001:3984:3989::/64\" gateway: \"2001:3984:3989::1\" Se especificas las networks que creamos y copiamos un clon de servicio de prod y pre teniendo en cuenta cambiar el path de la bbdd para que no se conflicten. docker-compose -f stack-billing.yml build --no-cache docker-compose -f stack-billing.yml up -d --force-recreate","title":"V3 SERVICES TEST/PROD"},{"location":"devops/#kubernettes","text":"","title":"KUBERNETTES"},{"location":"devops/#instalacion_1","text":"La arquitectura consta de un cluster donde hay nodos que trabajan(master y workers), donde dentro estan los pods. En este cluster tenemos Control manager, etcd(bbdd) y scheduler. Tenemos un Apiserver que es quien nos provee la interaccion con el cluster a traves de los comandos kubectl o manera grafica con el kubernetes dashboard . Normalmente se usa la infraestructura de amazon, azure o google. Minikube simula la infraestrura de un cluster de kubernetes y contiene todos los componentes necesarios en un solo nodo. Vemos si nuestro Linux tiene virtualizaci\u00f3n: grep -E --color 'vmx|svm' /proc/cpuinfo Descargar kubectl y hacerlo ejecutable: curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.20.0/bin/linux/amd64/kubectl && chmod +x kubectl Crear el directorio para kubectl: sudo mv ./kubectl /usr/local/bin/kubectl Verificar version: kubectl version --client Descargar minnukube y hacerlo ejecutable: curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube Crear el directorio para minikube: sudo mkdir -p /usr/local/bin/ Lanzar el ejecutable: sudo install minikube /usr/local/bin/ Comandos para operar minikube, lo lanzamos con minikube start : minikube start minikube status minikube stop Por defecto cuando hacemos start se ha de indicar cual es el hipervisor a trabajar, por defecto coge docker, pero se le puede poner hyperV, virtualbox,etc. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una capa de software para realizar una virtualizaci\u00f3n de hardware que permite utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Comprobamos que est\u00e1 encendido y el contenedor docker que se crea: [miguel@fedora Downloads]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured [miguel@fedora Downloads]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82f6891e5f44 gcr.io/k8s-minikube/kicbase:v0.0.28 \"/usr/local/bin/entr\u2026\" About a minute ago Up About a minute 127.0.0.1:49157->22/tcp, 127.0.0.1:49156->2376/tcp, 127.0.0.1:49155->5000/tcp, 127.0.0.1:49154->8443/tcp, 127.0.0.1:49153->32443/tcp minikube Lanzar el dashboard grafico donde nos manda a una url para manejarlo: minikube dashboard Eliminar el cluster de minikube: minikube delete Inicar un nuevo cluster de minikube usando el controlador hypervisor de virtualbox: minikube start --driver=virtualbox","title":"INSTALACI\u00d3N"},{"location":"devops/#pods","text":"Vamos a crear dentro de un cluster una app para que el cliente desde fuera pueda acceder desde el navegador. Para ello se crea un POD a trav\u00e9s de la imagen de la app, con volumenes persistentes y este pod tiene una IP no visible. Para que sea visible desde fuera, se crea un servicio para poder acceder a el desde una petici\u00f3n de fuera. Creamos un pod desde nuestra imagen de un repositorio dockerhub: kubectl run kbillingapp --image=sotobotero/udemy-devops:0.0.1 --port=80 80 Lo vemos con kubetctl get pods y kubectl describe pod name_pod . El pod tiene un IP no visible desde fuera y para que se pueda acceder se ha de exponer esta ip como si hicieramos un servicio: kubectl expose pod kbillingapp --type=LoadBalancer --port=8080 --target-port=80 [miguel@fedora Downloads]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kbillingapp LoadBalancer 10.105.34.39 <pending> 8080:30424/TCP 9s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 25m Al ser minikube y no tener mas nodos trabajando (vida real no pasaria) tenemos que pasar otro comando para obtener esa ip externa y acceder: [miguel@fedora Downloads]$ minikube service kbillingapp |-----------|-------------|-------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|---------------------------| | default | kbillingapp | 8080 | http://192.168.49.2:30424 | |-----------|-------------|-------------|---------------------------|","title":"PODS"},{"location":"devops/#variables-y-secrets","text":"En este caso vamos a tener dos pods: uno con una bbdd postgres que contiene volumenes persistentes y otro el pgadmin grafico. Esto tendr\u00e1 un servicio para poder acceder desde fuera de manera grafica. Para ello, vamos a crear en un SECRET fichero yaml, las credenciales de entrada para postgresql para que esten encriptadas y no sean visibles, lo hacemos en base64 con el comando echo -n \"palabra\" | base64 y para descodificarlo echo \"xxxx\" | base64 -d : #object that store enviroments variables that could be have sensitive data like a password apiVersion: v1 kind: Secret metadata: name: postgres-secret labels: app: postgres #meant that we can use arbitrary key pair values type: Opaque data: POSTGRES_DB: cG9zdGdyZXM= POSTGRES_USER: cG9zdGdyZXM= POSTGRES_PASSWORD: cXdlcnR5 Fichero secret-dev.yaml Creamos otro secret para el PGADMIN grafico: #object that store enviroments variables that could be have sensitive data like a password apiVersion: v1 kind: Secret metadata: name: pgadmin-secret labels: app: postgres #meant that we can use arbitrary key pair values type: Opaque data: PGADMIN_DEFAULT_EMAIL: YWRtaW5AYWRtaW4uY29t PGADMIN_DEFAULT_PASSWORD: cXdlcnR5 PGADMIN_PORT: ODA= Fichero secret-pgadmin.yaml","title":"VARIABLES Y SECRETS"},{"location":"devops/#volumes-y-configmaps","text":"Ahora creamos un volumen para la data persistente y un reclamo de espacio para ello. Tambi\u00e9n creamos un configmap para la configuraci\u00f3n de que lance un script a la hora de crear todo: #persistence volumen (PV) is a piece of storage that have idependent lifecycle from pods #thees preserve data throug restartin, rescheduling and even deleting pods #PersistenceVolumeCalin is a request for storage by the user that can be fulfilled by teh PV kind: PersistentVolume #version of ApiServer on control panel node (/api/v1) check using kubectl api-versions apiVersion: v1 metadata: name: postgres-volume labels: #it is aplugin suport many luke amazon EBS azure disk etc. local = local storage devices mounted on nodes. type: local app: postgres spec: storageClassName: manual capacity: storage: 5Gi #many pods on shcheduled on differents nodes can read and write accessModes: - ReadWriteMany #path on cluster's node hostPath: path: \"/mnt/data/\" #it is a reques of resource (persistence volume) from a pod by example, teh pod claim by a storage throug PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-claim labels: app: postgres spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 2Gi apiVersion: v1 kind: ConfigMap metadata: name: postgres-init-script-configmap data: initdb.sh: |- #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL CREATE USER billingapp WITH PASSWORD 'qwerty'; CREATE DATABASE billingapp_db; GRANT ALL PRIVILEGES ON DATABASE billingapp_db TO billingapp; EOSQL","title":"VOLUMES Y CONFIGMAPS"},{"location":"devops/#deploys-y-services","text":"Ahora definimos el deloy completo de la base de datos que contiene un fichero de variables de entorno, un volumen persistente, el reclamo de espacio del volumen y el configmap con el script de iniciaci\u00f3n. apiVersion: apps/v1 kind: Deployment metadata: name: postgres-deployment labels: app: postgres spec: #Pods number replicates replicas: 1 #Define how the Deployment identify the pods that it could manage selector: matchLabels: app: postgres #pod template specification template: metadata: #define teh labels for all pods labels: app: postgres spec: containers: - name: postgres image: postgres:latest imagePullPolicy: IfNotPresent #open the port to allow send and receive traffic in teh container ports: - containerPort: 5432 #read envars from secret file envFrom: - secretRef: name: postgres-secret volumeMounts: #This is the path in the container on which the mounting will take place. - mountPath: /var/lib/postgresql/data name: postgredb - mountPath: /docker-entrypoint-initdb.d name : init-script volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-claim - name: init-script configMap: name: postgres-init-script-configmap Ahora hacemos el deploy del pgadmin gr\u00e1fico para acceder a la bbdd graficamente con el puerto y fichero de varibles de entorno: apiVersion: apps/v1 kind: Deployment metadata: name: pgadmin-deployment spec: selector: matchLabels: app: pgadmin replicas: 1 template: metadata: labels: app: pgadmin spec: containers: - name: pgadmin4 image: dpage/pgadmin4 envFrom: - secretRef: name: pgadmin-secret ports: - containerPort: 80 name: pgadminport Ahora tenemos que crear los servicios de ambos pods para poder acceder desde fuera en internet a trav\u00e9s de un puerto publico y est\u00e1n en nodo port porque solo tenemos un nodo, sino seria por loadbalancer por diferentes ip y su carga: kind: Service apiVersion: v1 metadata: name: postgres-service labels: app: postgres spec: ports: - name: postgres port: 5432 nodePort : 30432 #type: LoadBalancer type: NodePort selector: app: postgres apiVersion: v1 kind: Service metadata: name: pgadmin-service labels: app: pgadmin spec: selector: app: pgadmin type: NodePort ports: - port: 80 nodePort: 30200 Para crearlo ahora en kubernetes usamos kubectl apply -f fichero.yaml . Primero los secrets, despues el configmap, despues los volumenes, despues los deploys y si est\u00e1 todo OK, por ultimo los servicios. Comprobamos todo con kubectl get all . Con todo lanzado ahora nos podemos conectar desde fuera por la ip de cada uno, no obstante, al ser nuestro un minikube, debemos usar la ip de este y el puerto de cada cosa. Para ver la ip usamos minikube ip . Ahora tenemos la ip y usamos el puerto de postgres y el puerto de pgadmin para conectarse a cada cosa. Dentro de pgadmin nos podemos conectar a la bbdd de postgres con la ip del pod o la ip del cluster y su puerto correspondente. Si no queremos usar el pgadmin como grafico de bbdd, podemos usar otro motor y nos conectamos al servicio postgres desde fuera y lo tendremos igual. Podemos eliminar todo lo realizado con kubectl delete -f ./ .","title":"DEPLOYS Y SERVICES"},{"location":"devops/#orquestacion-real","text":"Ahora a\u00f1adiremos el servicio backend y frontend de nuestra aplicaci\u00f3n. El frotend est\u00e1 basado en un server nginx y escrita en angular por lo que al usuario que hace la petici\u00f3n le daremos respuesta html. El backend est\u00e1 escrito en springboot. En este caso tambi\u00e9n se pone una ip fija al servicio postgres ya que la bbdd no ser\u00eda eficiente que vaya cambiando la ip y no encontrar\u00eda la info nuestro backend para poder dar respuesta a las peticiones de los usuarios en el frontend a trav\u00e9s del navegador. kind: Service apiVersion: v1 metadata: name: postgres-service labels: app: postgres spec: clusterIP: 10.96.1.2 ipFamilies: - IPv4 ports: - name: postgres port: 5432 nodePort : 30432 #type: LoadBalancer type: NodePort selector: app: postgres Para que el engine apunte al registro de imagenes en local usamos: minikube docker-env [miguel@fedora deploy_app]$ minikube docker-env export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.49.2:2376\" export DOCKER_CERT_PATH=\"/home/miguel/.minikube/certs\" export MINIKUBE_ACTIVE_DOCKERD=\"minikube\" # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) eval $(minikube docker-env) Ahora ya estamos listo para poder crear las imagenes personalizadas y se puedan usar en minikube. Esto solo valdr\u00e1 en esta terminal. Creamos la imagen de java jar: docker build -t billingapp-back:0.0.4 --no-cache --build-arg JAR_FILE=/*.jar . Creamos la imagen de angular: docker build -t billingapp-front:0.0.4 --no-cache . Una vez creadas las dos imagenes, creamos los dos deployments y los dos servicios de la aplicaci\u00f3n a partir de estas imagenes y le ponemos las replicas que necesitamos: apiVersion: apps/v1 kind: Deployment metadata: name: billing-app-back-deployment spec: selector: matchLabels: app: billing-app-back replicas: 3 template: metadata: labels: app: billing-app-back spec: containers: - name: billing-app-back image: billingapp-back:0.0.4 ports: - containerPort: 7080 name: billingappbport apiVersion: apps/v1 kind: Deployment metadata: name: billing-app-front-deployment spec: selector: matchLabels: app: billing-app-front replicas: 2 template: metadata: labels: app: billing-app-front spec: containers: - name: billing-app-front image: billingapp-front:0.0.4 ports: - containerPort: 80 name: billingappfport kind: Service apiVersion: v1 metadata: name: billing-app-back-service labels: app: billing-app-back spec: ports: - name: billing-app-back port: 7080 nodePort : 30780 #type: LoadBalancer type: NodePort selector: app: billing-app-back apiVersion: v1 kind: Service metadata: name: billing-app-front-service labels: app: billing-app-front spec: selector: app: billing-app-front type: NodePort ports: - port: 80 nodePort: 30100 Ahora con todos los servicios metidos, con minikube ip podemos ir con esta ip y el puerto de cada cosa para ver que todo funciona. INtroducir registros y ver como se guardan en la base de datos.","title":"ORQUESTACI\u00d3N REAL"},{"location":"devops/#git","text":"Es un sistema de gesti\u00f3n de versiones distribuido y opensource. Hay dos maneras de trabajo: trunked based es la manera m\u00e1s basica de dos ramas y merge; y gitflow que consiste en trabajar en 5 ramas: master, develop, features(donde se va poniendo el codigo nuevo), releases(donde se prueba develop unido de features en entorno de pruebas) y hotfix(rama que sale de urgencia de master para arreglar fallos urgentes de master en producci\u00f3n). Comandos b\u00e1sicos: git init git commit -m \"\" git push -u origin master git branch git checkout git branch nueva_rama git checkout -b nueva_rama git pull origin master git push origin features/ma-1.0 git merge master Para usar el tipo GITFLOW se instala: sudo dnf copr enable elegos/gitflow dnf install git-flow Resumen: En este art\u00edculo, hemos explicado el flujo de trabajo de Gitflow. Gitflow es uno de los muchos estilos de flujos de trabajo de Git que pod\u00e9is utilizar tu equipo y t\u00fa. El flujo general de Gitflow es el siguiente: Se crea una rama develop a partir de main. Se crea una rama release a partir de la develop. Se crean ramas feature a partir de la develop. Cuando se termina una rama feature, se fusiona en la rama develop. Cuando la rama release est\u00e1 lista, se fusiona en las ramas develop y main. Si se detecta un problema en main, se crea una rama hotfix a partir de main. Una vez terminada la rama hotfix, esta se fusiona tanto en develop como en main. Con comandos git flow: # inicio repo git flow git flow init # Creaci\u00f3n de una rama de funci\u00f3n git flow feature start feature_branch # Finalizaci\u00f3n de una rama de funci\u00f3n git flow feature finish feature_branch # Ramas de publicaci\u00f3n $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # Para finalizar una rama release, utiliza los siguientes m\u00e9todos: git flow release finish '0.1.0' # Ramas de correcci\u00f3n git flow hotfix start hotfix_branch # una rama hotfix se fusiona tanto en main como en develop. git flow hotfix finish hotfix_branch Con comandos git: # inicio repo git flow git init # rama developer git branch develop git push -u origin develop # Creaci\u00f3n de una rama de funci\u00f3n git checkout develop git checkout -b feature_branch # Finalizaci\u00f3n de una rama de funci\u00f3n git checkout develop git merge feature_branch # Ramas de publicaci\u00f3n git checkout develop git checkout -b release/0.1.0 Switched to a new branch 'release/0.1.0' # Para finalizar una rama release, utiliza los siguientes m\u00e9todos: git checkout main git merge release/0.1.0 # Ramas de correcci\u00f3n git checkout main git checkout -b hotfix_branch # una rama hotfix se fusiona tanto en main como en develop. git checkout main git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch Ejemplo: A continuaci\u00f3n, se incluye un ejemplo completo que demuestra un flujo de ramas de funci\u00f3n. Vamos a suponer que tenemos una configuraci\u00f3n del repositorio con una rama main: git checkout main git checkout -b develop git checkout -b feature_branch # work happens on feature branch git checkout develop git merge feature_branch git checkout main git merge develop git branch -d feature_branch Adem\u00e1s del flujo de feature y release, aqu\u00ed tenemos un ejemplo de hotfix: git checkout main git checkout -b hotfix_branch # work is done commits are added to the hotfix_branch git checkout develop git merge hotfix_branch git checkout main git merge hotfix_branch","title":"GIT"},{"location":"devops/#jenkins","text":"Usaremos una imagen jenkins con la capa blueocean y dentro contiene maven. Maven es una herramienta de software para la gesti\u00f3n y construcci\u00f3n de proyectos Java creada por Jason van Zyl, de Sonatype, en 2002. Es similar en funcionalidad a Apache Ant, pero tiene un modelo de configuraci\u00f3n de construcci\u00f3n m\u00e1s simple, basado en un formato XML: FROM jenkinsci/blueocean USER root RUN apk update && apk add wget RUN wget --no-verbose -O /tmp/apache-maven-3.6.3-bin.tar.gz https://downloads.apache.org/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz RUN tar xzf /tmp/apache-maven-3.6.3-bin.tar.gz -C /opt/ RUN ln -s /opt/apache-maven-3.6.3 /opt/maven RUN ln -s /opt/maven/bin/mvn /usr/local/bin RUN rm /tmp/apache-maven-3.6.3-bin.tar.gz RUN chown jenkins:jenkins /opt/maven; ENV MAVEN_HOME=/opt/mvn USER jenkins Construimos la imagen docker build -t jenkins/blueocean --no-cache . y el contenedor docker run --name jenkinsblue -p 8080:8080 -p 50000:50000 jenkins/blueocean . Entramos a localhost:8080 y pegamos el hash que nos dio al construir el container o lo vemos en el fichero que nos indica y seguimos los pasos de instalaci\u00f3n.","title":"JENKINS"},{"location":"devops/#pipelines","text":"","title":"PIPELINES"},{"location":"devops/#1","text":"tutorial Hacemos una peque\u00f1a prueba de construir con nuestro gitlab el codigo de billing/pom.xml, a\u00f1adiendo nuestra credenciales en un proyecto de top-level maven. Probamos la construcci\u00f3n y nos salga SUCCESS.","title":"1"},{"location":"devops/#2","text":"Ahora vamos a probar de a\u00f1adir un WEBHOOK con git para que cuando se haga un cambio en el codigo, automaticamente se lo comunique a jenkins y pueda lanzar un nuevo build. Para ello, usaremos la herramienta NGROK . ngrok es un ejecutable \u00fanico y sin dependencias que podemos descargar para Windows, Linux, macOS y FreeBSD, y que con una simple instrucci\u00f3n nos permite exponer hacia el exterior cualquier servicio web local que tengamos en nuestro ordenador, en cualquier puerto. Seguimos las instrucciones de registro y luego lanzamos el ngrok para que nos de la url a redirigir entre git y jenkins con ngrok http puerto_jenkins(8080) . Vamos a Github donde el codigo de nuestro repo, vamos a settings y a\u00f1adimos webhook con la url indicada en ngrok https://64c1-90-168-170-12.ngrok.io/github-webhook/ en json y que haga push en cada cosa que pase. Veremos en ngrok en la consola que nos sale STATUS 200 OK. Ahora haremos una nueva rama git branch feature/addtest master para hacer cambios y subirlo a git esta nueva rama y cuando detecte este cambio, se lanzar\u00e1 un nuevo webhook que lo podremos comprobar en la consola de ngrok y en github para ver que s\u00ed funciona y ahora podamos construir un nuevo pipeline automatico al hacer cambios en el repo. Session Status online Account isx46410800 (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding http://64c1-90-168-170-12.ngrok.io -> http://localhost:8080 Forwarding https://64c1-90-168-170-12.ngrok.io -> http://localhost:8080 Connections ttl opn rt1 rt5 p50 p90 4 0 0.00 0.00 5.19 6.19 HTTP Requests ------------- POST /github-webhook/ 200 OK POST /github-webhook/ 200 OK Ahora administraremos jenkins y agregaremos al sistema nuestro servidor github con un secret token que obtendremos de settings - developer options - token de github. Lo a\u00f1adimos y creamos un nuevo pipeline - github project - repo git con credenciales de usuario - proyecto maven y ponemos clean install y el pom.xml. Ahora hacemos algun cambio en la rama de test y lo subimos y veremos como se lanza correctamente. Una vez que est\u00e1 SUCCESS y estamos satisfechos , hacemos en github un PULL REQUEST(es hacer como un merge a la rama master para integrar lo nuevo) y vemos en settings si est\u00e1 activado lo de eliminar automaticamente al hacer un pull request. Hacemos merge y ya en nuestro local de consola podemos eliminar tambien esta rama.","title":"2"},{"location":"devops/#3","text":"En estos pasos hacemos un actualizaci\u00f3n del codigo de billing a\u00f1adiendo nuevos test de pruebas. Despues configuramos el build indicando github project, repo git, credenciales, la nueva rama feature/addtest como rama a construir, en comportamientos especiales indicamos jenkins y un email, quitamos el webhook automatico y en el build dejamos el clean install en un projecto maven y a\u00f1adimos con comando shell: git branch, git checkout origin/master y git merge de la nueva rama. Despues en acciones de despues del build hacemos push solo si est\u00e1 todo SUCCESS e indicamos las ramas de master y origin como si fuese el merge. Acabaremos teniendo en nuestra repositorio github solo una rama y el contenido unido automatizado todo. plugin git jenkins usado para acciones de postbuild,etc.","title":"3"},{"location":"devops/#4","text":"A\u00f1adiendo SLACK a la integraci\u00f3n del proceso automatizado. Creamos un canal normal y luego vamos a apliaciones y buscamos jenkins y lo integramos al canal que hemos creado antes para vincularlo. A continuaci\u00f3n, te indica los pasos para integrarlo en el servidor jenkins. Despues en jenkins vamos a plugins e instalamos slack notification . Despues una vez reiniciado, vamos a configurar - sistema - slack y metemos los datos que nos indica el tutorial de integraci\u00f3n. Probamos conexion y SUCCESS/SAVE. Ahora configuramos el projecto y al final del todo indicamos en postbuild las notificaciones de slack. Hacemos algun cambio en el projecto y vemos el resultado en slack.","title":"4"},{"location":"devops/#sonarqube","text":"","title":"SONARQUBE"},{"location":"devops/#instalacion-y-configuracion","text":"SonarQube es una plataforma para evaluar c\u00f3digo fuente. Es software libre y usa diversas herramientas de an\u00e1lisis est\u00e1tico de c\u00f3digo fuente como Checkstyle, PMD o FindBugs para obtener m\u00e9tricas que pueden ayudar a mejorar la calidad del c\u00f3digo de un programa. Integracion con jenkins Lo vamos a instalar en docker: docker pull sonarqube docker run --name sonarqube -p 9000:9000 -p 9002:9002 sonarqube -d Ahora creamos una red virtual para jenkins y sonarqube se puedan comunicar: [miguel@fedora ~]$ docker network create jenkins_sonarqube 0fb0f16f8d5b6db1c9416c0bcbb0b703696acfc41f90b4b84a9c113952a047f9 [miguel@fedora ~]$ docker network connect jenkins_sonarqube sonarqube [miguel@fedora ~]$ docker network connect jenkins_sonarqube jenkinsblue Podemos comprobar las redes con docker container inspect container_name Nos conectamos a sonarqube con localhost:9000 Configuramos sonarqube. Vamos a Administration - security - users - creamos token para jenkins ccca6584e7016f16486388e025e1ce0251c22d90 Ahora vamos a Jenkins, instalamos el plugin de Sonarqube scanner, vamos despues a manage jenkins - sistema - sonarqube y a\u00f1adimos el token como credenciales de sonarqube(nombre container o ip publica si es remoto). Despues volvemos a Manage jenkins - global tools y en Sonarqube scanner instalamos la ultima version. Ahora vamos a configurar nuestro ultimo pipeline de webhook con slack y a\u00f1adimos un nuevo paso que pondremos primero en BUILD: nuevo execute con sonarqube, le ponemos un nombre, le ponemos de argumento -X para debugging y despues ponemos las siguientes variables para indicarle donde tiene que analizar el c\u00f3digo: sonar.projectKey=sonarqube sonar.sources=billing/src/main/java sonar.java.binaries=billing/target/classes Nombre proyecto, codigo fuente y binarios. Veremos ahora un nuevo icono y menu de SUCCESS de Sonarqube.","title":"INSTALACI\u00d3N Y CONFIGURACI\u00d3N"},{"location":"devops/#analisis-de-resultados","text":"GUIA Sonarqube nos analiza las lineas de codigo y podemos encontrar: bugs, code smell(mejoras de codigo),etc. Podemos ir a ISSUES y analizar cada linea y ver donde se puede mejorar, asignar a gente, comentarlo, marcar que ya est\u00e1 corregido, ver que lineas afectan, etc.","title":"ANALISIS DE RESULTADOS"},{"location":"devops/#docker","text":"Ahora vamos a montar automaticamente los container con docker. Instalamos el plugin docker build and publish en jenkins. Creamos puente de conexion entre mi maquina y docker engine ip route show default | awk '/default/ {print $3}' En /lib/systemd/system/docker.service a\u00f1adimos ExecStart=/usr/bin/dockerd -H fd:// -H=tcp://0.0.0.0:2375 Reiniciamos ambos servicios y comprobamos que tengamos los containers encendidos systemctl daemon-reload y service restart docker . Comprobamos que podemos entrar a la API de docker engine curl http://localhost:2375/images/json En configurar nuestro pipeline de webhook. Ahora hemos cambiado los Dockerfiles y unos ficheros para poner otro puerto y hemos modificado el pipeline para construir la imagen automatizada y subida a dockerhub.","title":"DOCKER"},{"location":"devops/#kubernetes","text":"Ahora nos conectamos al contenedor de jenkins para instalar la herramienta kubect de administraci\u00f3n de kubernetes: docker exec -it --user=root jenkinsblue /bin/bash Descargar kubectl y hacerlo ejecutable: curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.20.0/bin/linux/amd64/kubectl && chmod +x kubectl Crear el directorio para kubectl: sudo mv ./kubectl /usr/local/bin/kubectl Verificar version: kubectl version --client Ahora salimos y conectamos jenkins a la red de minikube: docker network connect minikube jenkinsblue Ahora vamos a jenkins e instalamos el plugin de kubernetes. Ahora con los ficheros personalizados para Jenkins, creamos un service account para crear la conexion de jenkins con kubernetes. Despues vemos cual es nuestra url y como obtener el token para poder acceder despues al deploy de kubernetes. kubectl apply -f jenkins-account.yaml #this file define a service account for kubernetesplugins on jenkins --- apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: default --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: jenkins namespace: default rules: - apiGroups: [\"\"] resources: [\"pods\",\"services\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"apps\"] resources: [\"deployments\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/exec\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/log\"] verbs: [\"get\",\"list\",\"watch\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: jenkins namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkins subjects: - kind: ServiceAccount name: jenkins --- # Allows jenkins to create persistent volumes # This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace. kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: jenkins-crb subjects: - kind: ServiceAccount namespace: default name: jenkins roleRef: kind: ClusterRole name: jenkinsclusterrole apiGroup: rbac.authorization.k8s.io --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # \"namespace\" omitted since ClusterRoles are not namespaced name: jenkinsclusterrole rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] Obtenemos la url y el token: ver la configuracion de minikube kubectl config view consultar los server account kubectl --namespace default get serviceaccount vel el detalle del server account kubectl --namespace default get serviceaccount jenkins -o yaml obtener el token del server account kubectl describe secrets/jenkins-token-rk2mg Despues vamos a sistema - cloud. Abrimos en pesta\u00f1a nueva, indicamos kubernetes y con los comandos de antes indicamos la url y el contenido del certificado de autoridad. Indicamos las credenciales y test conection. Con esto ya confirmamos que tenemos conexion de jenkins con la api del cluster de kubernetes. pipeline { agent any stages { stage('clone repository') { steps { sh '''java -version mvn --version git --version''' } } stage('Deploy billing App') { steps { withCredentials(bindings: [ string(credentialsId: 'kubernete-jenkis-server-account', variable: 'api_token') ]) { sh 'kubectl --token $api_token --server https://192.168.49.2:8443 --insecure-skip-tls-verify=true apply -f deployment-billing-app-back-jenkins.yaml ' } } } } } Despues lo comprobamos el deploy en el kubernetes dashboard y probarlo en la ip de minikube con el puerto indicado en el yaml. Podemos unir los dos pipelines indicando en el de webhook accion de despues ejecutar con otros proyectos y le ponemos el nombre del deploy_kubernetes y har\u00eda los dos deploy seguidos de pipelines.","title":"KUBERNETES"},{"location":"devops/#prometeus","text":"Prometheus es un software especializado como sistema de monitorizaci\u00f3n y alertas escrito en el lenguaje de programaci\u00f3n Go. Todos los datos y m\u00e9tricas se almacenan en la base de datos como series temporales (junto al instante de tiempo en el que el valor se ha registrado). Tambi\u00e9n es posible a\u00f1adir etiquetas de tipo clave-valor junto a estas m\u00e9tricas. Prometheus tiene 3 componentes principales. Servidor: Almacena los datos de las m\u00e9tricas. Librer\u00eda Cliente: Se usa para calcular y exponer las m\u00e9tricas al cliente. Gestor de alertas: Genera alertas basadas en reglas. M\u00e9tricas en Prometheus: El tipo de m\u00e9trica contador es un valor que solo se puede incrementar o bien resetear. Se puede usar para contar el n\u00famero de peticiones o de errores en una aplicaci\u00f3n, que son m\u00e9tricas que nunca se reducen. Gauge o medidor es un valor num\u00e9rico que puede incrementarse o decrementar. Un ejemplo puede ser el n\u00famero de servidores en un sistema distribuido. El histograma representa los valores en agrupaciones predefinidas y acumuladas en el tiempo. Por ejemplo, se puede usar un histograma para medir los tiempos de respuesta de nuestra aplicaci\u00f3n en los intervalos de tiempo establecidos para cada petici\u00f3n de un cliente. Comandos seccion: 1- Namespaces: Clusters virtuales en un mismo cluster fisico (separacion logic ade clusters) https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2-verificar namespaces kubectl get namespace 3- Crear el namespace si no existe kubectl create namespace monitoring 4-Crear role de monitorizacion Referencia Authoriation https://kubernetes.io/docs/reference/access-authn-authz/rbac/ kubectl apply -f moniring-role.yaml 5- Crear fichero de configuracion para externalizar la configuracion de prometheus(independiente del ciclo de vida del contenedor) kubectl apply -f configmap-prometheus.yaml 6- Crear el contenedor y el servicio de prometheus (el contenedor) kubectl apply -f deployment-prometheus.yaml 7- verificar los pods del namespace monitoring kubectl get all --namespace=monitoring Vamos a minikube ip para saber la ip y el puerto 30000 para ver que tenemos ya prometheus en marcha. Dentro de prometheus, en status-target vemos los jobs creados en los jobs del yaml. Despues creamos servicios de metricas y alertas.","title":"PROMETEUS"},{"location":"devops/#grafana","text":"Grafana es el componente que nos permitir\u00e1 crear dashboards para comprender lo que est\u00e1 pasando en nuestro sistema monitorizado. Se integra perfectamente con el software de Prometheus que, aunque ya nos proporciona estos gr\u00e1ficos en su propia interfaz gr\u00e1fica, est\u00e1n pensados como un mecanismo de test o debug. apiVersion: apps/v1 kind: Deployment metadata: name: grafana namespace: monitoring spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: name: grafana labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:latest ports: - name: grafana containerPort: 3000 resources: limits: memory: \"1Gi\" cpu: \"1000m\" requests: memory: 500M cpu: \"500m\" volumeMounts: - mountPath: /var/lib/grafana name: grafana-storage - mountPath: /etc/grafana/provisioning/datasources name: grafana-datasources readOnly: false volumes: - name: grafana-storage emptyDir: {} - name: grafana-datasources configMap: defaultMode: 420 name: grafana-datasources --- apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring annotations: prometheus.io/scrape: 'true' prometheus.io/port: '3000' spec: selector: app: grafana type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 32000 ############################################### #grafana public templates #https://grafana.com/grafana/dashboards?dataSource #https://grafana.com/grafana/dashboards/6417 #https://grafana.com/grafana/dashboards/8588 apiVersion: v1 kind: ConfigMap metadata: name: grafana-datasources namespace: monitoring #This data source is fro prometheus, You can add more datasources here adding more .yml files data: prometheus.yaml: |- { \"apiVersion\": 1, \"datasources\": [ { \"access\":\"proxy\", \"editable\": true, \"name\": \"prometheus\", \"orgId\": 1, \"type\": \"prometheus\", \"url\": \"http://prometheus-service.monitoring.svc:8280\", \"version\": 1 } ] }","title":"GRAFANA"},{"location":"docker/","text":"DOCKER APUNTES DOCKER INSTALACI\u00d3N Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world COMANDOS Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container Redes en Docker: docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed Volumes en Docker: docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash Docker Compose: docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap Docker SWARM: docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode ARQUITECTURA Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real. DOCKER IMAGES Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images DOCKERFILE El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito CMD VS ENTRYPOINT CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"] CENTOS-PHP-SSL Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi NGINX-PHP Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh MULTI-STAGE-BUILD Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo. PRUEBA REAL La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php DOCKER CONTAINERS Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen LISTAR/MAPEO PUERTOS docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080. INICIAR/DETENE/PAUSAR Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id VARIABLES DE ENTORNO En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins MYSQL Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333 MONGODB Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones APACHE/NGINX/TOMCAT Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine POSTGRES Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows) JENKINS Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins. LIMITAR RECURSOS Ayuda con: docker --help | grep \"xxxx\" MEMORIA Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb CPU Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2. COPIA DE ARCHIVOS De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/. CONTENEDOR A IMAGEN Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!! SOBREESCRIBIR CMD Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos DESTRUIR CONTAINER Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm... DOCUMENT ROOT El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi. DOCKER VOLUMES Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes VOLUMES HOST Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 VOLUMES ANONYMOYS Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root VOLUMES NAMED VOLUMES Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root PRUEBA REAL Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev DOCKER NETWORK Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping CREAR REDES Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB VER REDES Para ver las redes creadas: docker network inspect netA AGREGAR/CONECTAR REDES Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known ELIMINAR REDES Para eliminar redes: docker network remove netA netB ASIGNAR IPs Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos RED HOST Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel RED NONE Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos EXPONER IPs CONCRETAS Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2 DOCKER COMPOSE Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes INSTALACI\u00d3N Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose EJEMPLO Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down VARIABLES ENTORNO Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env VOL\u00daMENES Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\" REDES Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms BUILD DOCKERFILE Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache CMD CAMBIADO Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net: LIMITAR RECURSOS Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\" POL\u00cdTICA DE REINICIO Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca. NOMBRE PROYECTO Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d DIFERENTE DOCKER-COMPOSE Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d OTROS COMANDOS docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap PROYECTOS DOCKER-COMPOSE MYSQL-WORDPRESS Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: DRUPAL-POSTGRESQL Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados: PRESTASHOP-MYSQL Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: JOOMLA-MYSQL Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: REACT-MONGODB-NODE.JS Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados: GUACAMOLE DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados: ZABBIX Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados: PHPMYADMIN-MYSL Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados: DOCKER SWARM Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio. COMANDOS B\u00c1SICOS docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2 INICIALIZAR Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377 DEPLOY SWARM Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago ESCALAR SERVICIOS Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp MODO GLOBAL Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] NODO DRAIN/PAUSE/ACTIVE DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName LABELS Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] DOCKER REGISTRY Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"Docker"},{"location":"docker/#docker","text":"APUNTES DOCKER","title":"DOCKER"},{"location":"docker/#instalacion","text":"Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world","title":"INSTALACI\u00d3N"},{"location":"docker/#comandos","text":"Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container","title":"COMANDOS"},{"location":"docker/#redes-en-docker","text":"docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed","title":"Redes en Docker:"},{"location":"docker/#volumes-en-docker","text":"docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash","title":"Volumes en Docker:"},{"location":"docker/#docker-compose","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"Docker Compose:"},{"location":"docker/#docker-swarm","text":"docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode","title":"Docker SWARM:"},{"location":"docker/#arquitectura","text":"Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real.","title":"ARQUITECTURA"},{"location":"docker/#docker-images","text":"Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images","title":"DOCKER IMAGES"},{"location":"docker/#dockerfile","text":"El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito","title":"DOCKERFILE"},{"location":"docker/#cmd-vs-entrypoint","text":"CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"]","title":"CMD VS ENTRYPOINT"},{"location":"docker/#centos-php-ssl","text":"Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi","title":"CENTOS-PHP-SSL"},{"location":"docker/#nginx-php","text":"Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh","title":"NGINX-PHP"},{"location":"docker/#multi-stage-build","text":"Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo.","title":"MULTI-STAGE-BUILD"},{"location":"docker/#prueba-real","text":"La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php","title":"PRUEBA REAL"},{"location":"docker/#docker-containers","text":"Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen","title":"DOCKER CONTAINERS"},{"location":"docker/#listarmapeo-puertos","text":"docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080.","title":"LISTAR/MAPEO PUERTOS"},{"location":"docker/#iniciardetenepausar","text":"Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id","title":"INICIAR/DETENE/PAUSAR"},{"location":"docker/#variables-de-entorno","text":"En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins","title":"VARIABLES DE ENTORNO"},{"location":"docker/#mysql","text":"Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333","title":"MYSQL"},{"location":"docker/#mongodb","text":"Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones","title":"MONGODB"},{"location":"docker/#apachenginxtomcat","text":"Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine","title":"APACHE/NGINX/TOMCAT"},{"location":"docker/#postgres","text":"Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows)","title":"POSTGRES"},{"location":"docker/#jenkins","text":"Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins.","title":"JENKINS"},{"location":"docker/#limitar-recursos","text":"Ayuda con: docker --help | grep \"xxxx\"","title":"LIMITAR RECURSOS"},{"location":"docker/#memoria","text":"Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb","title":"MEMORIA"},{"location":"docker/#cpu","text":"Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2.","title":"CPU"},{"location":"docker/#copia-de-archivos","text":"De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/.","title":"COPIA DE ARCHIVOS"},{"location":"docker/#contenedor-a-imagen","text":"Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!!","title":"CONTENEDOR A IMAGEN"},{"location":"docker/#sobreescribir-cmd","text":"Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos","title":"SOBREESCRIBIR CMD"},{"location":"docker/#destruir-container","text":"Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm...","title":"DESTRUIR CONTAINER"},{"location":"docker/#document-root","text":"El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi.","title":"DOCUMENT ROOT"},{"location":"docker/#docker-volumes","text":"Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes","title":"DOCKER VOLUMES"},{"location":"docker/#volumes-host","text":"Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7","title":"VOLUMES HOST"},{"location":"docker/#volumes-anonymoys","text":"Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES ANONYMOYS"},{"location":"docker/#volumes-named-volumes","text":"Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES NAMED VOLUMES"},{"location":"docker/#prueba-real_1","text":"Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev","title":"PRUEBA REAL"},{"location":"docker/#docker-network","text":"Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping","title":"DOCKER NETWORK"},{"location":"docker/#crear-redes","text":"Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB","title":"CREAR REDES"},{"location":"docker/#ver-redes","text":"Para ver las redes creadas: docker network inspect netA","title":"VER REDES"},{"location":"docker/#agregarconectar-redes","text":"Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known","title":"AGREGAR/CONECTAR REDES"},{"location":"docker/#eliminar-redes","text":"Para eliminar redes: docker network remove netA netB","title":"ELIMINAR REDES"},{"location":"docker/#asignar-ips","text":"Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos","title":"ASIGNAR IPs"},{"location":"docker/#red-host","text":"Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel","title":"RED HOST"},{"location":"docker/#red-none","text":"Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos","title":"RED NONE"},{"location":"docker/#exponer-ips-concretas","text":"Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2","title":"EXPONER IPs CONCRETAS"},{"location":"docker/#docker-compose_1","text":"Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes","title":"DOCKER COMPOSE"},{"location":"docker/#instalacion_1","text":"Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"INSTALACI\u00d3N"},{"location":"docker/#ejemplo","text":"Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down","title":"EJEMPLO"},{"location":"docker/#variables-entorno","text":"Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env","title":"VARIABLES ENTORNO"},{"location":"docker/#volumenes","text":"Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\"","title":"VOL\u00daMENES"},{"location":"docker/#redes","text":"Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms","title":"REDES"},{"location":"docker/#build-dockerfile","text":"Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache","title":"BUILD DOCKERFILE"},{"location":"docker/#cmd-cambiado","text":"Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net:","title":"CMD CAMBIADO"},{"location":"docker/#limitar-recursos_1","text":"Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\"","title":"LIMITAR RECURSOS"},{"location":"docker/#politica-de-reinicio","text":"Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca.","title":"POL\u00cdTICA DE REINICIO"},{"location":"docker/#nombre-proyecto","text":"Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d","title":"NOMBRE PROYECTO"},{"location":"docker/#diferente-docker-compose","text":"Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d","title":"DIFERENTE DOCKER-COMPOSE"},{"location":"docker/#otros-comandos","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"OTROS COMANDOS"},{"location":"docker/#proyectos-docker-compose","text":"","title":"PROYECTOS DOCKER-COMPOSE"},{"location":"docker/#mysql-wordpress","text":"Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"MYSQL-WORDPRESS"},{"location":"docker/#drupal-postgresql","text":"Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados:","title":"DRUPAL-POSTGRESQL"},{"location":"docker/#prestashop-mysql","text":"Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"PRESTASHOP-MYSQL"},{"location":"docker/#joomla-mysql","text":"Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"JOOMLA-MYSQL"},{"location":"docker/#react-mongodb-nodejs","text":"Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados:","title":"REACT-MONGODB-NODE.JS"},{"location":"docker/#guacamole","text":"DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados:","title":"GUACAMOLE"},{"location":"docker/#zabbix","text":"Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados:","title":"ZABBIX"},{"location":"docker/#phpmyadmin-mysl","text":"Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados:","title":"PHPMYADMIN-MYSL"},{"location":"docker/#docker-swarm_1","text":"Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio.","title":"DOCKER SWARM"},{"location":"docker/#comandos-basicos","text":"docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2","title":"COMANDOS B\u00c1SICOS"},{"location":"docker/#inicializar","text":"Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377","title":"INICIALIZAR"},{"location":"docker/#deploy-swarm","text":"Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago","title":"DEPLOY SWARM"},{"location":"docker/#escalar-servicios","text":"Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp","title":"ESCALAR SERVICIOS"},{"location":"docker/#modo-global","text":"Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"MODO GLOBAL"},{"location":"docker/#nodo-drainpauseactive","text":"DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName","title":"NODO DRAIN/PAUSE/ACTIVE"},{"location":"docker/#labels","text":"Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"LABELS"},{"location":"docker/#docker-registry","text":"Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"DOCKER REGISTRY"},{"location":"elastic/","text":"ELASTIC STACK ELK Stack (o Elastic Stack) es un conjunto de herramientas open source desarrolladas por Elastics que permite recoger datos de cualquier tipo de fuente y en cualquier formato para realizar b\u00fasquedas, an\u00e1lisis y visualizaci\u00f3n de los datos en tiempo real. La velocidad y escalabilidad de Elasticsearch y su capacidad de indexar muchos tipos de contenido significan que puede usarse para una variedad de casos de uso: B\u00fasqueda de aplicaciones B\u00fasqueda de sitio web B\u00fasqueda Empresarial Logging y anal\u00edticas de log M\u00e9tricas de infraestructura y monitoreo de contenedores Monitoreo de rendimiento de aplicaciones An\u00e1lisis y visualizaci\u00f3n de datos geoespaciales Anal\u00edtica de Seguridad Anal\u00edtica de Negocios P\u00e1gina oficial COMPONENTES/INSTALACI\u00d3N BEATS Los Beats son excelentes para recopilar datos. Se quedan en tus servidores, con tus contenedores, o se despliegan como funciones, y despu\u00e9s centralizan los datos en Elasticsearch. Los Beats env\u00edan datos que cumplen con Elastic Common Schema (ECS), y si deseas una mayor potencia de procesamiento, pueden enviarlos a Logstash para las tareas de transformaci\u00f3n y parseo. Filebeat y Metricbeat incluyen m\u00f3dulos que simplifican la recopilaci\u00f3n, el parseo y la visualizaci\u00f3n de la informaci\u00f3n de fuentes de datos clave, como sistemas, contenedores y plataformas cloud, y tecnolog\u00edas de red. Ejecuta un solo comando y explora m\u00e1s all\u00e1. Beats re\u00fane los logs y las m\u00e9tricas de tus entornos \u00fanicos y los documenta con metadatos esenciales de hosts, plataformas de contenedores como Docker y Kubernetes y Proveedores Cloud antes de enviarlos al Elastic Stack. Desde el monitoreo de contenedores hasta el env\u00edo de datos desde arquitecturas sin servidor, nos aseguramos de que tengas el contexto que necesitas. Descargar [isx46410800@miguel metricbeat-7.14.0-linux-x86_64]$ tar -zxvf metricbeat-7.14.0-linux-x86_64.tar.gz Abrimos el fichero de conf metricbeat.yaml, comentamos el apartado del output elasticsearch y le ponemos por ahora uno basico de consola: # ================================== Outputs =================================== # Configure what output to use when sending the data collected by the beat. output.console: pretty: true # ---------------------------- Elasticsearch Output ---------------------------- #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] + Lo encendemos con `[isx46410800@miguel metricbeat-7.14.0-linux-x86_64]$ ./metricbeat -c metricbeat.yml` Descargar [isx46410800@miguel elasticStack]$ tar -zxvf filebeat-7.14.0-linux-x86_64.tar.gz En el fichero de conf filebeat.yaml activamos el type log y comentamos la salida elasticsearch y activamos la de logstash: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - ../logs/example.log #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" # ------------------------------ Logstash Output ------------------------------- output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] + Se arranca con `./filebeat` LOGSTASH Logstash ingesta, transforma y env\u00eda de forma din\u00e1mica tus datos independientemente de su formato o complejidad. Deriva estructura a partir de datos no estructurados con grok, descifra las coordenadas geogr\u00e1ficas de las direcciones IP, anonimiza o excluye los campos sensibles y facilita el procesamiento general. Los datos a menudo se encuentran repartidos o en silos en muchos sistemas en diversos formatos. Logstash admite una variedad de entradas que extraen eventos de una multitud de fuentes comunes, todo al mismo tiempo. Ingesta f\u00e1cilmente desde tus logs, m\u00e9tricas, aplicaciones web, almacenes de datos y varios servicios de AWS, todo de una manera de transmisi\u00f3n continua. A medida que los datos viajan de la fuente al almac\u00e9n, los filtros Logstash parsean cada evento, identifican los campos con nombre para crear la estructura y los transforman para que converjan en un formato com\u00fan para un an\u00e1lisis y un valor comercial m\u00e1s poderosos. Logstash transforma y prepara de forma din\u00e1mica tus datos independientemente de su formato o complejidad: Deriva estructura a partir de datos no estructurados con grok Descifra las coordenadas geogr\u00e1ficas a partir de las direcciones IP Anonimiza datos PII y excluye campos sensibles por completo Facilita el procesamiento general, independientemente de la fuente de datos, el formato o el esquema. Las posibilidades son infinitas con nuestra completa biblioteca de filtros y el vers\u00e1til Elastic Common Schema. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf logstash-7.14.0-linux-x86_64.tar.gz Creamos un fichero b\u00e1sico de conf, example.conf y ponemos: ``` input { stdin{} } output { stdout { codec => json_lines } } `` + Encendemos con [isx46410800@miguel logstash-7.14.0]$ bin/logstash -f example.conf` y veremos que cuando arranca para introducir algo en por teclado y sale por pantalla. ELASTICSEARCH Elasticsearch es un motor de b\u00fasqueda y anal\u00edtica de RESTful distribuido capaz de abordar un n\u00famero creciente de casos de uso. Como n\u00facleo del Elastic Stack, almacena de forma central tus datos para una b\u00fasqueda a la velocidad de la luz, relevancia refinada y anal\u00edticas poderosas que escalan con facilidad. Elasticsearch te permite realizar y combinar muchos tipos de b\u00fasquedas: estructuradas, no estructuradas, geogr\u00e1ficas, m\u00e9tricas, de la forma que desees. Comienza por lo m\u00e1s simple con una pregunta y observa a d\u00f3nde te lleva. Una cosa es encontrar los 10 mejores documentos que coincidan con tu b\u00fasqueda. Pero \u00bfc\u00f3mo le das sentido, digamos, a mil millones de l\u00edneas de log? Las agregaciones de Elasticsearch te permiten obtener una vista m\u00e1s general para explorar tendencias y patrones en tus datos. Resultados r\u00e1pidos: Cuando obtienes respuestas al instante, la relaci\u00f3n con tus datos cambia. Puedes permitirte iterar y cubrir m\u00e1s terreno. Dise\u00f1o poderoso: Ser as\u00ed de r\u00e1pido no es f\u00e1cil. Hemos implementado \u00edndices invertidos con transductores de estado finito para b\u00fasquedas de texto completo, \u00e1rboles de BKD para almacenar datos num\u00e9ricos y geogr\u00e1ficos, y un almac\u00e9n de columnas para anal\u00edticas. Todo incluido: Y como todo est\u00e1 indexado, nunca te quedar\u00e1s con la envidia de la indexaci\u00f3n. Puedes aprovechar y acceder a todos tus datos a velocidades rid\u00edculamente asombrosas. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz En config tenemos los fichero de configuraci\u00f3n. Arrancamos como ./bin/elasticsearch -d Hacemos una prueba con curl localhost:9200 para probar el funcionamiento. KIBANA Kibana te da la libertad de seleccionar la manera en que les das forma a tus datos. Con sus visualizaciones interactivas, comienza con una pregunta y mira hacia d\u00f3nde te lleva. Kibana env\u00eda datos de forma central con los cl\u00e1sicos: histogramas, grafos de l\u00edneas, gr\u00e1ficos circulares, proyecciones solares y m\u00e1s. Y, por supuesto, puedes buscar en todos tus documentos. Aprovecha Elastic Maps para explorar datos de ubicaci\u00f3n o vu\u00e9lvete creativo y visualiza capas y formas de vectores personalizadas. Realiza un an\u00e1lisis avanzado de series temporales en tus datos de Elasticsearch con nuestras UI de series temporales seleccionadas. Describe b\u00fasquedas, transformaciones y visualizaciones con expresiones poderosas y f\u00e1ciles de aprender. Detecta las anomal\u00edas que se esconden en tus datos de Elasticsearch y explora las propiedades que influyen significativamente en ellas con caracter\u00edsticas de Machine Learning sin supervisi\u00f3n. Toma las capacidades de relevancia de un motor de b\u00fasqueda, comb\u00ednalas con la exploraci\u00f3n de grafos y descubre las relaciones inusualmente comunes en tus datos de Elasticsearch. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz Editamos el vim config/kibana.yaml e indicamos que pueda escuchar de otros sitios y no solo de local server.host: \"0.0.0.0\" si estamos en otro host. Para que funcione primero se tiene que arrancar elasticsearch y luego kibana. Encendemos [isx46410800@miguel kibana-7.14.0-linux-x86_64]$ ./bin/kibana Entramos en http://localhost:5601 DOCKER ELKS DOC Ejemplo de creaci\u00f3n de `docker-compose.yaml: version: '3.7' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.6.0 ulimits: memlock: soft: -1 hard: -1 environment: - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - discovery.type=single-node volumes: - ./elasticsearch/data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 logstash: image: docker.elastic.co/logstash/logstash:7.6.0 volumes: - ./logstash/pipeline:/usr/share/logstash/pipeline kibana: image: docker.elastic.co/kibana/kibana:7.6.0 ports: - 5601:5601 PRUEBA FUNCIONAMIENTO Creamos un directorio de logs con un fichero b\u00e1sico logs/example.log : hello world bye bye hola mundo adios Editamos info en FILEBEAT: En el fichero de conf filebeat.yaml activamos el type log y comentamos la salida elasticsearch y activamos la de logstash: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - ../logs/example.log #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" # ------------------------------ Logstash Out------------------------------- output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] Editamos info en LOGSTASH: Creamos un fichero de conf personalizado con test.yml : input { beats { port => 5044 } output { elasticsearch { hosts => \"localhost:9200\" } } Arrancamos primero elasticsearch [isx46410800@miguel elasticsearch-7.14.0]$ ./bin/elasticsearch -d Arrancamos KIBANA: ./bin/kibana Arrancamos LOGSTASH: [isx46410800@miguel logstash-7.14.0]$ bin/logstash -f test.yml Arrancamos FILEBEAT: [isx46410800@miguel filebeat-7.14.0-linux-x86_64]$ ./filebeat -c filebeat.yml Direcci\u00f3n: http://localhost:5601. Si entramos y vamos al apartado de Discover de Kibana veremos que tenemos un indice. Creamos uno de logsstah para que coincida con el que crea por defecto(logstash-*) y vemos el campo de timestamp. Vemos ahora como se ha indexada 4 cosas, que corresponden a las 4 lineas del fichero de logs creado. Si ahora a\u00f1adimos una nueva linea al documento, veremos como ahora se indexa 5 documentos a kibana [isx46410800@miguel logs]$ echo \"nueva linea introducida\" >> example.log . Si lo editamos con vim, entonces duplicar\u00eda las entradas como documento nuevo. Si paramos filebeat y volvemos a arrancar, sigue el inode por la ultima linea leida, si queremos volver desde el principio o alguna linea en concreto tenemos que tocar el fichero en filebeat de data/registry BEATS METRICBEATS DOCS Sirve para que nos saque estadisticas del sistema, docker, etc Se ha de configurar el fichero de metricbeat.yml . Primero nos indica los modulos que se van a cargar, activar la salida, importar los dashboards al kibana(descomentando la linea localhost) Si ponemos ./metricbeat modules list vemos los modulos activados y desactivados. Los modulos estan en su directorio modules.d. Para validar una configuraci\u00f3n se usa ./metricbeat test config -c metricbeat.yml Para activar otro modulo seria con la orden ./metricbeat modules enable docker y en el yml podriamos poner un ejemplo como: - module: docker metricsets: - container - cpu - diskio - event - healthcheck - info - memory - network - network_summary period: 10s hosts: [\"unix:///var/run/docker.sock\"] PACKETBEAT DOCS Sirve para monitorizar la red entrante como saliente. En el fichero de configuraci\u00f3n cambiamos lo necesario en ./packetbeat.yml . Para los inputs en este fichero, debemos saber cual es la interfaz por la que queremos que escuche, podemos saberlo con ./packetbeat devices . Podemos comprobar si est\u00e1 bien el fichero con ./packetbeat test config -c packetbeat.yml Como no tenemos permisos suficientes para monitorizar la red de mi interfaz. hay que hacerlo como sudo o como root cambiando el fichero a root(chown root packetbeat.yml) y arrancamos con sudo ./packetbeat -c packetbeat.yml HEARTBEAT Nos indica el estado de salud de los servicios que est\u00e1n corriendo. Podremos saber si algun servidor se ha caido, alguna base de datos, etc. DOCS El fichero de configuraci\u00f3n est\u00e1 en heartbeat.yml Arrancamos con ./heartbeat -c heartbeat.yml FILEBEAT DOCS Filebeat es un cargador ligero para reenviar y centralizar datos de registro. Instalado como un agente en sus servidores, Filebeat monitorea los archivos de registro o las ubicaciones que usted especifica, recopila eventos de registro y los reenv\u00eda a Elasticsearch o Logstash para su indexaci\u00f3n. Utilizamos una herramienta para generar logs y se usa [isx46410800@miguel logs]$ java -jar log-generator-1.0.jar -h como por ejemplo java -jar log-generator-1.0.jar -l -1 -d 0 Despues de crear unas trazas de logs para usarlos de ejemplo, modificamos el archivo de conf para indicar la ruta de los logs: - ../logs/log-generator*.log Para probar quitamos los outputs y ponemos uno de consola: output.console: pretty: true Vamos probando con java -jar log-generator-1.0.jar -l 20 -d 1000 Excluimos lineas DEBUG: exclude_lines: ['^DEBUG'] Hay grupos de lineas de traza en el log que no empiezan por DEBUG, INFO, TRACE...y contienen varias lineas que hace como si fuera un evento cada linea. Se ha de poner que sea solo un evento y se asigne al evento anterior. Esto se hace con expresiones regulares HERRAMIENTA RUBULAR # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ multiline.pattern: ^(DEBUG|INFO|ERROR|TRACE|FATAL).* # Defines if the pattern set under pattern should be negated or not. Default isfalse. multiline.negate: true # Match can be set to \"after\" or \"before\". It is used to define if lines should beappend to a pattern # that was (not) matched before or after or as long as a pattern is not matchedbased on negate. # Note: After is the equivalent to previous and before is the equivalent to to nextin Logstash multiline.match: after Se indica que se niegue todo lo que empieze tal y tal y se a\u00f1ada al evento anterior Vemos como ahora arrancando el filebeat y haciendo logs nos agrupa el log como hemos indicado: ./filebeat -c filebeat.yml java -jar log-generator-1.0.jar -l 20 -d 1000 \"log\": { \"offset\": 1111503, \"file\": { \"path\": \"/home/isx46410800/Documents/elasticStack/logs/log-generator.log\" }, \"flags\": [ \"multiline\" ] }, \"message\": \"ERROR 2021-08-25 02:08:16 [main] a.b.t.loggenerator.LogGenerator - Document not found|\\njava.lang.Exception: Document not found\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.generateExceptionTrace(LogGenerator.kt:66)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.generateTraces(LogGenerator.kt:31)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt:17)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt)\\n\\tat com.xenomachina.argparser.SystemExitExceptionKt.mainBody(SystemExitException.kt:74)\\n\\tat com.xenomachina.argparser.SystemExitExceptionKt.mainBody$default(SystemExitException.kt:72)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.main(LogGenerator.kt:14)\" } Hay tambi\u00e9n modulos preconfigurados y podemos verlos en ./filebeat modules list y para activar ./filebeat modules enable logstash system . Los podemos configurar en vim modules.d/logstah.yml y las rutas de log por defecto las tiene en logs/logstash-plain.log . - module: logstash # logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/logstash-plain.log\"] # Slow logs slowlog: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/logstash-slowlog-plain.log\"] Ahora activamos el setup de kibana, quitamos el output de antes y ponemos el de elasticsearch y activamos la monitorizaci\u00f3n. En el filebeat_reference.yml vemos todo mas extenso y explicado. Ahora preparamos el filebeat para poder tratarlo con logstash. Desactivamos los outputs y ponemos el de logstash, quitamos kibana y dejamos igual lo de los patterns y activamos los monitoring. OTROS BEATS WINLOGBEAT sirve para monitorizar los sistemas windows. AUDITBEAT sirve para monitorizar los sistemas linux. COMMUNITY BEATS son beats creados por la gente para diferentes productos y servicios. LOGSTASH De la info que hay en los beats, llega a logstash, la procesa y la enriquece de info extrayendo datos y en el caso de salida ir\u00e1 a elasticsearch aunque tambi\u00e9n la info se puede procesar a un fichero, email, naggios, redis, kafka, amazon etc. Todo el proceso se llama PIPELINE (input -> filter -> output). Tambi\u00e9n elastic hay un proceso m\u00e1s que se le puede llamar, el CODEC, que sirve para transformar la info en un formato en concreto, como por ejemplo en CODEC JSON, MULTILINE,etc. INPUTS PLUGIN STDIN Sirve para indicar cual es el metodo de entrada. Normalmente usado para el testing y comprobar cosas de funcionamiento. DOCS Un ejemplo b\u00e1sico de fichero de configuraci\u00f3n seria una entrada por teclado basica y salida por pantalla en formato json: input { stdin{} } output { stdout { codec => json_lines } } arrancamos logstash con ./logstach -f example.conf y luego escribimos por pantalla y nos devuelve la salida en formato json. Otro ejemplo es la entrada en multiline: input { stdin{ codec => multiline { pattern => \"fin\" negate => \"true\" what => \"next\" } } } output { stdout { codec => json_lines } } Esto hace que todo lo que se ponga en multilineas, hasta que no se escriba FIN, no termina el evento. Tambien se le puede pasar la entrada en json y lo detecta como json y saca la salida del evento bien: input { stdin{ codec => json_lines } } output { stdout { codec => json_lines } } FILE INPUT Sirve para poner un input de tipo archivo. DOCS Ejemplo donde se le pasa las trazas de log, indicando que lea desde el principio, y donde guarde el puntero desde donde ha acabado de leer(logs/log-generator.sincedb), donde el nombre se indica en el fichero de conf. Para comprobar el fichero de conf se usa ./bin/logstash -t -f example.conf input { file { path => \"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/log-generator*.log\" #excluse => \"*.gz\" //para excluir archivos .gz start_position => \"beginning\" # o end sincedb_path => \"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/log-generator.sincedb\" codec => multiline { pattern => \"^(DEBUG|INFO|ERROR|TRACE|FATAL|WARN).*\" negate => \"true\" what => \"previous\" } } } output { stdout { codec => json_lines } } BEATS INPUT Es un peque\u00f1o agente que instalamos en los servidores y que nos permite el env\u00edo de datos recolectados en nuestro servidor a un procesador (Logstash) o directamente al repositorio (Elasticsearch. Cabe destacar que existen distintos tipos de Beats, seg\u00fan nuestra necesidad: FileBeat (para el env\u00edo de logs/ficheros), WinLogBeat (env\u00edo de eventos en un servidor Windows), MetricBeat (env\u00edo de m\u00e9tricas), etc. DOCS Como vemos en la configuracion de filebeat.yml vemos cual es el puerto de salida para logstash: output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] Lo que hacemos es que desde la conf de beats est\u00e1 indicado de donde coge logs(log-generator) y lo envia de salida a logstash para que lo proceso y veremos por pantalla todo lo que va saliendo mientras vamos haciendo trazas: input { beats { port => 5044 #hosts => 0.0.0.0 } } output { stdout { #codec => json_lines } } Arrancamos primero filebeat con ./filebeat -c filebeat.yml y luego logstash con el nuevo fichero de conf ./bin/logstash -f beats.conf FILTERS FILTER GROK DOCS Nos sirve para poder ir recortando los campos con diferentes patrones de expresiones regulares. Podemos encontrarlo en el apartado de kibana -> dev tools -> grok debugger o directamente hay una web que hace los mismo y nos indican chuletas de patterns de grok Cogemos este evento y vemos como lo traducimos con un pattern: INFO 2021-08-25 21:23:57 [main] a.b.t.loggenerator.LogGenerator - LOGIN|150|Filemon|180.86.181.252 PATTERN: %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip} Cogemos este evento y vemos como lo traducimos con un pattern: INFO 2021-08-25 21:23:55 [main] a.b.t.loggenerator.LogGenerator - PERFORMANCE|3.37|SEND EMAIL|FAILURE|Pantuflo PATTERN: %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user} Cogemos este evento y vemos como lo traducimos con un pattern: ERROR 2021-08-25 21:24:02 [main] a.b.t.loggenerator.LogGenerator - User can not be null. Can not fetch information from the database| java.lang.Exception: User can not be null. Can not fetch information from the database at am.ballesteros.training.loggenerator.LogGeneratorKt.generateExceptionTrace(LogGenerator.kt:66) at am.ballesteros.training.loggenerator.LogGeneratorKt.generateTraces(LogGenerator.kt:31) at am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt:17) at am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt) at com.xenomachina.argparser.SystemExitExceptionKt.mainBody(SystemExitException.kt:74) at com.xenomachina.argparser.SystemExitExceptionKt.mainBody$default(SystemExitException.kt:72) at am.ballesteros.training.loggenerator.LogGeneratorKt.main(LogGenerator.kt:14) PATTERN: (?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|%{GREEDYDATA:stacktrace} EXPRESION REGULAR: (?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|(?<stacktrace>.*) Tambien podemos crear PATTERNS propios como por ejemplo algo que se repite mucho, en nuestro caso se repite esto %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - . Se crea en custom patterns o en fichero aparte: COMMON_LOG %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - PATTERN: (?m)%{COMMON_LOG} %{DATA:description}\\|(?<stacktrace>.*) El fichero de configuraci\u00f3n de FILEBEAT sin la variable de pattern quedar\u00eda: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { match => {\"message\" => [ \"%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\"}, \"%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ] } } } output { stdout { #codec => json_lines } } Con la variable en un directorio patterns y en un fichero log-generator-patterns : COMMON_LOG %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } } output { stdout { #codec => json_lines } } FILTER MUTATE Sirve para cambiar campos, eliminar, copiar, pasar a mayusculas, dividir un campo en varios,etc. DOC En este caso vamos a cambiar tipos de campo de string a number, a\u00f1adir campos, conficionales... input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } } } output { stdout { #codec => json_lines } } en grok se puede poner %{NUMBER:duration:int/float}, solo en estos tipos. FILTER DATE DOCS En este caso renombramos el campo timestamp para no confundir con la fecha del log, y la fecha del log le cambiamos el formato con el filter date: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } } date { match => [\"date\", \"YYYY-MM-dd HH:mm:ss\"] } } output { stdout { #codec => json_lines } } FILTER TRANSLATE DOCS Nos permite hacer traducciones de codigos o informaciones indexando bien la informaci\u00f3n. En este caso en el campo login a\u00f1adimos traducciones segun el codigo que nos den: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } translate { field => \"status\" destination => \"statusText\" dictionary => { \"200\" => \"Login correcto\" \"201\" => \"Login correcto tras varios intentos\" \"202\" => \"Login con actualizaci\u00f3n de password\" \"204\" => \"login automatico\" \"250\" => \"Login recordado\" \"100\" => \"Usuario suplantado\" \"150\" => \"Usuario suplantado automaticamente\" \"400\" => \"Usuario bloqeuado\" \"404\" => \"Usuario no encontrado\" \"500\" => \"Contrase\u00f1a expirada\" } } } date { match => [ \"date\", \"YYYY-MM-dd HH:mm:ss\" ] } } output { stdout { #codec => json_lines } } FILTER GEOLP DOCS Nos indica info de geolocalizaci\u00f3n de la red, ips. En este caso se los a\u00f1adimos a los de login para ver donde para su ip: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } translate { field => \"status\" destination => \"statusText\" dictionary => { \"200\" => \"Login correcto\" \"201\" => \"Login correcto tras varios intentos\" \"202\" => \"Login con actualizaci\u00f3n de password\" \"204\" => \"login automatico\" \"250\" => \"Login recordado\" \"100\" => \"Usuario suplantado\" \"150\" => \"Usuario suplantado automaticamente\" \"400\" => \"Usuario bloqeuado\" \"404\" => \"Usuario no encontrado\" \"500\" => \"Contrase\u00f1a expirada\" } } geoip { source => \"ip\" } } date { match => [ \"date\", \"YYYY-MM-dd HH:mm:ss\" ] } } output { stdout { #codec => json_lines } } FILTER RUBY DOCS Logstash est\u00e1 escrito en RUBY. Es el plugin m\u00e1s polivalente ya que con este podemos hacer todo lo que hacen el resto de plugins. Un ejemplo: input { stdin { codec => } } filter { ruby { code => \" event.set('name_normalized', event.get('name').gsub('\u00f1', 'n')) total_in_seconds = 0: event.get('total').split(':').each_with_index do [v, i] if i == 0 total_in_seconds += v.to_i * 3600; elsif i == 1 total_in_seconds += v.to_i * 60; elsif i == 2 total_in_seconds += v.to_i; end end event.set('total', total_in_seconds); \" } } output { stdout {} } ## { \"total\": \"00:00:00\", \"name\": \"Nombre\", \"equipo\": \"Equipo\"} ## convertir en segundos el tiempo y quitar las \u00d1 de los nombres OTROS FILTERS DOCS OUTPUTS DOCS Combinando configuraciones de la que se entra por pantalla o por la que entra del log generator. input { stdin { codec => json add_field => {\"application\" => \"results\"} id => \"stdin-input-results\" } #file { # path => \"/Users/ballesterosam/Personal/training/elasticstack/logs/log-generator*.log\" #exclude => \"*.gz\" # start_position => \"beginning\" # sincedb_path => \"/Users/ballesterosam/Personal/training/elasticstack/logs/log-generator.sincedb\" # codec => multiline { # pattern => \"^(DEBUG|INFO|ERROR|TRACE|FATAL|WARN).*\" # negate => \"true\" # what => \"previous\" # } #} } filter { if [application] == \"results\" { ruby { code => \" event.set('name_normalized', event.get('name').gsub('\u00f1', 'n')); total_in_seconds = 0; event.get('total').split(':').each_with_index do |v, i| if i == 0 total_in_seconds += v.to_i * 3600; elsif i == 1 total_in_seconds += v.to_i * 60; elsif i == 2 total_in_seconds += v.to_i; end end event.set('total', total_in_seconds); \" id => \"parse-total\" } } } output { if [application] == \"results\" { stdout { #codec => json_lines id => \"stdout-results\" } } } ELASTICSEARCH DOC Vamos a kibana -> dev tools Ejemplos de entradas que buscar: GET _nodes/(nombre de nodo si hay mas) GET /_cluster/allocation/explain { \"index\": \"nombre_indice\", \"shard\": 0, \"primary\": true } GET _cat/shards?v GET _cat/nodes?v GET _cat/health?v PUT new-index { \"settings\": { \"index\": { \"number_of_shards:\" 2, \"number_of_replicas: 1 } } } DELETE new-index PUT employees/_doc/ { \"name\": \"tyler\", \"surname\": \"lopex\", \"job\": \"IT\" } POST employees/_doc/ { \"name\": \"tyler\", \"surname\": \"lopex\", \"job\": \"IT\" } GET employees/_doc/2 GET employees/_search { \"query\": {\"match\": { \"name\": \"tyler\" }} } GET log-generator-*/_search { \"query\": {\"match\": { \"type\": \"JAVA_ERROR\" }} } GET employees/_search { \"query\": {\"match_all\"}: {} } Ejemplo de full query text GET /log-generator-*/_search { \"query\": { \"match\": { \"slackrace\": { \"query\": \"User not found\", \"operation\": \"and\" } } } } GET /log-generator-*/_search { \"query\": { \"multimatch\": { \"query\": \"STMP, \"fields\": [\"stackrace\",\"type\"] } } } Ejemplo de term queries GET /log-generator-*/_search { \"query\": { \"term\": { \"stackrace\": \"User not found\", } } } Ejemplo de boolean queries GET log-generator-*/_search { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, {\"match\" : { \"responseText\" : \"bloqueado\" }} ], \"minimum_should_match\" : 1, \"should\" : [ { \"term\" : { \"user\" : \"Carpanta\" } }, { \"term\" : { \"user\": { \"value\": \"SuperLopez\", \"boost\": 2 } }} { \"term\" : { \"user\" : \"Conan\" } }, { \"term\" : { \"user\" : \"Irongisg\" } } ], \"must_not\": [ { \"term\" : { \"geoip\" : \"Japon\" } } ] } } } Ejemplo de geo distance queries GET log-generator-*/_search { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, {\"match\" : { \"responseText\" : \"bloqueado\" }} ], \"must_not\": [ { \"term\" : { \"geoip\" : \"Japon\" } } ], \"filter\": [ { \"term\" : { \"geoip\" : \"EU\" } }, {\"geo_distance\": { \"distance\": \"1000km\", \"geoip.location\": { \"lat\": 40, \"lon\": -3 } } } ] } } } Ejemplo de agregaciones queries { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, ], \"filter\": [ { \"term\" : { \"geoip\" : \"EU\" } }, ] } }, \"aggs\": { \"ByUser\": { \"terms\": { \"field\": \"user\" } } } } Ejemplo de SQL queries: POST _sql { \"query\": \"SELECT * from \\\"log-generator-*\"\" } POST _sql?format=txt { \"query\": \"SELECT user, respondeCode from \\\"log-generator-*\"\" } POST _sql?format=txt { \"query\": \"SELECT user, respondeCode from \\\"log-generator-*\" where type=\"LOGIN\"\" } Los archivos importantes para configurar elasticsearch son config/jvm.options y elasticsearch.yml: ################################################################ ## ## JVM configuration ## ################################################################ ## ## WARNING: DO NOT EDIT THIS FILE. If you want to override the ## JVM options in this file, or set any additional options, you ## should create one or more files in the jvm.options.d ## directory containing your adjustments. ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/jvm-options.html ## for more information. ## ################################################################ ################################################################ ## IMPORTANT: JVM heap size ################################################################ ## ## The heap size is automatically configured by Elasticsearch ## based on the available memory in your system and the roles ## each node is configured to fulfill. If specifying heap is ## required, it should be done through a file in jvm.options.d, ## and the min and max should be set to the same value. For ## example, to set the heap to 4 GB, create a new file in the ## jvm.options.d directory containing these lines: ## -Xms2g -Xmx2g ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html ## for more information ## ################################################################ ################################################################ ## Expert settings ################################################################ ## ## All settings below here are considered expert settings. Do ## not adjust them unless you understand what you are doing. Do ## not edit them in this file; instead, create a new file in the ## jvm.options.d directory containing your adjustments. ## ################################################################ ## GC configuration 8-13:-XX:+UseConcMarkSweepGC 8-13:-XX:CMSInitiatingOccupancyFraction=75 8-13:-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1 GC is only supported on JDK version 10 or later # to use G1GC, uncomment the next two lines and update the version on the # following three lines to your version of the JDK # 10-13:-XX:-UseConcMarkSweepGC # 10-13:-XX:-UseCMSInitiatingOccupancyOnly 14-:-XX:+UseG1GC ## JVM temporary directory -Djava.io.tmpdir=${ES_TMPDIR} ## heap dumps # generate a heap dump when an allocation from the Java heap fails; heap dumps # are created in the working directory of the JVM unless an alternative path is # specified -XX:+HeapDumpOnOutOfMemoryError # specify an alternative path for heap dumps; ensure the directory exists and # has sufficient space -XX:HeapDumpPath=data # specify an alternative path for JVM fatal error logs -XX:ErrorFile=logs/hs_err_pid%p.log ## JDK 8 GC logging 8:-XX:+PrintGCDetails 8:-XX:+PrintGCDateStamps 8:-XX:+PrintTenuringDistribution 8:-XX:+PrintGCApplicationStoppedTime 8:-Xloggc:logs/gc.log 8:-XX:+UseGCLogFileRotation 8:-XX:NumberOfGCLogFiles=32 8:-XX:GCLogFileSize=64m # JDK 9+ GC logging 9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m # ======================== Elasticsearch Configuration ========================= # # NOTE: Elasticsearch comes with reasonable defaults for most settings. # Before you set out to tweak and tune the configuration, make sure you # understand what are you trying to accomplish and the consequences. # # The primary way of configuring a node is via this file. This template lists # the most important settings you may want to configure for a production cluster. # # Please consult the documentation for further information on configuration options: # https://www.elastic.co/guide/en/elasticsearch/reference/index.html # # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # cluster.name: my-application # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # node.name: node-1 # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # path.data: /path/to/data # # Path to log files: # path.logs: /path/to/logs # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # By default Elasticsearch is only accessible on localhost. Set a different # address here to expose this node on the network: # network.host: 192.168.0.1 # # By default Elasticsearch listens for HTTP traffic on the first free port it # finds starting at 9200. Set a specific HTTP port here: # http.port: 9200 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # discovery.seed_hosts: [\"host1\", \"192.168.10.2\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # cluster.initial_master_nodes: [\"node-1\", \"node-2\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true KIBANA Configuraciones en kibana -> manegement. Podemos tener varias visualizaciones como histograma, pie, gauge, mapas,etc. Para ello vamos a KIBANA - VISUALIZE - +CREAR. Se puede crear varios y luego en DASHBOARD personalizar tus tableros de visualizaci\u00f3n.","title":"ELKS"},{"location":"elastic/#elastic-stack","text":"ELK Stack (o Elastic Stack) es un conjunto de herramientas open source desarrolladas por Elastics que permite recoger datos de cualquier tipo de fuente y en cualquier formato para realizar b\u00fasquedas, an\u00e1lisis y visualizaci\u00f3n de los datos en tiempo real. La velocidad y escalabilidad de Elasticsearch y su capacidad de indexar muchos tipos de contenido significan que puede usarse para una variedad de casos de uso: B\u00fasqueda de aplicaciones B\u00fasqueda de sitio web B\u00fasqueda Empresarial Logging y anal\u00edticas de log M\u00e9tricas de infraestructura y monitoreo de contenedores Monitoreo de rendimiento de aplicaciones An\u00e1lisis y visualizaci\u00f3n de datos geoespaciales Anal\u00edtica de Seguridad Anal\u00edtica de Negocios P\u00e1gina oficial","title":"ELASTIC STACK"},{"location":"elastic/#componentesinstalacion","text":"","title":"COMPONENTES/INSTALACI\u00d3N"},{"location":"elastic/#beats","text":"Los Beats son excelentes para recopilar datos. Se quedan en tus servidores, con tus contenedores, o se despliegan como funciones, y despu\u00e9s centralizan los datos en Elasticsearch. Los Beats env\u00edan datos que cumplen con Elastic Common Schema (ECS), y si deseas una mayor potencia de procesamiento, pueden enviarlos a Logstash para las tareas de transformaci\u00f3n y parseo. Filebeat y Metricbeat incluyen m\u00f3dulos que simplifican la recopilaci\u00f3n, el parseo y la visualizaci\u00f3n de la informaci\u00f3n de fuentes de datos clave, como sistemas, contenedores y plataformas cloud, y tecnolog\u00edas de red. Ejecuta un solo comando y explora m\u00e1s all\u00e1. Beats re\u00fane los logs y las m\u00e9tricas de tus entornos \u00fanicos y los documenta con metadatos esenciales de hosts, plataformas de contenedores como Docker y Kubernetes y Proveedores Cloud antes de enviarlos al Elastic Stack. Desde el monitoreo de contenedores hasta el env\u00edo de datos desde arquitecturas sin servidor, nos aseguramos de que tengas el contexto que necesitas. Descargar [isx46410800@miguel metricbeat-7.14.0-linux-x86_64]$ tar -zxvf metricbeat-7.14.0-linux-x86_64.tar.gz Abrimos el fichero de conf metricbeat.yaml, comentamos el apartado del output elasticsearch y le ponemos por ahora uno basico de consola: # ================================== Outputs =================================== # Configure what output to use when sending the data collected by the beat. output.console: pretty: true # ---------------------------- Elasticsearch Output ---------------------------- #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] + Lo encendemos con `[isx46410800@miguel metricbeat-7.14.0-linux-x86_64]$ ./metricbeat -c metricbeat.yml` Descargar [isx46410800@miguel elasticStack]$ tar -zxvf filebeat-7.14.0-linux-x86_64.tar.gz En el fichero de conf filebeat.yaml activamos el type log y comentamos la salida elasticsearch y activamos la de logstash: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - ../logs/example.log #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" # ------------------------------ Logstash Output ------------------------------- output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] + Se arranca con `./filebeat`","title":"BEATS"},{"location":"elastic/#logstash","text":"Logstash ingesta, transforma y env\u00eda de forma din\u00e1mica tus datos independientemente de su formato o complejidad. Deriva estructura a partir de datos no estructurados con grok, descifra las coordenadas geogr\u00e1ficas de las direcciones IP, anonimiza o excluye los campos sensibles y facilita el procesamiento general. Los datos a menudo se encuentran repartidos o en silos en muchos sistemas en diversos formatos. Logstash admite una variedad de entradas que extraen eventos de una multitud de fuentes comunes, todo al mismo tiempo. Ingesta f\u00e1cilmente desde tus logs, m\u00e9tricas, aplicaciones web, almacenes de datos y varios servicios de AWS, todo de una manera de transmisi\u00f3n continua. A medida que los datos viajan de la fuente al almac\u00e9n, los filtros Logstash parsean cada evento, identifican los campos con nombre para crear la estructura y los transforman para que converjan en un formato com\u00fan para un an\u00e1lisis y un valor comercial m\u00e1s poderosos. Logstash transforma y prepara de forma din\u00e1mica tus datos independientemente de su formato o complejidad: Deriva estructura a partir de datos no estructurados con grok Descifra las coordenadas geogr\u00e1ficas a partir de las direcciones IP Anonimiza datos PII y excluye campos sensibles por completo Facilita el procesamiento general, independientemente de la fuente de datos, el formato o el esquema. Las posibilidades son infinitas con nuestra completa biblioteca de filtros y el vers\u00e1til Elastic Common Schema. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf logstash-7.14.0-linux-x86_64.tar.gz Creamos un fichero b\u00e1sico de conf, example.conf y ponemos: ``` input { stdin{} } output { stdout { codec => json_lines } } `` + Encendemos con [isx46410800@miguel logstash-7.14.0]$ bin/logstash -f example.conf` y veremos que cuando arranca para introducir algo en por teclado y sale por pantalla.","title":"LOGSTASH"},{"location":"elastic/#elasticsearch","text":"Elasticsearch es un motor de b\u00fasqueda y anal\u00edtica de RESTful distribuido capaz de abordar un n\u00famero creciente de casos de uso. Como n\u00facleo del Elastic Stack, almacena de forma central tus datos para una b\u00fasqueda a la velocidad de la luz, relevancia refinada y anal\u00edticas poderosas que escalan con facilidad. Elasticsearch te permite realizar y combinar muchos tipos de b\u00fasquedas: estructuradas, no estructuradas, geogr\u00e1ficas, m\u00e9tricas, de la forma que desees. Comienza por lo m\u00e1s simple con una pregunta y observa a d\u00f3nde te lleva. Una cosa es encontrar los 10 mejores documentos que coincidan con tu b\u00fasqueda. Pero \u00bfc\u00f3mo le das sentido, digamos, a mil millones de l\u00edneas de log? Las agregaciones de Elasticsearch te permiten obtener una vista m\u00e1s general para explorar tendencias y patrones en tus datos. Resultados r\u00e1pidos: Cuando obtienes respuestas al instante, la relaci\u00f3n con tus datos cambia. Puedes permitirte iterar y cubrir m\u00e1s terreno. Dise\u00f1o poderoso: Ser as\u00ed de r\u00e1pido no es f\u00e1cil. Hemos implementado \u00edndices invertidos con transductores de estado finito para b\u00fasquedas de texto completo, \u00e1rboles de BKD para almacenar datos num\u00e9ricos y geogr\u00e1ficos, y un almac\u00e9n de columnas para anal\u00edticas. Todo incluido: Y como todo est\u00e1 indexado, nunca te quedar\u00e1s con la envidia de la indexaci\u00f3n. Puedes aprovechar y acceder a todos tus datos a velocidades rid\u00edculamente asombrosas. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz En config tenemos los fichero de configuraci\u00f3n. Arrancamos como ./bin/elasticsearch -d Hacemos una prueba con curl localhost:9200 para probar el funcionamiento.","title":"ELASTICSEARCH"},{"location":"elastic/#kibana","text":"Kibana te da la libertad de seleccionar la manera en que les das forma a tus datos. Con sus visualizaciones interactivas, comienza con una pregunta y mira hacia d\u00f3nde te lleva. Kibana env\u00eda datos de forma central con los cl\u00e1sicos: histogramas, grafos de l\u00edneas, gr\u00e1ficos circulares, proyecciones solares y m\u00e1s. Y, por supuesto, puedes buscar en todos tus documentos. Aprovecha Elastic Maps para explorar datos de ubicaci\u00f3n o vu\u00e9lvete creativo y visualiza capas y formas de vectores personalizadas. Realiza un an\u00e1lisis avanzado de series temporales en tus datos de Elasticsearch con nuestras UI de series temporales seleccionadas. Describe b\u00fasquedas, transformaciones y visualizaciones con expresiones poderosas y f\u00e1ciles de aprender. Detecta las anomal\u00edas que se esconden en tus datos de Elasticsearch y explora las propiedades que influyen significativamente en ellas con caracter\u00edsticas de Machine Learning sin supervisi\u00f3n. Toma las capacidades de relevancia de un motor de b\u00fasqueda, comb\u00ednalas con la exploraci\u00f3n de grafos y descubre las relaciones inusualmente comunes en tus datos de Elasticsearch. Descargar [isx46410800@miguel elasticStack]$ tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz Editamos el vim config/kibana.yaml e indicamos que pueda escuchar de otros sitios y no solo de local server.host: \"0.0.0.0\" si estamos en otro host. Para que funcione primero se tiene que arrancar elasticsearch y luego kibana. Encendemos [isx46410800@miguel kibana-7.14.0-linux-x86_64]$ ./bin/kibana Entramos en http://localhost:5601","title":"KIBANA"},{"location":"elastic/#docker-elks","text":"DOC Ejemplo de creaci\u00f3n de `docker-compose.yaml: version: '3.7' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.6.0 ulimits: memlock: soft: -1 hard: -1 environment: - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - discovery.type=single-node volumes: - ./elasticsearch/data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 logstash: image: docker.elastic.co/logstash/logstash:7.6.0 volumes: - ./logstash/pipeline:/usr/share/logstash/pipeline kibana: image: docker.elastic.co/kibana/kibana:7.6.0 ports: - 5601:5601","title":"DOCKER ELKS"},{"location":"elastic/#prueba-funcionamiento","text":"Creamos un directorio de logs con un fichero b\u00e1sico logs/example.log : hello world bye bye hola mundo adios Editamos info en FILEBEAT: En el fichero de conf filebeat.yaml activamos el type log y comentamos la salida elasticsearch y activamos la de logstash: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - ../logs/example.log #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" # ------------------------------ Logstash Out------------------------------- output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] Editamos info en LOGSTASH: Creamos un fichero de conf personalizado con test.yml : input { beats { port => 5044 } output { elasticsearch { hosts => \"localhost:9200\" } } Arrancamos primero elasticsearch [isx46410800@miguel elasticsearch-7.14.0]$ ./bin/elasticsearch -d Arrancamos KIBANA: ./bin/kibana Arrancamos LOGSTASH: [isx46410800@miguel logstash-7.14.0]$ bin/logstash -f test.yml Arrancamos FILEBEAT: [isx46410800@miguel filebeat-7.14.0-linux-x86_64]$ ./filebeat -c filebeat.yml Direcci\u00f3n: http://localhost:5601. Si entramos y vamos al apartado de Discover de Kibana veremos que tenemos un indice. Creamos uno de logsstah para que coincida con el que crea por defecto(logstash-*) y vemos el campo de timestamp. Vemos ahora como se ha indexada 4 cosas, que corresponden a las 4 lineas del fichero de logs creado. Si ahora a\u00f1adimos una nueva linea al documento, veremos como ahora se indexa 5 documentos a kibana [isx46410800@miguel logs]$ echo \"nueva linea introducida\" >> example.log . Si lo editamos con vim, entonces duplicar\u00eda las entradas como documento nuevo. Si paramos filebeat y volvemos a arrancar, sigue el inode por la ultima linea leida, si queremos volver desde el principio o alguna linea en concreto tenemos que tocar el fichero en filebeat de data/registry","title":"PRUEBA FUNCIONAMIENTO"},{"location":"elastic/#beats_1","text":"","title":"BEATS"},{"location":"elastic/#metricbeats","text":"DOCS Sirve para que nos saque estadisticas del sistema, docker, etc Se ha de configurar el fichero de metricbeat.yml . Primero nos indica los modulos que se van a cargar, activar la salida, importar los dashboards al kibana(descomentando la linea localhost) Si ponemos ./metricbeat modules list vemos los modulos activados y desactivados. Los modulos estan en su directorio modules.d. Para validar una configuraci\u00f3n se usa ./metricbeat test config -c metricbeat.yml Para activar otro modulo seria con la orden ./metricbeat modules enable docker y en el yml podriamos poner un ejemplo como: - module: docker metricsets: - container - cpu - diskio - event - healthcheck - info - memory - network - network_summary period: 10s hosts: [\"unix:///var/run/docker.sock\"]","title":"METRICBEATS"},{"location":"elastic/#packetbeat","text":"DOCS Sirve para monitorizar la red entrante como saliente. En el fichero de configuraci\u00f3n cambiamos lo necesario en ./packetbeat.yml . Para los inputs en este fichero, debemos saber cual es la interfaz por la que queremos que escuche, podemos saberlo con ./packetbeat devices . Podemos comprobar si est\u00e1 bien el fichero con ./packetbeat test config -c packetbeat.yml Como no tenemos permisos suficientes para monitorizar la red de mi interfaz. hay que hacerlo como sudo o como root cambiando el fichero a root(chown root packetbeat.yml) y arrancamos con sudo ./packetbeat -c packetbeat.yml","title":"PACKETBEAT"},{"location":"elastic/#heartbeat","text":"Nos indica el estado de salud de los servicios que est\u00e1n corriendo. Podremos saber si algun servidor se ha caido, alguna base de datos, etc. DOCS El fichero de configuraci\u00f3n est\u00e1 en heartbeat.yml Arrancamos con ./heartbeat -c heartbeat.yml","title":"HEARTBEAT"},{"location":"elastic/#filebeat","text":"DOCS Filebeat es un cargador ligero para reenviar y centralizar datos de registro. Instalado como un agente en sus servidores, Filebeat monitorea los archivos de registro o las ubicaciones que usted especifica, recopila eventos de registro y los reenv\u00eda a Elasticsearch o Logstash para su indexaci\u00f3n. Utilizamos una herramienta para generar logs y se usa [isx46410800@miguel logs]$ java -jar log-generator-1.0.jar -h como por ejemplo java -jar log-generator-1.0.jar -l -1 -d 0 Despues de crear unas trazas de logs para usarlos de ejemplo, modificamos el archivo de conf para indicar la ruta de los logs: - ../logs/log-generator*.log Para probar quitamos los outputs y ponemos uno de consola: output.console: pretty: true Vamos probando con java -jar log-generator-1.0.jar -l 20 -d 1000 Excluimos lineas DEBUG: exclude_lines: ['^DEBUG'] Hay grupos de lineas de traza en el log que no empiezan por DEBUG, INFO, TRACE...y contienen varias lineas que hace como si fuera un evento cada linea. Se ha de poner que sea solo un evento y se asigne al evento anterior. Esto se hace con expresiones regulares HERRAMIENTA RUBULAR # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ multiline.pattern: ^(DEBUG|INFO|ERROR|TRACE|FATAL).* # Defines if the pattern set under pattern should be negated or not. Default isfalse. multiline.negate: true # Match can be set to \"after\" or \"before\". It is used to define if lines should beappend to a pattern # that was (not) matched before or after or as long as a pattern is not matchedbased on negate. # Note: After is the equivalent to previous and before is the equivalent to to nextin Logstash multiline.match: after Se indica que se niegue todo lo que empieze tal y tal y se a\u00f1ada al evento anterior Vemos como ahora arrancando el filebeat y haciendo logs nos agrupa el log como hemos indicado: ./filebeat -c filebeat.yml java -jar log-generator-1.0.jar -l 20 -d 1000 \"log\": { \"offset\": 1111503, \"file\": { \"path\": \"/home/isx46410800/Documents/elasticStack/logs/log-generator.log\" }, \"flags\": [ \"multiline\" ] }, \"message\": \"ERROR 2021-08-25 02:08:16 [main] a.b.t.loggenerator.LogGenerator - Document not found|\\njava.lang.Exception: Document not found\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.generateExceptionTrace(LogGenerator.kt:66)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.generateTraces(LogGenerator.kt:31)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt:17)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt)\\n\\tat com.xenomachina.argparser.SystemExitExceptionKt.mainBody(SystemExitException.kt:74)\\n\\tat com.xenomachina.argparser.SystemExitExceptionKt.mainBody$default(SystemExitException.kt:72)\\n\\tat am.ballesteros.training.loggenerator.LogGeneratorKt.main(LogGenerator.kt:14)\" } Hay tambi\u00e9n modulos preconfigurados y podemos verlos en ./filebeat modules list y para activar ./filebeat modules enable logstash system . Los podemos configurar en vim modules.d/logstah.yml y las rutas de log por defecto las tiene en logs/logstash-plain.log . - module: logstash # logs log: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/logstash-plain.log\"] # Slow logs slowlog: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/logstash-slowlog-plain.log\"] Ahora activamos el setup de kibana, quitamos el output de antes y ponemos el de elasticsearch y activamos la monitorizaci\u00f3n. En el filebeat_reference.yml vemos todo mas extenso y explicado. Ahora preparamos el filebeat para poder tratarlo con logstash. Desactivamos los outputs y ponemos el de logstash, quitamos kibana y dejamos igual lo de los patterns y activamos los monitoring.","title":"FILEBEAT"},{"location":"elastic/#otros-beats","text":"WINLOGBEAT sirve para monitorizar los sistemas windows. AUDITBEAT sirve para monitorizar los sistemas linux. COMMUNITY BEATS son beats creados por la gente para diferentes productos y servicios.","title":"OTROS BEATS"},{"location":"elastic/#logstash_1","text":"De la info que hay en los beats, llega a logstash, la procesa y la enriquece de info extrayendo datos y en el caso de salida ir\u00e1 a elasticsearch aunque tambi\u00e9n la info se puede procesar a un fichero, email, naggios, redis, kafka, amazon etc. Todo el proceso se llama PIPELINE (input -> filter -> output). Tambi\u00e9n elastic hay un proceso m\u00e1s que se le puede llamar, el CODEC, que sirve para transformar la info en un formato en concreto, como por ejemplo en CODEC JSON, MULTILINE,etc.","title":"LOGSTASH"},{"location":"elastic/#inputs","text":"","title":"INPUTS"},{"location":"elastic/#plugin-stdin","text":"Sirve para indicar cual es el metodo de entrada. Normalmente usado para el testing y comprobar cosas de funcionamiento. DOCS Un ejemplo b\u00e1sico de fichero de configuraci\u00f3n seria una entrada por teclado basica y salida por pantalla en formato json: input { stdin{} } output { stdout { codec => json_lines } } arrancamos logstash con ./logstach -f example.conf y luego escribimos por pantalla y nos devuelve la salida en formato json. Otro ejemplo es la entrada en multiline: input { stdin{ codec => multiline { pattern => \"fin\" negate => \"true\" what => \"next\" } } } output { stdout { codec => json_lines } } Esto hace que todo lo que se ponga en multilineas, hasta que no se escriba FIN, no termina el evento. Tambien se le puede pasar la entrada en json y lo detecta como json y saca la salida del evento bien: input { stdin{ codec => json_lines } } output { stdout { codec => json_lines } }","title":"PLUGIN STDIN"},{"location":"elastic/#file-input","text":"Sirve para poner un input de tipo archivo. DOCS Ejemplo donde se le pasa las trazas de log, indicando que lea desde el principio, y donde guarde el puntero desde donde ha acabado de leer(logs/log-generator.sincedb), donde el nombre se indica en el fichero de conf. Para comprobar el fichero de conf se usa ./bin/logstash -t -f example.conf input { file { path => \"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/log-generator*.log\" #excluse => \"*.gz\" //para excluir archivos .gz start_position => \"beginning\" # o end sincedb_path => \"/home/isx46410800/Documents/elasticStack/logstash-7.14.0/logs/log-generator.sincedb\" codec => multiline { pattern => \"^(DEBUG|INFO|ERROR|TRACE|FATAL|WARN).*\" negate => \"true\" what => \"previous\" } } } output { stdout { codec => json_lines } }","title":"FILE INPUT"},{"location":"elastic/#beats-input","text":"Es un peque\u00f1o agente que instalamos en los servidores y que nos permite el env\u00edo de datos recolectados en nuestro servidor a un procesador (Logstash) o directamente al repositorio (Elasticsearch. Cabe destacar que existen distintos tipos de Beats, seg\u00fan nuestra necesidad: FileBeat (para el env\u00edo de logs/ficheros), WinLogBeat (env\u00edo de eventos en un servidor Windows), MetricBeat (env\u00edo de m\u00e9tricas), etc. DOCS Como vemos en la configuracion de filebeat.yml vemos cual es el puerto de salida para logstash: output.logstash: # The Logstash hosts hosts: [\"localhost:5044\"] Lo que hacemos es que desde la conf de beats est\u00e1 indicado de donde coge logs(log-generator) y lo envia de salida a logstash para que lo proceso y veremos por pantalla todo lo que va saliendo mientras vamos haciendo trazas: input { beats { port => 5044 #hosts => 0.0.0.0 } } output { stdout { #codec => json_lines } } Arrancamos primero filebeat con ./filebeat -c filebeat.yml y luego logstash con el nuevo fichero de conf ./bin/logstash -f beats.conf","title":"BEATS INPUT"},{"location":"elastic/#filters","text":"","title":"FILTERS"},{"location":"elastic/#filter-grok","text":"DOCS Nos sirve para poder ir recortando los campos con diferentes patrones de expresiones regulares. Podemos encontrarlo en el apartado de kibana -> dev tools -> grok debugger o directamente hay una web que hace los mismo y nos indican chuletas de patterns de grok Cogemos este evento y vemos como lo traducimos con un pattern: INFO 2021-08-25 21:23:57 [main] a.b.t.loggenerator.LogGenerator - LOGIN|150|Filemon|180.86.181.252 PATTERN: %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip} Cogemos este evento y vemos como lo traducimos con un pattern: INFO 2021-08-25 21:23:55 [main] a.b.t.loggenerator.LogGenerator - PERFORMANCE|3.37|SEND EMAIL|FAILURE|Pantuflo PATTERN: %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user} Cogemos este evento y vemos como lo traducimos con un pattern: ERROR 2021-08-25 21:24:02 [main] a.b.t.loggenerator.LogGenerator - User can not be null. Can not fetch information from the database| java.lang.Exception: User can not be null. Can not fetch information from the database at am.ballesteros.training.loggenerator.LogGeneratorKt.generateExceptionTrace(LogGenerator.kt:66) at am.ballesteros.training.loggenerator.LogGeneratorKt.generateTraces(LogGenerator.kt:31) at am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt:17) at am.ballesteros.training.loggenerator.LogGeneratorKt$main$1.invoke(LogGenerator.kt) at com.xenomachina.argparser.SystemExitExceptionKt.mainBody(SystemExitException.kt:74) at com.xenomachina.argparser.SystemExitExceptionKt.mainBody$default(SystemExitException.kt:72) at am.ballesteros.training.loggenerator.LogGeneratorKt.main(LogGenerator.kt:14) PATTERN: (?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|%{GREEDYDATA:stacktrace} EXPRESION REGULAR: (?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|(?<stacktrace>.*) Tambien podemos crear PATTERNS propios como por ejemplo algo que se repite mucho, en nuestro caso se repite esto %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - . Se crea en custom patterns o en fichero aparte: COMMON_LOG %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - PATTERN: (?m)%{COMMON_LOG} %{DATA:description}\\|(?<stacktrace>.*) El fichero de configuraci\u00f3n de FILEBEAT sin la variable de pattern quedar\u00eda: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { match => {\"message\" => [ \"%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\"}, \"%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ] } } } output { stdout { #codec => json_lines } } Con la variable en un directorio patterns y en un fichero log-generator-patterns : COMMON_LOG %{WORD:level} %{TIMESTAMP_ISO8601:date} \\[%{WORD:thread}\\] %{JAVACLASS:class} - input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } } output { stdout { #codec => json_lines } }","title":"FILTER GROK"},{"location":"elastic/#filter-mutate","text":"Sirve para cambiar campos, eliminar, copiar, pasar a mayusculas, dividir un campo en varios,etc. DOC En este caso vamos a cambiar tipos de campo de string a number, a\u00f1adir campos, conficionales... input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } } } output { stdout { #codec => json_lines } } en grok se puede poner %{NUMBER:duration:int/float}, solo en estos tipos.","title":"FILTER MUTATE"},{"location":"elastic/#filter-date","text":"DOCS En este caso renombramos el campo timestamp para no confundir con la fecha del log, y la fecha del log le cambiamos el formato con el filter date: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } } date { match => [\"date\", \"YYYY-MM-dd HH:mm:ss\"] } } output { stdout { #codec => json_lines } }","title":"FILTER DATE"},{"location":"elastic/#filter-translate","text":"DOCS Nos permite hacer traducciones de codigos o informaciones indexando bien la informaci\u00f3n. En este caso en el campo login a\u00f1adimos traducciones segun el codigo que nos den: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } translate { field => \"status\" destination => \"statusText\" dictionary => { \"200\" => \"Login correcto\" \"201\" => \"Login correcto tras varios intentos\" \"202\" => \"Login con actualizaci\u00f3n de password\" \"204\" => \"login automatico\" \"250\" => \"Login recordado\" \"100\" => \"Usuario suplantado\" \"150\" => \"Usuario suplantado automaticamente\" \"400\" => \"Usuario bloqeuado\" \"404\" => \"Usuario no encontrado\" \"500\" => \"Contrase\u00f1a expirada\" } } } date { match => [ \"date\", \"YYYY-MM-dd HH:mm:ss\" ] } } output { stdout { #codec => json_lines } }","title":"FILTER TRANSLATE"},{"location":"elastic/#filter-geolp","text":"DOCS Nos indica info de geolocalizaci\u00f3n de la red, ips. En este caso se los a\u00f1adimos a los de login para ver donde para su ip: input { beats { port => 5044 #hosts => 0.0.0.0 } } filter { grok { patterns_dir => \"patterns\" match => {\"message\" => [ \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:status}\\|%{USERNAME:user}\\|%{IP:ip}\", \"%{COMMON_LOG} %{WORD:type}\\|%{NUMBER:duration}\\|%{DATA:action}\\|%{WORD:status}\\|%{USERNAME:user}\", \"(?m)%{COMMON_LOG} %{DATA:description}\\|%{GREEDYDATA:stacktrace}\" ]} } mutate { convert => {\"duration\" => \"float\"} #en grok se puede poner %{NUMBER:duration:int} remove_field => [\"message\", \"prospector\", \"input\"] rename => {\"@timestamp\" => \"processTime\"} } if ![type] { mutate { add_field => {\"type\" => \"JAVA_ERROR\"} #XK CUANDO NO SALE ESTE CAMPO, SON ERRORES } } if [type] == \"LOGIN\" { mutate { convert => {\"status\" => \"integer\"} add_field => {\"login\" => true} } if [status] > 300 { mutate { replace => {\"login\" => false} } } translate { field => \"status\" destination => \"statusText\" dictionary => { \"200\" => \"Login correcto\" \"201\" => \"Login correcto tras varios intentos\" \"202\" => \"Login con actualizaci\u00f3n de password\" \"204\" => \"login automatico\" \"250\" => \"Login recordado\" \"100\" => \"Usuario suplantado\" \"150\" => \"Usuario suplantado automaticamente\" \"400\" => \"Usuario bloqeuado\" \"404\" => \"Usuario no encontrado\" \"500\" => \"Contrase\u00f1a expirada\" } } geoip { source => \"ip\" } } date { match => [ \"date\", \"YYYY-MM-dd HH:mm:ss\" ] } } output { stdout { #codec => json_lines } }","title":"FILTER GEOLP"},{"location":"elastic/#filter-ruby","text":"DOCS Logstash est\u00e1 escrito en RUBY. Es el plugin m\u00e1s polivalente ya que con este podemos hacer todo lo que hacen el resto de plugins. Un ejemplo: input { stdin { codec => } } filter { ruby { code => \" event.set('name_normalized', event.get('name').gsub('\u00f1', 'n')) total_in_seconds = 0: event.get('total').split(':').each_with_index do [v, i] if i == 0 total_in_seconds += v.to_i * 3600; elsif i == 1 total_in_seconds += v.to_i * 60; elsif i == 2 total_in_seconds += v.to_i; end end event.set('total', total_in_seconds); \" } } output { stdout {} } ## { \"total\": \"00:00:00\", \"name\": \"Nombre\", \"equipo\": \"Equipo\"} ## convertir en segundos el tiempo y quitar las \u00d1 de los nombres","title":"FILTER RUBY"},{"location":"elastic/#otros-filters","text":"DOCS","title":"OTROS FILTERS"},{"location":"elastic/#outputs","text":"DOCS Combinando configuraciones de la que se entra por pantalla o por la que entra del log generator. input { stdin { codec => json add_field => {\"application\" => \"results\"} id => \"stdin-input-results\" } #file { # path => \"/Users/ballesterosam/Personal/training/elasticstack/logs/log-generator*.log\" #exclude => \"*.gz\" # start_position => \"beginning\" # sincedb_path => \"/Users/ballesterosam/Personal/training/elasticstack/logs/log-generator.sincedb\" # codec => multiline { # pattern => \"^(DEBUG|INFO|ERROR|TRACE|FATAL|WARN).*\" # negate => \"true\" # what => \"previous\" # } #} } filter { if [application] == \"results\" { ruby { code => \" event.set('name_normalized', event.get('name').gsub('\u00f1', 'n')); total_in_seconds = 0; event.get('total').split(':').each_with_index do |v, i| if i == 0 total_in_seconds += v.to_i * 3600; elsif i == 1 total_in_seconds += v.to_i * 60; elsif i == 2 total_in_seconds += v.to_i; end end event.set('total', total_in_seconds); \" id => \"parse-total\" } } } output { if [application] == \"results\" { stdout { #codec => json_lines id => \"stdout-results\" } } }","title":"OUTPUTS"},{"location":"elastic/#elasticsearch_1","text":"DOC Vamos a kibana -> dev tools Ejemplos de entradas que buscar: GET _nodes/(nombre de nodo si hay mas) GET /_cluster/allocation/explain { \"index\": \"nombre_indice\", \"shard\": 0, \"primary\": true } GET _cat/shards?v GET _cat/nodes?v GET _cat/health?v PUT new-index { \"settings\": { \"index\": { \"number_of_shards:\" 2, \"number_of_replicas: 1 } } } DELETE new-index PUT employees/_doc/ { \"name\": \"tyler\", \"surname\": \"lopex\", \"job\": \"IT\" } POST employees/_doc/ { \"name\": \"tyler\", \"surname\": \"lopex\", \"job\": \"IT\" } GET employees/_doc/2 GET employees/_search { \"query\": {\"match\": { \"name\": \"tyler\" }} } GET log-generator-*/_search { \"query\": {\"match\": { \"type\": \"JAVA_ERROR\" }} } GET employees/_search { \"query\": {\"match_all\"}: {} } Ejemplo de full query text GET /log-generator-*/_search { \"query\": { \"match\": { \"slackrace\": { \"query\": \"User not found\", \"operation\": \"and\" } } } } GET /log-generator-*/_search { \"query\": { \"multimatch\": { \"query\": \"STMP, \"fields\": [\"stackrace\",\"type\"] } } } Ejemplo de term queries GET /log-generator-*/_search { \"query\": { \"term\": { \"stackrace\": \"User not found\", } } } Ejemplo de boolean queries GET log-generator-*/_search { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, {\"match\" : { \"responseText\" : \"bloqueado\" }} ], \"minimum_should_match\" : 1, \"should\" : [ { \"term\" : { \"user\" : \"Carpanta\" } }, { \"term\" : { \"user\": { \"value\": \"SuperLopez\", \"boost\": 2 } }} { \"term\" : { \"user\" : \"Conan\" } }, { \"term\" : { \"user\" : \"Irongisg\" } } ], \"must_not\": [ { \"term\" : { \"geoip\" : \"Japon\" } } ] } } } Ejemplo de geo distance queries GET log-generator-*/_search { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, {\"match\" : { \"responseText\" : \"bloqueado\" }} ], \"must_not\": [ { \"term\" : { \"geoip\" : \"Japon\" } } ], \"filter\": [ { \"term\" : { \"geoip\" : \"EU\" } }, {\"geo_distance\": { \"distance\": \"1000km\", \"geoip.location\": { \"lat\": 40, \"lon\": -3 } } } ] } } } Ejemplo de agregaciones queries { \"query\": { \"bool\" : { \"must\" : [ {\"term\" : { \"type\" : \"LOGIN\" }}, ], \"filter\": [ { \"term\" : { \"geoip\" : \"EU\" } }, ] } }, \"aggs\": { \"ByUser\": { \"terms\": { \"field\": \"user\" } } } } Ejemplo de SQL queries: POST _sql { \"query\": \"SELECT * from \\\"log-generator-*\"\" } POST _sql?format=txt { \"query\": \"SELECT user, respondeCode from \\\"log-generator-*\"\" } POST _sql?format=txt { \"query\": \"SELECT user, respondeCode from \\\"log-generator-*\" where type=\"LOGIN\"\" } Los archivos importantes para configurar elasticsearch son config/jvm.options y elasticsearch.yml: ################################################################ ## ## JVM configuration ## ################################################################ ## ## WARNING: DO NOT EDIT THIS FILE. If you want to override the ## JVM options in this file, or set any additional options, you ## should create one or more files in the jvm.options.d ## directory containing your adjustments. ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/jvm-options.html ## for more information. ## ################################################################ ################################################################ ## IMPORTANT: JVM heap size ################################################################ ## ## The heap size is automatically configured by Elasticsearch ## based on the available memory in your system and the roles ## each node is configured to fulfill. If specifying heap is ## required, it should be done through a file in jvm.options.d, ## and the min and max should be set to the same value. For ## example, to set the heap to 4 GB, create a new file in the ## jvm.options.d directory containing these lines: ## -Xms2g -Xmx2g ## ## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html ## for more information ## ################################################################ ################################################################ ## Expert settings ################################################################ ## ## All settings below here are considered expert settings. Do ## not adjust them unless you understand what you are doing. Do ## not edit them in this file; instead, create a new file in the ## jvm.options.d directory containing your adjustments. ## ################################################################ ## GC configuration 8-13:-XX:+UseConcMarkSweepGC 8-13:-XX:CMSInitiatingOccupancyFraction=75 8-13:-XX:+UseCMSInitiatingOccupancyOnly ## G1GC Configuration # NOTE: G1 GC is only supported on JDK version 10 or later # to use G1GC, uncomment the next two lines and update the version on the # following three lines to your version of the JDK # 10-13:-XX:-UseConcMarkSweepGC # 10-13:-XX:-UseCMSInitiatingOccupancyOnly 14-:-XX:+UseG1GC ## JVM temporary directory -Djava.io.tmpdir=${ES_TMPDIR} ## heap dumps # generate a heap dump when an allocation from the Java heap fails; heap dumps # are created in the working directory of the JVM unless an alternative path is # specified -XX:+HeapDumpOnOutOfMemoryError # specify an alternative path for heap dumps; ensure the directory exists and # has sufficient space -XX:HeapDumpPath=data # specify an alternative path for JVM fatal error logs -XX:ErrorFile=logs/hs_err_pid%p.log ## JDK 8 GC logging 8:-XX:+PrintGCDetails 8:-XX:+PrintGCDateStamps 8:-XX:+PrintTenuringDistribution 8:-XX:+PrintGCApplicationStoppedTime 8:-Xloggc:logs/gc.log 8:-XX:+UseGCLogFileRotation 8:-XX:NumberOfGCLogFiles=32 8:-XX:GCLogFileSize=64m # JDK 9+ GC logging 9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m # ======================== Elasticsearch Configuration ========================= # # NOTE: Elasticsearch comes with reasonable defaults for most settings. # Before you set out to tweak and tune the configuration, make sure you # understand what are you trying to accomplish and the consequences. # # The primary way of configuring a node is via this file. This template lists # the most important settings you may want to configure for a production cluster. # # Please consult the documentation for further information on configuration options: # https://www.elastic.co/guide/en/elasticsearch/reference/index.html # # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # cluster.name: my-application # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # node.name: node-1 # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # path.data: /path/to/data # # Path to log files: # path.logs: /path/to/logs # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # By default Elasticsearch is only accessible on localhost. Set a different # address here to expose this node on the network: # network.host: 192.168.0.1 # # By default Elasticsearch listens for HTTP traffic on the first free port it # finds starting at 9200. Set a specific HTTP port here: # http.port: 9200 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # discovery.seed_hosts: [\"host1\", \"192.168.10.2\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # cluster.initial_master_nodes: [\"node-1\", \"node-2\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true","title":"ELASTICSEARCH"},{"location":"elastic/#kibana_1","text":"Configuraciones en kibana -> manegement. Podemos tener varias visualizaciones como histograma, pie, gauge, mapas,etc. Para ello vamos a KIBANA - VISUALIZE - +CREAR. Se puede crear varios y luego en DASHBOARD personalizar tus tableros de visualizaci\u00f3n.","title":"KIBANA"},{"location":"files/","text":"Archivos importantes de Linux Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos Destacados"},{"location":"files/#archivos-importantes-de-linux","text":"Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos importantes de Linux"},{"location":"gitlab/","text":"Comandos para GIT git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file Obtener claves para GIT ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io GitKraken Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken Git Tags/Releases Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0 Gitflow Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/ Features A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch Hotfix Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch Releases Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master. Emulador CMDER Descargar GITLAB CI/CD APUNTES","title":"Gitlab"},{"location":"gitlab/#comandos-para-git","text":"git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file","title":"Comandos para GIT"},{"location":"gitlab/#obtener-claves-para-git","text":"ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com","title":"Obtener claves para GIT"},{"location":"gitlab/#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"gitlab/#gitkraken","text":"Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken","title":"GitKraken"},{"location":"gitlab/#git-tagsreleases","text":"Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0","title":"Git Tags/Releases"},{"location":"gitlab/#gitflow","text":"Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/","title":"Gitflow"},{"location":"gitlab/#features","text":"A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch","title":"Features"},{"location":"gitlab/#hotfix","text":"Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch","title":"Hotfix"},{"location":"gitlab/#releases","text":"Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master.","title":"Releases"},{"location":"gitlab/#emulador-cmder","text":"Descargar","title":"Emulador CMDER"},{"location":"gitlab/#gitlab-cicd","text":"APUNTES","title":"GITLAB CI/CD"},{"location":"java/","text":"CURSO JAVA INSTALACION Instalamos JAVA # cd /opt # wget https://download.java.net/java/GA/jdk13/5b8a42f3905b406298b72d750b6919f6/33/GPL/openjdk-13_linux-x64_bin.tar.gz # tar -xvf openjdk-13_linux-x64_bin.tar.gz # export JAVA_HOME=/opt/jdk-13/ # export PATH=$PATH:/opt/jdk-13/bin Tambi\u00e9n puede definirlo en el archivo de inicio del shell global / etc / environment como se muestra. # vi /etc/environment # export JAVA_HOME=/usr/lib/jvm/java-17-openjdk # export PATH=$PATH:/usr/lib/jvm/java-17-openjdk/bin # source /etc/environment Instalamos Apache Netbeans : wget https://dlcdn.apache.org/netbeans/netbeans-installers/12.6/Apache-NetBeans-12.6-bin-linux-x64.sh chmod +x Apache-NetBeans-12.6-bin-linux-x64.sh sudo ./Apache-NetBeans-12.6-bin-linux-x64.sh Seguimos los pasos para instalar y arrancamos con netbeans . Podemos instalar plugins en my Netbeans - available plugins. DOCS Version oscura instalando el plugin DRACULA APACHE NETBEANS. HELLOWORLD Creamos un proyecto nuevo de Helloworld con las caracteristicas que le digamos. En pom.xml nos indica las caracteristicas de java. En source package hacemos boton derecho y nueva clase con el mismo nombre,siempre se empieza por mayusculas. //Mi clase en Java public class HelloWorld { public static void main(String args[]){ System.out.println(\"Hola Mundo desde Java\"); } } psvm y sout + tabulador y agrega ese codigo rapido Si se pone solo print y no println no lo imprime en otra linea. VARIABLES String Se puede poner el tipo como String, int, float, chart... o var en las nuevas versiones: public class HelloWorld { public static void main(String args[]){ String saludar = \"Saludos!\"; System.out.println(\"Hola Mundo desde Java\"); System.out.println(saludar); //nuevas versiones se va usando var para variables var despedirse = \"Adios\"; System.out.println(despedirse); // variable entero var numero = 1; System.out.println(numero); System.out.println(saludar + despedirse);//se junta y numeros primero lo suma System.out.println(saludar + \" \" + despedirse); // tipo input Scanner scanner = new Scanner(System.in); System.out.println(\"Proporciona un usuario: \"); var usuario = scanner.nextLine(); System.out.println(\"Usuario = \" + usuario); } } Enteros Short,int,long,byte: import java.util.Scanner; /* Esto es un comentario */ //Mi clase en Java public class HelloWorld { public static void main(String args[]){ //byte, short, int, long byte byteVar = 127; System.out.println(\"byteVar = \" + byteVar); System.out.println(\"bits tipo byte:\" + Byte.SIZE); System.out.println(\"bytes tipos byte:\" + Byte.BYTES); System.out.println(\"valor minimo tipo byte:\" + Byte.MIN_VALUE); System.out.println(\"valor maximo tipo byte:\" + Byte.MAX_VALUE); short shortVar = 32767; System.out.println(\"shortVar = \" + shortVar); System.out.println(\"bits tipo short:\" + Short.SIZE); System.out.println(\"bytes tipos short:\" + Short.BYTES); System.out.println(\"valor minimo tipo short:\" + Short.MIN_VALUE); System.out.println(\"valor maximo tipo short:\" + Short.MAX_VALUE); int intVar = 2147483647; System.out.println(\"intVar = \" + intVar); System.out.println(\"bits tipo int:\" + Integer.SIZE); System.out.println(\"bytes tipos int:\" + Integer.BYTES); System.out.println(\"valor minimo tipo int:\" + Integer.MIN_VALUE); System.out.println(\"valor maximo tipo int:\" + Integer.MAX_VALUE); long longVar = 9223372036854775807L; System.out.println(\"longVar = \" + longVar); System.out.println(\"bits tipo long:\" + Long.SIZE); System.out.println(\"bytes tipos long:\" + Long.BYTES); System.out.println(\"valor minimo tipo long:\" + Long.MIN_VALUE); System.out.println(\"valor maximo tipo long:\" + Long.MAX_VALUE); var numeroInt = 2147483647; System.out.println(\"numeroInt = \" + numeroInt); var numeroLong = 2147483648L; System.out.println(\"numeroLong = \" + numeroLong); } } Flotantes import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { var floatVar = 1000.10F; System.out.println(\"floatVar = \" + floatVar); System.out.println(\"bits tipo float:\" + Float.SIZE); System.out.println(\"bytes tipo float:\" + Float.BYTES); System.out.println(\"valor minimo tipo float:\" + Float.MIN_VALUE); System.out.println(\"valor maximo tipo float:\" + Float.MAX_VALUE); var doubleVar = 100D; System.out.println(\"doubleVar = \" + doubleVar); System.out.println(\"bits tipo double:\" + Double.SIZE); System.out.println(\"bytes tipo double:\" + Double.BYTES); System.out.println(\"valor minimo tipo double:\" + Double.MIN_VALUE); System.out.println(\"valor maximo tipo double:\" + Double.MAX_VALUE); } } Tipo chart Solo soporta un caracter. import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { System.out.println(\"bits tipo char:\" + Character.SIZE); System.out.println(\"bytes tipo char:\" + Character.BYTES); System.out.println(\"valor minimo tipo char:\" + Character.MIN_VALUE); System.out.println(\"valor maximo tipo char:\" + Character.MAX_VALUE); var varChar = '\\u0021'; System.out.println(\"varChar = \" + varChar); var varCharDecimal = 33; System.out.println(\"varCharDecimal = \" + varCharDecimal); var varCharSimbolo = '!'; System.out.println(\"varCharSimbolo = \" + varCharSimbolo); } } Boleanos True o False: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { //boolean System.out.println(\"true tipo boolean: \" + Boolean.TRUE); System.out.println(\"false tipo boolean: \" + Boolean.FALSE); boolean booleanVar = false; if(booleanVar){ System.out.println(\"el valor es verdadero\"); } else{ System.out.println(\"el valor es falso\"); } System.out.println(\"\"); var edad = 30; var esAdulto = edad >= 18; System.out.println(\"esAdulto = \" + esAdulto); } } Conversion de variables import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { //convertir un String a un tipo int var edad = Integer.parseInt(\"20\"); System.out.println(\"edad = \" + edad); double valorPI = Double.parseDouble(\"3.1416\"); System.out.println(\"valorPI = \" + valorPI); char c = \"hola\".charAt(3); System.out.println(\"c = \" + c); //introducir una edad var scanner = new Scanner(System.in); edad = Integer.parseInt(scanner.nextLine()) ; System.out.println(\"edad = \" + edad); char caracter = scanner.nextLine().charAt(0); System.out.println(\"caracter = \" + caracter); String edadTexto = String.valueOf(false); System.out.println(\"edadTexto = \" + edadTexto); short s = 129; byte b = (byte) s; System.out.println(\"b = \" + b); } } OPERACIONES Aritmeticos: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 2; var resultado = a + b; System.out.println(\"resultado suma = \" + resultado); System.out.println(\"resultado suma=\" + (a + b) ); resultado = a - b; System.out.println(\"resultado resta = \" + resultado); System.out.println(\"resultado resta = \" + (a - b)); resultado = a * b; System.out.println(\"resultado multiplicacion = \" + resultado); double resultado2 = 3D / b; System.out.println(\"resultado division = \" + resultado2); resultado = a % b; System.out.println(\"resultado modulo= \" + resultado); resultado = a % 2; System.out.println(\"resultado = \" + resultado); resultado = 123 % 2; System.out.println(\"resultado = \" + resultado); if(resultado == 0) System.out.println(\"es numero par\"); else System.out.println(\"es numero impar\"); } } De asignacion: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 2; int c = a + 5 - b; System.out.println(\"c = \" + c); a += 1;//a=a+1 System.out.println(\"a = \" + a); a += 3;//a=a+3 System.out.println(\"a = \" + a); b -= 1;//b=b-1 System.out.println(\"b = \" + b); // *=, /=, %= } } Unarios: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { // el negativo para invertir int a = 3; int b = -a; System.out.println(\"b = \" + b); // el ! para invertir el booleano boolean c = true; boolean d = !c; System.out.println(\"d = \" + d); //incremento //preincremento int e = 3; int f = ++e; //la e se incrementa 1 y f se queda con este valor System.out.println(\"e = \" + e); System.out.println(\"f = \" + f); //postincrement int g = 5; int h = g++;//la g se incrementa 1 y f se queda sin incremento System.out.println(\"g = \" + g); System.out.println(\"h = \" + h); //decremento //predecremento int i=2; int j = --i;//la i se disminuye 1 y f se queda con este valor System.out.println(\"i = \" + i); System.out.println(\"j = \" + j); //postdecremento int k=4; int l= k--;//la k se disminuye 1 y f se queda sin restar System.out.println(\"k = \" + k); System.out.println(\"l = \" + l); } } De comparacion: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 4; boolean c = (a == b); System.out.println(\"c = \" + c); c = (a != b); System.out.println(\"c = \" + c); String cadena = \"hola\"; String cadena2 = \"hola\"; // printa comparando las dos cadenas System.out.println(cadena.equals(cadena2)); boolean d = a <= b; System.out.println(\"d = \" + d); if (b % 2 == 0) { System.out.println(\"numero par\"); } else { System.out.println(\"numero impar\"); } int edad = 31; int adulto = 18; if (edad >= adulto) System.out.println(\"es un adulto\"); else System.out.println(\"es menor de edad\"); } } Booleanos: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 15; int valorMinimo = 0, valorMaximo=10; // && es un AND de si y si boolean resultado = a >= valorMinimo && a <= valorMaximo; System.out.println(\"resultado = \" + resultado); boolean vacaciones = false; boolean diaDescanso = true; // || es un OR es de si o si if(vacaciones || diaDescanso) System.out.println(\"Padre puede asistir al juego del hijo\"); else System.out.println(\"Padre ocupado\"); } } Ternario: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { // ternacio es una condicion // (expresion) ? true(si es verdadera) :false(o falsa) var resultado = (3 > 2) ? \"verdadero\" : false; System.out.println(\"resultado = \" + resultado); // o la expresion puede dar un resultado entre dos var numero = 8; var par = (numero % 2 == 0) ? \"numero par\" : \"numero impar\"; System.out.println(\"par = \" + par); } } Prioridad de operaciones: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { var x = 5; var y = 10; var z = ++x + y--;//x=6, y=9, z=(6+10)16 System.out.println(\"x = \" + x); System.out.println(\"y = \" + y); System.out.println(\"z = \" + z); System.out.println(\"\\nEjemplo 2 precedencia operadores\"); var resultado = 4 + 5 * 6 / 3;// 4+((5*6)/3) => 4+(30/3) => 4+10 => 14 System.out.println(\"resultado = \" + resultado); resultado = (4 + 5) * 6 / 3; //18 System.out.println(\"resultado = \" + resultado); } } CONDICIONALES","title":"Java"},{"location":"java/#curso-java","text":"","title":"CURSO JAVA"},{"location":"java/#instalacion","text":"Instalamos JAVA # cd /opt # wget https://download.java.net/java/GA/jdk13/5b8a42f3905b406298b72d750b6919f6/33/GPL/openjdk-13_linux-x64_bin.tar.gz # tar -xvf openjdk-13_linux-x64_bin.tar.gz # export JAVA_HOME=/opt/jdk-13/ # export PATH=$PATH:/opt/jdk-13/bin Tambi\u00e9n puede definirlo en el archivo de inicio del shell global / etc / environment como se muestra. # vi /etc/environment # export JAVA_HOME=/usr/lib/jvm/java-17-openjdk # export PATH=$PATH:/usr/lib/jvm/java-17-openjdk/bin # source /etc/environment Instalamos Apache Netbeans : wget https://dlcdn.apache.org/netbeans/netbeans-installers/12.6/Apache-NetBeans-12.6-bin-linux-x64.sh chmod +x Apache-NetBeans-12.6-bin-linux-x64.sh sudo ./Apache-NetBeans-12.6-bin-linux-x64.sh Seguimos los pasos para instalar y arrancamos con netbeans . Podemos instalar plugins en my Netbeans - available plugins. DOCS Version oscura instalando el plugin DRACULA APACHE NETBEANS.","title":"INSTALACION"},{"location":"java/#helloworld","text":"Creamos un proyecto nuevo de Helloworld con las caracteristicas que le digamos. En pom.xml nos indica las caracteristicas de java. En source package hacemos boton derecho y nueva clase con el mismo nombre,siempre se empieza por mayusculas. //Mi clase en Java public class HelloWorld { public static void main(String args[]){ System.out.println(\"Hola Mundo desde Java\"); } } psvm y sout + tabulador y agrega ese codigo rapido Si se pone solo print y no println no lo imprime en otra linea.","title":"HELLOWORLD"},{"location":"java/#variables","text":"","title":"VARIABLES"},{"location":"java/#string","text":"Se puede poner el tipo como String, int, float, chart... o var en las nuevas versiones: public class HelloWorld { public static void main(String args[]){ String saludar = \"Saludos!\"; System.out.println(\"Hola Mundo desde Java\"); System.out.println(saludar); //nuevas versiones se va usando var para variables var despedirse = \"Adios\"; System.out.println(despedirse); // variable entero var numero = 1; System.out.println(numero); System.out.println(saludar + despedirse);//se junta y numeros primero lo suma System.out.println(saludar + \" \" + despedirse); // tipo input Scanner scanner = new Scanner(System.in); System.out.println(\"Proporciona un usuario: \"); var usuario = scanner.nextLine(); System.out.println(\"Usuario = \" + usuario); } }","title":"String"},{"location":"java/#enteros","text":"Short,int,long,byte: import java.util.Scanner; /* Esto es un comentario */ //Mi clase en Java public class HelloWorld { public static void main(String args[]){ //byte, short, int, long byte byteVar = 127; System.out.println(\"byteVar = \" + byteVar); System.out.println(\"bits tipo byte:\" + Byte.SIZE); System.out.println(\"bytes tipos byte:\" + Byte.BYTES); System.out.println(\"valor minimo tipo byte:\" + Byte.MIN_VALUE); System.out.println(\"valor maximo tipo byte:\" + Byte.MAX_VALUE); short shortVar = 32767; System.out.println(\"shortVar = \" + shortVar); System.out.println(\"bits tipo short:\" + Short.SIZE); System.out.println(\"bytes tipos short:\" + Short.BYTES); System.out.println(\"valor minimo tipo short:\" + Short.MIN_VALUE); System.out.println(\"valor maximo tipo short:\" + Short.MAX_VALUE); int intVar = 2147483647; System.out.println(\"intVar = \" + intVar); System.out.println(\"bits tipo int:\" + Integer.SIZE); System.out.println(\"bytes tipos int:\" + Integer.BYTES); System.out.println(\"valor minimo tipo int:\" + Integer.MIN_VALUE); System.out.println(\"valor maximo tipo int:\" + Integer.MAX_VALUE); long longVar = 9223372036854775807L; System.out.println(\"longVar = \" + longVar); System.out.println(\"bits tipo long:\" + Long.SIZE); System.out.println(\"bytes tipos long:\" + Long.BYTES); System.out.println(\"valor minimo tipo long:\" + Long.MIN_VALUE); System.out.println(\"valor maximo tipo long:\" + Long.MAX_VALUE); var numeroInt = 2147483647; System.out.println(\"numeroInt = \" + numeroInt); var numeroLong = 2147483648L; System.out.println(\"numeroLong = \" + numeroLong); } }","title":"Enteros"},{"location":"java/#flotantes","text":"import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { var floatVar = 1000.10F; System.out.println(\"floatVar = \" + floatVar); System.out.println(\"bits tipo float:\" + Float.SIZE); System.out.println(\"bytes tipo float:\" + Float.BYTES); System.out.println(\"valor minimo tipo float:\" + Float.MIN_VALUE); System.out.println(\"valor maximo tipo float:\" + Float.MAX_VALUE); var doubleVar = 100D; System.out.println(\"doubleVar = \" + doubleVar); System.out.println(\"bits tipo double:\" + Double.SIZE); System.out.println(\"bytes tipo double:\" + Double.BYTES); System.out.println(\"valor minimo tipo double:\" + Double.MIN_VALUE); System.out.println(\"valor maximo tipo double:\" + Double.MAX_VALUE); } }","title":"Flotantes"},{"location":"java/#tipo-chart","text":"Solo soporta un caracter. import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { System.out.println(\"bits tipo char:\" + Character.SIZE); System.out.println(\"bytes tipo char:\" + Character.BYTES); System.out.println(\"valor minimo tipo char:\" + Character.MIN_VALUE); System.out.println(\"valor maximo tipo char:\" + Character.MAX_VALUE); var varChar = '\\u0021'; System.out.println(\"varChar = \" + varChar); var varCharDecimal = 33; System.out.println(\"varCharDecimal = \" + varCharDecimal); var varCharSimbolo = '!'; System.out.println(\"varCharSimbolo = \" + varCharSimbolo); } }","title":"Tipo chart"},{"location":"java/#boleanos","text":"True o False: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { //boolean System.out.println(\"true tipo boolean: \" + Boolean.TRUE); System.out.println(\"false tipo boolean: \" + Boolean.FALSE); boolean booleanVar = false; if(booleanVar){ System.out.println(\"el valor es verdadero\"); } else{ System.out.println(\"el valor es falso\"); } System.out.println(\"\"); var edad = 30; var esAdulto = edad >= 18; System.out.println(\"esAdulto = \" + esAdulto); } }","title":"Boleanos"},{"location":"java/#conversion-de-variables","text":"import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { //convertir un String a un tipo int var edad = Integer.parseInt(\"20\"); System.out.println(\"edad = \" + edad); double valorPI = Double.parseDouble(\"3.1416\"); System.out.println(\"valorPI = \" + valorPI); char c = \"hola\".charAt(3); System.out.println(\"c = \" + c); //introducir una edad var scanner = new Scanner(System.in); edad = Integer.parseInt(scanner.nextLine()) ; System.out.println(\"edad = \" + edad); char caracter = scanner.nextLine().charAt(0); System.out.println(\"caracter = \" + caracter); String edadTexto = String.valueOf(false); System.out.println(\"edadTexto = \" + edadTexto); short s = 129; byte b = (byte) s; System.out.println(\"b = \" + b); } }","title":"Conversion de variables"},{"location":"java/#operaciones","text":"Aritmeticos: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 2; var resultado = a + b; System.out.println(\"resultado suma = \" + resultado); System.out.println(\"resultado suma=\" + (a + b) ); resultado = a - b; System.out.println(\"resultado resta = \" + resultado); System.out.println(\"resultado resta = \" + (a - b)); resultado = a * b; System.out.println(\"resultado multiplicacion = \" + resultado); double resultado2 = 3D / b; System.out.println(\"resultado division = \" + resultado2); resultado = a % b; System.out.println(\"resultado modulo= \" + resultado); resultado = a % 2; System.out.println(\"resultado = \" + resultado); resultado = 123 % 2; System.out.println(\"resultado = \" + resultado); if(resultado == 0) System.out.println(\"es numero par\"); else System.out.println(\"es numero impar\"); } } De asignacion: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 2; int c = a + 5 - b; System.out.println(\"c = \" + c); a += 1;//a=a+1 System.out.println(\"a = \" + a); a += 3;//a=a+3 System.out.println(\"a = \" + a); b -= 1;//b=b-1 System.out.println(\"b = \" + b); // *=, /=, %= } } Unarios: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { // el negativo para invertir int a = 3; int b = -a; System.out.println(\"b = \" + b); // el ! para invertir el booleano boolean c = true; boolean d = !c; System.out.println(\"d = \" + d); //incremento //preincremento int e = 3; int f = ++e; //la e se incrementa 1 y f se queda con este valor System.out.println(\"e = \" + e); System.out.println(\"f = \" + f); //postincrement int g = 5; int h = g++;//la g se incrementa 1 y f se queda sin incremento System.out.println(\"g = \" + g); System.out.println(\"h = \" + h); //decremento //predecremento int i=2; int j = --i;//la i se disminuye 1 y f se queda con este valor System.out.println(\"i = \" + i); System.out.println(\"j = \" + j); //postdecremento int k=4; int l= k--;//la k se disminuye 1 y f se queda sin restar System.out.println(\"k = \" + k); System.out.println(\"l = \" + l); } } De comparacion: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 3, b = 4; boolean c = (a == b); System.out.println(\"c = \" + c); c = (a != b); System.out.println(\"c = \" + c); String cadena = \"hola\"; String cadena2 = \"hola\"; // printa comparando las dos cadenas System.out.println(cadena.equals(cadena2)); boolean d = a <= b; System.out.println(\"d = \" + d); if (b % 2 == 0) { System.out.println(\"numero par\"); } else { System.out.println(\"numero impar\"); } int edad = 31; int adulto = 18; if (edad >= adulto) System.out.println(\"es un adulto\"); else System.out.println(\"es menor de edad\"); } } Booleanos: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { int a = 15; int valorMinimo = 0, valorMaximo=10; // && es un AND de si y si boolean resultado = a >= valorMinimo && a <= valorMaximo; System.out.println(\"resultado = \" + resultado); boolean vacaciones = false; boolean diaDescanso = true; // || es un OR es de si o si if(vacaciones || diaDescanso) System.out.println(\"Padre puede asistir al juego del hijo\"); else System.out.println(\"Padre ocupado\"); } } Ternario: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { // ternacio es una condicion // (expresion) ? true(si es verdadera) :false(o falsa) var resultado = (3 > 2) ? \"verdadero\" : false; System.out.println(\"resultado = \" + resultado); // o la expresion puede dar un resultado entre dos var numero = 8; var par = (numero % 2 == 0) ? \"numero par\" : \"numero impar\"; System.out.println(\"par = \" + par); } } Prioridad de operaciones: import java.util.Scanner; public class HelloWorld { public static void main(String args[]) { var x = 5; var y = 10; var z = ++x + y--;//x=6, y=9, z=(6+10)16 System.out.println(\"x = \" + x); System.out.println(\"y = \" + y); System.out.println(\"z = \" + z); System.out.println(\"\\nEjemplo 2 precedencia operadores\"); var resultado = 4 + 5 * 6 / 3;// 4+((5*6)/3) => 4+(30/3) => 4+10 => 14 System.out.println(\"resultado = \" + resultado); resultado = (4 + 5) * 6 / 3; //18 System.out.println(\"resultado = \" + resultado); } }","title":"OPERACIONES"},{"location":"java/#condicionales","text":"","title":"CONDICIONALES"},{"location":"jenkins/","text":"Jenkins Instalaci\u00f3n FEDORA Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins UBUNTU/DEBIAN Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins DOCKER En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net: NOTAS A TENER EN CUENTA Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini... PROYECTO CON PARAMETROS Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script. SSH Creacion SSH container Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins Credenciales Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully. Ejercicio mandar un job a maquina remota En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota. BASE DE DATOS JENKINS Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p Creacion bbdd simple MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27); Creaci\u00f3n Buckets en amazon Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key Dump de la bbdd [root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql DUMP AUTOMATIZADO Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea). ANSIBLE Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Playbooks Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG TAGS Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job. DB MYSQL Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada. NGINX SERVER Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro. SECURITY JENKINS Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles: MANAGE USERS Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas: MANAGE ROLES Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron. TRIPS AND TICKS Variables de entorno Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A Cambio URL Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/ CRON Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue. GATILLAR JOBS Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22 MAIL Configurar envio de notificaciones Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent Gmail como server de correo Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo: Email de error Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email. MAVEN Instalacion Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app Configuracion de un job Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar Registrar los resultados A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results: Archivar los jar A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla: GIT SERVER Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido. CAMBIO URL MAVEN/GIT/JENKINS Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job. JOB DSL Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl SEED JOB Ejemplo estructura: job('job_dsl_example') { } DESCRIPCION Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada PAR\u00c1METROS Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas. SCM La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso. TRIGGERS Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron. STEPS Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world MAILER Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones. JOB DE ANSIBLE EN DSL En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/ JOB DE MAVEN EN DSL Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } DSL en GIT Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } PIPELINES Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline JENKINSFILE Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente. MULTIPLE-STEPS pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } } POST-ACTIONS pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } } RETRY pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } } TIMEOUT pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } } VARIABLES ENV pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } } CREDENCIALES pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } } CI/CD BUILD Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } } TEST Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } PUSH A MAQUINA REMOTA AWS Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa CERTIFICADO SSL REGISTRY CON AUTENTICACION Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } } DEPLOY En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } } CI/CD Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"Jenkins"},{"location":"jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"jenkins/#instalacion","text":"","title":"Instalaci\u00f3n"},{"location":"jenkins/#fedora","text":"Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins","title":"FEDORA"},{"location":"jenkins/#ubuntudebian","text":"Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins","title":"UBUNTU/DEBIAN"},{"location":"jenkins/#docker","text":"En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net:","title":"DOCKER"},{"location":"jenkins/#notas-a-tener-en-cuenta","text":"Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini...","title":"NOTAS A TENER EN CUENTA"},{"location":"jenkins/#proyecto-con-parametros","text":"Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script.","title":"PROYECTO CON PARAMETROS"},{"location":"jenkins/#ssh","text":"","title":"SSH"},{"location":"jenkins/#creacion-ssh-container","text":"Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins","title":"Creacion SSH container"},{"location":"jenkins/#credenciales","text":"Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully.","title":"Credenciales"},{"location":"jenkins/#ejercicio-mandar-un-job-a-maquina-remota","text":"En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota.","title":"Ejercicio mandar un job a maquina remota"},{"location":"jenkins/#base-de-datos-jenkins","text":"Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p","title":"BASE DE DATOS JENKINS"},{"location":"jenkins/#creacion-bbdd-simple","text":"MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27);","title":"Creacion bbdd simple"},{"location":"jenkins/#creacion-buckets-en-amazon","text":"Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key","title":"Creaci\u00f3n Buckets en amazon"},{"location":"jenkins/#dump-de-la-bbdd","text":"[root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql","title":"Dump de la bbdd"},{"location":"jenkins/#dump-automatizado","text":"Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea).","title":"DUMP AUTOMATIZADO"},{"location":"jenkins/#ansible","text":"Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"ANSIBLE"},{"location":"jenkins/#playbooks","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG","title":"Playbooks"},{"location":"jenkins/#tags","text":"Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job.","title":"TAGS"},{"location":"jenkins/#db-mysql","text":"Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada.","title":"DB MYSQL"},{"location":"jenkins/#nginx-server","text":"Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro.","title":"NGINX SERVER"},{"location":"jenkins/#security-jenkins","text":"Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles:","title":"SECURITY JENKINS"},{"location":"jenkins/#manage-users","text":"Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas:","title":"MANAGE USERS"},{"location":"jenkins/#manage-roles","text":"Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron.","title":"MANAGE ROLES"},{"location":"jenkins/#trips-and-ticks","text":"","title":"TRIPS AND TICKS"},{"location":"jenkins/#variables-de-entorno","text":"Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A","title":"Variables de entorno"},{"location":"jenkins/#cambio-url","text":"Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/","title":"Cambio URL"},{"location":"jenkins/#cron","text":"Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue.","title":"CRON"},{"location":"jenkins/#gatillar-jobs","text":"Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22","title":"GATILLAR JOBS"},{"location":"jenkins/#mail","text":"","title":"MAIL"},{"location":"jenkins/#configurar-envio-de-notificaciones","text":"Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent","title":"Configurar envio de notificaciones"},{"location":"jenkins/#gmail-como-server-de-correo","text":"Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo:","title":"Gmail como server de correo"},{"location":"jenkins/#email-de-error","text":"Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email.","title":"Email de error"},{"location":"jenkins/#maven","text":"","title":"MAVEN"},{"location":"jenkins/#instalacion_1","text":"Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app","title":"Instalacion"},{"location":"jenkins/#configuracion-de-un-job","text":"Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar","title":"Configuracion de un job"},{"location":"jenkins/#registrar-los-resultados","text":"A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results:","title":"Registrar los resultados"},{"location":"jenkins/#archivar-los-jar","text":"A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla:","title":"Archivar los jar"},{"location":"jenkins/#git-server","text":"Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido.","title":"GIT SERVER"},{"location":"jenkins/#cambio-url-mavengitjenkins","text":"Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job.","title":"CAMBIO URL MAVEN/GIT/JENKINS"},{"location":"jenkins/#job-dsl","text":"Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl","title":"JOB DSL"},{"location":"jenkins/#seed-job","text":"Ejemplo estructura: job('job_dsl_example') { }","title":"SEED JOB"},{"location":"jenkins/#descripcion","text":"Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada","title":"DESCRIPCION"},{"location":"jenkins/#parametros","text":"Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas.","title":"PAR\u00c1METROS"},{"location":"jenkins/#scm","text":"La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso.","title":"SCM"},{"location":"jenkins/#triggers","text":"Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron.","title":"TRIGGERS"},{"location":"jenkins/#steps","text":"Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world","title":"STEPS"},{"location":"jenkins/#mailer","text":"Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones.","title":"MAILER"},{"location":"jenkins/#job-de-ansible-en-dsl","text":"En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/","title":"JOB DE ANSIBLE EN DSL"},{"location":"jenkins/#job-de-maven-en-dsl","text":"Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"JOB DE MAVEN EN DSL"},{"location":"jenkins/#dsl-en-git","text":"Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"DSL en GIT"},{"location":"jenkins/#pipelines","text":"Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline","title":"PIPELINES"},{"location":"jenkins/#jenkinsfile","text":"Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente.","title":"JENKINSFILE"},{"location":"jenkins/#multiple-steps","text":"pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } }","title":"MULTIPLE-STEPS"},{"location":"jenkins/#post-actions","text":"pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } }","title":"POST-ACTIONS"},{"location":"jenkins/#retry","text":"pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } }","title":"RETRY"},{"location":"jenkins/#timeout","text":"pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } }","title":"TIMEOUT"},{"location":"jenkins/#variables-env","text":"pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } }","title":"VARIABLES ENV"},{"location":"jenkins/#credenciales_1","text":"pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } }","title":"CREDENCIALES"},{"location":"jenkins/#cicd","text":"","title":"CI/CD"},{"location":"jenkins/#build","text":"Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"BUILD"},{"location":"jenkins/#test","text":"Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } }","title":"TEST"},{"location":"jenkins/#push-a-maquina-remota-aws","text":"Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa","title":"PUSH A MAQUINA REMOTA AWS"},{"location":"jenkins/#certificado-ssl-registry-con-autenticacion","text":"Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"CERTIFICADO SSL REGISTRY CON AUTENTICACION"},{"location":"jenkins/#deploy","text":"En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } }","title":"DEPLOY"},{"location":"jenkins/#cicd_1","text":"Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"CI/CD"},{"location":"kerberos/","text":"KERBEROS Se conecta el user a keberos y recibe un ticket que demuestra que es quien es. Cuando un user se conecta al server kerbertizado, presenta un ticket y el server lo comprueba sin tener que loguearse todo el rato. Los tickets expiran Demonios: /usr/sbin/krb5kdc y /usr/sbin/kadmind Puertos: 88(keberos), 464(keberos passwd) y 749(keberos admin) COMANDOS klist /etc/krb5.conf /var/kerberos/krb5kdc/kdc.conf /var/kerberos/krb5kdc/kadm5.acl kadmin.local kadmin.local -q \"addprinc -pq kmiguel miguel\" kinit user kdestroy kadmin kadmin -p miguel kdb5_util create -s -P masterkey KSERVER Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-server passwd procps vim nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- cp /opt/docker/krb5.conf /etc/krb5.conf cp /opt/docker/kdc.conf /var/kerberos/krb5kdc/kdc.conf cp /opt/docker/kadm5.acl /var/kerberos/krb5kdc/kadm5.acl #creamos bbdd kdb5_util create -s -P masterkey #creamos unos users principales que desde el client podremos coger ticket (pass kpere para pere) kadmin.local -q \"addprinc -pw kpere pere\" kadmin.local -q \"addprinc -pw kmarta marta\" kadmin.local -q \"addprinc -pw kpau pau\" kadmin.local -q \"addprinc -pw kjordi jordi\" kadmin.local -q \"addprinc -pw kanna anna\" kadmin.local -q \"addprinc -pw kmarta marta/admin\" kadmin.local -q \"addprinc -pw kjulia julia\" kadmin.local -q \"addprinc -pw kadmin admin\" kadmin.local -q \"addprinc -pw kmiguel miguel\" kadmin.local -q \"addprinc -pw kuser01 kuser01\" kadmin.local -q \"addprinc -pw kuser02 kuser02\" kadmin.local -q \"addprinc -pw kuser03 kuser03\" kadmin.local -q \"addprinc -pw kuser04 kuser04\" kadmin.local -q \"addprinc -pw kuser05 kuser05\" kadmin.local -q \"addprinc -pw kuser06 kuser06\" kadmin.local -q \"addprinc -randkey host/sshd.edt.org\" Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis /usr/sbin/krb5kdc && echo \"krb5kdc Ok\" /usr/sbin/kadmind -nofork && echo \"kadmind Ok\" krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG kdc.conf: [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] EDT.ORG = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } kadm5.acl: */admin@EDT.ORG * superuser@EDT.ORG * pau@EDT.ORG * /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org KHOST Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-workstation passwd vim procps nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- #fitxer de conf del client cp /opt/docker/krb5.conf /etc/krb5.conf startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" /bin/bash krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG","title":"KERBEROS"},{"location":"kerberos/#kerberos","text":"Se conecta el user a keberos y recibe un ticket que demuestra que es quien es. Cuando un user se conecta al server kerbertizado, presenta un ticket y el server lo comprueba sin tener que loguearse todo el rato. Los tickets expiran Demonios: /usr/sbin/krb5kdc y /usr/sbin/kadmind Puertos: 88(keberos), 464(keberos passwd) y 749(keberos admin)","title":"KERBEROS"},{"location":"kerberos/#comandos","text":"klist /etc/krb5.conf /var/kerberos/krb5kdc/kdc.conf /var/kerberos/krb5kdc/kadm5.acl kadmin.local kadmin.local -q \"addprinc -pq kmiguel miguel\" kinit user kdestroy kadmin kadmin -p miguel kdb5_util create -s -P masterkey","title":"COMANDOS"},{"location":"kerberos/#kserver","text":"Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-server passwd procps vim nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- cp /opt/docker/krb5.conf /etc/krb5.conf cp /opt/docker/kdc.conf /var/kerberos/krb5kdc/kdc.conf cp /opt/docker/kadm5.acl /var/kerberos/krb5kdc/kadm5.acl #creamos bbdd kdb5_util create -s -P masterkey #creamos unos users principales que desde el client podremos coger ticket (pass kpere para pere) kadmin.local -q \"addprinc -pw kpere pere\" kadmin.local -q \"addprinc -pw kmarta marta\" kadmin.local -q \"addprinc -pw kpau pau\" kadmin.local -q \"addprinc -pw kjordi jordi\" kadmin.local -q \"addprinc -pw kanna anna\" kadmin.local -q \"addprinc -pw kmarta marta/admin\" kadmin.local -q \"addprinc -pw kjulia julia\" kadmin.local -q \"addprinc -pw kadmin admin\" kadmin.local -q \"addprinc -pw kmiguel miguel\" kadmin.local -q \"addprinc -pw kuser01 kuser01\" kadmin.local -q \"addprinc -pw kuser02 kuser02\" kadmin.local -q \"addprinc -pw kuser03 kuser03\" kadmin.local -q \"addprinc -pw kuser04 kuser04\" kadmin.local -q \"addprinc -pw kuser05 kuser05\" kadmin.local -q \"addprinc -pw kuser06 kuser06\" kadmin.local -q \"addprinc -randkey host/sshd.edt.org\" Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis /usr/sbin/krb5kdc && echo \"krb5kdc Ok\" /usr/sbin/kadmind -nofork && echo \"kadmind Ok\" krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG kdc.conf: [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] EDT.ORG = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } kadm5.acl: */admin@EDT.ORG * superuser@EDT.ORG * pau@EDT.ORG * /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org","title":"KSERVER"},{"location":"kerberos/#khost","text":"Dockerfile: # Version: 0.0.1 # @edt M11 2019-2020 # kerberos # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"KERBEROS server 2019-2020\" RUN dnf -y install krb5-workstation passwd vim procps nmap tree RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] /etc/hosts: 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.18.0.2 kserver.edt.org install.sh: #! /bin/bash # @edt ASIX M11 2019-2020 # instal.lacio # ------------------------------------- #fitxer de conf del client cp /opt/docker/krb5.conf /etc/krb5.conf startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" /bin/bash krb5.conf: # To opt out of the system crypto-policies configuration of krb5, remove the # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated. includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = EDT.ORG # default_ccache_name = KEYRING:persistent:%{uid} [realms] EDT.ORG = { kdc = kserver.edt.org admin_server = kserver.edt.org } [domain_realm] .edt.org = EDT.ORG edt.org = EDT.ORG","title":"KHOST"},{"location":"kubernetes/","text":"KUBERNETES K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal ARQUITECTURA MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc. INSTALACI\u00d3N MINIKUBE/KUBECTL MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes DASHBOARD Para ver la parte grafica usamos kubectl dashboard . Si no tenemos instalado esta opcion es porque necesitamos instalar addons. La lista de addons es minikube addons ls e instalamos minikube addons enable dashboard metrics-server PROXY Issuing the kubectl proxy command, kubectl authenticates with the API server on the master node and makes the Dashboard available on a slightly different URL than the one earlier, this time through the default proxy port 8001. kubectl proxy curl http://localhost:8001/: { \"paths\": [ \"/api\", \"/api/v1\", \"/apis\", \"/apis/apps\", ...... ...... \"/logs\", \"/metrics\", \"/openapi/v2\", \"/version\" ] } Podemos explorar urls del api server con http://localhost:8001/apis/apps/v1 PODS VS CONTENEDORES Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores. PODS CREAR POD Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso. LOGS PODS Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso. API-RESOURCES Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources ELIMINAR PODS Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all OBTENER YAML POD Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces. IP POD Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80 ENTRAR AL POD Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh CREAR POD YAML Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine 2+ CONTAINER POR POD Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2 LABELS Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas. PROBLEMAS PODs Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar. REPLICASETS Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet. CREAR REPLICASET Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s ELIMINAR/MODIFICAR En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s LOGS Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test OWNER REFERNCE Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 ADOPCI\u00d3N DE PODS PLANOS Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena. PROBLEMAS Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones. DEPLOYMENTS Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets CREAR DEPLOYMENT Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset ROLLING UPDATE Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge HISTORIAL DE DEPLOYMENTS Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> ROLL BACKS Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0 SERVICIOS Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio. CREAR SERVICIO Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80. INFO SERVICIO Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint. ENDPOINTS Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none> DNS Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888 SERVICIO CLUSTER-IP IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) SERVICIO NODE-PORT IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port. SERVICIO LOAD BALANCER Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP. GOLANG Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend. CREAR API REST GO DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png] CAMBIOS MENSAJE RESPUESTA MENSAJE 1 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png] MENSAJE 2 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png] DOCKERFILE GOLANG Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos. DEPLOYMENT GOLANG Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png] CONSUMO DEL SERVICIO Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ # FRONTED Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html MANIFIESTO FRONTED Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy NAMESPACES Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName CREAR NAMESPACE Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none> ASIGNAR NAMESPACES Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s BORRAR NAMESPACES Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns DEPLOY NAMESPACES Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m DNS NAMESPACES Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on CONTEXTOS NAMESPACES Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d LIMITAR RAM/CPU La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus. LIMITS/REQUEST Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique. RAM Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory. CPU Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu. QOS(Quality of Service) Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request. LIMITRANGE Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods VALORES POR DEFECTO Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi - VALORES POD Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M. LIMITES Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram RESOURCE QUOTA Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod. CREAR RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource. DEPLOY RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA. LIMITAR N\u00ba PODS EN NS Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota. PROBES Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP TIPOS PROBES Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse. CREAR LIVENESS PROBE Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness LIVENESS TCP Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName LIVENESS HTTP Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName READINESS PROBE Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod. VARIABLES Y CONFIGMAP CREAR VARIABLES Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1 VARIABLES REFERENCIADAS Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc. CONFIGMAP Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> MONTANDO VOLUMEN CONFIGMAP Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo. VOLUMEN-ENV CONFIGMAP Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user SECRETS Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen. CREAR Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode MANIFIESTOS Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test ENVSUBTS Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado. VOLUME SECRETS Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ # ENV SECRETS Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456 VOLUMES Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). EMPTYDIR Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / # HOSTPATH-PV En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none> PVC El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC. PVC-PV Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector. PVC-PODS De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql> CLOUD VOLUMES Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC. RECLAIM POLICY Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). USERS/GROUPS RBAC RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster. ROLES vs CLUSTERROLES En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo. ROLEBINDING vs CLUSTERROLEBINDING Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1. CREAR USERS & GROUPS Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" HABILITAR RBAC Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC SIMPLIFICAMOS CONTEXTO Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel CREAR ROLES Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user. ENLAZAR ROLE & USER Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\" CONFIG MAPS Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again. CREAR CLUSTEROLE Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m CREAR USER ADMIN Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m ROLES A GRUPOS Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m SERVICES ACCOUNT Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr SECRET SA Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc CREAR SA Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s RELACION POD-SA Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ REQUESTS A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones. REQUEST JWT Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante. SA DEPLOYMENT Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa ROLE SA Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1, INGRESS Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada. INGRESS CONTROLLER Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud. CREAR INGRESS CONTROLLER Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s IP INGRESS CONTROLLER Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602 APP INGRESS-CONTROLLER Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv EXPONER EL PUERTO AL EXTERIOR Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 2 APPS EN IC Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: AWS KUBERNETES Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: xxxx Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0 CREAR CLUSTER AWS EKSCTL docs install Creamos cluster master sin nodos: eksctl create cluster --name test-cluster --without-nodegroup --region eu-west-2 --zones eu-west-2a,eu-west-2b Vemos lo creado en el apartado EKS y CloudFormation : Eksctl lee de estos archivos para comunicarse: [isx46410800@miguel ~]$ cat .aws/credentials [default] aws_access_key_id = AKIA5RIFOUI3OMSWWHNM aws_secret_access_key = xxxxx [isx46410800@miguel ~]$ cat .aws/config [default] region = eu-west-2 Al crear el cluster nos crea un directorio ~/.kube/config Si eliminamos este directorio, como si no lo tuvieramos y nos queremos conectar a este cluster usamos la orden: aws eks --region eu-west-2 update-kubeconfig --name test-cluster Ahora si hacemos kubectl get svc y kubectl cluster-info vemos que estamos conectados y referenciados al cluster de AWS: [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7m11s [isx46410800@miguel ~]$ kubectl cluster-info Kubernetes master is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com CoreDNS is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ahora intentamos crear un POD pero vemos que no se acaba de crear porque no tenemos ningun nodo unido a nuestro CLUSTER: [isx46410800@miguel ~]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 0/1 Pending 0 10s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 15s default-scheduler no nodes available to schedule pods Ahora creamos nodos con eksctl con ami version kubernetes auto y asg access para ser escalable: eksctl create nodegroup --cluster test-cluster --region eu-west-2 --name test-workers --node-type t3.medium --node-ami auto --nodes 1 --nodes-min 1 --nodes-max 3 --asg-access Comprobamos que el pod de prueba est\u00e1 ahora running y asignado al nodo creado: [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 68s v1.17.11-eks-cfdc40 # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6m38s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Name: pod-test Namespace: default Priority: 0 Node: ip-192-168-38-128.eu-west-2.compute.internal/192.168.38.128 INGRESS AWS EKS Para exponerlo, crearemos un balanzador de carga, un ingress y un ingress controller. DOCS para crear el ingress controller nos dice que nuestro servicio(VPC) tiene que seguir una estructura de tag. Vemos los servicios VPC que se crearon automaticamente al crear el cluster y los nodos. Las subnets tambien tienen que seguir una estructura de tags. No obstante todos estos pasos al crearlos con EKSCTL ya vienen por defecto. IAM OIDC eksctl utils associate-iam-oidc-provider --region eu-west-2 --cluster test-cluster --approve Politica para crear recursos de balanceador de carga: [isx46410800@miguel ~]$ aws iam create-policy \\ > --policy-name ALBIngressControllerIAMPolicy \\ > --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json { \"Policy\": { \"PolicyName\": \"ALBIngressControllerIAMPolicy\", \"PolicyId\": \"ANPA5RIFOUI3IFJHOR5SB\", \"Arn\": \"arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy\", \"Path\": \"/\", \"DefaultVersionId\": \"v1\", \"AttachmentCount\": 0, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"CreateDate\": \"2020-10-21T17:06:33Z\", \"UpdateDate\": \"2020-10-21T17:06:33Z\" } } Creamos un service account para ingress con un clusterrole y un clusterrolebinding de ingress controller para balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml Creamos un service account para que nuestro ingress controller sea capaz de crear recursos en AWS: eksctl create iamserviceaccount \\ --region eu-west-2 \\ --name alb-ingress-controller \\ --namespace kube-system \\ --cluster test-cluster \\ --attach-policy-arn arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy \\ --override-existing-serviceaccounts \\ --approve La policy la vemos en IAM-POLICIES Resumen: creamos un service account que tiene un clusterrolebinding para ver los permisos de ingress y de balanzador de carga, por esto, de este ultimo, creamos una politica para que pueda crear recursos en AWS y en balanceador de carga. DEPLOY INGRESS CONTROLLER AWS Creamos un deployment que crea un pod de ingress controller con una imagen de aws ingress controller que lo que har\u00e1 es que si ve cambios, los modifica en el balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml Cambiamos unas lineas del deploy: kubectl edit deployment.apps/alb-ingress-controller -n kube-system spec: containers: - args: - --ingress-class=alb - --cluster-name=test-cluster Comprobamos que esto funciona: [isx46410800@miguel ~]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE alb-ingress-controller-868ddb9874-gzsvx 1/1 Running 0 41s aws-node-gcd69 1/1 Running 0 35m coredns-6ddcfb5bcf-h7qrx 1/1 Running 0 48m coredns-6ddcfb5bcf-t7wnz 1/1 Running 0 48m kube-proxy-jdnj5 1/1 Running 0 35m DEPLOY APP Creamos el ejemplo de aplicaci\u00f3n que es un juego, creamos un servicio, un deploy y namespaces: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml Comprobamos: [isx46410800@miguel ~]$ kubectl get all -n 2048-game NAME READY STATUS RESTARTS AGE pod/2048-deployment-dd74cc68d-88w46 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-gc9pp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-lw72w 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-wk8tp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-zlshx 1/1 Running 0 29s # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/service-2048 NodePort 10.100.179.203 <none> 80:30798/TCP 20s # NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/2048-deployment 5/5 5 5 30s # NAME DESIRED CURRENT READY AGE replicaset.apps/2048-deployment-dd74cc68d 5 5 5 30s Para comprobar que funciona la app internamente usamos: [isx46410800@miguel ~]$ kubectl port-forward pod/2048-deployment-dd74cc68d-88w46 -n 2048-game 7000:80 Forwarding from 127.0.0.1:7000 -> 80 Forwarding from [::1]:7000 -> 80 EXPONER LA APP EXTERNAMENTE Enrutamos con el ingress la app: [isx46410800@miguel ~]$ kubectl get ingress -n 2048-game NAME HOSTS ADDRESS PORTS AGE 2048-ingress * d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com 80 14s Si vamos a nuestro EC2 de amazon. a nuestro balanceador de carga veremos que nos sale la url en la que podemos ir a la aplicaci\u00f3n ya que la regla estaba asignada. MODIFICANDO REGLAS INGRESS Vemos que IPs apuntan al balanceador de carga que nos da la url del juego: [isx46410800@miguel ~]$ nslookup d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Server: 192.168.1.1 Address: 192.168.1.1#53 Non-authoritative answer: Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.134.190.250 Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.133.107.232 Las a\u00f1adimos a nuestro /etc/hosts: 18.134.190.250 app.aws.game.test 18.133.107.232 app.aws.game.test Cambiamos reglas para que utilicen el nombre y no la ip ni dns: kubectl edit ingress 2048-ingress -n 2048-game spec: rules: - host: app.aws.game.test http: paths: - path: /* backend: serviceName: service-2048 servicePort: 80 Ahora entraremos solo por nombre BORRAR TODO Borramos todo y vemos que no hay el balanceador de carga: kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml AWS HPA INSTALL HPA(Horizontal Pod Autoescaler) consulta unas metricas y se asocia a un deployment. Basado a unas metricas dice cuanta cantidad de pods creas, segun la carga que se pueda ir soportando. Solo escala por CPU. Se ha de instalar el Metrics Server : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml [isx46410800@miguel ~]$ kubectl get deployment metrics-server -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 6s CREAR UN HPA Ejemplo de una app: [isx46410800@miguel ~]$ kubectl apply -f https://k8s.io/examples/application/php-apache.yaml deployment.apps/php-apache created service/php-apache created [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 0/1 1 0 15s [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 105m php-apache ClusterIP 10.100.137.253 <none> 80/TCP 19s [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-h2xhh 1/1 Running 0 50s Ahora Escalamos. Esto quiere decir que si la carga pasa del 50% creee pods hasta un maximo de 10 pods: kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Comprobamos con kubectl get hpa : [isx46410800@miguel ~]$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 74s [isx46410800@miguel ~]$ kubectl get hpa -o yaml apiVersion: v1 items: - apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler AUTOESCALAR HPA Nuestra maquina de AWS es un t3.medium y tiene 2 cpus y 4 de ram. Creamos un container y dentro de el le hacemos muchas peticiones, veremos como se va cargando y se van creando pods para balancear esta carga: kubectl run -it --rm load-generator --image=busybox /bin/sh --generator=run-pod/v1 # while true; do wget -q -O- http://php-apache; done Vemos los pods y los hpa: [isx46410800@miguel ~]$ kubectl get pods -w NAME READY STATUS RESTARTS AGE apache-bench 1/1 Running 1 21m httpd 1/1 Running 0 2m10s load-generator 1/1 Running 0 20s php-apache-79544c9bd9-cnq6h 1/1 Running 0 43s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-ckcgb 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-m29bz 0/1 ContainerCreating 0 0s # [isx46410800@miguel ~]$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 35m php-apache Deployment/php-apache 54%/50% 1 10 1 35m php-apache Deployment/php-apache 250%/50% 1 10 1 36m php-apache Deployment/php-apache 250%/50% 1 10 4 36m php-apache Deployment/php-apache 250%/50% 1 10 5 36m php-apache Deployment/php-apache 74%/50% 1 10 5 37m php-apache Deployment/php-apache 74%/50% 1 10 8 37m php-apache Deployment/php-apache 68%/50% 1 10 8 38m php-apache Deployment/php-apache 68%/50% 1 10 8 39m php-apache Deployment/php-apache 0%/50% 1 10 8 40m CLUSTER AUTOSCALER Se dispara cuando el HPA dispara pods y no hay nodos donde colocarlos. Entonces se autoescala en nodos para ponerlos. Se dispara cuando desde fuera se hace un deploy y se llena el nodo. Si se dispara otro deploy, como no hay espacio, el cluster autoescaler crea otro nodo para poner los pods que falten por poner. La politica que se tiene que agregar al cluster de cluster autoscale se crea de por s\u00ed cuando creamos el cluster con la herramienta eksctl con la opcion --asg-access. DOCS autoscaler Trabaja como otro pod corriendo en mi cluster. Lo desplegamos: [isx46410800@miguel ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created # [isx46410800@miguel ~]$ kubectl get deploy -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE alb-ingress-controller 1/1 1 1 5h18m cluster-autoscaler 1/1 1 1 13s coredns 2/2 2 2 6h2m metrics-server 1/1 1 1 4h25m Editamos el deploy: kubectl -n kube-system edit deploy cluster-autoscaler - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/test-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false Borramos el HPA para que no haya conflictos. Editamos el deploy y ponemos 3 replicas: [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 3h55m [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-6zqcc 1/1 Running 0 5s php-apache-79544c9bd9-cnq6h 1/1 Running 0 3h56m php-apache-79544c9bd9-pfsrq 1/1 Running 0 5s Si editamos el deploy y a\u00f1adimos mas replicas, veremos que se nos crean varias maquinas, varios nodes. kubectl edit deploy php-apache Comprobamos: [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 114s php-apache-79544c9bd9-6zqcc 1/1 Running 0 6m59s php-apache-79544c9bd9-cnq6h 1/1 Running 0 4h3m php-apache-79544c9bd9-dlmrz 1/1 Running 0 114s php-apache-79544c9bd9-dq8f2 1/1 Running 0 3m29s php-apache-79544c9bd9-hbxnr 1/1 Running 0 3m29s php-apache-79544c9bd9-n594l 1/1 Running 0 114s php-apache-79544c9bd9-pfsrq 1/1 Running 0 6m59s php-apache-79544c9bd9-pv5cl 1/1 Running 0 114s php-apache-79544c9bd9-pzz4w 1/1 Running 0 114s php-apache-79544c9bd9-x4czh 1/1 Running 0 4m19s php-apache-79544c9bd9-zm7fj 1/1 Running 0 114s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-22-127.eu-west-2.compute.internal Ready <none> 41s v1.17.11-eks-cfdc40 ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h11m v1.17.11-eks-cfdc40 Ahora comprobamos que cuando no usa un nodo, el autoscale lo elimine automaticamente y va pasando pods a un solo nodo y dejar el minimo de maquinas running: [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 5m47s php-apache-79544c9bd9-dlmrz 1/1 Running 0 5m47s php-apache-79544c9bd9-n594l 1/1 Running 0 5m47s php-apache-79544c9bd9-pv5cl 1/1 Running 0 5m47s php-apache-79544c9bd9-pzz4w 1/1 Running 0 5m47s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h14m v1.17.11-eks-cfdc40 AUTHENTICATION AND AUTHORIZATION USUARIO This exercise guide assumes the following environment, which by default uses the certificate and key from /var/lib/minikube/certs/, and RBAC mode for authorization: Minikube v1.13.1 Kubernetes v1.19.2 Docker 19.03.12-ce This exercise guide was prepared for the video demonstration following on the next page. Start Minikube: $ minikube start View the content of the kubectl client's configuration manifest, observing the only context minikube and the only user minikube, created by default: $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/student/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/student/.minikube/profiles/minikube/client.crt client-key: /home/student/.minikube/profiles/minikube/client.key Create lfs158 namespace: $ kubectl create namespace lfs158 namespace/lfs158 created Create the rbac directory and cd into it: $ mkdir rbac $ cd rbac/ Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool: ~/rbac$ openssl genrsa -out student.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) .................................................+++++ .........................+++++ e is 65537 (0x010001) ~/rbac$ openssl req -new -key student.key -out student.csr -subj \"/CN=student/O=learner\" Create a YAML manifest for a certificate signing request object, and save it with a blank value for the request field: ~/rbac$ vim signing-request.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: <assign encoded value from next cat command> signerName: kubernetes.io/kube-apiserver-client usages: - digital signature - key encipherment - client auth View the certificate, encode it in base64, and assign it to the request field in the signing-request.yaml file: ~/rbac$ cat student.csr | base64 | tr -d '\\n','%' LS0tLS1CRUd...1QtLS0tLQo= ~/rbac$ vim signing-request.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: LS0tLS1CRUd...1QtLS0tLQo= signerName: kubernetes.io/kube-apiserver-client usages: - digital signature - key encipherment - client auth Create the certificate signing request object, then list the certificate signing request objects. It shows a pending state: ~/rbac$ kubectl create -f signing-request.yaml certificatesigningrequest.certificates.k8s.io/student-csr created ~/rbac$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION student-csr 12s kubernetes.io/kube-apiserver-client minikube-user Pending Approve the certificate signing request object, then list the certificate signing request objects again. It shows both approved and issued states: ~/rbac$ kubectl certificate approve student-csr certificatesigningrequest.certificates.k8s.io/student-csr approved ~/rbac$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION student-csr 57s kubernetes.io/kube-apiserver-client minikube-user Approved,Issued Extract the approved certificate from the certificate signing request, decode it with base64 and save it as a certificate file. Then view the certificate in the newly created certificate file: ~/rbac$ kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode > student.crt ~/rbac$ cat student.crt -----BEGIN CERTIFICATE----- MIIDGzCCA... ... ...NOZRRZBVunTjK7A== -----END CERTIFICATE----- Configure the kubectl client's configuration manifest with the student user's credentials by assigning the key and certificate: ~/rbac$ kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key User \"student\" set. Create a new context entry in the kubectl client's configuration manifest for the student user, associated with the lfs158 namespace in the minikube cluster: ~/rbac$ kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student Context \"student-context\" created. View the contents of the kubectl client's configuration manifest again, observing the new context entry student-context, and the new user entry student: ~/rbac$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/student/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube - context: cluster: minikube namespace: lfs158 user: student name: student-context current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/student/.minikube/profiles/minikube/client.crt client-key: /home/student/.minikube/profiles/minikube/client.key - name: student user: client-certificate: /home/student/rbac/student.crt client-key: /home/student/rbac/student.key While in the default minikube context, create a new deployment in the lfs158 namespace: ~/rbac$ kubectl -n lfs158 create deployment nginx --image=nginx:alpine deployment.apps/nginx created From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context: ~/rbac$ kubectl --context=student-context get pods Error from server (Forbidden): pods is forbidden: User \"student\" cannot list resource \"pods\" in API group \"\" in the namespace \"lfs158\" The following steps will assign a limited set of permissions to the student user in the student-context. Create a YAML configuration manifest for a pod-reader Role object, which allows only get, watch, list actions in the lfs158 namespace against pod objects. Then create the role object and list it from the default minikube context, but from the lfs158 namespace: ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ~/rbac$ kubectl create -f role.yaml role.rbac.authorization.k8s.io/pod-reader created ~/rbac$ kubectl -n lfs158 get roles NAME CREATED AT pod-reader 2020-10-07T03:47:45Z Create a YAML configuration manifest for a rolebinding object, which assigns the permissions of the pod-reader Role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace: ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME ROLE AGE pod-read-access Role/pod-reader 28s Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context. ~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-565785f75c-kl25r 1/1 Running 0 7m41s ELIMINAMOS TODO DE LA NUBE Vamos a AWS - CLOUD FORMATION y eliminamos todo. PRUEBAS DE SALUD LIVENES: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 3 failureThreshold: 1 periodSeconds: 5 Lo creamos con kubectl create -f liveness.yaml y lo vemos con kubectl get pod liveness_pod -w y vemos que cada rato se crea uno comprobando que est\u00e1 todo bien. LIVENESS HTTP REQUEST: livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 In the following example, the kubelet sends the HTTP GET request to the /healthz endpoint of the application, on port 8080. If that returns a failure, then the kubelet will restart the affected container; otherwise, it would consider the application to be alive. TCP LIVENESS PROBE: livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 With TCP Liveness Probe, the kubelet attempts to open the TCP Socket to the container which is running the application. If it succeeds, the application is considered healthy, otherwise the kubelet would mark it as unhealthy and restart the affected container. ANOTACIONES Se pueden poner comentarios como anotaciones en los ficheros yaml: apiVersion: apps/v1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019 KUBEADM GU\u00cdA para crear un cluster con master y nodos con kubeadm. INSTALAR KUBEADM . CREAR CLUSTER KUBEADM . LAB CLUSTERS KUBEADM HELM DOCS CHARTS instalables de helm para kubernetes. GUIA INSTALL HELM . Helm es un administrador de paquetes para Kubernetes que permite a los desarrolladores y operadores configurar e implementar de forma m\u00e1s sencilla aplicaciones en los cl\u00fasteres de Kubernetes. La mayor\u00eda de los sistemas operativos y de programaci\u00f3n de lenguaje tienen su propio administrador de paquetes para la instalaci\u00f3n y el mantenimiento de software. Helm proporciona el mismo conjunto de funciones b\u00e1sicas que muchos de los administradores que seguramente ya conoce, como apt de Debian o pip de Python. Helm puede: Instalar software Instalar de manera autom\u00e1tica dependencias de software Actualizar software Configurar implementaciones de software Obtener paquetes de software de repositorios Helm proporciona esta funcionalidad a trav\u00e9s de los siguientes componentes: Una herramienta de l\u00ednea de comandos, helm, que proporciona la interfaz de usuario para todas las funcionalidades de Helm. Un componente de servidor complementario, tiller, que funciona en su cl\u00faster de Kubernetes, escucha los comandos de helm y gestiona la configuraci\u00f3n e implementaci\u00f3n de versiones de software en el cl\u00faster. El formato de empaquetado de Helm, llamado charts. Un repositorio de charts oficiales seleccionados con charts empaquetados previamente para proyectos de software de c\u00f3digo abierto populares. INSTALACION INSTALACION Y PASO A PASO EXPLICACION INICIAL Iniciamos helm: kubectl apply -f helm-rbac.yml helm init --service-account helm-tiller kubectl get pods -n kube-system apiVersion: v1 kind: ServiceAccount metadata: name: helm-tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm-tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm-tiller namespace: kube-system NOTA: YA NO SE USA TILLER Podemos buscar charts instalables como por ejemplo: helm search hub/repo nginx Instalamos cualquier chart que salga con: helm install nginx-redi (--name nuevo_name) helm install my-releasr nginx-stable/nginx-ingress helm ls Instalar Tiller : Tiller es un complemento del comando helm que se ejecuta en su cl\u00faster, recibe comandos de helm y se comunica directamente con la API de Kubernetes para hacer el verdadero trabajo de crear y eliminar recursos. A fin de proporcionar a Tiller los permisos que necesita para ejecutarse en el cl\u00faster, crearemos un recurso serviceaccount de Kubernetes. kubectl -n kube-system create serviceaccount tiller kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller helm init --service-account tiller Vincularemos serviceaccount al rol de cl\u00faster de cluster-admin. Esto permitir\u00e1 que el superusuario del servicio de tiller acceda al cl\u00faster e instale todos los tipos de recursos en todos los espacios de nombres. Para verificar que Tiller est\u00e9 en ejecuci\u00f3n, enumere los pods en el espacio de nombres de kube-system : kubectl get pods --namespace kube-system Instalar un chart de Helm: Los paquetes de software de Helm se llaman charts. Hay un repositorio administrado de charts llamado stable, el cual consiste principalmente en tablas comunes que puede ver en su repositorio de GitHub. Helm no lo tiene preconfigurado en Helm. Por lo tanto, tendr\u00e1 que a\u00f1adirlo manualmente. Luego, a modo de ejemplo, instalar\u00e1 Kubernetes Dashboard. helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install dashboard-demo stable/kubernetes-dashboard --set rbac.clusterAdminRole=true Puede enumerar todas las versiones del cl\u00faster: helm list Ahora, podemos usar kubectl para verificar que se haya implementado un nuevo servicio en el cl\u00faster: kubectl get services El comando helm upgrade puede utilizarse para actualizar una versi\u00f3n con un chart nuevo o actualizado, o para actualizar sus opciones de configuraci\u00f3n (variables). Realizar\u00e1 un cambio sencillo en la versi\u00f3n de dashboard-demo para demostrar el proceso de actualizaci\u00f3n y reversi\u00f3n: actualizar\u00e1 el nombre del servicio de dashboard simplemente a kubernetes-dashboard en lugar de dashboard-demo-kubernetes-dashboard. El chart kubernetes-dashboard proporciona una opci\u00f3n de configuraci\u00f3n de fullnameOverride para controlar el nombre de servicio. Para cambiar el nombre de la versi\u00f3n, ejecute helm upgrade con esta opci\u00f3n establecida: helm upgrade dashboard-demo stable/kubernetes-dashboard --set fullnameOverride=\"kubernetes-dashboard\" --reuse-values Revertir y eliminar una versi\u00f3n: Cuando actualiz\u00f3 la versi\u00f3n de dashboard-demo en el paso anterior, cre\u00f3 una segunda revisi\u00f3n de la versi\u00f3n. Helm conserva todos los detalles de las versiones anteriores en caso de que deba realizar una reversi\u00f3n a una configuraci\u00f3n o un chart anterior. helm rollback dashboard-demo 1 Es posible eliminar versiones de helm con el comando helm delete: helm delete dashboard-demo","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal","title":"KUBERNETES"},{"location":"kubernetes/#arquitectura","text":"MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc.","title":"ARQUITECTURA"},{"location":"kubernetes/#instalacion-minikubekubectl","text":"MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes","title":"INSTALACI\u00d3N MINIKUBE/KUBECTL"},{"location":"kubernetes/#dashboard","text":"Para ver la parte grafica usamos kubectl dashboard . Si no tenemos instalado esta opcion es porque necesitamos instalar addons. La lista de addons es minikube addons ls e instalamos minikube addons enable dashboard metrics-server","title":"DASHBOARD"},{"location":"kubernetes/#proxy","text":"Issuing the kubectl proxy command, kubectl authenticates with the API server on the master node and makes the Dashboard available on a slightly different URL than the one earlier, this time through the default proxy port 8001. kubectl proxy curl http://localhost:8001/: { \"paths\": [ \"/api\", \"/api/v1\", \"/apis\", \"/apis/apps\", ...... ...... \"/logs\", \"/metrics\", \"/openapi/v2\", \"/version\" ] } Podemos explorar urls del api server con http://localhost:8001/apis/apps/v1","title":"PROXY"},{"location":"kubernetes/#pods-vs-contenedores","text":"Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.","title":"PODS VS CONTENEDORES"},{"location":"kubernetes/#pods","text":"","title":"PODS"},{"location":"kubernetes/#crear-pod","text":"Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso.","title":"CREAR POD"},{"location":"kubernetes/#logs-pods","text":"Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso.","title":"LOGS PODS"},{"location":"kubernetes/#api-resources","text":"Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources","title":"API-RESOURCES"},{"location":"kubernetes/#eliminar-pods","text":"Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all","title":"ELIMINAR PODS"},{"location":"kubernetes/#obtener-yaml-pod","text":"Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces.","title":"OBTENER YAML POD"},{"location":"kubernetes/#ip-pod","text":"Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80","title":"IP POD"},{"location":"kubernetes/#entrar-al-pod","text":"Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh","title":"ENTRAR AL POD"},{"location":"kubernetes/#crear-pod-yaml","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine","title":"CREAR POD YAML"},{"location":"kubernetes/#2-container-por-pod","text":"Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2","title":"2+ CONTAINER POR POD"},{"location":"kubernetes/#labels","text":"Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas.","title":"LABELS"},{"location":"kubernetes/#problemas-pods","text":"Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar.","title":"PROBLEMAS PODs"},{"location":"kubernetes/#replicasets","text":"Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.","title":"REPLICASETS"},{"location":"kubernetes/#crear-replicaset","text":"Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s","title":"CREAR REPLICASET"},{"location":"kubernetes/#eliminarmodificar","text":"En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s","title":"ELIMINAR/MODIFICAR"},{"location":"kubernetes/#logs","text":"Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test","title":"LOGS"},{"location":"kubernetes/#owner-refernce","text":"Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0","title":"OWNER REFERNCE"},{"location":"kubernetes/#adopcion-de-pods-planos","text":"Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.","title":"ADOPCI\u00d3N DE PODS PLANOS"},{"location":"kubernetes/#problemas","text":"Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.","title":"PROBLEMAS"},{"location":"kubernetes/#deployments","text":"Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets","title":"DEPLOYMENTS"},{"location":"kubernetes/#crear-deployment","text":"Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset","title":"CREAR DEPLOYMENT"},{"location":"kubernetes/#rolling-update","text":"Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge","title":"ROLLING UPDATE"},{"location":"kubernetes/#historial-de-deployments","text":"Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"HISTORIAL DE DEPLOYMENTS"},{"location":"kubernetes/#roll-backs","text":"Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0","title":"ROLL BACKS"},{"location":"kubernetes/#servicios","text":"Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.","title":"SERVICIOS"},{"location":"kubernetes/#crear-servicio","text":"Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.","title":"CREAR SERVICIO"},{"location":"kubernetes/#info-servicio","text":"Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.","title":"INFO SERVICIO"},{"location":"kubernetes/#endpoints","text":"Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none>","title":"ENDPOINTS"},{"location":"kubernetes/#dns","text":"Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888","title":"DNS"},{"location":"kubernetes/#servicio-cluster-ip","text":"IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)","title":"SERVICIO CLUSTER-IP"},{"location":"kubernetes/#servicio-node-port","text":"IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port.","title":"SERVICIO NODE-PORT"},{"location":"kubernetes/#servicio-load-balancer","text":"Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.","title":"SERVICIO LOAD BALANCER"},{"location":"kubernetes/#golang","text":"Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend.","title":"GOLANG"},{"location":"kubernetes/#crear-api-rest-go","text":"DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png]","title":"CREAR API REST GO"},{"location":"kubernetes/#cambios-mensaje-respuesta","text":"","title":"CAMBIOS MENSAJE RESPUESTA"},{"location":"kubernetes/#mensaje-1","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png]","title":"MENSAJE 1"},{"location":"kubernetes/#mensaje-2","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png]","title":"MENSAJE 2"},{"location":"kubernetes/#dockerfile-golang","text":"Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.","title":"DOCKERFILE GOLANG"},{"location":"kubernetes/#deployment-golang","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png]","title":"DEPLOYMENT GOLANG"},{"location":"kubernetes/#consumo-del-servicio","text":"Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ #","title":"CONSUMO DEL SERVICIO"},{"location":"kubernetes/#fronted","text":"Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html","title":"FRONTED"},{"location":"kubernetes/#manifiesto-fronted","text":"Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy","title":"MANIFIESTO FRONTED"},{"location":"kubernetes/#namespaces","text":"Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName","title":"NAMESPACES"},{"location":"kubernetes/#crear-namespace","text":"Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none>","title":"CREAR NAMESPACE"},{"location":"kubernetes/#asignar-namespaces","text":"Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s","title":"ASIGNAR NAMESPACES"},{"location":"kubernetes/#borrar-namespaces","text":"Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns","title":"BORRAR NAMESPACES"},{"location":"kubernetes/#deploy-namespaces","text":"Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m","title":"DEPLOY NAMESPACES"},{"location":"kubernetes/#dns-namespaces","text":"Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on","title":"DNS NAMESPACES"},{"location":"kubernetes/#contextos-namespaces","text":"Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d","title":"CONTEXTOS NAMESPACES"},{"location":"kubernetes/#limitar-ramcpu","text":"La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus.","title":"LIMITAR RAM/CPU"},{"location":"kubernetes/#limitsrequest","text":"Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique.","title":"LIMITS/REQUEST"},{"location":"kubernetes/#ram","text":"Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory.","title":"RAM"},{"location":"kubernetes/#cpu","text":"Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu.","title":"CPU"},{"location":"kubernetes/#qosquality-of-service","text":"Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request.","title":"QOS(Quality of Service)"},{"location":"kubernetes/#limitrange","text":"Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods","title":"LIMITRANGE"},{"location":"kubernetes/#valores-por-defecto","text":"Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi -","title":"VALORES POR DEFECTO"},{"location":"kubernetes/#valores-pod","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M.","title":"VALORES POD"},{"location":"kubernetes/#limites","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram","title":"LIMITES"},{"location":"kubernetes/#resource-quota","text":"Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod.","title":"RESOURCE QUOTA"},{"location":"kubernetes/#crear-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource.","title":"CREAR RQ"},{"location":"kubernetes/#deploy-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA.","title":"DEPLOY RQ"},{"location":"kubernetes/#limitar-no-pods-en-ns","text":"Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota.","title":"LIMITAR N\u00ba PODS EN NS"},{"location":"kubernetes/#probes","text":"Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP","title":"PROBES"},{"location":"kubernetes/#tipos-probes","text":"Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse.","title":"TIPOS PROBES"},{"location":"kubernetes/#crear-liveness-probe","text":"Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness","title":"CREAR LIVENESS PROBE"},{"location":"kubernetes/#liveness-tcp","text":"Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS TCP"},{"location":"kubernetes/#liveness-http","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS HTTP"},{"location":"kubernetes/#readiness-probe","text":"Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod.","title":"READINESS PROBE"},{"location":"kubernetes/#variables-y-configmap","text":"","title":"VARIABLES Y CONFIGMAP"},{"location":"kubernetes/#crear-variables","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1","title":"CREAR VARIABLES"},{"location":"kubernetes/#variables-referenciadas","text":"Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc.","title":"VARIABLES REFERENCIADAS"},{"location":"kubernetes/#configmap","text":"Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none>","title":"CONFIGMAP"},{"location":"kubernetes/#montando-volumen-configmap","text":"Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo.","title":"MONTANDO VOLUMEN CONFIGMAP"},{"location":"kubernetes/#volumen-env-configmap","text":"Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user","title":"VOLUMEN-ENV CONFIGMAP"},{"location":"kubernetes/#secrets","text":"Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen.","title":"SECRETS"},{"location":"kubernetes/#crear","text":"Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode","title":"CREAR"},{"location":"kubernetes/#manifiestos","text":"Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test","title":"MANIFIESTOS"},{"location":"kubernetes/#envsubts","text":"Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado.","title":"ENVSUBTS"},{"location":"kubernetes/#volume-secrets","text":"Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ #","title":"VOLUME SECRETS"},{"location":"kubernetes/#env-secrets","text":"Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456","title":"ENV SECRETS"},{"location":"kubernetes/#volumes","text":"Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"VOLUMES"},{"location":"kubernetes/#emptydir","text":"Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / #","title":"EMPTYDIR"},{"location":"kubernetes/#hostpath-pv","text":"En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none>","title":"HOSTPATH-PV"},{"location":"kubernetes/#pvc","text":"El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC.","title":"PVC"},{"location":"kubernetes/#pvc-pv","text":"Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector.","title":"PVC-PV"},{"location":"kubernetes/#pvc-pods","text":"De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql>","title":"PVC-PODS"},{"location":"kubernetes/#cloud-volumes","text":"Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC.","title":"CLOUD VOLUMES"},{"location":"kubernetes/#reclaim-policy","text":"Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"RECLAIM POLICY"},{"location":"kubernetes/#usersgroups-rbac","text":"RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster.","title":"USERS/GROUPS RBAC"},{"location":"kubernetes/#roles-vs-clusterroles","text":"En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo.","title":"ROLES vs CLUSTERROLES"},{"location":"kubernetes/#rolebinding-vs-clusterrolebinding","text":"Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1.","title":"ROLEBINDING vs CLUSTERROLEBINDING"},{"location":"kubernetes/#crear-users-groups","text":"Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\"","title":"CREAR USERS &amp; GROUPS"},{"location":"kubernetes/#habilitar-rbac","text":"Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC","title":"HABILITAR RBAC"},{"location":"kubernetes/#simplificamos-contexto","text":"Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel","title":"SIMPLIFICAMOS CONTEXTO"},{"location":"kubernetes/#crear-roles","text":"Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user.","title":"CREAR ROLES"},{"location":"kubernetes/#enlazar-role-user","text":"Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\"","title":"ENLAZAR ROLE &amp; USER"},{"location":"kubernetes/#config-maps","text":"Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again.","title":"CONFIG MAPS"},{"location":"kubernetes/#crear-clusterole","text":"Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m","title":"CREAR CLUSTEROLE"},{"location":"kubernetes/#crear-user-admin","text":"Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m","title":"CREAR USER ADMIN"},{"location":"kubernetes/#roles-a-grupos","text":"Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m","title":"ROLES A GRUPOS"},{"location":"kubernetes/#services-account","text":"Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr","title":"SERVICES ACCOUNT"},{"location":"kubernetes/#secret-sa","text":"Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc","title":"SECRET SA"},{"location":"kubernetes/#crear-sa","text":"Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s","title":"CREAR SA"},{"location":"kubernetes/#relacion-pod-sa","text":"Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/","title":"RELACION POD-SA"},{"location":"kubernetes/#requests","text":"A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones.","title":"REQUESTS"},{"location":"kubernetes/#request-jwt","text":"Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante.","title":"REQUEST JWT"},{"location":"kubernetes/#sa-deployment","text":"Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa","title":"SA DEPLOYMENT"},{"location":"kubernetes/#role-sa","text":"Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1,","title":"ROLE SA"},{"location":"kubernetes/#ingress","text":"Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada.","title":"INGRESS"},{"location":"kubernetes/#ingress-controller","text":"Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud.","title":"INGRESS CONTROLLER"},{"location":"kubernetes/#crear-ingress-controller","text":"Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s","title":"CREAR INGRESS CONTROLLER"},{"location":"kubernetes/#ip-ingress-controller","text":"Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602","title":"IP INGRESS CONTROLLER"},{"location":"kubernetes/#app-ingress-controller","text":"Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv","title":"APP INGRESS-CONTROLLER"},{"location":"kubernetes/#exponer-el-puerto-al-exterior","text":"Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080","title":"EXPONER EL PUERTO AL EXTERIOR"},{"location":"kubernetes/#2-apps-en-ic","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos:","title":"2 APPS EN IC"},{"location":"kubernetes/#aws-kubernetes","text":"Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: xxxx Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0","title":"AWS KUBERNETES"},{"location":"kubernetes/#crear-cluster-aws-eksctl","text":"docs install Creamos cluster master sin nodos: eksctl create cluster --name test-cluster --without-nodegroup --region eu-west-2 --zones eu-west-2a,eu-west-2b Vemos lo creado en el apartado EKS y CloudFormation : Eksctl lee de estos archivos para comunicarse: [isx46410800@miguel ~]$ cat .aws/credentials [default] aws_access_key_id = AKIA5RIFOUI3OMSWWHNM aws_secret_access_key = xxxxx [isx46410800@miguel ~]$ cat .aws/config [default] region = eu-west-2 Al crear el cluster nos crea un directorio ~/.kube/config Si eliminamos este directorio, como si no lo tuvieramos y nos queremos conectar a este cluster usamos la orden: aws eks --region eu-west-2 update-kubeconfig --name test-cluster Ahora si hacemos kubectl get svc y kubectl cluster-info vemos que estamos conectados y referenciados al cluster de AWS: [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7m11s [isx46410800@miguel ~]$ kubectl cluster-info Kubernetes master is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com CoreDNS is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ahora intentamos crear un POD pero vemos que no se acaba de crear porque no tenemos ningun nodo unido a nuestro CLUSTER: [isx46410800@miguel ~]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 0/1 Pending 0 10s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 15s default-scheduler no nodes available to schedule pods Ahora creamos nodos con eksctl con ami version kubernetes auto y asg access para ser escalable: eksctl create nodegroup --cluster test-cluster --region eu-west-2 --name test-workers --node-type t3.medium --node-ami auto --nodes 1 --nodes-min 1 --nodes-max 3 --asg-access Comprobamos que el pod de prueba est\u00e1 ahora running y asignado al nodo creado: [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 68s v1.17.11-eks-cfdc40 # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6m38s # [isx46410800@miguel ~]$ kubectl describe pod pod-test Name: pod-test Namespace: default Priority: 0 Node: ip-192-168-38-128.eu-west-2.compute.internal/192.168.38.128","title":"CREAR CLUSTER AWS EKSCTL"},{"location":"kubernetes/#ingress-aws-eks","text":"Para exponerlo, crearemos un balanzador de carga, un ingress y un ingress controller. DOCS para crear el ingress controller nos dice que nuestro servicio(VPC) tiene que seguir una estructura de tag. Vemos los servicios VPC que se crearon automaticamente al crear el cluster y los nodos. Las subnets tambien tienen que seguir una estructura de tags. No obstante todos estos pasos al crearlos con EKSCTL ya vienen por defecto. IAM OIDC eksctl utils associate-iam-oidc-provider --region eu-west-2 --cluster test-cluster --approve Politica para crear recursos de balanceador de carga: [isx46410800@miguel ~]$ aws iam create-policy \\ > --policy-name ALBIngressControllerIAMPolicy \\ > --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json { \"Policy\": { \"PolicyName\": \"ALBIngressControllerIAMPolicy\", \"PolicyId\": \"ANPA5RIFOUI3IFJHOR5SB\", \"Arn\": \"arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy\", \"Path\": \"/\", \"DefaultVersionId\": \"v1\", \"AttachmentCount\": 0, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"CreateDate\": \"2020-10-21T17:06:33Z\", \"UpdateDate\": \"2020-10-21T17:06:33Z\" } } Creamos un service account para ingress con un clusterrole y un clusterrolebinding de ingress controller para balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml Creamos un service account para que nuestro ingress controller sea capaz de crear recursos en AWS: eksctl create iamserviceaccount \\ --region eu-west-2 \\ --name alb-ingress-controller \\ --namespace kube-system \\ --cluster test-cluster \\ --attach-policy-arn arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy \\ --override-existing-serviceaccounts \\ --approve La policy la vemos en IAM-POLICIES Resumen: creamos un service account que tiene un clusterrolebinding para ver los permisos de ingress y de balanzador de carga, por esto, de este ultimo, creamos una politica para que pueda crear recursos en AWS y en balanceador de carga.","title":"INGRESS AWS EKS"},{"location":"kubernetes/#deploy-ingress-controller-aws","text":"Creamos un deployment que crea un pod de ingress controller con una imagen de aws ingress controller que lo que har\u00e1 es que si ve cambios, los modifica en el balanceador de carga: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml Cambiamos unas lineas del deploy: kubectl edit deployment.apps/alb-ingress-controller -n kube-system spec: containers: - args: - --ingress-class=alb - --cluster-name=test-cluster Comprobamos que esto funciona: [isx46410800@miguel ~]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE alb-ingress-controller-868ddb9874-gzsvx 1/1 Running 0 41s aws-node-gcd69 1/1 Running 0 35m coredns-6ddcfb5bcf-h7qrx 1/1 Running 0 48m coredns-6ddcfb5bcf-t7wnz 1/1 Running 0 48m kube-proxy-jdnj5 1/1 Running 0 35m","title":"DEPLOY INGRESS CONTROLLER AWS"},{"location":"kubernetes/#deploy-app","text":"Creamos el ejemplo de aplicaci\u00f3n que es un juego, creamos un servicio, un deploy y namespaces: kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml Comprobamos: [isx46410800@miguel ~]$ kubectl get all -n 2048-game NAME READY STATUS RESTARTS AGE pod/2048-deployment-dd74cc68d-88w46 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-gc9pp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-lw72w 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-wk8tp 1/1 Running 0 29s pod/2048-deployment-dd74cc68d-zlshx 1/1 Running 0 29s # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/service-2048 NodePort 10.100.179.203 <none> 80:30798/TCP 20s # NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/2048-deployment 5/5 5 5 30s # NAME DESIRED CURRENT READY AGE replicaset.apps/2048-deployment-dd74cc68d 5 5 5 30s Para comprobar que funciona la app internamente usamos: [isx46410800@miguel ~]$ kubectl port-forward pod/2048-deployment-dd74cc68d-88w46 -n 2048-game 7000:80 Forwarding from 127.0.0.1:7000 -> 80 Forwarding from [::1]:7000 -> 80","title":"DEPLOY APP"},{"location":"kubernetes/#exponer-la-app-externamente","text":"Enrutamos con el ingress la app: [isx46410800@miguel ~]$ kubectl get ingress -n 2048-game NAME HOSTS ADDRESS PORTS AGE 2048-ingress * d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com 80 14s Si vamos a nuestro EC2 de amazon. a nuestro balanceador de carga veremos que nos sale la url en la que podemos ir a la aplicaci\u00f3n ya que la regla estaba asignada.","title":"EXPONER LA APP EXTERNAMENTE"},{"location":"kubernetes/#modificando-reglas-ingress","text":"Vemos que IPs apuntan al balanceador de carga que nos da la url del juego: [isx46410800@miguel ~]$ nslookup d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Server: 192.168.1.1 Address: 192.168.1.1#53 Non-authoritative answer: Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.134.190.250 Name: d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com Address: 18.133.107.232 Las a\u00f1adimos a nuestro /etc/hosts: 18.134.190.250 app.aws.game.test 18.133.107.232 app.aws.game.test Cambiamos reglas para que utilicen el nombre y no la ip ni dns: kubectl edit ingress 2048-ingress -n 2048-game spec: rules: - host: app.aws.game.test http: paths: - path: /* backend: serviceName: service-2048 servicePort: 80 Ahora entraremos solo por nombre","title":"MODIFICANDO REGLAS INGRESS"},{"location":"kubernetes/#borrar-todo","text":"Borramos todo y vemos que no hay el balanceador de carga: kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml","title":"BORRAR TODO"},{"location":"kubernetes/#aws-hpa-install","text":"HPA(Horizontal Pod Autoescaler) consulta unas metricas y se asocia a un deployment. Basado a unas metricas dice cuanta cantidad de pods creas, segun la carga que se pueda ir soportando. Solo escala por CPU. Se ha de instalar el Metrics Server : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml [isx46410800@miguel ~]$ kubectl get deployment metrics-server -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 6s","title":"AWS HPA INSTALL"},{"location":"kubernetes/#crear-un-hpa","text":"Ejemplo de una app: [isx46410800@miguel ~]$ kubectl apply -f https://k8s.io/examples/application/php-apache.yaml deployment.apps/php-apache created service/php-apache created [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 0/1 1 0 15s [isx46410800@miguel ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 105m php-apache ClusterIP 10.100.137.253 <none> 80/TCP 19s [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-h2xhh 1/1 Running 0 50s Ahora Escalamos. Esto quiere decir que si la carga pasa del 50% creee pods hasta un maximo de 10 pods: kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Comprobamos con kubectl get hpa : [isx46410800@miguel ~]$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 74s [isx46410800@miguel ~]$ kubectl get hpa -o yaml apiVersion: v1 items: - apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler","title":"CREAR UN HPA"},{"location":"kubernetes/#autoescalar-hpa","text":"Nuestra maquina de AWS es un t3.medium y tiene 2 cpus y 4 de ram. Creamos un container y dentro de el le hacemos muchas peticiones, veremos como se va cargando y se van creando pods para balancear esta carga: kubectl run -it --rm load-generator --image=busybox /bin/sh --generator=run-pod/v1 # while true; do wget -q -O- http://php-apache; done Vemos los pods y los hpa: [isx46410800@miguel ~]$ kubectl get pods -w NAME READY STATUS RESTARTS AGE apache-bench 1/1 Running 1 21m httpd 1/1 Running 0 2m10s load-generator 1/1 Running 0 20s php-apache-79544c9bd9-cnq6h 1/1 Running 0 43s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-ckcgb 0/1 Pending 0 0s php-apache-79544c9bd9-m29bz 0/1 Pending 0 0s php-apache-79544c9bd9-xs4tl 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-ckcgb 0/1 ContainerCreating 0 0s php-apache-79544c9bd9-m29bz 0/1 ContainerCreating 0 0s # [isx46410800@miguel ~]$ kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 35m php-apache Deployment/php-apache 54%/50% 1 10 1 35m php-apache Deployment/php-apache 250%/50% 1 10 1 36m php-apache Deployment/php-apache 250%/50% 1 10 4 36m php-apache Deployment/php-apache 250%/50% 1 10 5 36m php-apache Deployment/php-apache 74%/50% 1 10 5 37m php-apache Deployment/php-apache 74%/50% 1 10 8 37m php-apache Deployment/php-apache 68%/50% 1 10 8 38m php-apache Deployment/php-apache 68%/50% 1 10 8 39m php-apache Deployment/php-apache 0%/50% 1 10 8 40m","title":"AUTOESCALAR HPA"},{"location":"kubernetes/#cluster-autoscaler","text":"Se dispara cuando el HPA dispara pods y no hay nodos donde colocarlos. Entonces se autoescala en nodos para ponerlos. Se dispara cuando desde fuera se hace un deploy y se llena el nodo. Si se dispara otro deploy, como no hay espacio, el cluster autoescaler crea otro nodo para poner los pods que falten por poner. La politica que se tiene que agregar al cluster de cluster autoscale se crea de por s\u00ed cuando creamos el cluster con la herramienta eksctl con la opcion --asg-access. DOCS autoscaler Trabaja como otro pod corriendo en mi cluster. Lo desplegamos: [isx46410800@miguel ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created # [isx46410800@miguel ~]$ kubectl get deploy -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE alb-ingress-controller 1/1 1 1 5h18m cluster-autoscaler 1/1 1 1 13s coredns 2/2 2 2 6h2m metrics-server 1/1 1 1 4h25m Editamos el deploy: kubectl -n kube-system edit deploy cluster-autoscaler - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/test-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false Borramos el HPA para que no haya conflictos. Editamos el deploy y ponemos 3 replicas: [isx46410800@miguel ~]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 3h55m [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-6zqcc 1/1 Running 0 5s php-apache-79544c9bd9-cnq6h 1/1 Running 0 3h56m php-apache-79544c9bd9-pfsrq 1/1 Running 0 5s Si editamos el deploy y a\u00f1adimos mas replicas, veremos que se nos crean varias maquinas, varios nodes. kubectl edit deploy php-apache Comprobamos: [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 114s php-apache-79544c9bd9-6zqcc 1/1 Running 0 6m59s php-apache-79544c9bd9-cnq6h 1/1 Running 0 4h3m php-apache-79544c9bd9-dlmrz 1/1 Running 0 114s php-apache-79544c9bd9-dq8f2 1/1 Running 0 3m29s php-apache-79544c9bd9-hbxnr 1/1 Running 0 3m29s php-apache-79544c9bd9-n594l 1/1 Running 0 114s php-apache-79544c9bd9-pfsrq 1/1 Running 0 6m59s php-apache-79544c9bd9-pv5cl 1/1 Running 0 114s php-apache-79544c9bd9-pzz4w 1/1 Running 0 114s php-apache-79544c9bd9-x4czh 1/1 Running 0 4m19s php-apache-79544c9bd9-zm7fj 1/1 Running 0 114s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-22-127.eu-west-2.compute.internal Ready <none> 41s v1.17.11-eks-cfdc40 ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h11m v1.17.11-eks-cfdc40 Ahora comprobamos que cuando no usa un nodo, el autoscale lo elimine automaticamente y va pasando pods a un solo nodo y dejar el minimo de maquinas running: [isx46410800@miguel ~]$ kubectl edit deploy php-apache deployment.apps/php-apache edited # [isx46410800@miguel ~]$ kubectl get pods NAME READY STATUS RESTARTS AGE php-apache-79544c9bd9-5slhp 1/1 Running 0 5m47s php-apache-79544c9bd9-dlmrz 1/1 Running 0 5m47s php-apache-79544c9bd9-n594l 1/1 Running 0 5m47s php-apache-79544c9bd9-pv5cl 1/1 Running 0 5m47s php-apache-79544c9bd9-pzz4w 1/1 Running 0 5m47s # [isx46410800@miguel ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-38-128.eu-west-2.compute.internal Ready <none> 6h14m v1.17.11-eks-cfdc40","title":"CLUSTER AUTOSCALER"},{"location":"kubernetes/#authentication-and-authorization-usuario","text":"This exercise guide assumes the following environment, which by default uses the certificate and key from /var/lib/minikube/certs/, and RBAC mode for authorization: Minikube v1.13.1 Kubernetes v1.19.2 Docker 19.03.12-ce This exercise guide was prepared for the video demonstration following on the next page. Start Minikube: $ minikube start View the content of the kubectl client's configuration manifest, observing the only context minikube and the only user minikube, created by default: $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/student/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/student/.minikube/profiles/minikube/client.crt client-key: /home/student/.minikube/profiles/minikube/client.key Create lfs158 namespace: $ kubectl create namespace lfs158 namespace/lfs158 created Create the rbac directory and cd into it: $ mkdir rbac $ cd rbac/ Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool: ~/rbac$ openssl genrsa -out student.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) .................................................+++++ .........................+++++ e is 65537 (0x010001) ~/rbac$ openssl req -new -key student.key -out student.csr -subj \"/CN=student/O=learner\" Create a YAML manifest for a certificate signing request object, and save it with a blank value for the request field: ~/rbac$ vim signing-request.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: <assign encoded value from next cat command> signerName: kubernetes.io/kube-apiserver-client usages: - digital signature - key encipherment - client auth View the certificate, encode it in base64, and assign it to the request field in the signing-request.yaml file: ~/rbac$ cat student.csr | base64 | tr -d '\\n','%' LS0tLS1CRUd...1QtLS0tLQo= ~/rbac$ vim signing-request.yaml apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: LS0tLS1CRUd...1QtLS0tLQo= signerName: kubernetes.io/kube-apiserver-client usages: - digital signature - key encipherment - client auth Create the certificate signing request object, then list the certificate signing request objects. It shows a pending state: ~/rbac$ kubectl create -f signing-request.yaml certificatesigningrequest.certificates.k8s.io/student-csr created ~/rbac$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION student-csr 12s kubernetes.io/kube-apiserver-client minikube-user Pending Approve the certificate signing request object, then list the certificate signing request objects again. It shows both approved and issued states: ~/rbac$ kubectl certificate approve student-csr certificatesigningrequest.certificates.k8s.io/student-csr approved ~/rbac$ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION student-csr 57s kubernetes.io/kube-apiserver-client minikube-user Approved,Issued Extract the approved certificate from the certificate signing request, decode it with base64 and save it as a certificate file. Then view the certificate in the newly created certificate file: ~/rbac$ kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode > student.crt ~/rbac$ cat student.crt -----BEGIN CERTIFICATE----- MIIDGzCCA... ... ...NOZRRZBVunTjK7A== -----END CERTIFICATE----- Configure the kubectl client's configuration manifest with the student user's credentials by assigning the key and certificate: ~/rbac$ kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key User \"student\" set. Create a new context entry in the kubectl client's configuration manifest for the student user, associated with the lfs158 namespace in the minikube cluster: ~/rbac$ kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student Context \"student-context\" created. View the contents of the kubectl client's configuration manifest again, observing the new context entry student-context, and the new user entry student: ~/rbac$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/student/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube - context: cluster: minikube namespace: lfs158 user: student name: student-context current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/student/.minikube/profiles/minikube/client.crt client-key: /home/student/.minikube/profiles/minikube/client.key - name: student user: client-certificate: /home/student/rbac/student.crt client-key: /home/student/rbac/student.key While in the default minikube context, create a new deployment in the lfs158 namespace: ~/rbac$ kubectl -n lfs158 create deployment nginx --image=nginx:alpine deployment.apps/nginx created From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context: ~/rbac$ kubectl --context=student-context get pods Error from server (Forbidden): pods is forbidden: User \"student\" cannot list resource \"pods\" in API group \"\" in the namespace \"lfs158\" The following steps will assign a limited set of permissions to the student user in the student-context. Create a YAML configuration manifest for a pod-reader Role object, which allows only get, watch, list actions in the lfs158 namespace against pod objects. Then create the role object and list it from the default minikube context, but from the lfs158 namespace: ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ~/rbac$ kubectl create -f role.yaml role.rbac.authorization.k8s.io/pod-reader created ~/rbac$ kubectl -n lfs158 get roles NAME CREATED AT pod-reader 2020-10-07T03:47:45Z Create a YAML configuration manifest for a rolebinding object, which assigns the permissions of the pod-reader Role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace: ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME ROLE AGE pod-read-access Role/pod-reader 28s Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context. ~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-565785f75c-kl25r 1/1 Running 0 7m41s","title":"AUTHENTICATION AND AUTHORIZATION USUARIO"},{"location":"kubernetes/#eliminamos-todo-de-la-nube","text":"Vamos a AWS - CLOUD FORMATION y eliminamos todo.","title":"ELIMINAMOS TODO DE LA NUBE"},{"location":"kubernetes/#pruebas-de-salud","text":"LIVENES: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 3 failureThreshold: 1 periodSeconds: 5 Lo creamos con kubectl create -f liveness.yaml y lo vemos con kubectl get pod liveness_pod -w y vemos que cada rato se crea uno comprobando que est\u00e1 todo bien. LIVENESS HTTP REQUEST: livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 In the following example, the kubelet sends the HTTP GET request to the /healthz endpoint of the application, on port 8080. If that returns a failure, then the kubelet will restart the affected container; otherwise, it would consider the application to be alive. TCP LIVENESS PROBE: livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 With TCP Liveness Probe, the kubelet attempts to open the TCP Socket to the container which is running the application. If it succeeds, the application is considered healthy, otherwise the kubelet would mark it as unhealthy and restart the affected container.","title":"PRUEBAS DE SALUD"},{"location":"kubernetes/#anotaciones","text":"Se pueden poner comentarios como anotaciones en los ficheros yaml: apiVersion: apps/v1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019","title":"ANOTACIONES"},{"location":"kubernetes/#kubeadm","text":"GU\u00cdA para crear un cluster con master y nodos con kubeadm. INSTALAR KUBEADM . CREAR CLUSTER KUBEADM . LAB CLUSTERS KUBEADM","title":"KUBEADM"},{"location":"kubernetes/#helm","text":"DOCS CHARTS instalables de helm para kubernetes. GUIA INSTALL HELM . Helm es un administrador de paquetes para Kubernetes que permite a los desarrolladores y operadores configurar e implementar de forma m\u00e1s sencilla aplicaciones en los cl\u00fasteres de Kubernetes. La mayor\u00eda de los sistemas operativos y de programaci\u00f3n de lenguaje tienen su propio administrador de paquetes para la instalaci\u00f3n y el mantenimiento de software. Helm proporciona el mismo conjunto de funciones b\u00e1sicas que muchos de los administradores que seguramente ya conoce, como apt de Debian o pip de Python. Helm puede: Instalar software Instalar de manera autom\u00e1tica dependencias de software Actualizar software Configurar implementaciones de software Obtener paquetes de software de repositorios Helm proporciona esta funcionalidad a trav\u00e9s de los siguientes componentes: Una herramienta de l\u00ednea de comandos, helm, que proporciona la interfaz de usuario para todas las funcionalidades de Helm. Un componente de servidor complementario, tiller, que funciona en su cl\u00faster de Kubernetes, escucha los comandos de helm y gestiona la configuraci\u00f3n e implementaci\u00f3n de versiones de software en el cl\u00faster. El formato de empaquetado de Helm, llamado charts. Un repositorio de charts oficiales seleccionados con charts empaquetados previamente para proyectos de software de c\u00f3digo abierto populares.","title":"HELM"},{"location":"kubernetes/#instalacion","text":"INSTALACION Y PASO A PASO EXPLICACION INICIAL Iniciamos helm: kubectl apply -f helm-rbac.yml helm init --service-account helm-tiller kubectl get pods -n kube-system apiVersion: v1 kind: ServiceAccount metadata: name: helm-tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm-tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm-tiller namespace: kube-system NOTA: YA NO SE USA TILLER Podemos buscar charts instalables como por ejemplo: helm search hub/repo nginx Instalamos cualquier chart que salga con: helm install nginx-redi (--name nuevo_name) helm install my-releasr nginx-stable/nginx-ingress helm ls Instalar Tiller : Tiller es un complemento del comando helm que se ejecuta en su cl\u00faster, recibe comandos de helm y se comunica directamente con la API de Kubernetes para hacer el verdadero trabajo de crear y eliminar recursos. A fin de proporcionar a Tiller los permisos que necesita para ejecutarse en el cl\u00faster, crearemos un recurso serviceaccount de Kubernetes. kubectl -n kube-system create serviceaccount tiller kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller helm init --service-account tiller Vincularemos serviceaccount al rol de cl\u00faster de cluster-admin. Esto permitir\u00e1 que el superusuario del servicio de tiller acceda al cl\u00faster e instale todos los tipos de recursos en todos los espacios de nombres. Para verificar que Tiller est\u00e9 en ejecuci\u00f3n, enumere los pods en el espacio de nombres de kube-system : kubectl get pods --namespace kube-system Instalar un chart de Helm: Los paquetes de software de Helm se llaman charts. Hay un repositorio administrado de charts llamado stable, el cual consiste principalmente en tablas comunes que puede ver en su repositorio de GitHub. Helm no lo tiene preconfigurado en Helm. Por lo tanto, tendr\u00e1 que a\u00f1adirlo manualmente. Luego, a modo de ejemplo, instalar\u00e1 Kubernetes Dashboard. helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install dashboard-demo stable/kubernetes-dashboard --set rbac.clusterAdminRole=true Puede enumerar todas las versiones del cl\u00faster: helm list Ahora, podemos usar kubectl para verificar que se haya implementado un nuevo servicio en el cl\u00faster: kubectl get services El comando helm upgrade puede utilizarse para actualizar una versi\u00f3n con un chart nuevo o actualizado, o para actualizar sus opciones de configuraci\u00f3n (variables). Realizar\u00e1 un cambio sencillo en la versi\u00f3n de dashboard-demo para demostrar el proceso de actualizaci\u00f3n y reversi\u00f3n: actualizar\u00e1 el nombre del servicio de dashboard simplemente a kubernetes-dashboard en lugar de dashboard-demo-kubernetes-dashboard. El chart kubernetes-dashboard proporciona una opci\u00f3n de configuraci\u00f3n de fullnameOverride para controlar el nombre de servicio. Para cambiar el nombre de la versi\u00f3n, ejecute helm upgrade con esta opci\u00f3n establecida: helm upgrade dashboard-demo stable/kubernetes-dashboard --set fullnameOverride=\"kubernetes-dashboard\" --reuse-values Revertir y eliminar una versi\u00f3n: Cuando actualiz\u00f3 la versi\u00f3n de dashboard-demo en el paso anterior, cre\u00f3 una segunda revisi\u00f3n de la versi\u00f3n. Helm conserva todos los detalles de las versiones anteriores en caso de que deba realizar una reversi\u00f3n a una configuraci\u00f3n o un chart anterior. helm rollback dashboard-demo 1 Es posible eliminar versiones de helm con el comando helm delete: helm delete dashboard-demo","title":"INSTALACION"},{"location":"ldap/","text":"LDAP Administra un servicio de directorios. Autenticaci\u00f3n: demuestra quien soy (auth) Autorizar: que derechos tiene (autz) Information Provider: la info de la cuenta de usuario Authenticaction Provider: quien da el password Es una base de datos no relacional: Jerarquica Distribuida Optimizada por las lecturas Forma de arbol Trabaja con identidades Ejemplo EDT: Escola utiliza LDAP/Kerberos y unix solo utiliza /etc/passwd. Una caja es una entidad y las caracteristicas de la caja son atributos. Formato ldif para los datos de LDAP Paquetes de instalaci\u00f3n: openldap-clients openldap-servers slapd ordenes de bajo nivel para el servidor /var/lib/ldap directorio donde se guarda las bbdd ldap.ldap /etc/openldap/slapd.d directorio de condiguraci\u00f3n formato ldap rootdn \"cn=Manager,dc=edt,dc=org\" es el root total de una bbdd. Puertos LDAP 389 Encender servicio /sbin/slapd -d CREACION Ejemplo ldap normal Dockerfile: # ldapserver FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"ldapserver\" RUN dnf install -y openldap-servers openldap-clients RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker CMD /opt/docker/startup.sh Install.sh: #! /bin/bash # Install ldap server rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* cp /opt/docker/DB_CONFIG /var/lib/ldap/. slaptest -f /opt/docker/slapd.conf -F /etc/openldap/slapd.d/ slapadd -F /etc/openldap/slapd.d/ -l /opt/docker/edt.org.ldif chown -R ldap.ldap /etc/openldap/slapd.d chown -R ldap.ldap /var/lib/ldap cp /opt/docker/ldap.conf /etc/openldap/. Startup.sh: #! /bin/bash bash /opt/docker/install.sh ulimit -n 1024 /sbin/slapd a=1 while [ $a -eq 1 ] do a=1 done FICHEROS LDAP Fichero slapd.conf. A partir de este file generamos el directorio de configuraci\u00f3n: # # See slapd.conf(5) for details on configuration options. # This file should NOT be world readable. # include /etc/openldap/schema/corba.schema include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/duaconf.schema include /etc/openldap/schema/dyngroup.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/java.schema include /etc/openldap/schema/misc.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/openldap.schema include /etc/openldap/schema/ppolicy.schema include /etc/openldap/schema/collective.schema # Allow LDAPv2 client connections. This is NOT the default. allow bind_v2 pidfile /var/run/openldap/slapd.pid #argsfile /var/run/openldap/slapd.args # ---------------------------------------------------------------------- database mdb suffix \"dc=edt,dc=org\" rootdn \"cn=Manager,dc=edt,dc=org\" rootpw secret directory /var/lib/ldap index objectClass eq,pres access to * by self write by * read # ---------------------------------------------------------------------- # ---------------------------------------------------------------------- database config rootdn \"cn=Sysadmin,cn=config\" rootpw {SSHA}5DfZc1WXeIwrP7C3fr23WLZiPZ5YHMgA # el passwd es syskey # ---------------------------------------------------------------------- # enable monitoring database monitor ldap.conf configuraci\u00f3n para conectar a ldap: # # LDAP Defaults # # See ldap.conf(5) for details # This file should be world readable but not world writable. #BASE dc=example,dc=com #URI ldap://ldap.example.com ldap://ldap-master.example.com:666 #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never TLS_CACERTDIR /etc/openldap/certs # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on URI ldap://ldapserver BASE dc=edt,dc=org Fichero edt.org.ldif fichero donde esta el arbol de root, subgrupos,usuarios, grupos: dn: dc=edt,dc=org dc: edt description: Escola del treball de Barcelona objectClass: dcObject objectClass: organization o: edt.org # dn: ou=maquines,dc=edt,dc=org ou: maquines description: Container per a maquines linux objectclass: organizationalunit # dn: ou=clients,dc=edt,dc=org ou: clients description: Container per a clients linux objectclass: organizationalunit # dn: ou=usuaris,dc=edt,dc=org ou: usuaris description: Container per usuaris del sistema linux objectclass: organizationalunit # dn: ou=grups,dc=edt,dc=org ou: grups description: Container per grups del sistema linux objectclass: organizationalunit # dn: cn=1asix,ou=grups,dc=edt,dc=org cn: 1asix gidNumber: 610 description: Grup de 1asix memberUid: user01 memberUid: user02 memberUid: user03 memberUid: user04 memberUid: user15 objectclass: posixGroup # dn: cn=2asix,ou=grups,dc=edt,dc=org cn: 2asix gidNumber: 611 description: Grup de 2asix memberUid: user06 memberUid: user07 memberUid: user08 memberUid: user09 memberUid: user10 objectclass: posixGroup # dn: cn=profesasix,ou=grups,dc=edt,dc=org cn: profesasix gidNumber: 612 description: Profes de asix memberUid: pere memberUid: anna memberUid: jordi memberUid: marta memberUid: pau objectclass: posixGroup # dn: cn=Pau Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pau Pou cn: Pauet Pou sn: Pou homephone: 555-222-2220 mail: pau@edt.org description: Watch out for this guy ou: Profes uid: pau uidNumber: 5000 gidNumber: 612 homeDirectory: /tmp/home/pau userPassword: {SSHA}NDkipesNQqTFDgGJfyraLz/csZAIlk2/ # dn: cn=Pere Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pere Pou sn: Pou homephone: 555-222-2221 mail: pere@edt.org description: Watch out for this guy ou: Profes uid: pere uidNumber: 5001 gidNumber: 612 homeDirectory: /tmp/home/pere userPassword: {SSHA}ghmtRL11YtXoUhIP7z6f7nb8RCNadFe+ DB_CONFIG configuracion de bbdd: # $OpenLDAP$ # Example DB_CONFIG file for use with slapd(8) BDB/HDB databases. # # See the Oracle Berkeley DB documentation # <http://www.oracle.com/technology/documentation/berkeley-db/db/ref/env/db_config.html> # for detail description of DB_CONFIG syntax and semantics. # # Hints can also be found in the OpenLDAP Software FAQ # <http://www.openldap.org/faq/index.cgi?file=2> # in particular: # <http://www.openldap.org/faq/index.cgi?file=1075> # Note: most DB_CONFIG settings will take effect only upon rebuilding # the DB environment. # one 0.25 GB cache set_cachesize 0 268435456 1 # Data Directory #set_data_dir db # Transaction Log settings set_lg_regionmax 262144 set_lg_bsize 2097152 #set_lg_dir logs # Note: special DB_CONFIG flags are no longer needed for \"quick\" # slapadd(8) or slapindex(8) access (see their -q option). SCHEMA Ejemplo de nombre.schema : attributetype ( 1.1.2.1.1 NAME 'x-equip' DESC 'equip del futbolista' EQUALITY caseIgnoreMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE ) attributetype ( 1.1.2.1.2 NAME 'x-dorsal' DESC 'dorsal del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.27 SINGLE-VALUE ) attributetype ( 1.1.2.1.3 NAME 'x-web' DESC 'pagina web del futbolista' EQUALITY caseExactMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 ) attributetype ( 1.1.2.1.4 NAME 'x-foto' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) attributetype ( 1.1.2.1.5 NAME 'x-lesionat' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.7 SINGLE-VALUE ) objectclass ( 1.1.2.2.1 NAME 'x-futbolistes' DESC 'futboleros' SUP inetOrgPerson STRUCTURAL MUST x-equip MAY ( x-dorsal $ x-web $ x-foto $ x-lesionat ) ) Creacion de un usuario con este schema: dn: cn=kaka,ou=Productes,dc=edt,dc=org objectclass: x-futbolistes cn: kaka sn: kaka x-equip: los pimientos x-dorsal: 7 x-web: www.kaka.com x-foto: //var/tmp/foto.jpg x-lesionat: FALSE A\u00f1adimos en el fichero slapd.conf el schema: include /opt/docker/futbolista-C.schema ACL En el slapd.conf est\u00e1 el user y pass del ACL. Ejemplos de ACL, son directrices y permisos que tienen ciertos grupos, usuarios etc: Implementar a la base de dades edt.org les seg\u00fcents ACLS: 1. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant de l\u2019administrador i t\u00e9 permisos per modificar-ho tot. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- **[implicitas aunque no salgan] [isx46410800@miguel-fedora27 ldapserver19:acl]$ ldapmodify -vx -c -h 172.17.0.2 -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi -f mod01.ldif ldap_initialize( ldap://172.17.0.2 ) replace mail: newmarta10@edt.org modifying entry \"cn=Marta Mas,ou=usuaris,dc=edt,dc=org\" ldap_modify: Insufficient access (50) replace mail: newmjordi10@edt.org modifying entry \"cn=Jordi Mas,ou=usuaris,dc=edt,dc=org\" modify complete # # 2. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant d\u2019administraci\u00f3. Tothom es pot modificar el seu propi email i homePhone. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- # # 3. Tot usuari es pot modificar el seu mail. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self write by * read [by * none] olcAccess: to * by * read [by * none] [acces to * by * none] --- # # 4. Tothom pot veure totes les dades de tothom, excepte els mail dels altres. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self read [by * none] olcAccess: to * by * read [acces to * by * none] --- [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna dn mail # # 5. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * read olcAccess: to * by * read --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword -- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --puedo ver todos los userpassword como ldapsearch -x -LLL ** by auth no es necesario porqie todo el mundo podemos ver el password y no requiere de identificacion # 6. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom, excepte els altres passwords. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth olcAccess: to * by * read --- **para que nadie lo vea, puedas cambiarlo, tienes que tener primero permiso de auth para autenticarte primero, sino serias un anonimo de fuera ----->puedo cambiar pass mio pero no otro [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s ann2 --->no puedo ver los userpassword con ldapsearch -x -LLL ni con [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword --> solo vere el de jordi por ser authorizado **sino ponemos el by auth, no podremos autenticarnos ya que al hacer la orden somos un user anonymous y hemos de hacer BIND con el -D para identificarnos y solo se consigue poniendo by auth -->** [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword ldap_bind: Invalid credentials (49)--> no deja sin el by auth # # 7. Tot usuari es pot modificar el seu propi password i tot usuari nom\u00e9s pot veure les seves pr\u00f2pies dades. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth [by * none] olcAccess: to * by self read by * search [by * none] ** permet la capacitat de llegir i navegar per tot el arbre ** si ponemos solo by self read, no podremos ver por toodo el contenido para poder ver sus datos y no es capaz de autenticarte. --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --> podemos cambiar nuestro pass pero no el de otro -->con el by * search tendremos acceso a los campos propios [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn: cn=Jordi Mas,ou=usuaris,dc=edt,dc=org objectClass: posixAccount **nos permite de todos los usuarios, ver solo el nuestro. en este caso jordi solo ve el suyo **como anonimo no sale nada porque no tiene datos dentro de la bbdd # 8. Tot usuari pot observar les seves pr\u00f2pies dades i modificar el seu propi password,email i homephone. L\u2019usuari \u201cAnna Pou\u201d pot modificar tots els atributs de tots excepte els passwords, que tampoc pot veure. L\u2019usuari \u201cPere Pou\u201d pot modificar els passwords de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by dn.exact=\"cn=Pere Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * auth [by * none] olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to * by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self read by * search [by * none] --- * by * search para poder ver todas las dades del arbol y hacer match cuando encuentre el suyo que entonces podra ver, el resto no. ORDENES LDAP rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* slaptest -F /etc/openldap/slapd.d/ -f /opt/docker/slapd.conf slapadd -f /etc/openldap/slapd.d -l /opt/docker/usuarios.ldif /sbin/slapd -d0 slapcat -n0 | grep dn ldapsearch -x -LLL -h ipLdap -b 'dc=edt,dc=org' ldapdelete -vx -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' ldapadd -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Sysadmin,cn=config' -w syskey 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif slappasswd -> genera passwd sha de tipo SHA slappasswd -h {md5/crypt} ldapwhoami -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna ldappassword -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s annanew ldapcompare -x -h ipLDAP 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' homePhone=555-222-222 slapacl -b 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' 'mail'","title":"LDAP"},{"location":"ldap/#ldap","text":"Administra un servicio de directorios. Autenticaci\u00f3n: demuestra quien soy (auth) Autorizar: que derechos tiene (autz) Information Provider: la info de la cuenta de usuario Authenticaction Provider: quien da el password Es una base de datos no relacional: Jerarquica Distribuida Optimizada por las lecturas Forma de arbol Trabaja con identidades Ejemplo EDT: Escola utiliza LDAP/Kerberos y unix solo utiliza /etc/passwd. Una caja es una entidad y las caracteristicas de la caja son atributos. Formato ldif para los datos de LDAP Paquetes de instalaci\u00f3n: openldap-clients openldap-servers slapd ordenes de bajo nivel para el servidor /var/lib/ldap directorio donde se guarda las bbdd ldap.ldap /etc/openldap/slapd.d directorio de condiguraci\u00f3n formato ldap rootdn \"cn=Manager,dc=edt,dc=org\" es el root total de una bbdd. Puertos LDAP 389 Encender servicio /sbin/slapd -d","title":"LDAP"},{"location":"ldap/#creacion","text":"Ejemplo ldap normal Dockerfile: # ldapserver FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"ldapserver\" RUN dnf install -y openldap-servers openldap-clients RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker CMD /opt/docker/startup.sh Install.sh: #! /bin/bash # Install ldap server rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* cp /opt/docker/DB_CONFIG /var/lib/ldap/. slaptest -f /opt/docker/slapd.conf -F /etc/openldap/slapd.d/ slapadd -F /etc/openldap/slapd.d/ -l /opt/docker/edt.org.ldif chown -R ldap.ldap /etc/openldap/slapd.d chown -R ldap.ldap /var/lib/ldap cp /opt/docker/ldap.conf /etc/openldap/. Startup.sh: #! /bin/bash bash /opt/docker/install.sh ulimit -n 1024 /sbin/slapd a=1 while [ $a -eq 1 ] do a=1 done","title":"CREACION"},{"location":"ldap/#ficheros-ldap","text":"Fichero slapd.conf. A partir de este file generamos el directorio de configuraci\u00f3n: # # See slapd.conf(5) for details on configuration options. # This file should NOT be world readable. # include /etc/openldap/schema/corba.schema include /etc/openldap/schema/core.schema include /etc/openldap/schema/cosine.schema include /etc/openldap/schema/duaconf.schema include /etc/openldap/schema/dyngroup.schema include /etc/openldap/schema/inetorgperson.schema include /etc/openldap/schema/java.schema include /etc/openldap/schema/misc.schema include /etc/openldap/schema/nis.schema include /etc/openldap/schema/openldap.schema include /etc/openldap/schema/ppolicy.schema include /etc/openldap/schema/collective.schema # Allow LDAPv2 client connections. This is NOT the default. allow bind_v2 pidfile /var/run/openldap/slapd.pid #argsfile /var/run/openldap/slapd.args # ---------------------------------------------------------------------- database mdb suffix \"dc=edt,dc=org\" rootdn \"cn=Manager,dc=edt,dc=org\" rootpw secret directory /var/lib/ldap index objectClass eq,pres access to * by self write by * read # ---------------------------------------------------------------------- # ---------------------------------------------------------------------- database config rootdn \"cn=Sysadmin,cn=config\" rootpw {SSHA}5DfZc1WXeIwrP7C3fr23WLZiPZ5YHMgA # el passwd es syskey # ---------------------------------------------------------------------- # enable monitoring database monitor ldap.conf configuraci\u00f3n para conectar a ldap: # # LDAP Defaults # # See ldap.conf(5) for details # This file should be world readable but not world writable. #BASE dc=example,dc=com #URI ldap://ldap.example.com ldap://ldap-master.example.com:666 #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never TLS_CACERTDIR /etc/openldap/certs # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on URI ldap://ldapserver BASE dc=edt,dc=org Fichero edt.org.ldif fichero donde esta el arbol de root, subgrupos,usuarios, grupos: dn: dc=edt,dc=org dc: edt description: Escola del treball de Barcelona objectClass: dcObject objectClass: organization o: edt.org # dn: ou=maquines,dc=edt,dc=org ou: maquines description: Container per a maquines linux objectclass: organizationalunit # dn: ou=clients,dc=edt,dc=org ou: clients description: Container per a clients linux objectclass: organizationalunit # dn: ou=usuaris,dc=edt,dc=org ou: usuaris description: Container per usuaris del sistema linux objectclass: organizationalunit # dn: ou=grups,dc=edt,dc=org ou: grups description: Container per grups del sistema linux objectclass: organizationalunit # dn: cn=1asix,ou=grups,dc=edt,dc=org cn: 1asix gidNumber: 610 description: Grup de 1asix memberUid: user01 memberUid: user02 memberUid: user03 memberUid: user04 memberUid: user15 objectclass: posixGroup # dn: cn=2asix,ou=grups,dc=edt,dc=org cn: 2asix gidNumber: 611 description: Grup de 2asix memberUid: user06 memberUid: user07 memberUid: user08 memberUid: user09 memberUid: user10 objectclass: posixGroup # dn: cn=profesasix,ou=grups,dc=edt,dc=org cn: profesasix gidNumber: 612 description: Profes de asix memberUid: pere memberUid: anna memberUid: jordi memberUid: marta memberUid: pau objectclass: posixGroup # dn: cn=Pau Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pau Pou cn: Pauet Pou sn: Pou homephone: 555-222-2220 mail: pau@edt.org description: Watch out for this guy ou: Profes uid: pau uidNumber: 5000 gidNumber: 612 homeDirectory: /tmp/home/pau userPassword: {SSHA}NDkipesNQqTFDgGJfyraLz/csZAIlk2/ # dn: cn=Pere Pou,ou=usuaris,dc=edt,dc=org objectclass: posixAccount objectclass: inetOrgPerson cn: Pere Pou sn: Pou homephone: 555-222-2221 mail: pere@edt.org description: Watch out for this guy ou: Profes uid: pere uidNumber: 5001 gidNumber: 612 homeDirectory: /tmp/home/pere userPassword: {SSHA}ghmtRL11YtXoUhIP7z6f7nb8RCNadFe+ DB_CONFIG configuracion de bbdd: # $OpenLDAP$ # Example DB_CONFIG file for use with slapd(8) BDB/HDB databases. # # See the Oracle Berkeley DB documentation # <http://www.oracle.com/technology/documentation/berkeley-db/db/ref/env/db_config.html> # for detail description of DB_CONFIG syntax and semantics. # # Hints can also be found in the OpenLDAP Software FAQ # <http://www.openldap.org/faq/index.cgi?file=2> # in particular: # <http://www.openldap.org/faq/index.cgi?file=1075> # Note: most DB_CONFIG settings will take effect only upon rebuilding # the DB environment. # one 0.25 GB cache set_cachesize 0 268435456 1 # Data Directory #set_data_dir db # Transaction Log settings set_lg_regionmax 262144 set_lg_bsize 2097152 #set_lg_dir logs # Note: special DB_CONFIG flags are no longer needed for \"quick\" # slapadd(8) or slapindex(8) access (see their -q option).","title":"FICHEROS LDAP"},{"location":"ldap/#schema","text":"Ejemplo de nombre.schema : attributetype ( 1.1.2.1.1 NAME 'x-equip' DESC 'equip del futbolista' EQUALITY caseIgnoreMatch SUBSTR caseIgnoreSubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 SINGLE-VALUE ) attributetype ( 1.1.2.1.2 NAME 'x-dorsal' DESC 'dorsal del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.27 SINGLE-VALUE ) attributetype ( 1.1.2.1.3 NAME 'x-web' DESC 'pagina web del futbolista' EQUALITY caseExactMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.15 ) attributetype ( 1.1.2.1.4 NAME 'x-foto' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.40 ) attributetype ( 1.1.2.1.5 NAME 'x-lesionat' DESC 'foto del futbolista' SYNTAX 1.3.6.1.4.1.1466.115.121.1.7 SINGLE-VALUE ) objectclass ( 1.1.2.2.1 NAME 'x-futbolistes' DESC 'futboleros' SUP inetOrgPerson STRUCTURAL MUST x-equip MAY ( x-dorsal $ x-web $ x-foto $ x-lesionat ) ) Creacion de un usuario con este schema: dn: cn=kaka,ou=Productes,dc=edt,dc=org objectclass: x-futbolistes cn: kaka sn: kaka x-equip: los pimientos x-dorsal: 7 x-web: www.kaka.com x-foto: //var/tmp/foto.jpg x-lesionat: FALSE A\u00f1adimos en el fichero slapd.conf el schema: include /opt/docker/futbolista-C.schema","title":"SCHEMA"},{"location":"ldap/#acl","text":"En el slapd.conf est\u00e1 el user y pass del ACL. Ejemplos de ACL, son directrices y permisos que tienen ciertos grupos, usuarios etc: Implementar a la base de dades edt.org les seg\u00fcents ACLS: 1. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant de l\u2019administrador i t\u00e9 permisos per modificar-ho tot. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- **[implicitas aunque no salgan] [isx46410800@miguel-fedora27 ldapserver19:acl]$ ldapmodify -vx -c -h 172.17.0.2 -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi -f mod01.ldif ldap_initialize( ldap://172.17.0.2 ) replace mail: newmarta10@edt.org modifying entry \"cn=Marta Mas,ou=usuaris,dc=edt,dc=org\" ldap_modify: Insufficient access (50) replace mail: newmjordi10@edt.org modifying entry \"cn=Jordi Mas,ou=usuaris,dc=edt,dc=org\" modify complete # # 2. L\u2019usuari \u201cAnna Pou\u201d \u00e9s ajudant d\u2019administraci\u00f3. Tothom es pot modificar el seu propi email i homePhone. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * read [by * none] olcAccess: to * by dn.exact=\u201dcn=Anna Pou,ou=usuaris,dc=edt,dc=org\u201d write by * read [by * none] [acces to * by * none] --- # # 3. Tot usuari es pot modificar el seu mail. Tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self write by * read [by * none] olcAccess: to * by * read [by * none] [acces to * by * none] --- # # 4. Tothom pot veure totes les dades de tothom, excepte els mail dels altres. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=mail by self read [by * none] olcAccess: to * by * read [acces to * by * none] --- [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna dn mail # # 5. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * read olcAccess: to * by * read --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword -- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --puedo ver todos los userpassword como ldapsearch -x -LLL ** by auth no es necesario porqie todo el mundo podemos ver el password y no requiere de identificacion # 6. Tot usuari es pot modificar el seu propi password i tothom pot veure totes les dades de tothom, excepte els altres passwords. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth olcAccess: to * by * read --- **para que nadie lo vea, puedas cambiarlo, tienes que tener primero permiso de auth para autenticarte primero, sino serias un anonimo de fuera ----->puedo cambiar pass mio pero no otro [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s ann2 --->no puedo ver los userpassword con ldapsearch -x -LLL ni con [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword --> solo vere el de jordi por ser authorizado **sino ponemos el by auth, no podremos autenticarnos ya que al hacer la orden somos un user anonymous y hemos de hacer BIND con el -D para identificarnos y solo se consigue poniendo by auth -->** [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn userPassword ldap_bind: Invalid credentials (49)--> no deja sin el by auth # # 7. Tot usuari es pot modificar el seu propi password i tot usuari nom\u00e9s pot veure les seves pr\u00f2pies dades. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by self write by * auth [by * none] olcAccess: to * by self read by * search [by * none] ** permet la capacitat de llegir i navegar per tot el arbre ** si ponemos solo by self read, no podremos ver por toodo el contenido para poder ver sus datos y no es capaz de autenticarte. --- [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s anna2 [root@ldapserver docker]# ldappasswd -x -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -s jordi2 Result: Insufficient access (50) --> podemos cambiar nuestro pass pero no el de otro -->con el by * search tendremos acceso a los campos propios [root@ldapserver docker]# ldapsearch -x -LLL -D 'cn=Jordi Mas,ou=usuaris,dc=edt,dc=org' -w jordi dn: cn=Jordi Mas,ou=usuaris,dc=edt,dc=org objectClass: posixAccount **nos permite de todos los usuarios, ver solo el nuestro. en este caso jordi solo ve el suyo **como anonimo no sale nada porque no tiene datos dentro de la bbdd # 8. Tot usuari pot observar les seves pr\u00f2pies dades i modificar el seu propi password,email i homephone. L\u2019usuari \u201cAnna Pou\u201d pot modificar tots els atributs de tots excepte els passwords, que tampoc pot veure. L\u2019usuari \u201cPere Pou\u201d pot modificar els passwords de tothom. --- dn: olcDatabase={1}mdb,cn=config changetype: modify replace: olcAccess olcAccess: to attrs=userPassword by dn.exact=\"cn=Pere Pou,ou=usuaris,dc=edt,dc=org\" write by self write by * auth [by * none] olcAccess: to attrs=mail by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to attrs=homePhone by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self write [by * none] olcAccess: to * by dn.exact=\"cn=Anna Pou,ou=usuaris,dc=edt,dc=org\" write by self read by * search [by * none] --- * by * search para poder ver todas las dades del arbol y hacer match cuando encuentre el suyo que entonces podra ver, el resto no.","title":"ACL"},{"location":"ldap/#ordenes-ldap","text":"rm -rf /etc/openldap/slapd.d/* rm -rf /var/lib/ldap/* slaptest -F /etc/openldap/slapd.d/ -f /opt/docker/slapd.conf slapadd -f /etc/openldap/slapd.d -l /opt/docker/usuarios.ldif /sbin/slapd -d0 slapcat -n0 | grep dn ldapsearch -x -LLL -h ipLdap -b 'dc=edt,dc=org' ldapdelete -vx -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' ldapadd -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Manager,dc=edt,dc=org' -w secret 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif ldapmodify -vx -c -h IpLDAP -D 'cn=Sysadmin,cn=config' -w syskey 'cn=Anna Pou,dc=edt,dc=org' -f modificaciones.ldif slappasswd -> genera passwd sha de tipo SHA slappasswd -h {md5/crypt} ldapwhoami -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna ldappassword -x -h IpLDAP -D 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' -w anna -s annanew ldapcompare -x -h ipLDAP 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' homePhone=555-222-222 slapacl -b 'cn=Anna Pou,ou=usuaris,dc=edt,dc=org' 'mail'","title":"ORDENES LDAP"},{"location":"linux/","text":"Comandos LINUX APUNTES LPIC Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd COMANDOS SYSADMIN Linux Commands frequently used by Linux Sysadmins \u2013 Part 1: 1. ip \u2013 from Iproute2, a collection of utilities for controlling TCP/IP networking and traffic control in Linux. 2. ls \u2013 list directory contents. 3. df \u2013 display disk space usage. 4. du \u2013 estimate file space usage. 5. free \u2013 display memory usage. 6. scp \u2013 securely Copy Files Using SCP, with examples. 7. find \u2013 locates files based on some user-specified criteria. 8. ncdu \u2013 a disk utility for Unix systems. 9. pstree \u2013 display a tree of processes. 10. last \u2013 show a listing of last logged in users. 11. w \u2013 show a list of currently logged in user sessions. 12. grep \u2013 Search a file for a pattern of characters, then display all matching lines. Linux Commands frequently used by Linux Sysadmins \u2013 Part 2: 13. uptime \u2013 shows system uptime and load average. 14. top \u2013 shows an overall system view. 15. vmstat \u2013 shows system memory, processes, interrupts, paging, block I/O, and CPU info. 16. htop \u2013 interactive process viewer and manager. 17. dstat \u2013 view processes, memory, paging, I/O, CPU, etc., in real-time. All-in-one for vmstat, iostat, netstat, and ifstat. 18. iftop \u2013 network traffic viewer. 19. nethogs \u2013 network traffic analyzer. 20. iotop \u2013 interactive I/O viewer. Get an overview of storage r/w activity. 21. iostat \u2013 for storage I/O statistics. 22. netstat \u2013 for network statistics. 23. ss \u2013 utility to investigate sockets. 24. atop \u2013 For Linux server performance analysis. 25. Glances and nmon \u2013 htop and top Alternatives: 26. ssh \u2013 secure command-line access to remote Linux systems. 27. sudo \u2013 execute commands with administrative privilege. 28. cd \u2013 directory navigation. 29. pwd \u2013 shows your current directory location. 30. cp \u2013 copying files and folders. 31. mv \u2013 moving files and folders. 32. rm \u2013 removing files and folders. 33. mkdir \u2013 create or make new directories. 34. touch \u2013 used to update the access date and/or modification date of a computer file or directory. 35. man \u2013 for reading system reference manuals. 36. apropos \u2013 Search man page names and descriptions. Linux Commands frequently used by Linux Sysadmins \u2013 Part 3: 37. rsync \u2013 remote file transfers and syncing. 38. tar \u2013 an archiving utility. 39. gzip \u2013 file compression and decompression. 40. b2zip \u2013 similar to gzip. It uses a different compression algorithm. 41. zip \u2013 for packaging and compressing (to archive) files. 42. locate \u2013 search files in Linux. 43. ps \u2013 information about the currently running processes. 44. Making use of Bash scripts. Example: ./bashscript.sh 45. cron \u2013 set up scheduled tasks to run. 46. nmcli \u2013 network management. 47. ping \u2013 send ICMP ECHO_REQUEST to network hosts. 48. traceroute \u2013 check the route packets take to a specified host. 49. mtr \u2013 network diagnostic tool. 50. nslookup \u2013 query Internet name servers (NS) interactively. 51. host \u2013 perform DNS lookups in Linux. 52. dig \u2013 DNS lookup utility. Linux Commands frequently used by Linux Sysadmins \u2013 Part 4: 53. wget \u2013 retrieve files over HTTP, HTTPS, FTP, and FTPS. 54. curl \u2013 transferring data using various network protocols. (supports more protocols than wget) 55. dd \u2013 convert and copy files. 56. fdisk \u2013 manipulate the disk partition table. 57. parted \u2013 for creating and manipulating partition tables. 58. blkid \u2013 command-line utility to locate/print block device attributes. 59. mkfs \u2013 build a Linux file system. 60. fsck \u2013 tool for checking the consistency of a file system. 61. whois \u2013 client for the whois directory service. 62. nc \u2013 command-line networking utility. (Also, see 60 Linux Networking commands and scripts.) 63. umask \u2013 set file mode creation mask. 64. chmod \u2013 change the access permissions of file system objects. 65. chown \u2013 change file owner and group. 66. chroot \u2013 run command or interactive shell with a special root directory. 67. useradd \u2013 create a new user or update default new user information. 68. userdel \u2013 used to delete a user account and all related files. 69. usermod \u2013 used to modify or change any attributes of an existing user account. Linux Commands frequently used by Linux Sysadmins \u2013 Part 5: 70. vi \u2013 text editor. 71. cat \u2013 display file contents. 72. tac \u2013 output file contents, in reverse. 73. more \u2013 display file contents one screen/page at a time. 74. less \u2013 similar to the more command with additional features. 75. tail \u2013 used to display the tail end of a text file or piped data. 76. dmesg \u2013 prints the message buffer of the kernel ring. 77. journalctl \u2013 query the systemd journal. 78. kill \u2013 terminate a process. 79. killall \u2013 Sends a kill signal to all instances of a process by name. 80. sleep \u2013 suspends program execution for a specified time. 81. wait \u2013 Suspend script execution until all jobs running in the background have been terminated. 82. nohup \u2013 Run Commands in the Background. 83. screen \u2013 hold a session open on a remote server. (also a full-screen window manager) 84. tmux \u2013 a terminal multiplexer. 85. passwd \u2013 change a user\u2019s password. 86. chpassword \u2013 87. mount / umount \u2013 provides access to an entire filesystem in one directory. 88. systemctl \u2013 Managing Services (Daemons). 89. clear \u2013 clears the screen of the terminal. 90. env -Run a command in a modified environment. Misc commands: 91. cheat \u2013 allows you to create and view interactive cheatsheets on the command-line.\u201d 92. tldr \u2013 Collaborative cheatsheets for console commands. 93. bashtop \u2013 the \u2018cool\u2019 top alternative. 94. bpytop \u2013 Python port of bashtop. This list of Linux Networking commands and scripts will receive ongoing updates, similar to the other lists on this blog\u2026 aria2 \u2013 downloading just about everything. Torrents included. arpwatch \u2013 Ethernet Activity Monitor. bmon \u2013 bandwidth monitor and rate estimator. bwm-ng \u2013 live network bandwidth monitor. curl \u2013 transferring data with URLs. (or try httpie) darkstat \u2013 captures network traffic, usage statistics. dhclient \u2013 Dynamic Host Configuration Protocol Client dig \u2013 query DNS servers for information. dstat \u2013 replacement for vmstat, iostat, mpstat, netstat and ifstat. ethtool \u2013 utility for controlling network drivers and hardware. gated \u2013 gateway routing daemon. host \u2013 DNS lookup utility. hping \u2013 TCP/IP packet assembler/analyzer. ibmonitor \u2013 shows bandwidth and total data transferred. ifstat \u2013 report network interfaces bandwidth. iftop \u2013 display bandwidth usage. ip (PDF file) \u2013 a command with more features that ifconfig (net-tools). iperf3 \u2013 network bandwidth measurement tool. (above screenshot Stacklinux VPS) iproute2 \u2013 collection of utilities for controlling TCP/IP. iptables \u2013 take control of network traffic. IPTraf \u2013 An IP Network Monitor. iputils \u2013 set of small useful utilities for Linux networking. iw \u2013 a new nl80211 based CLI configuration utility for wireless devices. jwhois (whois) \u2013 client for the whois service. \u201clsof -i\u201d \u2013 reveal information about your network sockets. mtr \u2013 network diagnostic tool. net-tools \u2013 utilities include: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool, iptunnel and ipmaddr. ncat \u2013 improved re-implementation of the venerable netcat. netcat \u2013 networking utility for reading/writing network connections. nethogs \u2013 a small \u2018net top\u2019 tool. Netperf \u2013 Network bandwidth Testing. netplan \u2013 Netplan is a utility for easily configuring networking on a linux system. netsniff-ng \u2013 Swiss army knife for daily Linux network plumbing. netwatch \u2013 monitoring Network Connections. ngrep \u2013 grep applied to the network layer. nload \u2013 display network usage. nmap \u2013 network discovery and security auditing. nmcli \u2013 a command-line tool for controlling NetworkManager and reporting network status. nmtui \u2013 provides a text interface to configure networking by controlling NetworkManager. nslookup \u2013 query Internet name servers interactively. ping \u2013 send icmp echo_request to network hosts. route \u2013 show / manipulate the IP routing table. slurm \u2013 network load monitor. snort \u2013 Network Intrusion Detection and Prevention System. smokeping \u2013 keeps track of your network latency. socat \u2013 establishes two bidirectional byte streams and transfers data between them. speedometer \u2013 Measure and display the rate of data across a network. speedtest-cli \u2013 test internet bandwidth using speedtest.net ss \u2013 utility to investigate sockets. ssh \u2013 secure system administration and file transfers over insecure networks. tcpdump \u2013 command-line packet analyzer. tcptrack \u2013 Displays information about tcp connections on a network interface. telnet \u2013 user interface to the TELNET protocol. tracepath \u2013 very similar function to traceroute. traceroute \u2013 print the route packets trace to network host. vnStat \u2013 network traffic monitor. websocat \u2013 Connection forwarder from/to web sockets to/from usual sockets, in style of socat. wget \u2013 retrieving files using HTTP, HTTPS, FTP and FTPS. Wireless Tools for Linux \u2013 includes iwconfig, iwlist, iwspy, iwpriv and ifrename. Wireshark \u2013 network protocol analyzer.","title":"Linux"},{"location":"linux/#comandos-linux","text":"APUNTES LPIC Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd","title":"Comandos LINUX"},{"location":"linux/#comandos-sysadmin","text":"Linux Commands frequently used by Linux Sysadmins \u2013 Part 1: 1. ip \u2013 from Iproute2, a collection of utilities for controlling TCP/IP networking and traffic control in Linux. 2. ls \u2013 list directory contents. 3. df \u2013 display disk space usage. 4. du \u2013 estimate file space usage. 5. free \u2013 display memory usage. 6. scp \u2013 securely Copy Files Using SCP, with examples. 7. find \u2013 locates files based on some user-specified criteria. 8. ncdu \u2013 a disk utility for Unix systems. 9. pstree \u2013 display a tree of processes. 10. last \u2013 show a listing of last logged in users. 11. w \u2013 show a list of currently logged in user sessions. 12. grep \u2013 Search a file for a pattern of characters, then display all matching lines. Linux Commands frequently used by Linux Sysadmins \u2013 Part 2: 13. uptime \u2013 shows system uptime and load average. 14. top \u2013 shows an overall system view. 15. vmstat \u2013 shows system memory, processes, interrupts, paging, block I/O, and CPU info. 16. htop \u2013 interactive process viewer and manager. 17. dstat \u2013 view processes, memory, paging, I/O, CPU, etc., in real-time. All-in-one for vmstat, iostat, netstat, and ifstat. 18. iftop \u2013 network traffic viewer. 19. nethogs \u2013 network traffic analyzer. 20. iotop \u2013 interactive I/O viewer. Get an overview of storage r/w activity. 21. iostat \u2013 for storage I/O statistics. 22. netstat \u2013 for network statistics. 23. ss \u2013 utility to investigate sockets. 24. atop \u2013 For Linux server performance analysis. 25. Glances and nmon \u2013 htop and top Alternatives: 26. ssh \u2013 secure command-line access to remote Linux systems. 27. sudo \u2013 execute commands with administrative privilege. 28. cd \u2013 directory navigation. 29. pwd \u2013 shows your current directory location. 30. cp \u2013 copying files and folders. 31. mv \u2013 moving files and folders. 32. rm \u2013 removing files and folders. 33. mkdir \u2013 create or make new directories. 34. touch \u2013 used to update the access date and/or modification date of a computer file or directory. 35. man \u2013 for reading system reference manuals. 36. apropos \u2013 Search man page names and descriptions. Linux Commands frequently used by Linux Sysadmins \u2013 Part 3: 37. rsync \u2013 remote file transfers and syncing. 38. tar \u2013 an archiving utility. 39. gzip \u2013 file compression and decompression. 40. b2zip \u2013 similar to gzip. It uses a different compression algorithm. 41. zip \u2013 for packaging and compressing (to archive) files. 42. locate \u2013 search files in Linux. 43. ps \u2013 information about the currently running processes. 44. Making use of Bash scripts. Example: ./bashscript.sh 45. cron \u2013 set up scheduled tasks to run. 46. nmcli \u2013 network management. 47. ping \u2013 send ICMP ECHO_REQUEST to network hosts. 48. traceroute \u2013 check the route packets take to a specified host. 49. mtr \u2013 network diagnostic tool. 50. nslookup \u2013 query Internet name servers (NS) interactively. 51. host \u2013 perform DNS lookups in Linux. 52. dig \u2013 DNS lookup utility. Linux Commands frequently used by Linux Sysadmins \u2013 Part 4: 53. wget \u2013 retrieve files over HTTP, HTTPS, FTP, and FTPS. 54. curl \u2013 transferring data using various network protocols. (supports more protocols than wget) 55. dd \u2013 convert and copy files. 56. fdisk \u2013 manipulate the disk partition table. 57. parted \u2013 for creating and manipulating partition tables. 58. blkid \u2013 command-line utility to locate/print block device attributes. 59. mkfs \u2013 build a Linux file system. 60. fsck \u2013 tool for checking the consistency of a file system. 61. whois \u2013 client for the whois directory service. 62. nc \u2013 command-line networking utility. (Also, see 60 Linux Networking commands and scripts.) 63. umask \u2013 set file mode creation mask. 64. chmod \u2013 change the access permissions of file system objects. 65. chown \u2013 change file owner and group. 66. chroot \u2013 run command or interactive shell with a special root directory. 67. useradd \u2013 create a new user or update default new user information. 68. userdel \u2013 used to delete a user account and all related files. 69. usermod \u2013 used to modify or change any attributes of an existing user account. Linux Commands frequently used by Linux Sysadmins \u2013 Part 5: 70. vi \u2013 text editor. 71. cat \u2013 display file contents. 72. tac \u2013 output file contents, in reverse. 73. more \u2013 display file contents one screen/page at a time. 74. less \u2013 similar to the more command with additional features. 75. tail \u2013 used to display the tail end of a text file or piped data. 76. dmesg \u2013 prints the message buffer of the kernel ring. 77. journalctl \u2013 query the systemd journal. 78. kill \u2013 terminate a process. 79. killall \u2013 Sends a kill signal to all instances of a process by name. 80. sleep \u2013 suspends program execution for a specified time. 81. wait \u2013 Suspend script execution until all jobs running in the background have been terminated. 82. nohup \u2013 Run Commands in the Background. 83. screen \u2013 hold a session open on a remote server. (also a full-screen window manager) 84. tmux \u2013 a terminal multiplexer. 85. passwd \u2013 change a user\u2019s password. 86. chpassword \u2013 87. mount / umount \u2013 provides access to an entire filesystem in one directory. 88. systemctl \u2013 Managing Services (Daemons). 89. clear \u2013 clears the screen of the terminal. 90. env -Run a command in a modified environment. Misc commands: 91. cheat \u2013 allows you to create and view interactive cheatsheets on the command-line.\u201d 92. tldr \u2013 Collaborative cheatsheets for console commands. 93. bashtop \u2013 the \u2018cool\u2019 top alternative. 94. bpytop \u2013 Python port of bashtop. This list of Linux Networking commands and scripts will receive ongoing updates, similar to the other lists on this blog\u2026 aria2 \u2013 downloading just about everything. Torrents included. arpwatch \u2013 Ethernet Activity Monitor. bmon \u2013 bandwidth monitor and rate estimator. bwm-ng \u2013 live network bandwidth monitor. curl \u2013 transferring data with URLs. (or try httpie) darkstat \u2013 captures network traffic, usage statistics. dhclient \u2013 Dynamic Host Configuration Protocol Client dig \u2013 query DNS servers for information. dstat \u2013 replacement for vmstat, iostat, mpstat, netstat and ifstat. ethtool \u2013 utility for controlling network drivers and hardware. gated \u2013 gateway routing daemon. host \u2013 DNS lookup utility. hping \u2013 TCP/IP packet assembler/analyzer. ibmonitor \u2013 shows bandwidth and total data transferred. ifstat \u2013 report network interfaces bandwidth. iftop \u2013 display bandwidth usage. ip (PDF file) \u2013 a command with more features that ifconfig (net-tools). iperf3 \u2013 network bandwidth measurement tool. (above screenshot Stacklinux VPS) iproute2 \u2013 collection of utilities for controlling TCP/IP. iptables \u2013 take control of network traffic. IPTraf \u2013 An IP Network Monitor. iputils \u2013 set of small useful utilities for Linux networking. iw \u2013 a new nl80211 based CLI configuration utility for wireless devices. jwhois (whois) \u2013 client for the whois service. \u201clsof -i\u201d \u2013 reveal information about your network sockets. mtr \u2013 network diagnostic tool. net-tools \u2013 utilities include: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool, iptunnel and ipmaddr. ncat \u2013 improved re-implementation of the venerable netcat. netcat \u2013 networking utility for reading/writing network connections. nethogs \u2013 a small \u2018net top\u2019 tool. Netperf \u2013 Network bandwidth Testing. netplan \u2013 Netplan is a utility for easily configuring networking on a linux system. netsniff-ng \u2013 Swiss army knife for daily Linux network plumbing. netwatch \u2013 monitoring Network Connections. ngrep \u2013 grep applied to the network layer. nload \u2013 display network usage. nmap \u2013 network discovery and security auditing. nmcli \u2013 a command-line tool for controlling NetworkManager and reporting network status. nmtui \u2013 provides a text interface to configure networking by controlling NetworkManager. nslookup \u2013 query Internet name servers interactively. ping \u2013 send icmp echo_request to network hosts. route \u2013 show / manipulate the IP routing table. slurm \u2013 network load monitor. snort \u2013 Network Intrusion Detection and Prevention System. smokeping \u2013 keeps track of your network latency. socat \u2013 establishes two bidirectional byte streams and transfers data between them. speedometer \u2013 Measure and display the rate of data across a network. speedtest-cli \u2013 test internet bandwidth using speedtest.net ss \u2013 utility to investigate sockets. ssh \u2013 secure system administration and file transfers over insecure networks. tcpdump \u2013 command-line packet analyzer. tcptrack \u2013 Displays information about tcp connections on a network interface. telnet \u2013 user interface to the TELNET protocol. tracepath \u2013 very similar function to traceroute. traceroute \u2013 print the route packets trace to network host. vnStat \u2013 network traffic monitor. websocat \u2013 Connection forwarder from/to web sockets to/from usual sockets, in style of socat. wget \u2013 retrieving files using HTTP, HTTPS, FTP and FTPS. Wireless Tools for Linux \u2013 includes iwconfig, iwlist, iwspy, iwpriv and ifrename. Wireshark \u2013 network protocol analyzer.","title":"COMANDOS SYSADMIN"},{"location":"lvm/","text":"LVM Logical Volum Management 1 particion extensa, 4 primarias, 16 logicas. COMANDOS CREAMOS IMAGEN Y ASIGNAMOS A UN LOOP dd if=/dev/zero of=file.img bs=1024 count=1024 losetup /dev/loop0 file.img losetup -a/-d CREAMOS UN PHYSICAL VOLUM pvcreate /dev/loop0 pvdisplay /dev/loop0 CREAMOS UN VOLUME GROUP vgcreate nameVG /dev/loop0{...} vgdisplay nameVG blkid tree /dev/disk CREAMOS UN LOGICAL VOLUME lvcreate -L tama\u00f1o -n nameLV /dev/nomVG lvcreate -l 100%libre -n nameLV /dev/nomVG lvdisplay /dev/nameVG/nameLV FORMATEAMOS mkfs -t ext4 /dev/nameVG/nameLV mkdir /mnt/dades mount /dev/nameVG/nameLV /mnt/dades df -h -t ext4 EXTENDEMOS vgextend /dev/nameVG /dev/loop2 lvextend -L +30M /dev/nameVG/nameLV /dev/loop2 resize2fs /dev/nameVG/nameLV REDUCIMOS umount /dev/nameVG/nameLV e2fsck -f /dev/nameVG/nameLV resize2fs /dev/nameVG/nameLV 56M mount /dev/nameVG/nameLV /mnt/xxx lvreduce -L 56M -r /dev/nameVG/nameLV DESHACEMOS umount /mnt/dades lvremove /dev/nameVG/nameLV vgremove /dev/nameVG pvremove /dev/loop0 losetup -d /dev/loop0 EJERCICIO PRACTICA RAID RAID 1. Crear tres particions de 5GBytes al HD, corresponents a sda2, sda3 i sda4. Entramos en nuestra tabla de particiones de /dev/sda para crear las particiones: [root@i21 ~]# fdisk /dev/sda creamos 3 particiones primarias(mismos pasos para las 3): opcion n para nueva particion: Command (m for help): n indicamos que es primaria Partition type p primary (0 primary, 1 extended, 3 free) l logical (numbered from 5) Select (default p): p indicamos el numero de particion, enter para por defecto (este caso 2) Partition number (2-4, default 2): indicamos desde donde empieza(por defecto desde el primer sitio libre) First sector (429918208-468862127, default 429918208): indicamos la medida que queremos, en este caso 5GB Last sector, +sectors or +size{K,M,G,T,P} (429918208-468862127, default 468862127): +5G Created a new partition 2 of type 'Linux' and of size 5 GiB. Partition #2 contains a ext4 signature. borramos la firma Do you want to remove the signature? [Y]es/[N]o: y Disk /dev/sda: 223.6 GiB, 240057409536 bytes, 468862128 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x173e314d Device Boot Start End Sectors Size Id Type /dev/sda1 2048 429918207 429916160 205G 5 Extended /dev/sda2 429918208 440403967 10485760 5G 83 Linux /dev/sda3 440403968 450889727 10485760 5G 83 Linux /dev/sda4 450889728 461375487 10485760 5G 83 Linux /dev/sda5 4096 209719295 209715200 100G 83 Linux /dev/sda6 * 209721344 419436543 209715200 100G 83 Linux /dev/sda7 419438592 429918207 10479616 5G 82 Linux swap / Solaris Para finalizar apretamos \u2018w\u2019 para guardar y hacemos un partprobe [root@i21 ~]# partprobe Crear un RAID de nivell 1 utilitzant les particions anteriors. Usar dos discs m\u00e9s un de spare. Mostrar el raid: la descripci\u00f3 i el proc\u00e9s.. creamos el raid 1 con dos discos (sda2 y sda3) y uno de spare(sda4) [root@i21 ~]# mdadm -v --create /dev/md/raid --level=1 --raid-devices=2 /dev/sda2 /dev/sda3 --spare-devices=1 /dev/sda4 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 mdadm: size set to 5238784K Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md/raid started. Vemos la descripcion [root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid1 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 10:57:07 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 17 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 Vemos el proceso [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] md127 : active raid1 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 [2/2] [UU] unused devices: Assignar format al RAID i muntar-lo al directori /mnt/raid. Copiar-hi tot el directori /bin i posar-hi un fitxer xixa.dat de 3G. Mostrar amb df -h l'ocupaci\u00f3. formateamos [root@i21 ~]# mkfs -t ext4 /dev/md/raid mke2fs 1.43.5 (04-Aug-2017) Discarding device blocks: done Creating filesystem with 1309696 4k blocks and 327680 inodes Filesystem UUID: 983460ab-c0f1-41dc-91e0-7c99fa6a266c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done creamos el directorio en el cual montaremos el raid [root@i21 ~]# mkdir /mnt/raid lo montamos [root@i21 ~]# mount /dev/md/raid /mnt/raid vemos que est\u00e1 montado [root@i21 ~]# ll /mnt/raid/ total 16 drwx------. 2 root root 16384 Feb 13 11:02 lost+found copiamos el bin (estaba con simbolic link por lo que copiamos la info real) [root@i21 ~]# cp -r /usr/bin/ /mnt/raid/ creamos un file xixat.dat de 3g [root@i21 ~]# dd if=/dev/zero of=xixa.dat bs=1k count=3M 3145728+0 records in 3145728+0 records out 3221225472 bytes (3.2 GB, 3.0 GiB) copied, 4.41942 s, 729 MB/s copiamos este fichero a mnt/raid [root@i21 ~]# cp xixa.dat /mnt/raid/. vemos el contenido del directorio y su ocupacion [root@i21 ~]# ll /mnt/raid/ total 3145800 dr-xr-xr-x. 2 root root 53248 Feb 13 11:12 bin drwx------. 2 root root 16384 Feb 13 11:02 lost+found -rw-r--r--. 1 root root 3221225472 Feb 13 11:10 xixa.dat [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Passar el RAID a nivell 5. Contesta i mostra quants discs en total t\u00e9 el raid i quants d\u2019actius (proc i descripci\u00f3). pasamos a raid 5 [root@i21 ~]# mdadm --grow /dev/md/raid --level=5 mdadm: level of /dev/md/raid changed to raid5 vemos el proceso [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU] unused devices: vemos como est\u00e1 formado el raid5 [root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:15:04 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 18 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 tiene 3 discos , dos activos y uno en spare Fes els passos necessaris perqu\u00e8 el RAID tingui tres discs actius. Mostrar-ho amb proc i descripci\u00f3. creo un nuevo disco y lo asigno a un loop, despues lo a\u00f1ado al raid 5 [root@i21 ~]# dd if=/dev/zero of=disc04.img bs=1k count=5M 5242880+0 records in 5242880+0 records out 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 7.36486 s, 729 MB/s [root@i21 ~]# losetup /dev/loop0 disc04.img [root@i21 ~]# mdadm --grow /dev/md127 --level=5 mdadm: level of /dev/md127 changed to raid5 [root@i21 ~]# mdadm --grow /dev/md127 --raid-devices=3 --add /dev/loop0 mdadm: added /dev/loop0 provoco un fallo de este disco que estaba activo en el raid [root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid Active Devices : 2 Working Devices : 3 Failed Devices : 1 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Reshape Status : 97% complete Delta Devices : 1, (2->3) Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 45 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 3 7 0 2 faulty /dev/loop0 2 8 4 - spare /dev/sda4 borro el disco y lo vuelvo a\u00f1adir entonces el de spare ocupa su puesto y se activa y el nuevo disco se queda como spare [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --add /dev/loop0 mdadm: added /dev/loop0 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 loop0 3 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:33:48 2020 State : clean Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 67 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 3 7 0 - spare /dev/loop0 provocamos fallo del loop y lo borramos [root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid mostramos resultados [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: Contesta i mostra: l\u2019espai de disc disponible al raid, l\u2019espai actual i l\u2019ocupaci\u00f3 de disc que mostra df del raid muntat. espacio deisponible del raid [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 ocupacion en disco montado [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Fes els canvis pertinents per tal de que el sistema de fitxers ocupi totalment l\u2019espai disponible al raid, i mostra-ho. hacemos un resize2fs para que ocupe todo el espacio [root@i21 ~]# resize2fs /dev/md/raid resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/md/raid is mounted on /mnt/raid; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 2 The filesystem on /dev/md/raid is now 2619392 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Descriu les ordres a fer per tal de poder reiniciar el sistema i disposar del raid actiu. Cal fer-ho (reboot) i mostrar amb df l\u2019ocupaci\u00f3. copiamos en el fichero etc/mdadm.conf la configuracion del raid [root@i21 ~]# mdadm --examine -scan > /etc/mdadm.conf [root@i21 ~]# cat /etc/mdadm.conf ARRAY /dev/md/raid metadata=1.2 UUID=25fba14f:434b6e31:97a0d544:0dd4bfd1 name=i21:raid spares=1 en el fstab ponemos el montaje para que al reiniciarlo salga /dev/md/raid /mnt/raid ext4 defaults 0 0 hacemos el reboot [root@i21 ~]# df -h -t ext4 /dev/md/raid Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Un cop fet desmunta /mnt/raid i elimina el muntatge automatittzat del fstab. Eliminariamos la linea introducida en el fstab [root@i21 ~]# vim /etc/fstab Desmontariamos /mnt/raid [root@i21 ~]# umount /mnt/raid parariamos el raid mdadm \u2013stop /dev/md/raid eliminariamos el fichero de conf /etc/mdadm.conf hariamos un mdadm \u2013zero-superblock a cada particion LVM LVM Partint del RAID creat a l'apartat anterior (Raid 1 dels discs sda2, sda3 i sda4) fer: 1. Crear dins dos Volums L\u00f2gics anomenats hdsystem (1024 MBytes) i hddata (500 Mbytes). Mostrar clarament tots els passos per fer-ho. creamos el pv [root@i21 ~]# pvcreate /dev/md/raid WARNING: ext4 signature detected on /dev/md/raid at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/md/raid. Physical volume \"/dev/md/raid\" successfully created. [root@i21 ~]# vgcreate mydisc /dev/md/raid creamos el vg Volume group \"mydisc\" successfully created creamos las lv [root@i21 ~]# lvcreate -L 1024M -n hdsystem /dev/mydisc Logical volume \"hdsystem\" created. [root@i21 ~]# lvcreate -L 500M -n hddata /dev/mydisc Logical volume \"hddata\" created. Mostrar la informaci\u00f3 del volum f\u00edsic, el grup de volum i els volums l\u00f2gics. info del pv [root@i21 ~]# pvdisplay /dev/md/raid --- Physical volume --- PV Name /dev/md127 VG Name mydisc PV Size 9.99 GiB / not usable 4.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 2557 Free PE 2176 Allocated PE 381 PV UUID 1pbO3s-krID-iQhm-xNzZ-ocAi-N9vw-uXdicW info del vg [root@i21 ~]# vgdisplay /dev/mydisc --- Volume group --- VG Name mydisc System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size <9.99 GiB PE Size 4.00 MiB Total PE 2557 Alloc PE / Size 381 / <1.49 GiB Free PE / Size 2176 / 8.50 GiB VG UUID IzpMcC-71Ri-1Xwc-8Vsw-Pwj1-N6lY-R33fKC info dels lv [root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 0 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 0 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 Muntar a /mnt/hdsystem i a /mnt/hddata els corresponents Volums L\u00f2gics. Copiar-hi a hdsystem tot el contingut de /usr/share/man i a hddata de /usr/share/doc. Mostrar amb df -h l'ocupaci\u00f3. damos formato a las lv [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hdsystem mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 262144 4k blocks and 65536 inodes Filesystem UUID: 16687c70-a2f0-477b-b7cd-7cb1c4a172da Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hddata mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 512000 1k blocks and 128016 inodes Filesystem UUID: 002310bc-b92e-491c-bcb0-52f104a69323 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done creamos los directorios hdsystem y hddata a mnt [root@i21 ~]# mkdir /mnt/hddata [root@i21 ~]# mkdir /mnt/hdsystem montamos los lv [root@i21 ~]# mount /dev/mydisc/hdsystem /mnt/hdsystem [root@i21 ~]# mount /dev/mydisc/hddata /mnt/hddata copiamos los ficheros indicados en su directorio correspondiente y vemos que se haya copiado: [root@i21 ~]# cp -r /usr/share/man /mnt/hdsystem/. [root@i21 ~]# cp -r /usr/share/doc /mnt/hddata/. [root@i21 ~]# ll /mnt/hddata/ total 48 drwxr-xr-x. 1113 root root 34816 Feb 13 12:07 doc drwx------. 2 root root 12288 Feb 13 12:04 lost+found [root@i21 ~]# ll /mnt/hdsystem/ total 20 drwx------. 2 root root 16384 Feb 13 12:04 lost+found drwxr-xr-x. 47 root root 4096 Feb 13 12:06 man vemos la ocupacion [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Fer que aquests canvis siguin permanents en reiniciar el sistema. Mostra que es fa un reinici i els sistemes de fitxers estan muntats i quina \u00e9s la seva ocupaci\u00f3. copiamos las siguientes lineas en el fstab [root@i21 ~]# vim /etc/fstab /dev/mydisc/hddata /mnt/hddata ext4 defaults 0 0 /dev/mydisc/hdsystem /mnt/hdsystem ext4 defaults 0 0 reboot [root@i21 ~]# reboot ocupacion Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Aprofitant que encara hi ha espai disponible assignar un 50% de l'espai lliure al Volum L\u00f2gic hdsystem. Mostrar clarament aquests canvi en el Volum L\u00f2gic. extendemos la lv hdsystem [root@i21 ~]# lvextend -l +50%FREE /dev/mydisc/hdsystem Size of logical volume mydisc/hdsystem changed from 1.00 GiB (256 extents) to 5.25 GiB (1344 extents). Logical volume mydisc/hdsystem successfully resized. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem como vemos que el file sysrem sigue igual , hacemos un resize2fs para que ocupe todo el espacio [root@i21 ~]# resize2fs /dev/mydisc/hdsystem resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/mydisc/hdsystem is mounted on /mnt/system; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/mydisc/hdsystem is now 1376256 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem vemos que ha cambiado la lv [root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 [root@i21 ~]# lvdisplay /dev/mydisc/hdsystem --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 5.25 GiB Current LE 1344 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 [root@i21 ~]# lvdisplay /dev/mydisc/hddata --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 6. Usant l\u2019ordre df -h mostrar que el sisteme de fitxers muntat a /mnt/hdsystem s'ha ampliat fins al m\u00e0xim del seu espai disponible (i si no ho ha fet, fer-ho!). como hemos hecho antes un resize2fs, tenemos el maximo del espacio asignado. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem EXTRA Partint del RAID i el LVM creats en els apartats anteriors fer: 1. Escriu les ordres necess\u00e0ries per eliminar tota l\u2019automatitzaci\u00f3 de l\u2019arrancada. borramos las lineas del fstab [root@i21 ~]# vim /etc/fstab borramos el file de conf de raid para que no detecte nada [root@i21 ~]# rm -rf /etc/mdadm.conf desmontamos los directorios [root@i21 ~]# umount /mnt/raid [root@i21 ~]# umount /mnt/hdsystem [root@i21 ~]# umount /mnt/hddata Escriu les ordres necess\u00e0ries per eliminar els LVM. borramos las lv [root@i21 ~]# lvremove /dev/mydisc/hdsystem Do you really want to remove active logical volume mydisc/hdsystem? [y/n]: y Logical volume \"hdsystem\" successfully removed [root@i21 ~]# lvremove /dev/mydisc/hddata Do you really want to remove active logical volume mydisc/hddata? [y/n]: y Logical volume \"hddata\" successfully removed borramos las vg [root@i21 ~]# vgremove /dev/mydisc Volume group \"mydisc\" successfully removed booramos las pv [root@i21 ~]# pvremove /dev/md/raid Labels on physical volume \"/dev/md/raid\" successfully wiped. Escriu les ordres necess\u00e0ries per eliminar el raid. paramos el raid [root@i21 ~]# mdadm --stop /dev/md/raid mdadm: stopped /dev/md/raid Verifica que les particions sda2, sda3 i sda4 no tenen cap marca especial. hacemos un zero-superblock para eliminar todo tipo de marca [root@i21 ~]# mdadm --zero-superblock /dev/sda2 [root@i21 ~]# mdadm --zero-superblock /dev/sda3 [root@i21 ~]# mdadm --zero-superblock /dev/sda4 verificamos [root@i21 ~]# mdadm --examine /dev/sda2 mdadm: No md superblock detected on /dev/sda2. [root@i21 ~]# mdadm --examine /dev/sda3 mdadm: No md superblock detected on /dev/sda3. [root@i21 ~]# mdadm --examine /dev/sda4 mdadm: No md superblock detected on /dev/sda4.","title":"LVM/RAID"},{"location":"lvm/#lvm","text":"Logical Volum Management 1 particion extensa, 4 primarias, 16 logicas.","title":"LVM"},{"location":"lvm/#comandos","text":"CREAMOS IMAGEN Y ASIGNAMOS A UN LOOP dd if=/dev/zero of=file.img bs=1024 count=1024 losetup /dev/loop0 file.img losetup -a/-d CREAMOS UN PHYSICAL VOLUM pvcreate /dev/loop0 pvdisplay /dev/loop0 CREAMOS UN VOLUME GROUP vgcreate nameVG /dev/loop0{...} vgdisplay nameVG blkid tree /dev/disk CREAMOS UN LOGICAL VOLUME lvcreate -L tama\u00f1o -n nameLV /dev/nomVG lvcreate -l 100%libre -n nameLV /dev/nomVG lvdisplay /dev/nameVG/nameLV FORMATEAMOS mkfs -t ext4 /dev/nameVG/nameLV mkdir /mnt/dades mount /dev/nameVG/nameLV /mnt/dades df -h -t ext4 EXTENDEMOS vgextend /dev/nameVG /dev/loop2 lvextend -L +30M /dev/nameVG/nameLV /dev/loop2 resize2fs /dev/nameVG/nameLV REDUCIMOS umount /dev/nameVG/nameLV e2fsck -f /dev/nameVG/nameLV resize2fs /dev/nameVG/nameLV 56M mount /dev/nameVG/nameLV /mnt/xxx lvreduce -L 56M -r /dev/nameVG/nameLV DESHACEMOS umount /mnt/dades lvremove /dev/nameVG/nameLV vgremove /dev/nameVG pvremove /dev/loop0 losetup -d /dev/loop0","title":"COMANDOS"},{"location":"lvm/#ejercicio-practica","text":"","title":"EJERCICIO PRACTICA"},{"location":"lvm/#raid","text":"RAID 1. Crear tres particions de 5GBytes al HD, corresponents a sda2, sda3 i sda4.","title":"RAID"},{"location":"lvm/#entramos-en-nuestra-tabla-de-particiones-de-devsda-para-crear-las-particiones","text":"[root@i21 ~]# fdisk /dev/sda","title":"Entramos en nuestra tabla de particiones de /dev/sda para crear las particiones:"},{"location":"lvm/#creamos-3-particiones-primariasmismos-pasos-para-las-3","text":"","title":"creamos 3 particiones primarias(mismos pasos para las 3):"},{"location":"lvm/#opcion-n-para-nueva-particion","text":"Command (m for help): n","title":"opcion n para nueva particion:"},{"location":"lvm/#indicamos-que-es-primaria","text":"Partition type p primary (0 primary, 1 extended, 3 free) l logical (numbered from 5) Select (default p): p","title":"indicamos que es primaria"},{"location":"lvm/#indicamos-el-numero-de-particion-enter-para-por-defecto-este-caso-2","text":"Partition number (2-4, default 2):","title":"indicamos el numero de particion, enter para por defecto (este caso 2)"},{"location":"lvm/#indicamos-desde-donde-empiezapor-defecto-desde-el-primer-sitio-libre","text":"First sector (429918208-468862127, default 429918208):","title":"indicamos desde donde empieza(por defecto desde el primer sitio libre)"},{"location":"lvm/#indicamos-la-medida-que-queremos-en-este-caso-5gb","text":"Last sector, +sectors or +size{K,M,G,T,P} (429918208-468862127, default 468862127): +5G Created a new partition 2 of type 'Linux' and of size 5 GiB. Partition #2 contains a ext4 signature.","title":"indicamos la medida que queremos, en este caso 5GB"},{"location":"lvm/#borramos-la-firma","text":"Do you want to remove the signature? [Y]es/[N]o: y Disk /dev/sda: 223.6 GiB, 240057409536 bytes, 468862128 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x173e314d Device Boot Start End Sectors Size Id Type /dev/sda1 2048 429918207 429916160 205G 5 Extended /dev/sda2 429918208 440403967 10485760 5G 83 Linux /dev/sda3 440403968 450889727 10485760 5G 83 Linux /dev/sda4 450889728 461375487 10485760 5G 83 Linux /dev/sda5 4096 209719295 209715200 100G 83 Linux /dev/sda6 * 209721344 419436543 209715200 100G 83 Linux /dev/sda7 419438592 429918207 10479616 5G 82 Linux swap / Solaris","title":"borramos la firma"},{"location":"lvm/#para-finalizar-apretamos-w-para-guardar-y-hacemos-un-partprobe","text":"[root@i21 ~]# partprobe Crear un RAID de nivell 1 utilitzant les particions anteriors. Usar dos discs m\u00e9s un de spare. Mostrar el raid: la descripci\u00f3 i el proc\u00e9s..","title":"Para finalizar apretamos \u2018w\u2019 para guardar y hacemos un partprobe"},{"location":"lvm/#creamos-el-raid-1-con-dos-discos-sda2-y-sda3-y-uno-de-sparesda4","text":"[root@i21 ~]# mdadm -v --create /dev/md/raid --level=1 --raid-devices=2 /dev/sda2 /dev/sda3 --spare-devices=1 /dev/sda4 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 mdadm: size set to 5238784K Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md/raid started.","title":"creamos el raid 1 con dos discos (sda2 y sda3) y uno de spare(sda4)"},{"location":"lvm/#vemos-la-descripcion","text":"[root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid1 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 10:57:07 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 17 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4","title":"Vemos la descripcion"},{"location":"lvm/#vemos-el-proceso","text":"[root@i21 ~]# cat /proc/mdstat Personalities : [raid1] md127 : active raid1 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 [2/2] [UU] unused devices: Assignar format al RAID i muntar-lo al directori /mnt/raid. Copiar-hi tot el directori /bin i posar-hi un fitxer xixa.dat de 3G. Mostrar amb df -h l'ocupaci\u00f3.","title":"Vemos el proceso"},{"location":"lvm/#formateamos","text":"[root@i21 ~]# mkfs -t ext4 /dev/md/raid mke2fs 1.43.5 (04-Aug-2017) Discarding device blocks: done Creating filesystem with 1309696 4k blocks and 327680 inodes Filesystem UUID: 983460ab-c0f1-41dc-91e0-7c99fa6a266c Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done","title":"formateamos"},{"location":"lvm/#creamos-el-directorio-en-el-cual-montaremos-el-raid","text":"[root@i21 ~]# mkdir /mnt/raid","title":"creamos el directorio en el cual montaremos el raid"},{"location":"lvm/#lo-montamos","text":"[root@i21 ~]# mount /dev/md/raid /mnt/raid","title":"lo montamos"},{"location":"lvm/#vemos-que-esta-montado","text":"[root@i21 ~]# ll /mnt/raid/ total 16 drwx------. 2 root root 16384 Feb 13 11:02 lost+found","title":"vemos que est\u00e1 montado"},{"location":"lvm/#copiamos-el-bin-estaba-con-simbolic-link-por-lo-que-copiamos-la-info-real","text":"[root@i21 ~]# cp -r /usr/bin/ /mnt/raid/","title":"copiamos el bin (estaba con simbolic link por lo que copiamos la info real)"},{"location":"lvm/#creamos-un-file-xixatdat-de-3g","text":"[root@i21 ~]# dd if=/dev/zero of=xixa.dat bs=1k count=3M 3145728+0 records in 3145728+0 records out 3221225472 bytes (3.2 GB, 3.0 GiB) copied, 4.41942 s, 729 MB/s","title":"creamos un file xixat.dat de 3g"},{"location":"lvm/#copiamos-este-fichero-a-mntraid","text":"[root@i21 ~]# cp xixa.dat /mnt/raid/.","title":"copiamos este fichero a mnt/raid"},{"location":"lvm/#vemos-el-contenido-del-directorio-y-su-ocupacion","text":"[root@i21 ~]# ll /mnt/raid/ total 3145800 dr-xr-xr-x. 2 root root 53248 Feb 13 11:12 bin drwx------. 2 root root 16384 Feb 13 11:02 lost+found -rw-r--r--. 1 root root 3221225472 Feb 13 11:10 xixa.dat [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Passar el RAID a nivell 5. Contesta i mostra quants discs en total t\u00e9 el raid i quants d\u2019actius (proc i descripci\u00f3).","title":"vemos el contenido del directorio y su ocupacion"},{"location":"lvm/#pasamos-a-raid-5","text":"[root@i21 ~]# mdadm --grow /dev/md/raid --level=5 mdadm: level of /dev/md/raid changed to raid5","title":"pasamos a raid 5"},{"location":"lvm/#vemos-el-proceso_1","text":"[root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4 2 sda3[1] sda2[0] 5238784 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU] unused devices:","title":"vemos el proceso"},{"location":"lvm/#vemos-como-esta-formado-el-raid5","text":"[root@i21 ~]# mdadm --detail /dev/md/raid /dev/md/raid: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 5238784 (5.00 GiB 5.36 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 2 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:15:04 2020 State : clean Active Devices : 2 Working Devices : 3 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 18 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 - spare /dev/sda4 tiene 3 discos , dos activos y uno en spare Fes els passos necessaris perqu\u00e8 el RAID tingui tres discs actius. Mostrar-ho amb proc i descripci\u00f3.","title":"vemos como est\u00e1 formado el raid5"},{"location":"lvm/#creo-un-nuevo-disco-y-lo-asigno-a-un-loop-despues-lo-anado-al-raid-5","text":"[root@i21 ~]# dd if=/dev/zero of=disc04.img bs=1k count=5M 5242880+0 records in 5242880+0 records out 5368709120 bytes (5.4 GB, 5.0 GiB) copied, 7.36486 s, 729 MB/s [root@i21 ~]# losetup /dev/loop0 disc04.img [root@i21 ~]# mdadm --grow /dev/md127 --level=5 mdadm: level of /dev/md127 changed to raid5 [root@i21 ~]# mdadm --grow /dev/md127 --raid-devices=3 --add /dev/loop0 mdadm: added /dev/loop0","title":"creo un nuevo disco y lo asigno a un loop, despues lo a\u00f1ado al raid 5"},{"location":"lvm/#provoco-un-fallo-de-este-disco-que-estaba-activo-en-el-raid","text":"[root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid Active Devices : 2 Working Devices : 3 Failed Devices : 1 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Reshape Status : 97% complete Delta Devices : 1, (2->3) Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 45 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 3 7 0 2 faulty /dev/loop0 2 8 4 - spare /dev/sda4","title":"provoco un fallo de este disco que estaba activo en el raid"},{"location":"lvm/#borro-el-disco-y-lo-vuelvo-anadir","text":"","title":"borro el disco y lo vuelvo a\u00f1adir"},{"location":"lvm/#entonces-el-de-spare-ocupa-su-puesto-y-se-activa-y-el-nuevo-disco-se-queda-como-spare","text":"[root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --add /dev/loop0 mdadm: added /dev/loop0 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 loop0 3 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: [root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:33:48 2020 State : clean Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 67 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 3 7 0 - spare /dev/loop0","title":"entonces el de spare ocupa su puesto y se activa y el nuevo disco se queda como spare"},{"location":"lvm/#provocamos-fallo-del-loop-y-lo-borramos","text":"[root@i21 ~]# mdadm /dev/md/raid --fail /dev/loop0 mdadm: set /dev/loop0 faulty in /dev/md/raid [root@i21 ~]# mdadm /dev/md/raid --remove /dev/loop0 mdadm: hot removed /dev/loop0 from /dev/md/raid","title":"provocamos fallo del loop y lo borramos"},{"location":"lvm/#mostramos-resultados","text":"[root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4 [root@i21 ~]# cat /proc/mdstat Personalities : [raid1] [raid6] [raid5] [raid4] md127 : active raid5 sda4[2] sda3[1] sda2[0] 10477568 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU] unused devices: Contesta i mostra: l\u2019espai de disc disponible al raid, l\u2019espai actual i l\u2019ocupaci\u00f3 de disc que mostra df del raid muntat.","title":"mostramos resultados"},{"location":"lvm/#espacio-deisponible-del-raid","text":"[root@i21 ~]# mdadm --detail /dev/md127 /dev/md127: Version : 1.2 Creation Time : Thu Feb 13 10:56:41 2020 Raid Level : raid5 Array Size : 10477568 (9.99 GiB 10.73 GB) Used Dev Size : 5238784 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 3 Persistence : Superblock is persistent Update Time : Thu Feb 13 11:37:36 2020 State : clean Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 64K Name : i21:raid (local to host i21) UUID : 25fba14f:434b6e31:97a0d544:0dd4bfd1 Events : 69 Number Major Minor RaidDevice State 0 8 2 0 active sync /dev/sda2 1 8 3 1 active sync /dev/sda3 2 8 4 2 active sync /dev/sda4","title":"espacio deisponible del raid"},{"location":"lvm/#ocupacion-en-disco-montado","text":"[root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 4.9G 3.8G 829M 83% /mnt/raid Fes els canvis pertinents per tal de que el sistema de fitxers ocupi totalment l\u2019espai disponible al raid, i mostra-ho.","title":"ocupacion en disco montado"},{"location":"lvm/#hacemos-un-resize2fs-para-que-ocupe-todo-el-espacio","text":"[root@i21 ~]# resize2fs /dev/md/raid resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/md/raid is mounted on /mnt/raid; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 2 The filesystem on /dev/md/raid is now 2619392 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/md127 Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Descriu les ordres a fer per tal de poder reiniciar el sistema i disposar del raid actiu. Cal fer-ho (reboot) i mostrar amb df l\u2019ocupaci\u00f3.","title":"hacemos un resize2fs para que ocupe todo el espacio"},{"location":"lvm/#copiamos-en-el-fichero-etcmdadmconf-la-configuracion-del-raid","text":"[root@i21 ~]# mdadm --examine -scan > /etc/mdadm.conf [root@i21 ~]# cat /etc/mdadm.conf ARRAY /dev/md/raid metadata=1.2 UUID=25fba14f:434b6e31:97a0d544:0dd4bfd1 name=i21:raid spares=1","title":"copiamos en el fichero etc/mdadm.conf la configuracion del raid"},{"location":"lvm/#en-el-fstab-ponemos-el-montaje-para-que-al-reiniciarlo-salga","text":"/dev/md/raid /mnt/raid ext4 defaults 0 0","title":"en el fstab ponemos el montaje para que al reiniciarlo salga"},{"location":"lvm/#hacemos-el-reboot","text":"[root@i21 ~]# df -h -t ext4 /dev/md/raid Filesystem Size Used Avail Use% Mounted on /dev/md127 9.8G 3.8G 5.6G 41% /mnt/raid Un cop fet desmunta /mnt/raid i elimina el muntatge automatittzat del fstab.","title":"hacemos el reboot"},{"location":"lvm/#eliminariamos-la-linea-introducida-en-el-fstab","text":"[root@i21 ~]# vim /etc/fstab","title":"Eliminariamos la linea introducida en el fstab"},{"location":"lvm/#desmontariamos-mntraid","text":"[root@i21 ~]# umount /mnt/raid","title":"Desmontariamos /mnt/raid"},{"location":"lvm/#parariamos-el-raid","text":"mdadm \u2013stop /dev/md/raid","title":"parariamos el raid"},{"location":"lvm/#eliminariamos-el-fichero-de-conf-etcmdadmconf","text":"","title":"eliminariamos el fichero de conf /etc/mdadm.conf"},{"location":"lvm/#hariamos-un-mdadm-zero-superblock-a-cada-particion","text":"","title":"hariamos un mdadm \u2013zero-superblock a cada particion"},{"location":"lvm/#lvm_1","text":"LVM Partint del RAID creat a l'apartat anterior (Raid 1 dels discs sda2, sda3 i sda4) fer: 1. Crear dins dos Volums L\u00f2gics anomenats hdsystem (1024 MBytes) i hddata (500 Mbytes). Mostrar clarament tots els passos per fer-ho.","title":"LVM"},{"location":"lvm/#creamos-el-pv","text":"[root@i21 ~]# pvcreate /dev/md/raid WARNING: ext4 signature detected on /dev/md/raid at offset 1080. Wipe it? [y/n]: y Wiping ext4 signature on /dev/md/raid. Physical volume \"/dev/md/raid\" successfully created. [root@i21 ~]# vgcreate mydisc /dev/md/raid","title":"creamos el pv"},{"location":"lvm/#creamos-el-vg","text":"Volume group \"mydisc\" successfully created","title":"creamos el vg"},{"location":"lvm/#creamos-las-lv","text":"[root@i21 ~]# lvcreate -L 1024M -n hdsystem /dev/mydisc Logical volume \"hdsystem\" created. [root@i21 ~]# lvcreate -L 500M -n hddata /dev/mydisc Logical volume \"hddata\" created. Mostrar la informaci\u00f3 del volum f\u00edsic, el grup de volum i els volums l\u00f2gics.","title":"creamos las lv"},{"location":"lvm/#info-del-pv","text":"[root@i21 ~]# pvdisplay /dev/md/raid --- Physical volume --- PV Name /dev/md127 VG Name mydisc PV Size 9.99 GiB / not usable 4.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 2557 Free PE 2176 Allocated PE 381 PV UUID 1pbO3s-krID-iQhm-xNzZ-ocAi-N9vw-uXdicW","title":"info del pv"},{"location":"lvm/#info-del-vg","text":"[root@i21 ~]# vgdisplay /dev/mydisc --- Volume group --- VG Name mydisc System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size <9.99 GiB PE Size 4.00 MiB Total PE 2557 Alloc PE / Size 381 / <1.49 GiB Free PE / Size 2176 / 8.50 GiB VG UUID IzpMcC-71Ri-1Xwc-8Vsw-Pwj1-N6lY-R33fKC","title":"info del vg"},{"location":"lvm/#info-dels-lv","text":"[root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 0 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 0 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 Muntar a /mnt/hdsystem i a /mnt/hddata els corresponents Volums L\u00f2gics. Copiar-hi a hdsystem tot el contingut de /usr/share/man i a hddata de /usr/share/doc. Mostrar amb df -h l'ocupaci\u00f3.","title":"info dels lv"},{"location":"lvm/#damos-formato-a-las-lv","text":"[root@i21 ~]# mkfs -t ext4 /dev/mydisc/hdsystem mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 262144 4k blocks and 65536 inodes Filesystem UUID: 16687c70-a2f0-477b-b7cd-7cb1c4a172da Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done [root@i21 ~]# mkfs -t ext4 /dev/mydisc/hddata mke2fs 1.43.5 (04-Aug-2017) Creating filesystem with 512000 1k blocks and 128016 inodes Filesystem UUID: 002310bc-b92e-491c-bcb0-52f104a69323 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done","title":"damos formato a las lv"},{"location":"lvm/#creamos-los-directorios-hdsystem-y-hddata-a-mnt","text":"[root@i21 ~]# mkdir /mnt/hddata [root@i21 ~]# mkdir /mnt/hdsystem","title":"creamos los directorios hdsystem y hddata a mnt"},{"location":"lvm/#montamos-los-lv","text":"[root@i21 ~]# mount /dev/mydisc/hdsystem /mnt/hdsystem [root@i21 ~]# mount /dev/mydisc/hddata /mnt/hddata","title":"montamos los lv"},{"location":"lvm/#copiamos-los-ficheros-indicados-en-su-directorio-correspondiente-y-vemos-que-se-haya-copiado","text":"[root@i21 ~]# cp -r /usr/share/man /mnt/hdsystem/. [root@i21 ~]# cp -r /usr/share/doc /mnt/hddata/. [root@i21 ~]# ll /mnt/hddata/ total 48 drwxr-xr-x. 1113 root root 34816 Feb 13 12:07 doc drwx------. 2 root root 12288 Feb 13 12:04 lost+found [root@i21 ~]# ll /mnt/hdsystem/ total 20 drwx------. 2 root root 16384 Feb 13 12:04 lost+found drwxr-xr-x. 47 root root 4096 Feb 13 12:06 man","title":"copiamos los ficheros indicados en su directorio correspondiente y vemos que se haya copiado:"},{"location":"lvm/#vemos-la-ocupacion","text":"[root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Fer que aquests canvis siguin permanents en reiniciar el sistema. Mostra que es fa un reinici i els sistemes de fitxers estan muntats i quina \u00e9s la seva ocupaci\u00f3.","title":"vemos la ocupacion"},{"location":"lvm/#copiamos-las-siguientes-lineas-en-el-fstab","text":"[root@i21 ~]# vim /etc/fstab /dev/mydisc/hddata /mnt/hddata ext4 defaults 0 0 /dev/mydisc/hdsystem /mnt/hdsystem ext4 defaults 0 0","title":"copiamos las siguientes lineas en el fstab"},{"location":"lvm/#reboot","text":"[root@i21 ~]# reboot","title":"reboot"},{"location":"lvm/#ocupacion","text":"Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem Aprofitant que encara hi ha espai disponible assignar un 50% de l'espai lliure al Volum L\u00f2gic hdsystem. Mostrar clarament aquests canvi en el Volum L\u00f2gic.","title":"ocupacion"},{"location":"lvm/#extendemos-la-lv-hdsystem","text":"[root@i21 ~]# lvextend -l +50%FREE /dev/mydisc/hdsystem Size of logical volume mydisc/hdsystem changed from 1.00 GiB (256 extents) to 5.25 GiB (1344 extents). Logical volume mydisc/hdsystem successfully resized. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 976M 47M 863M 6% /mnt/hdsystem","title":"extendemos la lv hdsystem"},{"location":"lvm/#como-vemos-que-el-file-sysrem-sigue-igual-hacemos-un-resize2fs-para-que-ocupe-todo-el-espacio","text":"[root@i21 ~]# resize2fs /dev/mydisc/hdsystem resize2fs 1.43.5 (04-Aug-2017) Filesystem at /dev/mydisc/hdsystem is mounted on /mnt/system; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/mydisc/hdsystem is now 1376256 (4k) blocks long. [root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem","title":"como vemos que el file sysrem sigue igual , hacemos un resize2fs para que ocupe todo el espacio"},{"location":"lvm/#vemos-que-ha-cambiado-la-lv","text":"[root@i21 ~]# lvdisplay --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 [root@i21 ~]# lvdisplay /dev/mydisc/hdsystem --- Logical volume --- LV Path /dev/mydisc/hdsystem LV Name hdsystem VG Name mydisc LV UUID 2jSCBs-01XX-P5pn-4lp2-OsKc-H5Jc-FsJD9h LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:57:49 +0100 LV Status available # open 1 LV Size 5.25 GiB Current LE 1344 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:0 [root@i21 ~]# lvdisplay /dev/mydisc/hddata --- Logical volume --- LV Path /dev/mydisc/hddata LV Name hddata VG Name mydisc LV UUID nL29my-hgY5-8uDr-BlYq-uxYI-yUye-1ZQnxA LV Write Access read/write LV Creation host, time i21, 2020-02-13 11:58:07 +0100 LV Status available # open 1 LV Size 500.00 MiB Current LE 125 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 512 Block device 253:1 6. Usant l\u2019ordre df -h mostrar que el sisteme de fitxers muntat a /mnt/hdsystem s'ha ampliat fins al m\u00e0xim del seu espai disponible (i si no ho ha fet, fer-ho!).","title":"vemos que ha cambiado la lv"},{"location":"lvm/#como-hemos-hecho-antes-un-resize2fs-tenemos-el-maximo-del-espacio-asignado","text":"[root@i21 ~]# df -h -t ext4 /dev/mydisc/* Filesystem Size Used Avail Use% Mounted on /dev/mapper/mydisc-hddata 477M 109M 339M 25% /mnt/hddata /dev/mapper/mydisc-hdsystem 5.2G 48M 4.9G 1% /mnt/hdsystem","title":"como hemos hecho antes un resize2fs, tenemos el maximo del espacio asignado."},{"location":"lvm/#extra","text":"Partint del RAID i el LVM creats en els apartats anteriors fer: 1. Escriu les ordres necess\u00e0ries per eliminar tota l\u2019automatitzaci\u00f3 de l\u2019arrancada.","title":"EXTRA"},{"location":"lvm/#borramos-las-lineas-del-fstab","text":"[root@i21 ~]# vim /etc/fstab","title":"borramos las lineas del fstab"},{"location":"lvm/#borramos-el-file-de-conf-de-raid-para-que-no-detecte-nada","text":"[root@i21 ~]# rm -rf /etc/mdadm.conf","title":"borramos el file de conf de raid para que no detecte nada"},{"location":"lvm/#desmontamos-los-directorios","text":"[root@i21 ~]# umount /mnt/raid [root@i21 ~]# umount /mnt/hdsystem [root@i21 ~]# umount /mnt/hddata Escriu les ordres necess\u00e0ries per eliminar els LVM.","title":"desmontamos los directorios"},{"location":"lvm/#borramos-las-lv","text":"[root@i21 ~]# lvremove /dev/mydisc/hdsystem Do you really want to remove active logical volume mydisc/hdsystem? [y/n]: y Logical volume \"hdsystem\" successfully removed [root@i21 ~]# lvremove /dev/mydisc/hddata Do you really want to remove active logical volume mydisc/hddata? [y/n]: y Logical volume \"hddata\" successfully removed","title":"borramos las lv"},{"location":"lvm/#borramos-las-vg","text":"[root@i21 ~]# vgremove /dev/mydisc Volume group \"mydisc\" successfully removed","title":"borramos las vg"},{"location":"lvm/#booramos-las-pv","text":"[root@i21 ~]# pvremove /dev/md/raid Labels on physical volume \"/dev/md/raid\" successfully wiped. Escriu les ordres necess\u00e0ries per eliminar el raid.","title":"booramos las pv"},{"location":"lvm/#paramos-el-raid","text":"[root@i21 ~]# mdadm --stop /dev/md/raid mdadm: stopped /dev/md/raid Verifica que les particions sda2, sda3 i sda4 no tenen cap marca especial.","title":"paramos el raid"},{"location":"lvm/#hacemos-un-zero-superblock-para-eliminar-todo-tipo-de-marca","text":"[root@i21 ~]# mdadm --zero-superblock /dev/sda2 [root@i21 ~]# mdadm --zero-superblock /dev/sda3 [root@i21 ~]# mdadm --zero-superblock /dev/sda4","title":"hacemos un zero-superblock para eliminar todo tipo de marca"},{"location":"lvm/#verificamos","text":"[root@i21 ~]# mdadm --examine /dev/sda2 mdadm: No md superblock detected on /dev/sda2. [root@i21 ~]# mdadm --examine /dev/sda3 mdadm: No md superblock detected on /dev/sda3. [root@i21 ~]# mdadm --examine /dev/sda4 mdadm: No md superblock detected on /dev/sda4.","title":"verificamos"},{"location":"markdown/","text":"Comandos lenguaje MARKDOWN T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea Crear indice: 1. [Primer apartado](#id1) -- ## Primer apartado","title":"Markdown"},{"location":"markdown/#comandos-lenguaje-markdown","text":"T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea Crear indice: 1. [Primer apartado](#id1) -- ## Primer apartado","title":"Comandos lenguaje MARKDOWN"},{"location":"monitorizacion/","text":"ZABBIX Zabbix es un software de monitorizaci\u00f3n de c\u00f3digo abierto para redes y aplicaciones. Ofrece monitorizaci\u00f3n en tiempo real de miles de m\u00e9tricas recogidas de servidores, equipos virtuales, dispositivos de red y aplicaciones web. Estas m\u00e9tricas pueden ayudarle a determinar el estado actual de su infraestructura TI y a detectar problemas con los componentes de hardware o software antes de que los clientes se quejen. La informaci\u00f3n \u00fatil se almacena en una base de datos para que pueda analizar datos a lo largo del tiempo y mejorar la calidad de los servicios proporcionados o planificar mejoras de su equipo. INSTALACION PASO A PASO Zabbix usa varias opciones para recoger m\u00e9tricas, incluyendo la monitorizaci\u00f3n sin agente de los servicios del usuario y de la arquitectura cliente-servidor. Para recoger m\u00e9tricas del servidor, utiliza un agente peque\u00f1o en el cliente monitorizado para recopilar datos y enviarlos al servidor Zabbix. Zabbix admite la comunicaci\u00f3n cifrada entre el servidor y los clientes conectados, de forma que sus datos est\u00e1n protegidos mientras recorren redes inseguras. El servidor Zabbix almacena sus datos en una base de datos relacional alimentada con MySQL o PostgreSQL. Tambi\u00e9n puede almacenar datos hist\u00f3ricos en bases de datos NoSQL como Elasticsearch y TimescaleDB. Zabbix ofrece una interfaz web para que pueda ver los datos y configurar los ajustes del sistema. En este tutorial, configurar\u00e1 Zabbix en dos equipos Ubuntu 20.04. Uno ser\u00e1 configurado como el servidor Zabbix y el otro como un cliente que monitorizar\u00e1. El servidor Zabbix usar\u00e1 una base de datos MySQL para registrar los datos de monitorizaci\u00f3n y utilizar\u00e1 Nginx para presentar la interfaz web. NAGGIOS GRAFANA Grafana es una herramienta de visualizaci\u00f3n y monitoreo de datos de c\u00f3digo abierto que se integra con datos complejos de fuentes como Prometheus, InfluxDB, Graphite y ElasticSearch. Grafana le permite crear alertas, notificaciones y filtros ad-hoc para sus datos, a la vez que facilita la colaboraci\u00f3n con los compa\u00f1eros de equipo mediante caracter\u00edsticas de uso compartido integradas. INSTALACION En este tutorial, instalar\u00e1 Grafana y la proteger\u00e1 con un certificado SSL y un proxy inverso de Nginx. Una vez que haya configurado Grafana, tendr\u00e1 la opci\u00f3n de configurar la autenticaci\u00f3n del usuario a trav\u00e9s de GitHub, lo que le permitir\u00e1 organizar mejor los permisos de su equipo. Podemos instalar plugins en grafana. Un caso muy usado es instalar Telegraf que es un motor que envia datos de monitorizacion a una base de datos InfluxDB para poder mostrar los datos de esta bbdd en grafana. Se pueden crear paneles y alertas. PROMETHEUS Prometheus es otra herramienta de metricas y alertas en tiempo real. Se suele conectar con grafana para una mejor visualizacion y mejor grafica. cADVISOR Sirve para monitoreo de estadisticas. Ejemplo en docker: sudo docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest Por defecto puerto 8080","title":"Monitorizacion"},{"location":"monitorizacion/#zabbix","text":"Zabbix es un software de monitorizaci\u00f3n de c\u00f3digo abierto para redes y aplicaciones. Ofrece monitorizaci\u00f3n en tiempo real de miles de m\u00e9tricas recogidas de servidores, equipos virtuales, dispositivos de red y aplicaciones web. Estas m\u00e9tricas pueden ayudarle a determinar el estado actual de su infraestructura TI y a detectar problemas con los componentes de hardware o software antes de que los clientes se quejen. La informaci\u00f3n \u00fatil se almacena en una base de datos para que pueda analizar datos a lo largo del tiempo y mejorar la calidad de los servicios proporcionados o planificar mejoras de su equipo. INSTALACION PASO A PASO Zabbix usa varias opciones para recoger m\u00e9tricas, incluyendo la monitorizaci\u00f3n sin agente de los servicios del usuario y de la arquitectura cliente-servidor. Para recoger m\u00e9tricas del servidor, utiliza un agente peque\u00f1o en el cliente monitorizado para recopilar datos y enviarlos al servidor Zabbix. Zabbix admite la comunicaci\u00f3n cifrada entre el servidor y los clientes conectados, de forma que sus datos est\u00e1n protegidos mientras recorren redes inseguras. El servidor Zabbix almacena sus datos en una base de datos relacional alimentada con MySQL o PostgreSQL. Tambi\u00e9n puede almacenar datos hist\u00f3ricos en bases de datos NoSQL como Elasticsearch y TimescaleDB. Zabbix ofrece una interfaz web para que pueda ver los datos y configurar los ajustes del sistema. En este tutorial, configurar\u00e1 Zabbix en dos equipos Ubuntu 20.04. Uno ser\u00e1 configurado como el servidor Zabbix y el otro como un cliente que monitorizar\u00e1. El servidor Zabbix usar\u00e1 una base de datos MySQL para registrar los datos de monitorizaci\u00f3n y utilizar\u00e1 Nginx para presentar la interfaz web.","title":"ZABBIX"},{"location":"monitorizacion/#naggios","text":"","title":"NAGGIOS"},{"location":"monitorizacion/#grafana","text":"Grafana es una herramienta de visualizaci\u00f3n y monitoreo de datos de c\u00f3digo abierto que se integra con datos complejos de fuentes como Prometheus, InfluxDB, Graphite y ElasticSearch. Grafana le permite crear alertas, notificaciones y filtros ad-hoc para sus datos, a la vez que facilita la colaboraci\u00f3n con los compa\u00f1eros de equipo mediante caracter\u00edsticas de uso compartido integradas. INSTALACION En este tutorial, instalar\u00e1 Grafana y la proteger\u00e1 con un certificado SSL y un proxy inverso de Nginx. Una vez que haya configurado Grafana, tendr\u00e1 la opci\u00f3n de configurar la autenticaci\u00f3n del usuario a trav\u00e9s de GitHub, lo que le permitir\u00e1 organizar mejor los permisos de su equipo. Podemos instalar plugins en grafana. Un caso muy usado es instalar Telegraf que es un motor que envia datos de monitorizacion a una base de datos InfluxDB para poder mostrar los datos de esta bbdd en grafana. Se pueden crear paneles y alertas.","title":"GRAFANA"},{"location":"monitorizacion/#prometheus","text":"Prometheus es otra herramienta de metricas y alertas en tiempo real. Se suele conectar con grafana para una mejor visualizacion y mejor grafica.","title":"PROMETHEUS"},{"location":"monitorizacion/#cadvisor","text":"Sirve para monitoreo de estadisticas. Ejemplo en docker: sudo docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest Por defecto puerto 8080","title":"cADVISOR"},{"location":"office/","text":"OFFICE 365 DOC OFFICE 365 Y APPS CURSO OFFICE 365 Microsoft 365 ayuda a los usuarios con la innovaci\u00f3n m\u00e1s reciente en experiencias de productividad nuevas y familiares, como Teams, Word, Excel, PowerPoint, Outlook y Windows. A diferencia de otros servicios de productividad, Microsoft 365 aprende de los usuarios y recopila informaci\u00f3n valiosa a trav\u00e9s de Microsoft Graph para ofrecer experiencias mejoradas que mejoran continuamente a lo largo del tiempo y mantienen protegidos a los usuarios. Microsoft 365 ayuda a las organizaciones con las siguientes caracter\u00edsticas: Productividad y trabajo en equipo: Incluye mensajer\u00eda instant\u00e1nea y reuniones en l\u00ednea con Microsoft Teams, correo electr\u00f3nico y calendarios con Outlook, aplicaciones conocidas de Office en todos los dispositivos, almacenamiento avanzado y uso compartido de archivos con OneDrive para la Empresa, sitios de grupo e intranet, y redes sociales empresariales con Yammer. Administraci\u00f3n de empresaS: Incluye administraci\u00f3n de TI simplificada con Microsoft Endpoint Manager, automatizaci\u00f3n de procesos empresariales, extensibilidad con Teams y Power Platform, telefon\u00eda empresarial y sistema telef\u00f3nico con Teams, administraci\u00f3n de flujos de trabajo y Formularios, inteligencia empresarial con Workplace Analytics y administraci\u00f3n del trabajo con Project Online. Seguridad y cumplimiento: Incluye soluciones de administraci\u00f3n de identidad y acceso, control y protecci\u00f3n de la informaci\u00f3n, protecci\u00f3n contra amenazas, administraci\u00f3n de seguridad, administraci\u00f3n de riesgos internos, administraci\u00f3n de cumplimiento y eDiscovery. Algunos de los componentes de Microsoft 365, como las Aplicaciones de Microsoft 365 y Windows, se entregan con el modelo de software como servicio (SaaS). SaaS es un software que un proveedor de servicios en la nube (CSP) hospeda y administra de forma centralizada para clientes. En general, los CSP proporcionan una versi\u00f3n de una aplicaci\u00f3n para todos los clientes que otorgan mediante una suscripci\u00f3n mensual o anual. Ventajas: Permitir el trabajo en equipo y simplificar el flujo de trabajo:Colaborar, reunirse, llamar y conectar aplicaciones empresariales en un \u00fanico sitio con Microsoft Teams. Productividad desde cualquier lugar: Pase f\u00e1cilmente de ordenadores a dispositivos m\u00f3viles con aplicaciones m\u00f3viles innovadoras y eficaces. Mayor productividad con las herramientas habilitadas para IA: Potencie la creatividad, descubra nuevas perspectivas, mejore las b\u00fasquedas y obtenga asistencia personalizada con caracter\u00edsticas de inteligencia integradas. Productividad en la organizaci\u00f3n: Las organizaciones siempre intentan destacar en un entorno comercial en constante evoluci\u00f3n. Quieren impulsar el crecimiento, reducir los costos y servir mejor a sus clientes. Quieren desbloquear el potencial de sus empleados, impulsar la automatizaci\u00f3n de procesos, capturar el conocimiento colectivo de su organizaci\u00f3n y evitar posibles riesgos de seguridad, cumplimiento normativo y privacidad que puedan interferir en el progreso. Aumentar los conocimientos de la organizaci\u00f3n: Convierta r\u00e1pidamente los datos en perspectivas y ofrezca a los empleados la informaci\u00f3n y la experiencia que necesitan para realizar su trabajo con Workplace Analytics. Administrar todos los puntos de conexi\u00f3n: Implemente una soluci\u00f3n de administraci\u00f3n fluida de un extremo a otro y mejore la visibilidad entre todos los dispositivos conectados con Microsoft Endpoint Manager. Proteger su empresa: Eleve y modernice su seguridad, administre los riesgos y cumpla los est\u00e1ndares de cumplimiento en la nube de confianza de Microsoft. SUSCRIPCIONES Microsoft 365 Enterprise Microsoft 365 Enterprise ofrece servicios de clase empresarial para organizaciones que quieren una soluci\u00f3n de productividad que incluya caracter\u00edsticas seguras de protecci\u00f3n contra amenazas, seguridad, cumplimiento de normas y an\u00e1lisis. Hay tres planes disponibles de Microsoft 365 Enterprise, que le permiten ajustar a\u00fan m\u00e1s lo que se incluye en su implementaci\u00f3n: E3, E5 y F3 (anteriormente conocido como F1). E5 incluye las mismas caracter\u00edsticas que E3 y las herramientas m\u00e1s recientes de protecci\u00f3n contra amenazas avanzada, seguridad y colaboraci\u00f3n. F3 est\u00e1 dise\u00f1ado para trabajadores de primera l\u00ednea mediante recursos y herramientas dedicadas que les permiten dar lo mejor de s\u00ed. Microsoft 365 para empresas Microsoft 365 para empresas est\u00e1 dise\u00f1ado para peque\u00f1as y medianas organizaciones. Como Microsoft 365 Enterprise, Microsoft 365 para empresas ofrece el conjunto completo de herramientas de productividad de Office 365 e incluye caracter\u00edsticas de seguridad y administraci\u00f3n de dispositivos. No incluye algunas de las herramientas de protecci\u00f3n de la informaci\u00f3n, cumplimiento y an\u00e1lisis m\u00e1s avanzadas disponibles para los suscriptores de Enterprise. Se ha dise\u00f1ado para organizaciones que necesitan hasta 300 licencias. Si su organizaci\u00f3n es m\u00e1s grande, tendr\u00e1 que suscribirse a un plan de Microsoft 365 Enterprise. Microsoft 365 Educaci\u00f3n Microsoft 365 Educaci\u00f3n est\u00e1 disponible para organizaciones educativas y permite a los profesores dar rienda suelta a la creatividad, fomentar el trabajo en equipo y proporcionar una experiencia segura y sencilla en una \u00fanica soluci\u00f3n econ\u00f3mica dise\u00f1ada para el \u00e1mbito educativo. Las licencias acad\u00e9micas se pueden modificar para adaptarse a las necesidades de cualquier instituci\u00f3n, incluidas soluciones de productividad y seguridad para profesores, miembros del personal y estudiantes. Microsoft 365 Hogar Microsoft 365 Hogar tiene el prop\u00f3sito de ofrecer las mismas ventajas de productividad en su vida personal y familiar. Microsoft 365 Hogar tiene dos planes: Microsoft 365 Familia y Microsoft 365 Personal. Office Home y Estudiantes 2019 est\u00e1n disponibles como compras de pago \u00fanico, pero no incluyen ninguno de los beneficios de la nube de Microsoft 365. Compare planes para ver el plan que mejor le convenga. CREACIONES GRUPOS y AD Ejercicio 1: Iniciar sesi\u00f3n en el espacio empresarial Abra Microsoft Edge. Vaya a www.office.com. Inicie sesi\u00f3n con las credenciales de la cuenta de administrador global de su espacio empresarial de Office 365. Consulte la introducci\u00f3n al Laboratorio para adquirir un espacio empresarial de prueba de Office 365. Haga clic en el icono Administraci\u00f3n. Ejercicio 2: Explorar el Centro de administraci\u00f3n de Microsoft 365 En el Centro de administraci\u00f3n de Microsoft 365, en el panel de navegaci\u00f3n, seleccione Mostrar todo. Expanda Usuarios y seleccione Usuarios activos. Vea las cuentas disponibles. Haga clic en el primer nombre de usuario de la lista para seleccionarlo. Se abre una hoja en la que se muestra informaci\u00f3n m\u00e1s detallada sobre la cuenta. Para cerrar la hoja, seleccione la X en la esquina superior derecha de la hoja. Expanda Grupos y seleccione Grupos. Si usa una versi\u00f3n de prueba de espacio empresarial de Office 365 reci\u00e9n creada, esta p\u00e1gina probablemente estar\u00e1 vac\u00eda. Si a\u00fan no tiene grupos, haga clic en Agregar un grupo para agregar uno. Expanda Facturaci\u00f3n y seleccione Licencias. Se deber\u00eda mostrar al menos un conjunto de licencias. Ejercicio 3: Explorar el Centro de administraci\u00f3n de Azure Active Directory Expanda centros de administraci\u00f3n y seleccione Azure Active Directory. Observe que se abre una pesta\u00f1a nueva en Microsoft Edge. En el Centro de administraci\u00f3n de Azure Active Directory, en el Panel, seleccione Azure Active Directory en el panel de navegaci\u00f3n. Haga clic en Usuarios. Observe que se muestran las mismas cuentas de usuario de Office 365. Cierre la hoja Usuarios: Todos los usuarios. Observe que en el panel del \u00e1rea Usuarios y grupos se muestra el grupo que cre\u00f3 anteriormente. Puede ver los mismos grupos de Office 365. Puede hacer clic en Buscar un grupo en el \u00e1rea Tareas r\u00e1pidas para buscar un grupo espec\u00edfico. Cierre la hoja Grupos: Todos los grupos. En el panel del Centro de administraci\u00f3n de Azure Active Directory, haga clic en Personalizaci\u00f3n de marca de la empresa. Observe las opciones configuradas para la personalizaci\u00f3n de marca. Cierre la hoja de personalizaci\u00f3n de marca de la empresa. APLICACIONES 365 \u2013 Word \u2013 El famoso procesador de textos \u2013 Excel \u2013 La potente hoja de c\u00e1lculo \u2013 Outlook \u2013 El servicio de correo de Microsoft \u2013 PowerPoint \u2013 El programa para realizar presentaciones \u2013 Access \u2013 Para la creaci\u00f3n de bases de datos \u2013 Skype \u2013 el conocido programa para llamadas VOIP y videollamadas \u2013 OneNote \u2013 una magn\u00edfica aplicaci\u00f3n para tomar notas \u2013 OneDrive \u2013 El servicio en la nube de Microsoft, gratuito pero que con Office 365 se incluye un TB adicional. \u2013 Publisher \u2013 Un software de autoedici\u00f3n para crear folletos, boletines, etc. *** EXCHANGE + Trabaja de forma m\u00e1s inteligente con calendario y correo electr\u00f3nico de categor\u00eda empresarial. + Exchange te ayuda a colaborar en los documentos cr\u00edticos y te proporciona la Bandeja de entrada Prioritarios, que muestra primero los mensajes importantes y se adapta a tu estilo de trabajo para que mejores tu productividad. + Obt\u00e9n acceso a una bandeja de entrada m\u00e1s personalizada con caracter\u00edsticas \u00fatiles y una forma m\u00e1s inteligente y organizada de ver el correo electr\u00f3nico y de realizar con \u00e9l las acciones oportunas. Las mejoras de b\u00fasqueda te ofrecen resultados m\u00e1s r\u00e1pidos y completos. Con los complementos, obtendr\u00e1s una personalizaci\u00f3n y una extensibilidad avanzadas, que te pondr\u00e1n en contacto con servicios modernos y aplicaciones internas de l\u00ednea de negocio. + Organiza tu tiempo con un sistema de calendario que va m\u00e1s all\u00e1 de la programaci\u00f3n b\u00e1sica de citas y compromisos. Captura autom\u00e1ticamente los eventos del correo electr\u00f3nico, como vuelos y reservas de hotel. Y obt\u00e9n sugerencias sobre los lugares en los que puedes reunirte en funci\u00f3n de cu\u00e1l sea tu ubicaci\u00f3n. Aplicaciones web \u00bfCu\u00e1les son todas las aplicaciones que vienen con Office 365? La gran novedad de Office 365 es que algunas de estas aplicaciones tienen una versi\u00f3n web que no son exactamente como la versi\u00f3n de escritorio. Por un lado, no utilizan nuestro disco duro para archivar los documentos creados, si no que se almacenan autom\u00e1ticamente en OneDrive, por lo que podemos acceder a ellos desde cualquier lugar y dispositivo. Por otra parte, las aplicaciones web est\u00e1n conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, favoreciendo la sincronizaci\u00f3n de contenidos. Por otra parte, estas aplicaciones web est\u00e1n mejor conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, con una mejor sincronizaci\u00f3n. Estas son las aplicaciones web que incluye Office 365: Word, Excel, OneNote, Skype y PowerPoint \u2013 b\u00e1sicamente son iguales a las versiones de escritorio, con algunas peque\u00f1as diferencias. Outlook \u2013 Aqu\u00ed comienzan las diferencias: algunas funciones de la versi\u00f3n de escritorio aqu\u00ed aparecen como funciones independientes: \u2013 Contactos \u2013 Calendario \u2013 Tareas Sway \u2013 Una aplicaci\u00f3n para crear informes, presentaciones, recursos de aprendizaje, etc. Flow \u2013 Una herramienta para automatizar tareas, capaz de trabajar con m\u00e1s de 200 servicios entre los que se incluyen Facebook, Instagram, Twitter, Dropbox, etc. Estas son las aplicaciones que podemos encontrar en Office 365. Adem\u00e1s, existen m\u00e1s servicios a\u00f1adidos como soporte t\u00e9cnico, actualizaciones autom\u00e1ticas, etc. APLICACIONES ADICIONALES Aplicaciones y servicios adicionales: Access (solo PC) Advanced Threat Analytics1 Bookings Cloud App Security Microsoft Edge Enterprise Mobility + Security Exchange Forms Intune Microsoft 365 Defender Microsoft Defender para punto de conexi\u00f3n Microsoft Defender for Identity Microsoft Defender para Office 365 Editor Microsoft Microsoft Family Safety Microsoft Lists Microsoft Stream Microsoft To Do Aplicaciones m\u00f3viles MyAnalytics Planner Power Apps Power Automate Project Publisher (solo PC) Microsoft Endpoint Manager SharePoint SharePoint Syntex Skype Skype Empresarial Sway Visio Whiteboard Windows Workplace Analytics Yammer ADMIN Agregamos un dominio en el portal de admin de 365. Configuraci\u00f3n - dominios - agregar dominio - nombre - crear registro dns - no agregar servicios. Creamos un usuario con usuarios - usuario activo - rellenar datos. Para asignar licencias se selecciona el contacto y en licencias, se le a\u00f1ade licencia y aplicaciones. Tambien en este apartado podemos modificar nombre, datos de perfil de usuario. Para eliminar un usuario, selecionamos y eliminamos, pero se quedan 30 dias en elimnados. Para eliminar permanentemente, vamos a mostrar todo - azure active directory - usuarios - usuarios eliminados y eliminar total. Cambiamos contrase\u00f1a selecionando usuario y le damos arriba a restablecer contrase\u00f1a, Si queremos que lo haga un usuario de manera normal. Configuracion - conf de la organizacion - seguridad y privacidad - autoservicio de restablecimiento de contrase\u00f1a - vaya al portal de azure - y selecionamos que opcion coger. Podemos activar la verificaci\u00f3n de dos pasos: usuario - usuario activo y damos autenticaci\u00f3n multifactor y elegios usuario. Otra manera para toda la organizacion: mostrar todo - portal azure AD - azure AD - propiedades - admiinistracion valores predeterminados de seguridad(activar o no). Bloquear acceso a usuario. Selecionamos usuario y arriba bloquear inicio de sesion. Crear un grupo: grupos - crear nuevo grupo activo - agregar - tipo de grupo y selecionamos miembros. Crear un buzon compartido: grupos - buzones compartidos - agregar nuevo - nombre y despues selecionamos los miembros. Cambiar caducidad contrase\u00f1as: configuracion - conf de la organizacion - seguridad y privacidad - directiva de expiraci\u00f3n de contrase\u00f1as y cambiamos. Comprobar estado del servicio: mostrar todo - mantenimiento - estado del servicio. Lo mismo pero centro de mensajes, nos indica las novedades que vendran de actualizaciones. Contactar con soporte Microsoft: mostrar todo - soporte tecnico - nueva solicitud. Poner alias de email donde varios emails llegar\u00e1n al email principal, solo sirven para llegadas, para enviar solo desde el principal. Usuarios - usuarios activos - adminisrar nombre y email y ponemos alias. Habilitar usuarios invitados: configuracion - conf de la organizacion - servicios - grupos de 365 y activar las dos pesta\u00f1as. La otra opcion: ir al a teams - portal teams - configuracion de toda la org - acceso de invitado y activado. La otra es ir al portal de Azure AD - azure AD - informacion general - external identities - conf de coloboracion externa y activamos lo que toque. Sharepoint - directivas - uso comportido - activar lo primero. Cancelar suscripion 365: facturaci\u00f3n - sus productos - selecionamos - tres puntos - cancelar. SOLUCIONES DE FALLOS OFFICE 365 Ventajas de adquirir una licencia: Siempre version actualizada Tambien para moviles Integracion con la nube Versiones de escritorio de acces y publisher 1Tb en onedrive. Inicio de sesion Al portal correcto de office www.office.com / www.admin.microsoft.com Guardarselo en favoritos Cuenta expirada/bloqueada/olvidada Soluciones: Ir a usuarios del portal y ver el estado de usuarios bloequeados y darle a desbloquear. Si olvida su contrase\u00f1a vamos a usuarios y le damos a restablecer contrase\u00f1a. Puede ser que no tenga licencia, editamos usuario y le damos licencia. Podemos bloquear tambien el inicio de sesion de alguien. Conectividad Herramienta de diagnostico de windows Probar ping a un sitio Mirar comando tracer al destino Pathping nslookup para mirar el dns Soluciones: Damos nueva ip ipconfig /release y luego /renew ping localhost / ping google.com nslookup para ver nuestro dns y luego probamos poniendo direcciones(podemos poner registros set type=MX). tracert www.linkedin.com para que nos resuelva a una ip y los saltos correctos hasta llegar correctamente al destino pasando por routers. pathping -n www.linkedin.com para hacer lo mismo que tracer mas estadisticas extras de si falla en los saltos. ipconfig /flushdns para liberar toda la cache de dns ipconfig /displaydns para ver la cache del dns ipconfig /registerdns para forzar el registro Correo electronico Fallos de conectividad o te pide constante la contrase\u00f1a para entrar. Tenemos que tener acceso a internet, comunicacion online con exchange, el firewall permitir trafico de office y acceso a trav\u00e9s de puertos 443/80. Los servidores exchange almacenas los buzones de correo. Herramientas de solucion: Hacer un ping de conectividad del cliente a outlook.office365.com Revisar la configuracion de outlook www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. www.testconnectivity.microsoft.com web con una coleccion de herramientas para ver posibles fallos de conectividad con exchange, ews, office 365, internos, etc. Aplicaciones office365 Aplicaciones que se detienen o bloquean Mensaje de error al abrir office Activacion de la licencia Fallo de inicio de sesion Dentro de por ejemplo el word Inicio - Cuenta, se puede activar las actualizaciones En panel control - programas, seleccionamos y le damos a cambiar y podemos reparar en linea. Creamos usuario y vamos a office.com, iniciamos sesion y vamos logueando en las aplicaciones para comprobar conectividad. Problemas: www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. CMD: outlook/profile y podemos crear un nuevo perfil de outlook. En mi buzon outllok darle a mi nombre email, boton derecho y mirar permisos En inicio, informacion, confguracion de cuenta, delegar permisos a otros usuarios para hacerlo en mi nombre. Sharepoint Nube para subir archivos y compartir con los otros usuarios. Problemas de navegador, servicio, no poder instalar el controlador ActiveX. Problemas: En el centro de administracion de office 365, que est\u00e1 al entrar en office.com, en mantenimiento - estado del servicio En directivas de grupo, podemos incorporar sharepoint como sitio de confianza. En los admin s\u00ed pueden restaurar papeleras de reciclaje ya eliminadas por el usuario, maximo 93 dias (SHAREPOINT - COLECCION DE SITIOS). www.protection.office.com - gobierno de datos - retencion. Creamos una poniendo el tiempo que queremos de borrado de archivos. Onedrive Almacenaje en la nube. No mas de 10GB. No repetir nombres, caracteres invalidos. Herramientas: onedrive.exe /reset en CMD para resetear la conf de onedrive www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. Actualizar version En los admin s\u00ed pueden restaurar papeleras de reciclaje ya eliminadas por el usuario, maximo 93 dias (SHAREPOINT - COLECCION DE SITIOS). Restaurar sitios completos eliminadors, powershell: Restore-SPDeletedSite Skype/Teams Chat de grupo, videollamadas, ficheros, etc Problemas de calidad sonido/video, red, firewall, puertos... Herramientas: Verificar conf micro/camara, cerrar otras apps, internet malo, actualizar hardware/software Puertos por encima de 1024 skype o 80/443 y en teams 80/443 o 3478/3481. En el centro de admin de office, en mantenimiento y estado para ver si hay fallos. En el mismo crento de admin, ir a teams - calidad de llamada Centro de informes En google: informes de actividad de office365 o en el centro de admin de 365. centro de admin - informes - uso y veremos los informes de cada cosa. Dentro de aqui podemos ver cada aplicacion, dias, filtros de columnas, exportar,etc. centro de admin - informes - seguridad hay dos sitios url donde ir para ver informes de exchange(admin de cumpliliemto-auditoria) y 365(informes). Mejor visualizacion y programar hora de informes. Para ocultar los nombres de los usuarios, podemos ir a configuracion - servicio y complementos - informes y ponerlo animo, todo esto en el centro d admin 365. Monitoreo de estado de servicio En el centro de administracion de office 365, que est\u00e1 al entrar en office.com, en mantenimiento - estado del servicio. Podemos ver si hay incidencias en nuestros servicios. En el centro de mensajes podemos personalizar los mensajes, apps, etc. En los usuarios podemos seleccionar uno y en el apartado de roles podemos editar y darle acceso a que pueda ver los estados d servicio, mensajes etc. OFFICE 365 MANAGEMENT PACK es una herramienta m\u00e1s que se puede descargar para monitorear servicios, mensajes, a\u00f1adir aletar, suscripciones. Tiene que ser en windows server y tener System Center Operation managers antes. Desde powershell con el comando de documentacion Search-AdminAuditLog Podemos pedir ayuda en el centro d admin por preguntas o correo electronico. Remote Connectivity Analyzer: www.testconnectivity.microsoft.com web con una coleccion de herramientas para ver posibles fallos de conectividad con exchange, ews, office 365, internos, etc. En el pesta\u00f1a cliente podemos descargar tambien la herramienta localmente. En wwww.aka.ms/hybridwizard es un asistente para soluciones hibridas. www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. Enterprise Mobility + Security E5 Enterprise Mobility + Security Suite: Es una robusta plataforma de seguridad, basada en la identidad dise\u00f1ada para ayudar a las empresas a administrar y proteger sus dispositivos, aplicaciones y datos dentro de la empresa. Es robusta porque se crean \u201cl\u00edneas de defensa\u201d virtuales que permite atrapar las incidencias en alguna de ellas. Realmente tendr\u00e1s a la mano herramientas de seguridad con las que podr\u00e1s evitar el robo de informaci\u00f3n ya sea intencional o por accidente, as\u00ed como evitar el robo por descuido de los empleados, entre otros.","title":"Office 365"},{"location":"office/#office-365","text":"DOC OFFICE 365 Y APPS CURSO OFFICE 365 Microsoft 365 ayuda a los usuarios con la innovaci\u00f3n m\u00e1s reciente en experiencias de productividad nuevas y familiares, como Teams, Word, Excel, PowerPoint, Outlook y Windows. A diferencia de otros servicios de productividad, Microsoft 365 aprende de los usuarios y recopila informaci\u00f3n valiosa a trav\u00e9s de Microsoft Graph para ofrecer experiencias mejoradas que mejoran continuamente a lo largo del tiempo y mantienen protegidos a los usuarios. Microsoft 365 ayuda a las organizaciones con las siguientes caracter\u00edsticas: Productividad y trabajo en equipo: Incluye mensajer\u00eda instant\u00e1nea y reuniones en l\u00ednea con Microsoft Teams, correo electr\u00f3nico y calendarios con Outlook, aplicaciones conocidas de Office en todos los dispositivos, almacenamiento avanzado y uso compartido de archivos con OneDrive para la Empresa, sitios de grupo e intranet, y redes sociales empresariales con Yammer. Administraci\u00f3n de empresaS: Incluye administraci\u00f3n de TI simplificada con Microsoft Endpoint Manager, automatizaci\u00f3n de procesos empresariales, extensibilidad con Teams y Power Platform, telefon\u00eda empresarial y sistema telef\u00f3nico con Teams, administraci\u00f3n de flujos de trabajo y Formularios, inteligencia empresarial con Workplace Analytics y administraci\u00f3n del trabajo con Project Online. Seguridad y cumplimiento: Incluye soluciones de administraci\u00f3n de identidad y acceso, control y protecci\u00f3n de la informaci\u00f3n, protecci\u00f3n contra amenazas, administraci\u00f3n de seguridad, administraci\u00f3n de riesgos internos, administraci\u00f3n de cumplimiento y eDiscovery. Algunos de los componentes de Microsoft 365, como las Aplicaciones de Microsoft 365 y Windows, se entregan con el modelo de software como servicio (SaaS). SaaS es un software que un proveedor de servicios en la nube (CSP) hospeda y administra de forma centralizada para clientes. En general, los CSP proporcionan una versi\u00f3n de una aplicaci\u00f3n para todos los clientes que otorgan mediante una suscripci\u00f3n mensual o anual. Ventajas: Permitir el trabajo en equipo y simplificar el flujo de trabajo:Colaborar, reunirse, llamar y conectar aplicaciones empresariales en un \u00fanico sitio con Microsoft Teams. Productividad desde cualquier lugar: Pase f\u00e1cilmente de ordenadores a dispositivos m\u00f3viles con aplicaciones m\u00f3viles innovadoras y eficaces. Mayor productividad con las herramientas habilitadas para IA: Potencie la creatividad, descubra nuevas perspectivas, mejore las b\u00fasquedas y obtenga asistencia personalizada con caracter\u00edsticas de inteligencia integradas. Productividad en la organizaci\u00f3n: Las organizaciones siempre intentan destacar en un entorno comercial en constante evoluci\u00f3n. Quieren impulsar el crecimiento, reducir los costos y servir mejor a sus clientes. Quieren desbloquear el potencial de sus empleados, impulsar la automatizaci\u00f3n de procesos, capturar el conocimiento colectivo de su organizaci\u00f3n y evitar posibles riesgos de seguridad, cumplimiento normativo y privacidad que puedan interferir en el progreso. Aumentar los conocimientos de la organizaci\u00f3n: Convierta r\u00e1pidamente los datos en perspectivas y ofrezca a los empleados la informaci\u00f3n y la experiencia que necesitan para realizar su trabajo con Workplace Analytics. Administrar todos los puntos de conexi\u00f3n: Implemente una soluci\u00f3n de administraci\u00f3n fluida de un extremo a otro y mejore la visibilidad entre todos los dispositivos conectados con Microsoft Endpoint Manager. Proteger su empresa: Eleve y modernice su seguridad, administre los riesgos y cumpla los est\u00e1ndares de cumplimiento en la nube de confianza de Microsoft.","title":"OFFICE 365"},{"location":"office/#suscripciones","text":"Microsoft 365 Enterprise Microsoft 365 Enterprise ofrece servicios de clase empresarial para organizaciones que quieren una soluci\u00f3n de productividad que incluya caracter\u00edsticas seguras de protecci\u00f3n contra amenazas, seguridad, cumplimiento de normas y an\u00e1lisis. Hay tres planes disponibles de Microsoft 365 Enterprise, que le permiten ajustar a\u00fan m\u00e1s lo que se incluye en su implementaci\u00f3n: E3, E5 y F3 (anteriormente conocido como F1). E5 incluye las mismas caracter\u00edsticas que E3 y las herramientas m\u00e1s recientes de protecci\u00f3n contra amenazas avanzada, seguridad y colaboraci\u00f3n. F3 est\u00e1 dise\u00f1ado para trabajadores de primera l\u00ednea mediante recursos y herramientas dedicadas que les permiten dar lo mejor de s\u00ed. Microsoft 365 para empresas Microsoft 365 para empresas est\u00e1 dise\u00f1ado para peque\u00f1as y medianas organizaciones. Como Microsoft 365 Enterprise, Microsoft 365 para empresas ofrece el conjunto completo de herramientas de productividad de Office 365 e incluye caracter\u00edsticas de seguridad y administraci\u00f3n de dispositivos. No incluye algunas de las herramientas de protecci\u00f3n de la informaci\u00f3n, cumplimiento y an\u00e1lisis m\u00e1s avanzadas disponibles para los suscriptores de Enterprise. Se ha dise\u00f1ado para organizaciones que necesitan hasta 300 licencias. Si su organizaci\u00f3n es m\u00e1s grande, tendr\u00e1 que suscribirse a un plan de Microsoft 365 Enterprise. Microsoft 365 Educaci\u00f3n Microsoft 365 Educaci\u00f3n est\u00e1 disponible para organizaciones educativas y permite a los profesores dar rienda suelta a la creatividad, fomentar el trabajo en equipo y proporcionar una experiencia segura y sencilla en una \u00fanica soluci\u00f3n econ\u00f3mica dise\u00f1ada para el \u00e1mbito educativo. Las licencias acad\u00e9micas se pueden modificar para adaptarse a las necesidades de cualquier instituci\u00f3n, incluidas soluciones de productividad y seguridad para profesores, miembros del personal y estudiantes. Microsoft 365 Hogar Microsoft 365 Hogar tiene el prop\u00f3sito de ofrecer las mismas ventajas de productividad en su vida personal y familiar. Microsoft 365 Hogar tiene dos planes: Microsoft 365 Familia y Microsoft 365 Personal. Office Home y Estudiantes 2019 est\u00e1n disponibles como compras de pago \u00fanico, pero no incluyen ninguno de los beneficios de la nube de Microsoft 365. Compare planes para ver el plan que mejor le convenga.","title":"SUSCRIPCIONES"},{"location":"office/#creaciones-grupos-y-ad","text":"Ejercicio 1: Iniciar sesi\u00f3n en el espacio empresarial Abra Microsoft Edge. Vaya a www.office.com. Inicie sesi\u00f3n con las credenciales de la cuenta de administrador global de su espacio empresarial de Office 365. Consulte la introducci\u00f3n al Laboratorio para adquirir un espacio empresarial de prueba de Office 365. Haga clic en el icono Administraci\u00f3n. Ejercicio 2: Explorar el Centro de administraci\u00f3n de Microsoft 365 En el Centro de administraci\u00f3n de Microsoft 365, en el panel de navegaci\u00f3n, seleccione Mostrar todo. Expanda Usuarios y seleccione Usuarios activos. Vea las cuentas disponibles. Haga clic en el primer nombre de usuario de la lista para seleccionarlo. Se abre una hoja en la que se muestra informaci\u00f3n m\u00e1s detallada sobre la cuenta. Para cerrar la hoja, seleccione la X en la esquina superior derecha de la hoja. Expanda Grupos y seleccione Grupos. Si usa una versi\u00f3n de prueba de espacio empresarial de Office 365 reci\u00e9n creada, esta p\u00e1gina probablemente estar\u00e1 vac\u00eda. Si a\u00fan no tiene grupos, haga clic en Agregar un grupo para agregar uno. Expanda Facturaci\u00f3n y seleccione Licencias. Se deber\u00eda mostrar al menos un conjunto de licencias. Ejercicio 3: Explorar el Centro de administraci\u00f3n de Azure Active Directory Expanda centros de administraci\u00f3n y seleccione Azure Active Directory. Observe que se abre una pesta\u00f1a nueva en Microsoft Edge. En el Centro de administraci\u00f3n de Azure Active Directory, en el Panel, seleccione Azure Active Directory en el panel de navegaci\u00f3n. Haga clic en Usuarios. Observe que se muestran las mismas cuentas de usuario de Office 365. Cierre la hoja Usuarios: Todos los usuarios. Observe que en el panel del \u00e1rea Usuarios y grupos se muestra el grupo que cre\u00f3 anteriormente. Puede ver los mismos grupos de Office 365. Puede hacer clic en Buscar un grupo en el \u00e1rea Tareas r\u00e1pidas para buscar un grupo espec\u00edfico. Cierre la hoja Grupos: Todos los grupos. En el panel del Centro de administraci\u00f3n de Azure Active Directory, haga clic en Personalizaci\u00f3n de marca de la empresa. Observe las opciones configuradas para la personalizaci\u00f3n de marca. Cierre la hoja de personalizaci\u00f3n de marca de la empresa.","title":"CREACIONES GRUPOS y AD"},{"location":"office/#aplicaciones-365","text":"\u2013 Word \u2013 El famoso procesador de textos \u2013 Excel \u2013 La potente hoja de c\u00e1lculo \u2013 Outlook \u2013 El servicio de correo de Microsoft \u2013 PowerPoint \u2013 El programa para realizar presentaciones \u2013 Access \u2013 Para la creaci\u00f3n de bases de datos \u2013 Skype \u2013 el conocido programa para llamadas VOIP y videollamadas \u2013 OneNote \u2013 una magn\u00edfica aplicaci\u00f3n para tomar notas \u2013 OneDrive \u2013 El servicio en la nube de Microsoft, gratuito pero que con Office 365 se incluye un TB adicional. \u2013 Publisher \u2013 Un software de autoedici\u00f3n para crear folletos, boletines, etc. *** EXCHANGE + Trabaja de forma m\u00e1s inteligente con calendario y correo electr\u00f3nico de categor\u00eda empresarial. + Exchange te ayuda a colaborar en los documentos cr\u00edticos y te proporciona la Bandeja de entrada Prioritarios, que muestra primero los mensajes importantes y se adapta a tu estilo de trabajo para que mejores tu productividad. + Obt\u00e9n acceso a una bandeja de entrada m\u00e1s personalizada con caracter\u00edsticas \u00fatiles y una forma m\u00e1s inteligente y organizada de ver el correo electr\u00f3nico y de realizar con \u00e9l las acciones oportunas. Las mejoras de b\u00fasqueda te ofrecen resultados m\u00e1s r\u00e1pidos y completos. Con los complementos, obtendr\u00e1s una personalizaci\u00f3n y una extensibilidad avanzadas, que te pondr\u00e1n en contacto con servicios modernos y aplicaciones internas de l\u00ednea de negocio. + Organiza tu tiempo con un sistema de calendario que va m\u00e1s all\u00e1 de la programaci\u00f3n b\u00e1sica de citas y compromisos. Captura autom\u00e1ticamente los eventos del correo electr\u00f3nico, como vuelos y reservas de hotel. Y obt\u00e9n sugerencias sobre los lugares en los que puedes reunirte en funci\u00f3n de cu\u00e1l sea tu ubicaci\u00f3n. Aplicaciones web \u00bfCu\u00e1les son todas las aplicaciones que vienen con Office 365? La gran novedad de Office 365 es que algunas de estas aplicaciones tienen una versi\u00f3n web que no son exactamente como la versi\u00f3n de escritorio. Por un lado, no utilizan nuestro disco duro para archivar los documentos creados, si no que se almacenan autom\u00e1ticamente en OneDrive, por lo que podemos acceder a ellos desde cualquier lugar y dispositivo. Por otra parte, las aplicaciones web est\u00e1n conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, favoreciendo la sincronizaci\u00f3n de contenidos. Por otra parte, estas aplicaciones web est\u00e1n mejor conectadas entre s\u00ed, de forma que funcionan mucho mejor como suite, con una mejor sincronizaci\u00f3n. Estas son las aplicaciones web que incluye Office 365: Word, Excel, OneNote, Skype y PowerPoint \u2013 b\u00e1sicamente son iguales a las versiones de escritorio, con algunas peque\u00f1as diferencias. Outlook \u2013 Aqu\u00ed comienzan las diferencias: algunas funciones de la versi\u00f3n de escritorio aqu\u00ed aparecen como funciones independientes: \u2013 Contactos \u2013 Calendario \u2013 Tareas Sway \u2013 Una aplicaci\u00f3n para crear informes, presentaciones, recursos de aprendizaje, etc. Flow \u2013 Una herramienta para automatizar tareas, capaz de trabajar con m\u00e1s de 200 servicios entre los que se incluyen Facebook, Instagram, Twitter, Dropbox, etc. Estas son las aplicaciones que podemos encontrar en Office 365. Adem\u00e1s, existen m\u00e1s servicios a\u00f1adidos como soporte t\u00e9cnico, actualizaciones autom\u00e1ticas, etc.","title":"APLICACIONES 365"},{"location":"office/#aplicaciones-adicionales","text":"Aplicaciones y servicios adicionales: Access (solo PC) Advanced Threat Analytics1 Bookings Cloud App Security Microsoft Edge Enterprise Mobility + Security Exchange Forms Intune Microsoft 365 Defender Microsoft Defender para punto de conexi\u00f3n Microsoft Defender for Identity Microsoft Defender para Office 365 Editor Microsoft Microsoft Family Safety Microsoft Lists Microsoft Stream Microsoft To Do Aplicaciones m\u00f3viles MyAnalytics Planner Power Apps Power Automate Project Publisher (solo PC) Microsoft Endpoint Manager SharePoint SharePoint Syntex Skype Skype Empresarial Sway Visio Whiteboard Windows Workplace Analytics Yammer","title":"APLICACIONES ADICIONALES"},{"location":"office/#admin","text":"Agregamos un dominio en el portal de admin de 365. Configuraci\u00f3n - dominios - agregar dominio - nombre - crear registro dns - no agregar servicios. Creamos un usuario con usuarios - usuario activo - rellenar datos. Para asignar licencias se selecciona el contacto y en licencias, se le a\u00f1ade licencia y aplicaciones. Tambien en este apartado podemos modificar nombre, datos de perfil de usuario. Para eliminar un usuario, selecionamos y eliminamos, pero se quedan 30 dias en elimnados. Para eliminar permanentemente, vamos a mostrar todo - azure active directory - usuarios - usuarios eliminados y eliminar total. Cambiamos contrase\u00f1a selecionando usuario y le damos arriba a restablecer contrase\u00f1a, Si queremos que lo haga un usuario de manera normal. Configuracion - conf de la organizacion - seguridad y privacidad - autoservicio de restablecimiento de contrase\u00f1a - vaya al portal de azure - y selecionamos que opcion coger. Podemos activar la verificaci\u00f3n de dos pasos: usuario - usuario activo y damos autenticaci\u00f3n multifactor y elegios usuario. Otra manera para toda la organizacion: mostrar todo - portal azure AD - azure AD - propiedades - admiinistracion valores predeterminados de seguridad(activar o no). Bloquear acceso a usuario. Selecionamos usuario y arriba bloquear inicio de sesion. Crear un grupo: grupos - crear nuevo grupo activo - agregar - tipo de grupo y selecionamos miembros. Crear un buzon compartido: grupos - buzones compartidos - agregar nuevo - nombre y despues selecionamos los miembros. Cambiar caducidad contrase\u00f1as: configuracion - conf de la organizacion - seguridad y privacidad - directiva de expiraci\u00f3n de contrase\u00f1as y cambiamos. Comprobar estado del servicio: mostrar todo - mantenimiento - estado del servicio. Lo mismo pero centro de mensajes, nos indica las novedades que vendran de actualizaciones. Contactar con soporte Microsoft: mostrar todo - soporte tecnico - nueva solicitud. Poner alias de email donde varios emails llegar\u00e1n al email principal, solo sirven para llegadas, para enviar solo desde el principal. Usuarios - usuarios activos - adminisrar nombre y email y ponemos alias. Habilitar usuarios invitados: configuracion - conf de la organizacion - servicios - grupos de 365 y activar las dos pesta\u00f1as. La otra opcion: ir al a teams - portal teams - configuracion de toda la org - acceso de invitado y activado. La otra es ir al portal de Azure AD - azure AD - informacion general - external identities - conf de coloboracion externa y activamos lo que toque. Sharepoint - directivas - uso comportido - activar lo primero. Cancelar suscripion 365: facturaci\u00f3n - sus productos - selecionamos - tres puntos - cancelar.","title":"ADMIN"},{"location":"office/#soluciones-de-fallos-office-365","text":"Ventajas de adquirir una licencia: Siempre version actualizada Tambien para moviles Integracion con la nube Versiones de escritorio de acces y publisher 1Tb en onedrive.","title":"SOLUCIONES DE FALLOS OFFICE 365"},{"location":"office/#inicio-de-sesion","text":"Al portal correcto de office www.office.com / www.admin.microsoft.com Guardarselo en favoritos Cuenta expirada/bloqueada/olvidada Soluciones: Ir a usuarios del portal y ver el estado de usuarios bloequeados y darle a desbloquear. Si olvida su contrase\u00f1a vamos a usuarios y le damos a restablecer contrase\u00f1a. Puede ser que no tenga licencia, editamos usuario y le damos licencia. Podemos bloquear tambien el inicio de sesion de alguien.","title":"Inicio de sesion"},{"location":"office/#conectividad","text":"Herramienta de diagnostico de windows Probar ping a un sitio Mirar comando tracer al destino Pathping nslookup para mirar el dns Soluciones: Damos nueva ip ipconfig /release y luego /renew ping localhost / ping google.com nslookup para ver nuestro dns y luego probamos poniendo direcciones(podemos poner registros set type=MX). tracert www.linkedin.com para que nos resuelva a una ip y los saltos correctos hasta llegar correctamente al destino pasando por routers. pathping -n www.linkedin.com para hacer lo mismo que tracer mas estadisticas extras de si falla en los saltos. ipconfig /flushdns para liberar toda la cache de dns ipconfig /displaydns para ver la cache del dns ipconfig /registerdns para forzar el registro","title":"Conectividad"},{"location":"office/#correo-electronico","text":"Fallos de conectividad o te pide constante la contrase\u00f1a para entrar. Tenemos que tener acceso a internet, comunicacion online con exchange, el firewall permitir trafico de office y acceso a trav\u00e9s de puertos 443/80. Los servidores exchange almacenas los buzones de correo. Herramientas de solucion: Hacer un ping de conectividad del cliente a outlook.office365.com Revisar la configuracion de outlook www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. www.testconnectivity.microsoft.com web con una coleccion de herramientas para ver posibles fallos de conectividad con exchange, ews, office 365, internos, etc.","title":"Correo electronico"},{"location":"office/#aplicaciones-office365","text":"Aplicaciones que se detienen o bloquean Mensaje de error al abrir office Activacion de la licencia Fallo de inicio de sesion Dentro de por ejemplo el word Inicio - Cuenta, se puede activar las actualizaciones En panel control - programas, seleccionamos y le damos a cambiar y podemos reparar en linea. Creamos usuario y vamos a office.com, iniciamos sesion y vamos logueando en las aplicaciones para comprobar conectividad. Problemas: www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. CMD: outlook/profile y podemos crear un nuevo perfil de outlook. En mi buzon outllok darle a mi nombre email, boton derecho y mirar permisos En inicio, informacion, confguracion de cuenta, delegar permisos a otros usuarios para hacerlo en mi nombre.","title":"Aplicaciones office365"},{"location":"office/#sharepoint","text":"Nube para subir archivos y compartir con los otros usuarios. Problemas de navegador, servicio, no poder instalar el controlador ActiveX. Problemas: En el centro de administracion de office 365, que est\u00e1 al entrar en office.com, en mantenimiento - estado del servicio En directivas de grupo, podemos incorporar sharepoint como sitio de confianza. En los admin s\u00ed pueden restaurar papeleras de reciclaje ya eliminadas por el usuario, maximo 93 dias (SHAREPOINT - COLECCION DE SITIOS). www.protection.office.com - gobierno de datos - retencion. Creamos una poniendo el tiempo que queremos de borrado de archivos.","title":"Sharepoint"},{"location":"office/#onedrive","text":"Almacenaje en la nube. No mas de 10GB. No repetir nombres, caracteres invalidos. Herramientas: onedrive.exe /reset en CMD para resetear la conf de onedrive www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles. Actualizar version En los admin s\u00ed pueden restaurar papeleras de reciclaje ya eliminadas por el usuario, maximo 93 dias (SHAREPOINT - COLECCION DE SITIOS). Restaurar sitios completos eliminadors, powershell: Restore-SPDeletedSite","title":"Onedrive"},{"location":"office/#skypeteams","text":"Chat de grupo, videollamadas, ficheros, etc Problemas de calidad sonido/video, red, firewall, puertos... Herramientas: Verificar conf micro/camara, cerrar otras apps, internet malo, actualizar hardware/software Puertos por encima de 1024 skype o 80/443 y en teams 80/443 o 3478/3481. En el centro de admin de office, en mantenimiento y estado para ver si hay fallos. En el mismo crento de admin, ir a teams - calidad de llamada","title":"Skype/Teams"},{"location":"office/#centro-de-informes","text":"En google: informes de actividad de office365 o en el centro de admin de 365. centro de admin - informes - uso y veremos los informes de cada cosa. Dentro de aqui podemos ver cada aplicacion, dias, filtros de columnas, exportar,etc. centro de admin - informes - seguridad hay dos sitios url donde ir para ver informes de exchange(admin de cumpliliemto-auditoria) y 365(informes). Mejor visualizacion y programar hora de informes. Para ocultar los nombres de los usuarios, podemos ir a configuracion - servicio y complementos - informes y ponerlo animo, todo esto en el centro d admin 365.","title":"Centro de informes"},{"location":"office/#monitoreo-de-estado-de-servicio","text":"En el centro de administracion de office 365, que est\u00e1 al entrar en office.com, en mantenimiento - estado del servicio. Podemos ver si hay incidencias en nuestros servicios. En el centro de mensajes podemos personalizar los mensajes, apps, etc. En los usuarios podemos seleccionar uno y en el apartado de roles podemos editar y darle acceso a que pueda ver los estados d servicio, mensajes etc. OFFICE 365 MANAGEMENT PACK es una herramienta m\u00e1s que se puede descargar para monitorear servicios, mensajes, a\u00f1adir aletar, suscripciones. Tiene que ser en windows server y tener System Center Operation managers antes. Desde powershell con el comando de documentacion Search-AdminAuditLog Podemos pedir ayuda en el centro d admin por preguntas o correo electronico. Remote Connectivity Analyzer: www.testconnectivity.microsoft.com web con una coleccion de herramientas para ver posibles fallos de conectividad con exchange, ews, office 365, internos, etc. En el pesta\u00f1a cliente podemos descargar tambien la herramienta localmente. En wwww.aka.ms/hybridwizard es un asistente para soluciones hibridas. www.diagnostics.outllok.com herramienta diagnostico para descargar y de ahi miraremos la opcion de si es outlook, office, diagnostico avanzado... seleccionamos el motivo como por ejemplo Outlook, problema de conectar y seguimos los pasos y te dar\u00e1 un resumen de los fallos posibles.","title":"Monitoreo de estado de servicio"},{"location":"office/#enterprise-mobility-security-e5","text":"Enterprise Mobility + Security Suite: Es una robusta plataforma de seguridad, basada en la identidad dise\u00f1ada para ayudar a las empresas a administrar y proteger sus dispositivos, aplicaciones y datos dentro de la empresa. Es robusta porque se crean \u201cl\u00edneas de defensa\u201d virtuales que permite atrapar las incidencias en alguna de ellas. Realmente tendr\u00e1s a la mano herramientas de seguridad con las que podr\u00e1s evitar el robo de informaci\u00f3n ya sea intencional o por accidente, as\u00ed como evitar el robo por descuido de los empleados, entre otros.","title":"Enterprise Mobility + Security E5"},{"location":"openshift/","text":"OPENSHIFT Ejemplo muy resumen de montaje de insfraestructura: Minishift es una m\u00e1quina virtual que te crea un cluster de un solo nodo en local para aprendizaje y desarrollo. Solo disponible en versi\u00f3n 3, por ahora. Es de OKD (Opensource). La herramienta para openshift 4 es usar CRC, RedHat CodeReady Containers gestionada por red hat. Hay otra herramienta online de Openshift Online pero ahora solo est\u00e1 con versi\u00f3n gratis de 30 dias (https://manage.openshift.com/) MINISHIFT Minishift solo funciona en estos momentos para versi\u00f3n 3 de Openshift. Por lo tanto no la vamos a usar durante el curso. Sin embargo y dado que a nivel de l\u00ednea de comandos es pr\u00e1cticamente id\u00e9ntica a la versi\u00f3n 4, puede ser muy \u00fatil si en vuestro trabajo ten\u00e9is esta versi\u00f3n. Por eso me he animado a incluir unos v\u00eddeos donde indico como instalarla y usarla. La diferencia radica sobre todo en la parte de la consola WEB que ha variado de forma notable con respecto a la 4. Para instalar minishift (https://www.okd.io/) ponemos /minishift y tambi\u00e9n los pasos de las diferentes releases (https://github.com/minishift/minishift/releases) y (https://github.com/minishift/minishift). Instalaci\u00f3n Instalacion virtualbox(https://computingforgeeks.com/how-to-install-virtualbox-on-fedora-linux/). Problemas virtualizaci\u00f3n (https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/) [isx46410800@miguel minishift]$ ll total 26312 -rw-r--r--. 1 isx46410800 isx46410800 11374 Sep 26 2020 LICENSE -rwxrwxr-x. 1 isx46410800 isx46410800 26927104 Sep 26 2020 minishift -rw-r--r--. 1 isx46410800 isx46410800 3508 Sep 26 2020 README.adoc [isx46410800@miguel minishift]$ ./minishift config set vm-driver virtualbox No Minishift instance exists. New 'vm-driver' setting will be applied on next 'minishift start' [isx46410800@miguel minishift]$ ./minishift start --vm-driver virtualbox Despues arrancado te da una serie de claves, ip(web grafica) y el OC que ser\u00e1 como el comando para utilizar el OPENSHIFT CLIENT RH CODEREADY CONTAINERS Instalaci\u00f3n PASOS(https://www.itsimplenow.com/instalando-un-cluster-local-de-openshift-con-coderady-containers/) Esta es la versi\u00f3n similar a minishift pero utilizando la 4 pero instalado sobre un redhat. Utilizado mejor para 30 dias ya que luego se resetea la info. Cogemos la descarga de (https://developers.redhat.com/products/codeready-containers/overview). Una vez descargado antes seguimos con una serie de comandos: yum install NetworkManager yum install libvirt Descomprimimos el archivo, renonbramos crc al archivo, a\u00f1adimos el usuario normal a trabajar a sudo. Como root en el /etc/sudoers: ## Allow root to run any commands anywhere root ALL=(ALL) ALL isx46410800 ALL=(ALL) ALL Despues ./crc setup para empezar a preparar el entorno de virtualizaci\u00f3n de crc. Despues ./crc start para arrancar la maquina. Nos pide un secret que est\u00e1 en los archivos del directorio crc bajado de la descarga, pegamos y enter. Luego podemos entrar en modo web o modo comando(copiar info en algun fichero que ser\u00e1 como podemos entrar en modo kubeadmin y modo developer). En los directorios vemos que tenemos creados ahora un .crc(configuraci\u00f3n y componentes de nuestro cluster) y un .kube(configuraci\u00f3n de kubernetes). Dentro de .crc est\u00e1n en bin el driver de virtualizaci\u00f3n y oc que es la herramienta para gestionar todas las ordenes de nuestro cluster en openshift. Para a\u00f1adir en el path directamente el binario de la herramienta crc y no tener que escribir siempre ./crc, vamos al home editamos el .bashrc y ponemos export PATH=$PATH:/home/openshift/crc es decir, ponemos la ruta de donde est\u00e1 la herramienta para poder usarlo. Copiamos el ejecutable en el $PATH de usuario: cd crc sudo cp crc /usr/local/bin Configuraci\u00f3n crc oc-env hace que para este entorno se pueda utilizar la herramienta oc, parecido a lo de .bashrc con lo de crc. Copiamos el oc login -u developer -p developer https://api.crc.testing:6443 , info sacada de la instalaci\u00f3n, y nos logueamos por comando. Si despues ponemos crc ip vemos la ip de nuestra maquina virtual creada. Si luego copiamos el otro de oc login -u kubeadmin -p xxxxxxxxx podemos loguearnos y usar ordenes de superadmin del cluster, como por ejemplo oc get nodes Podemos usar oc login para loguearnos de la manera que queramos. Si no nos acordamos podemos usar crc console --credentials para recordar de que manera podemos entrar. Para acceder por una navegador a la consola podemos usar crc console o crc console --url y nos da la direcci\u00f3n o entra directamente. Entramos con el usuario developer o kubeadmin para poder administrar todo lo que veremos en el curso. Una vez dentro vemos la interfaz web, a la izquierda el menu con las opciones de proyectos, pods, la red, monitoring, storage... Tambi\u00e9n podemos ver la VIEW de como seria de admin y como developer y lo que puede manejar uno u otro. El copy login command del menu, sirve para dar un ticket, un tocken para conectarme en modo remoto desde otro sitio. OPENSHIFT ONLINE https://www.openshift.com/products/online/ https://cloud.redhat.com/openshift/create/local Nuestra plataforma online es: https://console-openshift-console.apps.sandbox.x8i5.p1.openshiftapps.com/topology/ns/miguel-ito7-dev?view=graph Para conectarme tenemos que ir a la herramienta de ayuda -> command line tools y nos descargamos la de nuestro sistema operativo. [isx46410800@miguel openshift]$ tar xvf oc.tar [isx46410800@miguel openshift]$ ./oc Despues vamos a nuestro perfil -> copy login command y nos dar\u00e1 un token para conectarnos remotamente a nuestro server openshift. Your API token is sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg Log in with this token oc login --token=sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg --server=https://api.sandbox.x8i5.p1.openshiftapps.com:6443 Use this token directly against the API curl -H \"Authorization: Bearer sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg\" \"https://api.sandbox.x8i5.p1.openshiftapps.com:6443/apis/user.openshift.io/v1/users/~\" Copiamos el token de oc en la terminal y ya nos conectamos. Para un cloud de prueba: https://cloud.redhat.com/openshift/ PROYECTOS oc get ns oc project nombre_proyecto oc login Cuando creamos un proyecto se crea automaticamente un namespace oc new-project nombre_proyecto oc get projects oc get project nombre_proyecto oc get project miguel-ito7-dev -o yaml oc describe project nombre_proyecto Crear un proyecto Por comando: oc new-project nombre_proyecto Por fichero: apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: Esto es la descripcion del proyecto openshift.io/display-name: Ejemplo de creacion de una proyecto Openshift openshift.io/requester: developer documentacion: Ejemplo para crear un proyecto en openshift name: desa2 labels: tipo: desa spec: finalizers: - kubernetes Desplegamos el fichero .yaml con oc apply -f proyecto.yaml Borrar: oc delete project nombre_proyecto Cuando creamos un namespace se crea un proyecto nuevo oc create namespace nombre_ns Y si lo borramos, tambien se borra el proyecto oc delete ns nombre_ns CREAR OBJETOS KUBERNETES NOTA IMPORTANTE: Configurar permisos para poder ejecutar algunos contenedores Hola, algunas im\u00e1genes que usaremos durante el curso requiere privilegios de acceso como ROOT o bien necesitan ciertos permisos para acceder a vol\u00famenes o puertos. Por ejemplo postgres, redis, Apache, etc. Aunque este curso no es de Administraci\u00f3n, necesitamos dar ciertos permisos al usuario para que pueda trabajar. Es necesario ejecutar el siguiente comando en cada uno de los proyectos que creemos durante el curso Hay que modificar \"default\" por el nombre del proyecto.... oc adm policy add-scc-to-user anyuid -z default De esa forma podremos crear objetos y contenedores sin problemas. PODS Creamos un pod por comando: oc run --generator=run-pod/v1 nginx(name_pod) --image=nginx oc get pods / oc get pods -o wide/yaml oc describe pod name_pod oc logs name_pod oc delete pod name_pod En la consola web dentro del pod-> terminal, podemos entrar dentro del container. Creamos por manifiesto yaml: creamos un dockerfile personalizado y lo subimos a dockerhub ##Descargamos una versi\u00f3n concreta de UBUNTU, a trav\u00e9s del tag FROM ubuntu:18.04 MAINTAINER Apasoft Formacion \"apasoft.formacion@gmail.com\" ##Actualizamos el sistema RUN apt-get update ##Instalamos nginx RUN apt-get install -y nginx ##Creamos un fichero index.html en el directorio por defecto de nginx RUN echo 'Ejemplo de POD para el curso de OPENSHIFT de Apasoft Formacion' > /usr/share/nginx/www/index.html ##Arrancamos NGINX a trav\u00e9s de ENTRYPOINT para que no pueda ser modificado en la creaci\u00f3n del contenedor ENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"] ##Exponemos el Puerto 80 EXPOSE 80 el manifiesto pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx1 labels: zone: prod version: v1 spec: containers: - name: nginx image: apasoft/nginx:v1 DEPLOYMENT Ejemplo: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: desa1 spec: selector: matchLabels: app: ejemplo-deploy replicas: 3 template: metadata: labels: app: ejemplo-deploy spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Deploy con oc apply -f crear_deploy.yaml. oc get deploy oc describe deploy nombre SERVICIO Ejemplo: apiVersion: v1 kind: Service metadata: name: servicio labels: app: ejemplo-deploy spec: type: NodePort ports: - port: 8080 nodePort: 30005 protocol: TCP selector: app: ejemplo-deploy oc get svc oc describe svc nombre oc get all crc ip oc expose svc_nombre oc get route DESPLIEGUE APLICACIONES Esquema de un despliegue: Seguridad: Estimado alumno, desde la versi\u00f3n 4.5 de Openshift (que corresponde con CRC 1.13 o superior), el comando \"new-app\" que utilizamos durante parte del curso ha cambiado y ahora genera un Deployment en vez de un DeploymentConfig. # Dado que en este curso estudiamos sobre todo DeploymentConfig (que es un recurso propio de Openshift, mientras que un Deployment es un recurso de Kubernetes) es necesario indicar la siguiente opci\u00f3n al ejecutar el comando. --as-deployment-config (con 2 guiones por delante) Para crear una aplicaci\u00f3n, le damos una imagen, primero buscar\u00e1 en local sino en las nubes o repos: oc new-app nombre_imagen Vemos que crea un imagenstream: oc get is . Esto es como un puntero que se\u00f1ala a las imagenes de verdad, como si fuese una librer\u00eda a las imagenes a usar. oc get is nombre Tambi\u00e9n crea un deploymentconfig: oc get dc y oc describe dc nombre En openshift se utiliza replication controller, no replicasets como kubernetes oc get rc Cuando desplega el pod, primero despliega como un pod deploy como si fuese un test y luego lo completa el real. Al ponerle un label de blog podemos ver todo lo que se crea oc get all -o name -l app=blog : [isx46410800@miguel OC]$ oc get all -o name -l app=blog pod/blog-1-kpq8w replicationcontroller/blog-1 service/blog deploymentconfig.apps.openshift.io/blog imagestream.image.openshift.io/blog Podemos exponer el servicio: [isx46410800@miguel OC]$ oc expose svc blog route.route.openshift.io/blog exposed [isx46410800@miguel OC]$ oc get route blog NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD blog blog-miguel-ito7-dev.apps.sandbox.x8i5.p1.openshiftapps.com blog 8080-tcp None Podemos escalar los deploy con oc scale --replicas=3 dc name [isx46410800@miguel OC]$ oc scale --replicas=3 dc blog deploymentconfig.apps.openshift.io/blog scaled [isx46410800@miguel OC]$ oc get pods NAME READY STATUS RESTARTS AGE blog-1-deploy 0/1 Completed 0 6m23s blog-1-k4sch 1/1 Running 0 22s blog-1-kpq8w 1/1 Running 0 6m19s blog-1-nn9bf 1/1 Running 0 22s [isx46410800@miguel OC]$ oc get pod -l app=blog NAME READY STATUS RESTARTS AGE blog-1-k4sch 1/1 Running 0 73s blog-1-kpq8w 1/1 Running 0 7m10s blog-1-nn9bf 1/1 Running 0 73s Podemos borrar todo lo realizado con oc delete all -o name -l app=blog Desde consola podemos crear esto en TOPOLOGY - CONTAINER IMAGE Y PONERLE LA IMAGEN - DEPLOYMENTCONFIG, ETC. para crear el servicio se va a NETWORKING - CREATE ROUTE - SE PONE SERVICIO y ya est\u00e1. Ejemplo wordpress Crear un proyecto oc new-project wordpress Recordemos activar los permisos necesarios para poder desplegar im\u00e1genes que trabajen como ROOT o que activen determinados puertos oc adm policy add-scc-to-user anyuid -z default Crear una nueva aplicaci\u00f3n con Mysql oc new-app mysql:5.7 --name=mysql1 -e MYSQL_ROOT_PASSWORD=secret -e MYSQL_USER=usu1 -e MYSQL_PASSWORD=secret MYSQL_DATABASE=wordpress Crear una nueva aplicaci\u00f3n de tipo Wordpress y enlazarla con la anterior oc new-app wordpress --name=wordpress1 -e WORDPRESS_DB_HOST=mysql1 -e WORDPRESS_DB_USER=usu1 -e WORDPRESS_DB_PASSWORD=secret -e WORDPRESS_DB_NAME=wordpress Crear un route para poder acceder a Wordpress oc expose svc wordpress1 Comprobamos que tenemos todos los componentes oc get dc oc get rc oc get pod oc get svc oc get is oc get route Luego podemos probar la aplicaci\u00f3n a trav\u00e9s del router Maneras de construir una imagen Desde el codigo fuente de un repo: Lo generemos con oc new-app builder~linkrepo: oc new-app (python:3.5)~https://github.com/apasofttraining/blog.git (--name nombre_queremos) Esto te crear\u00e1 un imagestream un builder. un deploymentconfig y un servicio. En los pods crea uno de build, uno de deploy y el real. oc get buildconfig(bc) nombre_app oc get build Por consola web desde topology - from git y ponemos el repo, detecta rapido cual es el builder, sino ponemos python. Ahora desde un dockerfile ser\u00eda: oc new-app --name blog3 --strategy=docker https://github.com/apasofttraining/blog.git esto har\u00e1 que automaticamente construye los objetos desde el dockerfile oc logs pod_build Desde la consola web vamos a topology - from dockerfile - ponemos el repo Desde consola web tambi\u00e9n podemos crear app desde catalogo y usar las plantillas para construir la app. Las versiones ephemeral es porque los datos no se almacenan persitentemente. Tambi\u00e9n desde consola web - desde YAML podemos crear app completa o componentes. Ejemplo de un replication-controller. Estos son los que gobiernan las replicas de los pods: apiVersion: v1 kind: ReplicationController metadata: name: rep-controller1 spec: replicas: 3 selector: app: apasoft-rc template: metadata: name: apasoft-rc labels: app: apasoft-rc spec: containers: - name: apasoft-rc image: apasoft/blog ports: - containerPort: 80 APLICATION GROUPS: desde la consola en topology, elegimos el ejemplo de wordpress en el que hay dos apps en un proyecto. Vamos a actions - edit aplication grouping - creamos una app - ponemos nombre y nos crea como una burbuja(en la bbdd). Ahora vamos al worpress y hacemos lo mismo y le asignamos al mismo aplication groups y vemos que las dos apps estan agrupadas como si las dos dependieran d ellas. Esto en labels se ve como a\u00f1ade un label mas. Se puede a\u00f1adir flechas(conectores) para poner graficamente que relaciona una cosa con otra. VARIABLES El selector permite localizar los pods. El template define las caracteristicas de los pods. Podemos asignar variables en un yaml de la siguiente manera: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-variables spec: replicas: 2 selector: app: variables template: metadata: labels: app: variables spec: containers: - name: variables image: gcr.io/google-samples/node-hello:1.0 env: - name: NOMBRE value: \"CURSO DE OPENSHIFT\" - name: PROPIETARIO value: \"Apasoft Training\" Nos logamos en un pod para comprobar las variables: oc rsh nombre_pod y despues dentro env Podemos modificar el fichero yaml en caliente con el oc edit nombre_dc en este caso. Miramos que suma una revision y otro pods al ser modificados con oc get dc y oc get pods . Esto lo hace automaticamente porque en triggers estan de type: configchange. podemos listar las variables creadas del pod o dc con: oc set env pod/nombre_pod --list Podemos a\u00f1adir variables despues de hacer el pod o el dc con: oc set env dc/nombre_dc VARIABLE=valor Podemos tambien de una variable ya creada, sobreescribir el valor: oc set env dc/nombre_dc --overwrite VARIABLE=valor Para quitar la variable: oc set env dc/nombre_dc VARIABLE- Desde consola web vamos al deploymentconfig creado y en environment podemos a\u00f1adir variables o modificarlas. Ejemplo con ODOO y POSTGRES Creamos un deploymentconfig de postgres con las variables que necesita: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume env: - name: POSTGRES_PASSWORD value: \"secret\" - name: POSTGRES_USER value: \"odoo\" - name: POSTGRES_DB value: \"postgres\" volumes: - emptyDir: {} name: postgres-db-volume test: false triggers: - type: ConfigChange Creamos el servicio postgres: apiVersion: v1 kind: Service metadata: labels: app: postgres-db name: postgres-svc spec: ports: - name: 5432-tcp port: 5432 protocol: TCP targetPort: 5432 selector: deploymentconfig: postgres-db type: ClusterIP Sino sale, hacemos crc oc-env y luego lo que sale, lo ejecutamos. Ahora creamos el dc de odoo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST value: postgres-svc - name: PASSWORD value: secret - name: USER value: odoo image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange Creamos servicio: apiVersion: v1 kind: Service metadata: name: odoo-svc spec: ports: - name: 8069-tcp port: 8069 protocol: TCP targetPort: 8069 - name: 8071-tcp port: 8071 protocol: TCP targetPort: 8071 - name: 8072-tcp port: 8072 protocol: TCP targetPort: 8072 selector: deploymentconfig: odoo sessionAffinity: None type: ClusterIP status: loadBalancer: {} Luego exponemos el servicio de odoo y copiamos la url y entramos. CONFIGMAPS Son ficheros que contienen clave-valor de variables, para no tener que poner todas las variables en el yaml y ponerlo solo con el nombre de fichero del configmap. Creamos con oc create configmap cf1 --from-literal=VARIABLE=VALOR --from-literal=VARIABLE=VALOR ... oc get cm o oc describe cm cf1 oc get cm cf1 -o yaml Ejemplo de como usarlo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-variables spec: replicas: 2 selector: app: variables template: metadata: labels: app: variables spec: containers: - name: variables image: gcr.io/google-samples/node-hello:1.0 envFrom: - configMapRef: name: cf1 Ahora modificando el ejercicio practico de ODOO y POSTGRES, creando primero las variables en un fichero en vez de antes ponerlas from literal: create configmap postrges-cm --from-env-file fichero_variables_postgres apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume envFrom: - configMapRef: name: postgres-cm volumes: - emptyDir: {} name: postgres-db-volume test: false triggers: - type: ConfigChange apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST valueFrom: configMapKeyRef: name: odoo-cm key: SERVIDOR - name: PASSWORD valueFrom: configMapKeyRef: name: odoo-cm key: CONTRASENA - name: USER valueFrom: configMapKeyRef: name: odoo-cm key: USUARIO image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange Comprobamos lo mismo que antes y saldr\u00e1 igual. SECRETS Son como configmaps de clave-valor pero el valor est\u00e1 encriptado, no es visible directamente. Para crear un secret: oc create secret generic secret-cm --from-literal=usuario=usu1 --from-literal=passowrd=secret Vemos: oc get secret y oc describe secret secret-cm Para desplegarlo en un dc y usarlo seria: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-secrets spec: replicas: 1 selector: app: secret template: metadata: labels: app: secret spec: containers: - name: secret image: gcr.io/google-samples/node-hello:1.0 envFrom: - secretRef: name: secret-cm Si entramos al pod con oc rsh pod_name cuando hacemos env lo vuelve a poner en claro y no encriptado. Ahora seguimos con el ejemplo practico. Creamos un secret por yaml: apiVersion: v1 kind: Secret metadata: name: secreto1 type: Opaque data: PASSWORD: c2VjcmV0Cg== password con echo secret | base64 Y ahora en el dc de odoo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST valueFrom: configMapKeyRef: name: odoo-cm key: SERVIDOR - name: PASSWORD valueFrom: secretKeyRef: name: secreto1 key: PASSWORD - name: USER valueFrom: configMapKeyRef: name: odoo-cm key: USUARIO image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange En CONSOLA WEB vamos al apartado confimaps o secrets y damos a crear. Tambien se puede crear a\u00f1adiendo un workload ya existente. IMAGESTREAMS Forma de crearlos: Se crea: oc new-app wordpress(imagen) --name=w1 (en este caso no hay fase de build al ser de un repo) NOTA ERROR: c import-image test:latest --from=docker-registry.default.svc:5000/devproject/test:v2 ! error: Import failed (InternalError): Internal error occurred: Get https://docker-registry.default.svc:5000/v2/: x509: certificate signed by unknown authority ====================================================================================== Es debido a que a partir de la versi\u00f3n 3.10, OpenShift no registra la CA en el trusted store de CA del sistema operativo, por lo que la API maestra no puede acceder al registro interno de docker para importar las im\u00e1genes. Desgraciadamente, no es posible solucionarlo en CRC debido a que no podemos acceder por SSH al servidor Si estamos en un cluster normal de Openshift, para habilitar este acceso, hay que acceder por ssh al maestro y registrar /etc/origin/master/ca.crt en el almac\u00e9n de confianza de CA del sistema operativo. Los pasos para solucionarlo son: - Acceder al servidor master de openshift - A\u00f1adir lo siguiente /etc/origin/node/pods/apiserver.yaml ============================================ - mountPath: /etc/pki name: certs - hostPath: path: /etc/pki name: certs ============================================== - Ejecutar lo siguiente oc patch dc docker-registry -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"registry\",\"volumeMounts\":[{\"mountPath\":\"/etc/pki\",\"name\":\"certs\"}]}],\"volumes\":[{\"hostPath\":{\"path\":\"/etc/pki\",\"type\":\"Directory\"},\"name\":\"certs\"}]}}}}' - y lo siguiente oc adm policy add-scc-to-user hostaccess -z registry A trav\u00e9s de una imagen externa. Con este comando nos indica de que tipo de imagen tratamos: oc import-image cassandra:latest --from=\"docker.io/cassandra:latest\" (--confirm) Luego podremos ya con oc new-app cassandra:latest --name=c1 y tardar\u00e1 porque ahora es cuando se descarga la imagen a la que se apuntaba A trav\u00e9s de una imagen interna que la descargamos: oc import-image mariadb:10.2 --confirm Luego podremos ya con oc new-app mariadb:10.2 --name=maria1 -e MYSQL_ROOT_PASSWORD=secret Para etiquetas imagenes usamos oc tad . Descargamos oc import-image odoo:13. oc tag oddo:13 oddo:latest y con oc get is vemos que tiene dos tags de una misma imagen. Si ahora ponemos oc tag docker.io/oddo:12 oddo:12 tenemos otro tag pero otra imagen. Luego podemos construir una app como queramos oc new-app odoo:12 --name=oddoapp Crear un imagestream desde YAML: apiVersion: v1 kind: ImageStream metadata: name: mi-web spec: tags: - name: \"1.0\" from: kind: DockerImage name: apasoft/web Podemos actualizar cambios directamente en las imagenes poniendo oc import-image apasoft/ejemplo_docker --scheduled=true --confirm En las propiedades de imagePullPolicy . Always nos permite hacer un pull, If not present, se puede si aun no se ha traido la imagen, Never nunca se pueda hacer pull. BUILDS Podemos poner una imagen desde catalog en consola y un imagen to source como por ejemplo de node y ver las caracteristicas del build o build config que crea. Se puede crear solo la parte de build con oc new-build nodejs~https://github.com/sclorg/nodejs-ex.git --name=node1 . Solo crea el build y el is, no crea un DC con pod. oc get builds // oc get bc Se puede arrancar, parar otro build de un build con oc start-build bc nombre_bc o oc start-build nombre_build . Tambien oc cancel-build nombre_build o oc delete bc nombre build . Podemos crear un BUILD INLINE: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: docker-input spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: docker-input:latest postCommit: {} resources: {} runPolicy: Serial source: dockerfile: \"FROM centos:7\\nCMD echo 'Hola, estas probando un dockerfile inline' && exec sleep infinity\" type: dockerfile strategy: dockerStrategy: type: Docker successfulBuildsHistoryLimit: 5 triggers: - imageChange: type: ImageChange - type: ConfigChange Despues hacemos un oc create imagestream nombre_bc y luego ya podemos hacer un start-build y un new-app si queremos. Creamos uno desde un fichero externo: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: binary spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: binary:latest postCommit: {} resources: {} runPolicy: Serial source: git: uri: https://github.com/ApasoftTraining/ejemplophp.git type: Git strategy: sourceStrategy: from: kind: ImageStreamTag name: php:latest namespace: openshift type: Source successfulBuildsHistoryLimit: 5 triggers: - imageChange: type: ImageChange - type: ConfigChange apiVersion: v1 kind: ImageStream metadata: labels: application: ejemplo-binary version: \"2\" name: binary Luego vemos que se nos crea automaticamente todo por los triggers(triggers: - imageChange: type: ImageChange- type: ConfigChange) y ya podemos usarlo para un new-app. Un ejemplo de TRIGGERS: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: pythonapp spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: pythonapp:latest postCommit: {} resources: {} runPolicy: Serial source: git: uri: https://github.com/ApasoftTraining/python-app type: Git strategy: sourceStrategy: from: kind: ImageStreamTag name: mi-imagen:v1 type: Source successfulBuildsHistoryLimit: 5 triggers: - generic: secretReference: name: pythonapp-generic-webhook-secret type: Generic - github: secretReference: name: pythonapp-github-webhook-secret type: GitHub - imageChange: type: ImageChange - type: ConfigChange apiVersion: v1 kind: ImageStream metadata: name: mi-imagen spec: tags: - name: \"v1\" from: kind: DockerImage name: openshift/python-33-centos7 auto lo hace por el tipo de triggers y sino creamos un oc create imagestream pythonapp y lo hace. DEPLOYMENTCONFIG Y DEPLOY Partimos de un ejemplo PERP creado por catalog en consola: kind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: annotations: app.openshift.io/vcs-ref: '' app.openshift.io/vcs-uri: 'https://github.com/sclorg/dancer-ex.git' openshift.io/generated-by: OpenShiftWebConsole resourceVersion: '244792607' name: perl uid: 4c5c65cb-0d4e-4f12-ba24-fd1b159c6bc5 creationTimestamp: '2021-07-26T11:50:22Z' generation: 1 managedFields: - manager: Mozilla operation: Update apiVersion: apps.openshift.io/v1 time: '2021-07-26T11:50:22Z' fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:annotations': .: {} 'f:app.openshift.io/vcs-ref': {} 'f:app.openshift.io/vcs-uri': {} 'f:openshift.io/generated-by': {} 'f:labels': .: {} 'f:app': {} 'f:app.kubernetes.io/component': {} 'f:app.kubernetes.io/instance': {} 'f:app.kubernetes.io/name': {} 'f:app.kubernetes.io/part-of': {} 'f:app.openshift.io/runtime': {} 'f:app.openshift.io/runtime-version': {} 'f:spec': 'f:replicas': {} 'f:selector': .: {} 'f:app': {} 'f:deploymentconfig': {} 'f:strategy': 'f:activeDeadlineSeconds': {} 'f:rollingParams': .: {} 'f:intervalSeconds': {} 'f:maxSurge': {} 'f:maxUnavailable': {} 'f:timeoutSeconds': {} 'f:updatePeriodSeconds': {} 'f:type': {} 'f:template': .: {} 'f:metadata': .: {} 'f:creationTimestamp': {} 'f:labels': .: {} 'f:app': {} 'f:deploymentconfig': {} 'f:spec': .: {} 'f:containers': .: {} 'k:{\"name\":\"perl\"}': .: {} 'f:image': {} 'f:imagePullPolicy': {} 'f:name': {} 'f:ports': .: {} 'k:{\"containerPort\":8080,\"protocol\":\"TCP\"}': .: {} 'f:containerPort': {} 'f:protocol': {} 'f:resources': {} 'f:terminationMessagePath': {} 'f:terminationMessagePolicy': {} 'f:dnsPolicy': {} 'f:restartPolicy': {} 'f:schedulerName': {} 'f:securityContext': {} 'f:terminationGracePeriodSeconds': {} 'f:triggers': {} - manager: openshift-controller-manager operation: Update apiVersion: apps.openshift.io/v1 time: '2021-07-26T11:50:22Z' fieldsType: FieldsV1 fieldsV1: 'f:status': 'f:conditions': .: {} 'k:{\"type\":\"Available\"}': .: {} 'f:lastTransitionTime': {} 'f:lastUpdateTime': {} 'f:message': {} 'f:status': {} 'f:type': {} 'f:observedGeneration': {} namespace: miguel-ito7-dev labels: app: perl app.kubernetes.io/component: perl app.kubernetes.io/instance: perl app.kubernetes.io/name: perl app.kubernetes.io/part-of: perl app.openshift.io/runtime: perl app.openshift.io/runtime-version: 5.30-el7 spec: strategy: type: Rolling rollingParams: updatePeriodSeconds: 1 intervalSeconds: 1 timeoutSeconds: 600 maxUnavailable: 25% maxSurge: 25% resources: {} activeDeadlineSeconds: 21600 triggers: - type: ImageChange imageChangeParams: automatic: true containerNames: - perl from: kind: ImageStreamTag namespace: miguel-ito7-dev name: 'perl:latest' - type: ConfigChange replicas: 1 revisionHistoryLimit: 10 test: false selector: app: perl deploymentconfig: perl template: metadata: creationTimestamp: null labels: app: perl deploymentconfig: perl spec: containers: - name: perl image: 'perl:latest' ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler status: latestVersion: 0 observedGeneration: 1 replicas: 0 updatedReplicas: 0 availableReplicas: 0 unavailableReplicas: 0 conditions: - type: Available status: 'False' lastUpdateTime: '2021-07-26T11:50:22Z' lastTransitionTime: '2021-07-26T11:50:22Z' message: Deployment config does not have minimum availability. Con el oc rollout latest dc/php1 . Hace volver al dc ultimo registrado. Con el oc rollout status dc/php1 vemos el estado del rollout Con el oc rollout history dc/php1 vemos el historial del rollout y con el oc rollout history dc/php1 --revision=2 vemos el estado de uno en concreto. Con el oc rollout undo dc/php1 --to-revision=2 volvemos al estado del 2 Type rolling para poder hacer cambios, escalar etc. Si cambiamos el type por recrate, termina todo, crea uno nuevo en modo deploy y si funciona, despliega todo de golpe. PLANTILLAS Partimos de una plantilla: apiVersion: v1 kind: Template metadata: name: redis-plantilla annotations: description: \"Description\" iconClass: \"icon-redis\" tags: \"database,nosql\" objects: - apiVersion: v1 kind: Pod metadata: name: ${SERVIDOR} spec: containers: - env: - name: REDIS_PASSWORD value: ${REDIS_PASSWORD} image: redis name: ${NOMBRE_CONTENEDOR} ports: - containerPort: 6379 protocol: TCP parameters: - description: Password used for Redis authentication from: '[A-Z0-9]{8}' generate: expression name: REDIS_PASSWORD - name: SERVIDOR from: 'servidor[a-z0-9]{5}' generate: expression - name: NOMBRE_CONTENEDOR value: redis-mio labels: redis: master En los parameters especificamos las variables a utilizar en una plantilla personalizada. Lo vemos con oc get templates y oc describe templates nombre_template El comando oc process nombre_plantilla -o yaml te dice lo que generar\u00eda de los objetos que hace la plantilla. Para pasar par\u00e1metros a una plantilla: oc process nombre-plantilla --parameters nos indica que parametros hemos pasado a esa plantilla. Si le quiero a\u00f1adir oc process nombre-plantilla -p SERVIDOR=servidor2 | oc apply -f - y le pasamos a la plantilla lo nuevo. Si le pasamos parametros a trav\u00e9s de un fichero con los parametros se usa oc process nombre-plantilla --param-file=file.txt Con este ejemplo de plantilla vemos como construye una imagen a traves de un buildconfig: kind: Template apiVersion: v1 metadata: name: plantilla-build annotations: description: Plantilla con BuildConfig para aplicacion PHP objects: - kind: BuildConfig apiVersion: v1 metadata: name: \"${APLICACION}\" annotations: description: Ejemplo de plantilla con Buildconfig spec: output: to: kind: ImageStreamTag name: '${APLICACION}:latest' source: type: Git git: uri: \"${SOURCE_REPOSITORY_URL}\" strategy: sourceStrategy: from: kind: ImageStreamTag name: php:7.2 namespace: openshift type: Source triggers: - imageChange: {} type: ImageChange - kind: ImageStream apiVersion: v1 metadata: name: '${APLICACION}' spec: dockerImageRepository: '' tags: - name: latest parameters: - name: SOURCE_REPOSITORY_URL displayName: Repositorio GIT de la aplicacion description: Donde se encuentra la aplicacion value: https://github.com/ApasoftTraining/cakephp-ex required: true - description: Nombre aplicacion. name: APLICACION value: aplicacion message: \"... Aplicacion ${APLICACION} creada desde una plantilla.\" ahora podemos crear una nueva apllicacion con oc new-app aplicacion --name=app1 Las plantillas de dentro de openshift predefinidas se miran en oc get templates -n openshift . Y podemos crear una app con oc new-app --template=nombre_plantilla_predefinida --name=app_predefinida . Tambien podemos coger el fichero de una y luego modificarlo al gusto oc get templates nombre_plantilla -n openshift -o yaml > plantilla_custom.yaml Tambien podemos crear DC desde consola y luego guardar los ficheros en consola y crear una oc get bc,is,dc,service,route -o yaml > plantilla_web.yaml ALMACENAMIENTO Hay volumenes con datos persistentes y no. Ejemplo de un volumen donde se crean dentros los directorios de montajes que se crean dentro del contenedor y su nombre, y fuera donde esta la info: apiVersion: v1 kind: Pod metadata: name: volumenes spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /desarrollo name: desarrollo - mountPath: /git name: git readOnly: true - mountPath: /temp name: temp volumes: - name: desarrollo hostPath: path: /home/openshift/datos - name: git gitRepo: repository: https://github.com/apasoftTraining/cursoopenshift.git - name: temp emptyDir: {} oc get pv donde vemos los physical volumen creados. Bound significa que estan ligados a un PVClaim. oc describe pv pv_name donde el source nos indica a qu\u00e9 est\u00e1 asociado. Ejemplo de hacer un postgres con un pv y un asociado pvc: apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume labels: type: local spec: storageClassName: sc-ficheros capacity: storage: 20Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data/pv-volumen\" el storageclassname es una clase de pv que se crea cuando es para varias cosas en general. Si se crea aqui es solo interno y no sale como clase como objeto general, solo est\u00e1 nivel d este pv. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-claim spec: storageClassName: sc-ficheros accessModes: - ReadWriteOnce resources: requests: storage: 3Gi apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume env: - name: POSTGRES_PASSWORD value: \"secret\" - name: POSTGRES_USER value: \"odoo\" - name: POSTGRES_DB value: \"postgres\" volumes: - name: postgres-db-volume persistentVolumeClaim: claimName: pvc-claim test: false triggers: - type: ConfigChange Para un class storage generico. Creamos un proyecto, luego un storage class y ponemos el tipo de que sera en el 'provisioner'. Ahora creamos dos PV con el nombre y tama\u00f1o que queramos, asociados a este storage. Ahora creamos un PVC asociado al storage, el tama\u00f1o. Hace al hacer el apply del yaml veremos como el PVC clama, bound a unos de los PV que mas ajuste al estar asociados.","title":"Openshift"},{"location":"openshift/#openshift","text":"Ejemplo muy resumen de montaje de insfraestructura: Minishift es una m\u00e1quina virtual que te crea un cluster de un solo nodo en local para aprendizaje y desarrollo. Solo disponible en versi\u00f3n 3, por ahora. Es de OKD (Opensource). La herramienta para openshift 4 es usar CRC, RedHat CodeReady Containers gestionada por red hat. Hay otra herramienta online de Openshift Online pero ahora solo est\u00e1 con versi\u00f3n gratis de 30 dias (https://manage.openshift.com/)","title":"OPENSHIFT"},{"location":"openshift/#minishift","text":"Minishift solo funciona en estos momentos para versi\u00f3n 3 de Openshift. Por lo tanto no la vamos a usar durante el curso. Sin embargo y dado que a nivel de l\u00ednea de comandos es pr\u00e1cticamente id\u00e9ntica a la versi\u00f3n 4, puede ser muy \u00fatil si en vuestro trabajo ten\u00e9is esta versi\u00f3n. Por eso me he animado a incluir unos v\u00eddeos donde indico como instalarla y usarla. La diferencia radica sobre todo en la parte de la consola WEB que ha variado de forma notable con respecto a la 4. Para instalar minishift (https://www.okd.io/) ponemos /minishift y tambi\u00e9n los pasos de las diferentes releases (https://github.com/minishift/minishift/releases) y (https://github.com/minishift/minishift).","title":"MINISHIFT"},{"location":"openshift/#instalacion","text":"Instalacion virtualbox(https://computingforgeeks.com/how-to-install-virtualbox-on-fedora-linux/). Problemas virtualizaci\u00f3n (https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/) [isx46410800@miguel minishift]$ ll total 26312 -rw-r--r--. 1 isx46410800 isx46410800 11374 Sep 26 2020 LICENSE -rwxrwxr-x. 1 isx46410800 isx46410800 26927104 Sep 26 2020 minishift -rw-r--r--. 1 isx46410800 isx46410800 3508 Sep 26 2020 README.adoc [isx46410800@miguel minishift]$ ./minishift config set vm-driver virtualbox No Minishift instance exists. New 'vm-driver' setting will be applied on next 'minishift start' [isx46410800@miguel minishift]$ ./minishift start --vm-driver virtualbox Despues arrancado te da una serie de claves, ip(web grafica) y el OC que ser\u00e1 como el comando para utilizar el OPENSHIFT CLIENT","title":"Instalaci\u00f3n"},{"location":"openshift/#rh-codeready-containers","text":"","title":"RH CODEREADY CONTAINERS"},{"location":"openshift/#instalacion_1","text":"PASOS(https://www.itsimplenow.com/instalando-un-cluster-local-de-openshift-con-coderady-containers/) Esta es la versi\u00f3n similar a minishift pero utilizando la 4 pero instalado sobre un redhat. Utilizado mejor para 30 dias ya que luego se resetea la info. Cogemos la descarga de (https://developers.redhat.com/products/codeready-containers/overview). Una vez descargado antes seguimos con una serie de comandos: yum install NetworkManager yum install libvirt Descomprimimos el archivo, renonbramos crc al archivo, a\u00f1adimos el usuario normal a trabajar a sudo. Como root en el /etc/sudoers: ## Allow root to run any commands anywhere root ALL=(ALL) ALL isx46410800 ALL=(ALL) ALL Despues ./crc setup para empezar a preparar el entorno de virtualizaci\u00f3n de crc. Despues ./crc start para arrancar la maquina. Nos pide un secret que est\u00e1 en los archivos del directorio crc bajado de la descarga, pegamos y enter. Luego podemos entrar en modo web o modo comando(copiar info en algun fichero que ser\u00e1 como podemos entrar en modo kubeadmin y modo developer). En los directorios vemos que tenemos creados ahora un .crc(configuraci\u00f3n y componentes de nuestro cluster) y un .kube(configuraci\u00f3n de kubernetes). Dentro de .crc est\u00e1n en bin el driver de virtualizaci\u00f3n y oc que es la herramienta para gestionar todas las ordenes de nuestro cluster en openshift. Para a\u00f1adir en el path directamente el binario de la herramienta crc y no tener que escribir siempre ./crc, vamos al home editamos el .bashrc y ponemos export PATH=$PATH:/home/openshift/crc es decir, ponemos la ruta de donde est\u00e1 la herramienta para poder usarlo. Copiamos el ejecutable en el $PATH de usuario: cd crc sudo cp crc /usr/local/bin","title":"Instalaci\u00f3n"},{"location":"openshift/#configuracion","text":"crc oc-env hace que para este entorno se pueda utilizar la herramienta oc, parecido a lo de .bashrc con lo de crc. Copiamos el oc login -u developer -p developer https://api.crc.testing:6443 , info sacada de la instalaci\u00f3n, y nos logueamos por comando. Si despues ponemos crc ip vemos la ip de nuestra maquina virtual creada. Si luego copiamos el otro de oc login -u kubeadmin -p xxxxxxxxx podemos loguearnos y usar ordenes de superadmin del cluster, como por ejemplo oc get nodes Podemos usar oc login para loguearnos de la manera que queramos. Si no nos acordamos podemos usar crc console --credentials para recordar de que manera podemos entrar. Para acceder por una navegador a la consola podemos usar crc console o crc console --url y nos da la direcci\u00f3n o entra directamente. Entramos con el usuario developer o kubeadmin para poder administrar todo lo que veremos en el curso. Una vez dentro vemos la interfaz web, a la izquierda el menu con las opciones de proyectos, pods, la red, monitoring, storage... Tambi\u00e9n podemos ver la VIEW de como seria de admin y como developer y lo que puede manejar uno u otro. El copy login command del menu, sirve para dar un ticket, un tocken para conectarme en modo remoto desde otro sitio.","title":"Configuraci\u00f3n"},{"location":"openshift/#openshift-online","text":"https://www.openshift.com/products/online/ https://cloud.redhat.com/openshift/create/local Nuestra plataforma online es: https://console-openshift-console.apps.sandbox.x8i5.p1.openshiftapps.com/topology/ns/miguel-ito7-dev?view=graph Para conectarme tenemos que ir a la herramienta de ayuda -> command line tools y nos descargamos la de nuestro sistema operativo. [isx46410800@miguel openshift]$ tar xvf oc.tar [isx46410800@miguel openshift]$ ./oc Despues vamos a nuestro perfil -> copy login command y nos dar\u00e1 un token para conectarnos remotamente a nuestro server openshift. Your API token is sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg Log in with this token oc login --token=sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg --server=https://api.sandbox.x8i5.p1.openshiftapps.com:6443 Use this token directly against the API curl -H \"Authorization: Bearer sha256~b1c3cvoqItQKQvR3AgF6hqPDiGoUP3FnIo4H5TjjBgg\" \"https://api.sandbox.x8i5.p1.openshiftapps.com:6443/apis/user.openshift.io/v1/users/~\" Copiamos el token de oc en la terminal y ya nos conectamos. Para un cloud de prueba: https://cloud.redhat.com/openshift/","title":"OPENSHIFT ONLINE"},{"location":"openshift/#proyectos","text":"oc get ns oc project nombre_proyecto oc login Cuando creamos un proyecto se crea automaticamente un namespace oc new-project nombre_proyecto oc get projects oc get project nombre_proyecto oc get project miguel-ito7-dev -o yaml oc describe project nombre_proyecto","title":"PROYECTOS"},{"location":"openshift/#crear-un-proyecto","text":"Por comando: oc new-project nombre_proyecto Por fichero: apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: Esto es la descripcion del proyecto openshift.io/display-name: Ejemplo de creacion de una proyecto Openshift openshift.io/requester: developer documentacion: Ejemplo para crear un proyecto en openshift name: desa2 labels: tipo: desa spec: finalizers: - kubernetes Desplegamos el fichero .yaml con oc apply -f proyecto.yaml Borrar: oc delete project nombre_proyecto Cuando creamos un namespace se crea un proyecto nuevo oc create namespace nombre_ns Y si lo borramos, tambien se borra el proyecto oc delete ns nombre_ns","title":"Crear un proyecto"},{"location":"openshift/#crear-objetos-kubernetes","text":"","title":"CREAR OBJETOS KUBERNETES"},{"location":"openshift/#nota","text":"IMPORTANTE: Configurar permisos para poder ejecutar algunos contenedores Hola, algunas im\u00e1genes que usaremos durante el curso requiere privilegios de acceso como ROOT o bien necesitan ciertos permisos para acceder a vol\u00famenes o puertos. Por ejemplo postgres, redis, Apache, etc. Aunque este curso no es de Administraci\u00f3n, necesitamos dar ciertos permisos al usuario para que pueda trabajar. Es necesario ejecutar el siguiente comando en cada uno de los proyectos que creemos durante el curso Hay que modificar \"default\" por el nombre del proyecto.... oc adm policy add-scc-to-user anyuid -z default De esa forma podremos crear objetos y contenedores sin problemas.","title":"NOTA"},{"location":"openshift/#pods","text":"Creamos un pod por comando: oc run --generator=run-pod/v1 nginx(name_pod) --image=nginx oc get pods / oc get pods -o wide/yaml oc describe pod name_pod oc logs name_pod oc delete pod name_pod En la consola web dentro del pod-> terminal, podemos entrar dentro del container. Creamos por manifiesto yaml: creamos un dockerfile personalizado y lo subimos a dockerhub ##Descargamos una versi\u00f3n concreta de UBUNTU, a trav\u00e9s del tag FROM ubuntu:18.04 MAINTAINER Apasoft Formacion \"apasoft.formacion@gmail.com\" ##Actualizamos el sistema RUN apt-get update ##Instalamos nginx RUN apt-get install -y nginx ##Creamos un fichero index.html en el directorio por defecto de nginx RUN echo 'Ejemplo de POD para el curso de OPENSHIFT de Apasoft Formacion' > /usr/share/nginx/www/index.html ##Arrancamos NGINX a trav\u00e9s de ENTRYPOINT para que no pueda ser modificado en la creaci\u00f3n del contenedor ENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"] ##Exponemos el Puerto 80 EXPOSE 80 el manifiesto pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx1 labels: zone: prod version: v1 spec: containers: - name: nginx image: apasoft/nginx:v1","title":"PODS"},{"location":"openshift/#deployment","text":"Ejemplo: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: desa1 spec: selector: matchLabels: app: ejemplo-deploy replicas: 3 template: metadata: labels: app: ejemplo-deploy spec: containers: - name: hello-openshift image: openshift/hello-openshift ports: - containerPort: 8080 Deploy con oc apply -f crear_deploy.yaml. oc get deploy oc describe deploy nombre","title":"DEPLOYMENT"},{"location":"openshift/#servicio","text":"Ejemplo: apiVersion: v1 kind: Service metadata: name: servicio labels: app: ejemplo-deploy spec: type: NodePort ports: - port: 8080 nodePort: 30005 protocol: TCP selector: app: ejemplo-deploy oc get svc oc describe svc nombre oc get all crc ip oc expose svc_nombre oc get route","title":"SERVICIO"},{"location":"openshift/#despliegue-aplicaciones","text":"Esquema de un despliegue: Seguridad: Estimado alumno, desde la versi\u00f3n 4.5 de Openshift (que corresponde con CRC 1.13 o superior), el comando \"new-app\" que utilizamos durante parte del curso ha cambiado y ahora genera un Deployment en vez de un DeploymentConfig. # Dado que en este curso estudiamos sobre todo DeploymentConfig (que es un recurso propio de Openshift, mientras que un Deployment es un recurso de Kubernetes) es necesario indicar la siguiente opci\u00f3n al ejecutar el comando. --as-deployment-config (con 2 guiones por delante) Para crear una aplicaci\u00f3n, le damos una imagen, primero buscar\u00e1 en local sino en las nubes o repos: oc new-app nombre_imagen Vemos que crea un imagenstream: oc get is . Esto es como un puntero que se\u00f1ala a las imagenes de verdad, como si fuese una librer\u00eda a las imagenes a usar. oc get is nombre Tambi\u00e9n crea un deploymentconfig: oc get dc y oc describe dc nombre En openshift se utiliza replication controller, no replicasets como kubernetes oc get rc Cuando desplega el pod, primero despliega como un pod deploy como si fuese un test y luego lo completa el real. Al ponerle un label de blog podemos ver todo lo que se crea oc get all -o name -l app=blog : [isx46410800@miguel OC]$ oc get all -o name -l app=blog pod/blog-1-kpq8w replicationcontroller/blog-1 service/blog deploymentconfig.apps.openshift.io/blog imagestream.image.openshift.io/blog Podemos exponer el servicio: [isx46410800@miguel OC]$ oc expose svc blog route.route.openshift.io/blog exposed [isx46410800@miguel OC]$ oc get route blog NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD blog blog-miguel-ito7-dev.apps.sandbox.x8i5.p1.openshiftapps.com blog 8080-tcp None Podemos escalar los deploy con oc scale --replicas=3 dc name [isx46410800@miguel OC]$ oc scale --replicas=3 dc blog deploymentconfig.apps.openshift.io/blog scaled [isx46410800@miguel OC]$ oc get pods NAME READY STATUS RESTARTS AGE blog-1-deploy 0/1 Completed 0 6m23s blog-1-k4sch 1/1 Running 0 22s blog-1-kpq8w 1/1 Running 0 6m19s blog-1-nn9bf 1/1 Running 0 22s [isx46410800@miguel OC]$ oc get pod -l app=blog NAME READY STATUS RESTARTS AGE blog-1-k4sch 1/1 Running 0 73s blog-1-kpq8w 1/1 Running 0 7m10s blog-1-nn9bf 1/1 Running 0 73s Podemos borrar todo lo realizado con oc delete all -o name -l app=blog Desde consola podemos crear esto en TOPOLOGY - CONTAINER IMAGE Y PONERLE LA IMAGEN - DEPLOYMENTCONFIG, ETC. para crear el servicio se va a NETWORKING - CREATE ROUTE - SE PONE SERVICIO y ya est\u00e1.","title":"DESPLIEGUE APLICACIONES"},{"location":"openshift/#ejemplo-wordpress","text":"Crear un proyecto oc new-project wordpress Recordemos activar los permisos necesarios para poder desplegar im\u00e1genes que trabajen como ROOT o que activen determinados puertos oc adm policy add-scc-to-user anyuid -z default Crear una nueva aplicaci\u00f3n con Mysql oc new-app mysql:5.7 --name=mysql1 -e MYSQL_ROOT_PASSWORD=secret -e MYSQL_USER=usu1 -e MYSQL_PASSWORD=secret MYSQL_DATABASE=wordpress Crear una nueva aplicaci\u00f3n de tipo Wordpress y enlazarla con la anterior oc new-app wordpress --name=wordpress1 -e WORDPRESS_DB_HOST=mysql1 -e WORDPRESS_DB_USER=usu1 -e WORDPRESS_DB_PASSWORD=secret -e WORDPRESS_DB_NAME=wordpress Crear un route para poder acceder a Wordpress oc expose svc wordpress1 Comprobamos que tenemos todos los componentes oc get dc oc get rc oc get pod oc get svc oc get is oc get route Luego podemos probar la aplicaci\u00f3n a trav\u00e9s del router","title":"Ejemplo wordpress"},{"location":"openshift/#maneras-de-construir-una-imagen","text":"Desde el codigo fuente de un repo: Lo generemos con oc new-app builder~linkrepo: oc new-app (python:3.5)~https://github.com/apasofttraining/blog.git (--name nombre_queremos) Esto te crear\u00e1 un imagestream un builder. un deploymentconfig y un servicio. En los pods crea uno de build, uno de deploy y el real. oc get buildconfig(bc) nombre_app oc get build Por consola web desde topology - from git y ponemos el repo, detecta rapido cual es el builder, sino ponemos python. Ahora desde un dockerfile ser\u00eda: oc new-app --name blog3 --strategy=docker https://github.com/apasofttraining/blog.git esto har\u00e1 que automaticamente construye los objetos desde el dockerfile oc logs pod_build Desde la consola web vamos a topology - from dockerfile - ponemos el repo Desde consola web tambi\u00e9n podemos crear app desde catalogo y usar las plantillas para construir la app. Las versiones ephemeral es porque los datos no se almacenan persitentemente. Tambi\u00e9n desde consola web - desde YAML podemos crear app completa o componentes. Ejemplo de un replication-controller. Estos son los que gobiernan las replicas de los pods: apiVersion: v1 kind: ReplicationController metadata: name: rep-controller1 spec: replicas: 3 selector: app: apasoft-rc template: metadata: name: apasoft-rc labels: app: apasoft-rc spec: containers: - name: apasoft-rc image: apasoft/blog ports: - containerPort: 80 APLICATION GROUPS: desde la consola en topology, elegimos el ejemplo de wordpress en el que hay dos apps en un proyecto. Vamos a actions - edit aplication grouping - creamos una app - ponemos nombre y nos crea como una burbuja(en la bbdd). Ahora vamos al worpress y hacemos lo mismo y le asignamos al mismo aplication groups y vemos que las dos apps estan agrupadas como si las dos dependieran d ellas. Esto en labels se ve como a\u00f1ade un label mas. Se puede a\u00f1adir flechas(conectores) para poner graficamente que relaciona una cosa con otra.","title":"Maneras de construir una imagen"},{"location":"openshift/#variables","text":"El selector permite localizar los pods. El template define las caracteristicas de los pods. Podemos asignar variables en un yaml de la siguiente manera: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-variables spec: replicas: 2 selector: app: variables template: metadata: labels: app: variables spec: containers: - name: variables image: gcr.io/google-samples/node-hello:1.0 env: - name: NOMBRE value: \"CURSO DE OPENSHIFT\" - name: PROPIETARIO value: \"Apasoft Training\" Nos logamos en un pod para comprobar las variables: oc rsh nombre_pod y despues dentro env Podemos modificar el fichero yaml en caliente con el oc edit nombre_dc en este caso. Miramos que suma una revision y otro pods al ser modificados con oc get dc y oc get pods . Esto lo hace automaticamente porque en triggers estan de type: configchange. podemos listar las variables creadas del pod o dc con: oc set env pod/nombre_pod --list Podemos a\u00f1adir variables despues de hacer el pod o el dc con: oc set env dc/nombre_dc VARIABLE=valor Podemos tambien de una variable ya creada, sobreescribir el valor: oc set env dc/nombre_dc --overwrite VARIABLE=valor Para quitar la variable: oc set env dc/nombre_dc VARIABLE- Desde consola web vamos al deploymentconfig creado y en environment podemos a\u00f1adir variables o modificarlas.","title":"VARIABLES"},{"location":"openshift/#ejemplo-con-odoo-y-postgres","text":"Creamos un deploymentconfig de postgres con las variables que necesita: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume env: - name: POSTGRES_PASSWORD value: \"secret\" - name: POSTGRES_USER value: \"odoo\" - name: POSTGRES_DB value: \"postgres\" volumes: - emptyDir: {} name: postgres-db-volume test: false triggers: - type: ConfigChange Creamos el servicio postgres: apiVersion: v1 kind: Service metadata: labels: app: postgres-db name: postgres-svc spec: ports: - name: 5432-tcp port: 5432 protocol: TCP targetPort: 5432 selector: deploymentconfig: postgres-db type: ClusterIP Sino sale, hacemos crc oc-env y luego lo que sale, lo ejecutamos. Ahora creamos el dc de odoo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST value: postgres-svc - name: PASSWORD value: secret - name: USER value: odoo image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange Creamos servicio: apiVersion: v1 kind: Service metadata: name: odoo-svc spec: ports: - name: 8069-tcp port: 8069 protocol: TCP targetPort: 8069 - name: 8071-tcp port: 8071 protocol: TCP targetPort: 8071 - name: 8072-tcp port: 8072 protocol: TCP targetPort: 8072 selector: deploymentconfig: odoo sessionAffinity: None type: ClusterIP status: loadBalancer: {} Luego exponemos el servicio de odoo y copiamos la url y entramos.","title":"Ejemplo con ODOO y POSTGRES"},{"location":"openshift/#configmaps","text":"Son ficheros que contienen clave-valor de variables, para no tener que poner todas las variables en el yaml y ponerlo solo con el nombre de fichero del configmap. Creamos con oc create configmap cf1 --from-literal=VARIABLE=VALOR --from-literal=VARIABLE=VALOR ... oc get cm o oc describe cm cf1 oc get cm cf1 -o yaml Ejemplo de como usarlo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-variables spec: replicas: 2 selector: app: variables template: metadata: labels: app: variables spec: containers: - name: variables image: gcr.io/google-samples/node-hello:1.0 envFrom: - configMapRef: name: cf1 Ahora modificando el ejercicio practico de ODOO y POSTGRES, creando primero las variables en un fichero en vez de antes ponerlas from literal: create configmap postrges-cm --from-env-file fichero_variables_postgres apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume envFrom: - configMapRef: name: postgres-cm volumes: - emptyDir: {} name: postgres-db-volume test: false triggers: - type: ConfigChange apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST valueFrom: configMapKeyRef: name: odoo-cm key: SERVIDOR - name: PASSWORD valueFrom: configMapKeyRef: name: odoo-cm key: CONTRASENA - name: USER valueFrom: configMapKeyRef: name: odoo-cm key: USUARIO image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange Comprobamos lo mismo que antes y saldr\u00e1 igual.","title":"CONFIGMAPS"},{"location":"openshift/#secrets","text":"Son como configmaps de clave-valor pero el valor est\u00e1 encriptado, no es visible directamente. Para crear un secret: oc create secret generic secret-cm --from-literal=usuario=usu1 --from-literal=passowrd=secret Vemos: oc get secret y oc describe secret secret-cm Para desplegarlo en un dc y usarlo seria: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: ejemplo-secrets spec: replicas: 1 selector: app: secret template: metadata: labels: app: secret spec: containers: - name: secret image: gcr.io/google-samples/node-hello:1.0 envFrom: - secretRef: name: secret-cm Si entramos al pod con oc rsh pod_name cuando hacemos env lo vuelve a poner en claro y no encriptado. Ahora seguimos con el ejemplo practico. Creamos un secret por yaml: apiVersion: v1 kind: Secret metadata: name: secreto1 type: Opaque data: PASSWORD: c2VjcmV0Cg== password con echo secret | base64 Y ahora en el dc de odoo: apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: odoo spec: replicas: 1 selector: deploymentconfig: odoo template: metadata: labels: deploymentconfig: odoo spec: containers: - env: - name: HOST valueFrom: configMapKeyRef: name: odoo-cm key: SERVIDOR - name: PASSWORD valueFrom: secretKeyRef: name: secreto1 key: PASSWORD - name: USER valueFrom: configMapKeyRef: name: odoo-cm key: USUARIO image: odoo imagePullPolicy: Always name: odoo ports: - containerPort: 8069 protocol: TCP - containerPort: 8071 protocol: TCP - containerPort: 8072 protocol: TCP resources: {} volumeMounts: - mountPath: /mnt/extra-addons name: odoo-volume-1 - mountPath: /var/lib/odoo name: odoo-volume-2 volumes: - emptyDir: {} name: odoo-volume-1 - emptyDir: {} name: odoo-volume-2 test: false triggers: - type: ConfigChange En CONSOLA WEB vamos al apartado confimaps o secrets y damos a crear. Tambien se puede crear a\u00f1adiendo un workload ya existente.","title":"SECRETS"},{"location":"openshift/#imagestreams","text":"Forma de crearlos: Se crea: oc new-app wordpress(imagen) --name=w1 (en este caso no hay fase de build al ser de un repo) NOTA ERROR: c import-image test:latest --from=docker-registry.default.svc:5000/devproject/test:v2 ! error: Import failed (InternalError): Internal error occurred: Get https://docker-registry.default.svc:5000/v2/: x509: certificate signed by unknown authority ====================================================================================== Es debido a que a partir de la versi\u00f3n 3.10, OpenShift no registra la CA en el trusted store de CA del sistema operativo, por lo que la API maestra no puede acceder al registro interno de docker para importar las im\u00e1genes. Desgraciadamente, no es posible solucionarlo en CRC debido a que no podemos acceder por SSH al servidor Si estamos en un cluster normal de Openshift, para habilitar este acceso, hay que acceder por ssh al maestro y registrar /etc/origin/master/ca.crt en el almac\u00e9n de confianza de CA del sistema operativo. Los pasos para solucionarlo son: - Acceder al servidor master de openshift - A\u00f1adir lo siguiente /etc/origin/node/pods/apiserver.yaml ============================================ - mountPath: /etc/pki name: certs - hostPath: path: /etc/pki name: certs ============================================== - Ejecutar lo siguiente oc patch dc docker-registry -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"registry\",\"volumeMounts\":[{\"mountPath\":\"/etc/pki\",\"name\":\"certs\"}]}],\"volumes\":[{\"hostPath\":{\"path\":\"/etc/pki\",\"type\":\"Directory\"},\"name\":\"certs\"}]}}}}' - y lo siguiente oc adm policy add-scc-to-user hostaccess -z registry A trav\u00e9s de una imagen externa. Con este comando nos indica de que tipo de imagen tratamos: oc import-image cassandra:latest --from=\"docker.io/cassandra:latest\" (--confirm) Luego podremos ya con oc new-app cassandra:latest --name=c1 y tardar\u00e1 porque ahora es cuando se descarga la imagen a la que se apuntaba A trav\u00e9s de una imagen interna que la descargamos: oc import-image mariadb:10.2 --confirm Luego podremos ya con oc new-app mariadb:10.2 --name=maria1 -e MYSQL_ROOT_PASSWORD=secret Para etiquetas imagenes usamos oc tad . Descargamos oc import-image odoo:13. oc tag oddo:13 oddo:latest y con oc get is vemos que tiene dos tags de una misma imagen. Si ahora ponemos oc tag docker.io/oddo:12 oddo:12 tenemos otro tag pero otra imagen. Luego podemos construir una app como queramos oc new-app odoo:12 --name=oddoapp Crear un imagestream desde YAML: apiVersion: v1 kind: ImageStream metadata: name: mi-web spec: tags: - name: \"1.0\" from: kind: DockerImage name: apasoft/web Podemos actualizar cambios directamente en las imagenes poniendo oc import-image apasoft/ejemplo_docker --scheduled=true --confirm En las propiedades de imagePullPolicy . Always nos permite hacer un pull, If not present, se puede si aun no se ha traido la imagen, Never nunca se pueda hacer pull.","title":"IMAGESTREAMS"},{"location":"openshift/#builds","text":"Podemos poner una imagen desde catalog en consola y un imagen to source como por ejemplo de node y ver las caracteristicas del build o build config que crea. Se puede crear solo la parte de build con oc new-build nodejs~https://github.com/sclorg/nodejs-ex.git --name=node1 . Solo crea el build y el is, no crea un DC con pod. oc get builds // oc get bc Se puede arrancar, parar otro build de un build con oc start-build bc nombre_bc o oc start-build nombre_build . Tambien oc cancel-build nombre_build o oc delete bc nombre build . Podemos crear un BUILD INLINE: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: docker-input spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: docker-input:latest postCommit: {} resources: {} runPolicy: Serial source: dockerfile: \"FROM centos:7\\nCMD echo 'Hola, estas probando un dockerfile inline' && exec sleep infinity\" type: dockerfile strategy: dockerStrategy: type: Docker successfulBuildsHistoryLimit: 5 triggers: - imageChange: type: ImageChange - type: ConfigChange Despues hacemos un oc create imagestream nombre_bc y luego ya podemos hacer un start-build y un new-app si queremos. Creamos uno desde un fichero externo: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: binary spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: binary:latest postCommit: {} resources: {} runPolicy: Serial source: git: uri: https://github.com/ApasoftTraining/ejemplophp.git type: Git strategy: sourceStrategy: from: kind: ImageStreamTag name: php:latest namespace: openshift type: Source successfulBuildsHistoryLimit: 5 triggers: - imageChange: type: ImageChange - type: ConfigChange apiVersion: v1 kind: ImageStream metadata: labels: application: ejemplo-binary version: \"2\" name: binary Luego vemos que se nos crea automaticamente todo por los triggers(triggers: - imageChange: type: ImageChange- type: ConfigChange) y ya podemos usarlo para un new-app. Un ejemplo de TRIGGERS: apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: pythonapp spec: failedBuildsHistoryLimit: 5 nodeSelector: null output: to: kind: ImageStreamTag name: pythonapp:latest postCommit: {} resources: {} runPolicy: Serial source: git: uri: https://github.com/ApasoftTraining/python-app type: Git strategy: sourceStrategy: from: kind: ImageStreamTag name: mi-imagen:v1 type: Source successfulBuildsHistoryLimit: 5 triggers: - generic: secretReference: name: pythonapp-generic-webhook-secret type: Generic - github: secretReference: name: pythonapp-github-webhook-secret type: GitHub - imageChange: type: ImageChange - type: ConfigChange apiVersion: v1 kind: ImageStream metadata: name: mi-imagen spec: tags: - name: \"v1\" from: kind: DockerImage name: openshift/python-33-centos7 auto lo hace por el tipo de triggers y sino creamos un oc create imagestream pythonapp y lo hace.","title":"BUILDS"},{"location":"openshift/#deploymentconfig-y-deploy","text":"Partimos de un ejemplo PERP creado por catalog en consola: kind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: annotations: app.openshift.io/vcs-ref: '' app.openshift.io/vcs-uri: 'https://github.com/sclorg/dancer-ex.git' openshift.io/generated-by: OpenShiftWebConsole resourceVersion: '244792607' name: perl uid: 4c5c65cb-0d4e-4f12-ba24-fd1b159c6bc5 creationTimestamp: '2021-07-26T11:50:22Z' generation: 1 managedFields: - manager: Mozilla operation: Update apiVersion: apps.openshift.io/v1 time: '2021-07-26T11:50:22Z' fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:annotations': .: {} 'f:app.openshift.io/vcs-ref': {} 'f:app.openshift.io/vcs-uri': {} 'f:openshift.io/generated-by': {} 'f:labels': .: {} 'f:app': {} 'f:app.kubernetes.io/component': {} 'f:app.kubernetes.io/instance': {} 'f:app.kubernetes.io/name': {} 'f:app.kubernetes.io/part-of': {} 'f:app.openshift.io/runtime': {} 'f:app.openshift.io/runtime-version': {} 'f:spec': 'f:replicas': {} 'f:selector': .: {} 'f:app': {} 'f:deploymentconfig': {} 'f:strategy': 'f:activeDeadlineSeconds': {} 'f:rollingParams': .: {} 'f:intervalSeconds': {} 'f:maxSurge': {} 'f:maxUnavailable': {} 'f:timeoutSeconds': {} 'f:updatePeriodSeconds': {} 'f:type': {} 'f:template': .: {} 'f:metadata': .: {} 'f:creationTimestamp': {} 'f:labels': .: {} 'f:app': {} 'f:deploymentconfig': {} 'f:spec': .: {} 'f:containers': .: {} 'k:{\"name\":\"perl\"}': .: {} 'f:image': {} 'f:imagePullPolicy': {} 'f:name': {} 'f:ports': .: {} 'k:{\"containerPort\":8080,\"protocol\":\"TCP\"}': .: {} 'f:containerPort': {} 'f:protocol': {} 'f:resources': {} 'f:terminationMessagePath': {} 'f:terminationMessagePolicy': {} 'f:dnsPolicy': {} 'f:restartPolicy': {} 'f:schedulerName': {} 'f:securityContext': {} 'f:terminationGracePeriodSeconds': {} 'f:triggers': {} - manager: openshift-controller-manager operation: Update apiVersion: apps.openshift.io/v1 time: '2021-07-26T11:50:22Z' fieldsType: FieldsV1 fieldsV1: 'f:status': 'f:conditions': .: {} 'k:{\"type\":\"Available\"}': .: {} 'f:lastTransitionTime': {} 'f:lastUpdateTime': {} 'f:message': {} 'f:status': {} 'f:type': {} 'f:observedGeneration': {} namespace: miguel-ito7-dev labels: app: perl app.kubernetes.io/component: perl app.kubernetes.io/instance: perl app.kubernetes.io/name: perl app.kubernetes.io/part-of: perl app.openshift.io/runtime: perl app.openshift.io/runtime-version: 5.30-el7 spec: strategy: type: Rolling rollingParams: updatePeriodSeconds: 1 intervalSeconds: 1 timeoutSeconds: 600 maxUnavailable: 25% maxSurge: 25% resources: {} activeDeadlineSeconds: 21600 triggers: - type: ImageChange imageChangeParams: automatic: true containerNames: - perl from: kind: ImageStreamTag namespace: miguel-ito7-dev name: 'perl:latest' - type: ConfigChange replicas: 1 revisionHistoryLimit: 10 test: false selector: app: perl deploymentconfig: perl template: metadata: creationTimestamp: null labels: app: perl deploymentconfig: perl spec: containers: - name: perl image: 'perl:latest' ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File imagePullPolicy: Always restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler status: latestVersion: 0 observedGeneration: 1 replicas: 0 updatedReplicas: 0 availableReplicas: 0 unavailableReplicas: 0 conditions: - type: Available status: 'False' lastUpdateTime: '2021-07-26T11:50:22Z' lastTransitionTime: '2021-07-26T11:50:22Z' message: Deployment config does not have minimum availability. Con el oc rollout latest dc/php1 . Hace volver al dc ultimo registrado. Con el oc rollout status dc/php1 vemos el estado del rollout Con el oc rollout history dc/php1 vemos el historial del rollout y con el oc rollout history dc/php1 --revision=2 vemos el estado de uno en concreto. Con el oc rollout undo dc/php1 --to-revision=2 volvemos al estado del 2 Type rolling para poder hacer cambios, escalar etc. Si cambiamos el type por recrate, termina todo, crea uno nuevo en modo deploy y si funciona, despliega todo de golpe.","title":"DEPLOYMENTCONFIG Y DEPLOY"},{"location":"openshift/#plantillas","text":"Partimos de una plantilla: apiVersion: v1 kind: Template metadata: name: redis-plantilla annotations: description: \"Description\" iconClass: \"icon-redis\" tags: \"database,nosql\" objects: - apiVersion: v1 kind: Pod metadata: name: ${SERVIDOR} spec: containers: - env: - name: REDIS_PASSWORD value: ${REDIS_PASSWORD} image: redis name: ${NOMBRE_CONTENEDOR} ports: - containerPort: 6379 protocol: TCP parameters: - description: Password used for Redis authentication from: '[A-Z0-9]{8}' generate: expression name: REDIS_PASSWORD - name: SERVIDOR from: 'servidor[a-z0-9]{5}' generate: expression - name: NOMBRE_CONTENEDOR value: redis-mio labels: redis: master En los parameters especificamos las variables a utilizar en una plantilla personalizada. Lo vemos con oc get templates y oc describe templates nombre_template El comando oc process nombre_plantilla -o yaml te dice lo que generar\u00eda de los objetos que hace la plantilla. Para pasar par\u00e1metros a una plantilla: oc process nombre-plantilla --parameters nos indica que parametros hemos pasado a esa plantilla. Si le quiero a\u00f1adir oc process nombre-plantilla -p SERVIDOR=servidor2 | oc apply -f - y le pasamos a la plantilla lo nuevo. Si le pasamos parametros a trav\u00e9s de un fichero con los parametros se usa oc process nombre-plantilla --param-file=file.txt Con este ejemplo de plantilla vemos como construye una imagen a traves de un buildconfig: kind: Template apiVersion: v1 metadata: name: plantilla-build annotations: description: Plantilla con BuildConfig para aplicacion PHP objects: - kind: BuildConfig apiVersion: v1 metadata: name: \"${APLICACION}\" annotations: description: Ejemplo de plantilla con Buildconfig spec: output: to: kind: ImageStreamTag name: '${APLICACION}:latest' source: type: Git git: uri: \"${SOURCE_REPOSITORY_URL}\" strategy: sourceStrategy: from: kind: ImageStreamTag name: php:7.2 namespace: openshift type: Source triggers: - imageChange: {} type: ImageChange - kind: ImageStream apiVersion: v1 metadata: name: '${APLICACION}' spec: dockerImageRepository: '' tags: - name: latest parameters: - name: SOURCE_REPOSITORY_URL displayName: Repositorio GIT de la aplicacion description: Donde se encuentra la aplicacion value: https://github.com/ApasoftTraining/cakephp-ex required: true - description: Nombre aplicacion. name: APLICACION value: aplicacion message: \"... Aplicacion ${APLICACION} creada desde una plantilla.\" ahora podemos crear una nueva apllicacion con oc new-app aplicacion --name=app1 Las plantillas de dentro de openshift predefinidas se miran en oc get templates -n openshift . Y podemos crear una app con oc new-app --template=nombre_plantilla_predefinida --name=app_predefinida . Tambien podemos coger el fichero de una y luego modificarlo al gusto oc get templates nombre_plantilla -n openshift -o yaml > plantilla_custom.yaml Tambien podemos crear DC desde consola y luego guardar los ficheros en consola y crear una oc get bc,is,dc,service,route -o yaml > plantilla_web.yaml","title":"PLANTILLAS"},{"location":"openshift/#almacenamiento","text":"Hay volumenes con datos persistentes y no. Ejemplo de un volumen donde se crean dentros los directorios de montajes que se crean dentro del contenedor y su nombre, y fuera donde esta la info: apiVersion: v1 kind: Pod metadata: name: volumenes spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /desarrollo name: desarrollo - mountPath: /git name: git readOnly: true - mountPath: /temp name: temp volumes: - name: desarrollo hostPath: path: /home/openshift/datos - name: git gitRepo: repository: https://github.com/apasoftTraining/cursoopenshift.git - name: temp emptyDir: {} oc get pv donde vemos los physical volumen creados. Bound significa que estan ligados a un PVClaim. oc describe pv pv_name donde el source nos indica a qu\u00e9 est\u00e1 asociado. Ejemplo de hacer un postgres con un pv y un asociado pvc: apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume labels: type: local spec: storageClassName: sc-ficheros capacity: storage: 20Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data/pv-volumen\" el storageclassname es una clase de pv que se crea cuando es para varias cosas en general. Si se crea aqui es solo interno y no sale como clase como objeto general, solo est\u00e1 nivel d este pv. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-claim spec: storageClassName: sc-ficheros accessModes: - ReadWriteOnce resources: requests: storage: 3Gi apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: name: postgres-db spec: replicas: 1 selector: deploymentconfig: postgres-db template: metadata: labels: deploymentconfig: postgres-db spec: containers: - image: postgres:11 imagePullPolicy: Always name: postgres-db ports: - containerPort: 5432 protocol: TCP volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-db-volume env: - name: POSTGRES_PASSWORD value: \"secret\" - name: POSTGRES_USER value: \"odoo\" - name: POSTGRES_DB value: \"postgres\" volumes: - name: postgres-db-volume persistentVolumeClaim: claimName: pvc-claim test: false triggers: - type: ConfigChange Para un class storage generico. Creamos un proyecto, luego un storage class y ponemos el tipo de que sera en el 'provisioner'. Ahora creamos dos PV con el nombre y tama\u00f1o que queramos, asociados a este storage. Ahora creamos un PVC asociado al storage, el tama\u00f1o. Hace al hacer el apply del yaml veremos como el PVC clama, bound a unos de los PV que mas ajuste al estar asociados.","title":"ALMACENAMIENTO"},{"location":"pam/","text":"PAM Conjunto de librerias que permiten la autenticaci\u00f3n de aplicaciones en el sistema. Dan una API para dar ciertos privilegios a programas para su autenticacaci\u00f3n. Los clientes de PAM son las aplicaciones que necesitan la autenticaci\u00f3n. API cuando se proporcionan preguntas y te retorna respuestas. APP Pam Aware es una aplicaci\u00f3n construida para utilizar pam a traves de su API. Ir\u00e1 a mirar el fichero de /etc/pam.d Modulos plugables en /usr/lib64/security Linea de un file PAM: type - control - module_path - module_arguments Types: auth, account, password, session. Control: required, requisite, sufficient, optional, include, substack. pam_echo: siempre da SUCCESS pam_permit: permite cambiar chfn sin poner paasswd pam_deny: niega que puedas cambiar pam_unix: permite si es un user valid ldd /usr/lib/chfn ver dependencias Calidad de un password: /etc/pam.d/passwd // /etc/security/pwquality.conf Time: /etc/security/time.conf Definimos volumen a compartir en: /etc/security/pam_mount.conf.xml Comprobamos conexion con LDAP con getent passwd user/ getent group group Ficheros importantes de nscd.conf, nslcd.conf y nsswitch.conf INSTALACION Dockerfile: # hostpam FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"hostpam:scratch\" RUN dnf install -y vim util-linux-user-2.30.2-3.fc27.x86_64 finger passwd pam_mount nss-pam-ldapd authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker Startup.sh: #! /bin/bash bash /opt/docker/install.sh /sbin/nscd /sbin/nslcd -d Install.sh: #! /bin/bash useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 cp /opt/docker/login.defs /etc/login.defs cp /opt/docker/nslcd.conf /etc/nslcd.conf cp /opt/docker/nslcd.conf /etc/nscd.conf cp /opt/docker/nsswitch.conf /etc/nsswitch.conf authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall #bash /opt/docker/auth.sh FICHEROS nsswitch.conf: # # /etc/nsswitch.conf # # An example Name Service Switch config file. This file should be # sorted with the most-used services at the beginning. # # The entry '[NOTFOUND=return]' means that the search for an # entry should stop if the search in the previous entry turned # up nothing. Note that if the search failed due to some other reason # (like no NIS server responding) then the search continues with the # next entry. # # Valid entries include: # # nisplus Use NIS+ (NIS version 3) # nis Use NIS (NIS version 2), also called YP # dns Use DNS (Domain Name Service) # files Use the local files # db Use the local database (.db) files # compat Use NIS on compat mode # hesiod Use Hesiod for user lookups # [NOTFOUND=return] Stop searching if not found so far # # To use db, put the \"db\" in front of \"files\" for entries you want to be # looked up first in the databases # # Example: #passwd: db files nisplus nis #shadow: db files nisplus nis #group: db files nisplus nis passwd: files ldap systemd shadow: files ldap group: files ldap systemd #hosts: db files nisplus nis dns hosts: files dns myhostname # Example - obey only what nisplus tells us... #services: nisplus [NOTFOUND=return] files #networks: nisplus [NOTFOUND=return] files #protocols: nisplus [NOTFOUND=return] files #rpc: nisplus [NOTFOUND=return] files #ethers: nisplus [NOTFOUND=return] files #netmasks: nisplus [NOTFOUND=return] files bootparams: nisplus [NOTFOUND=return] files ethers: files netmasks: files networks: files protocols: files rpc: files services: files sss netgroup: nisplus sss publickey: nisplus automount: files nisplus aliases: files nisplus nslcd.conf: # This is the configuration file for the LDAP nameservice # switch library's nslcd daemon. It configures the mapping # between NSS names (see /etc/nsswitch.conf) and LDAP # information in the directory. # See the manual page nslcd.conf(5) for more information. # The user and group nslcd should run as. uid nslcd gid ldap # The uri pointing to the LDAP server to use for name lookups. # Multiple entries may be specified. The address that is used # here should be resolvable without using LDAP (obviously). #uri ldap://127.0.0.1/ #uri ldaps://127.0.0.1/ #uri ldapi://%2fvar%2frun%2fldapi_sock/ # Note: %2f encodes the '/' used as directory separator uri ldap://ldapserver # The LDAP version to use (defaults to 3 # if supported by client library) #ldap_version 3 # The distinguished name of the search base. base dc=edt,dc=org nscd.conf no hacemos nada Login.defs.conf: # # Please note that the parameters in this configuration file control the # behavior of the tools from the shadow-utils component. None of these # tools uses the PAM mechanism, and the utilities that use PAM (such as the # passwd command) should therefore be configured elsewhere. Refer to # /etc/pam.d/system-auth for more information. # # *REQUIRED* # Directory where mailboxes reside, _or_ name of file, relative to the # home directory. If you _do_ define both, MAIL_DIR takes precedence. # QMAIL_DIR is for Qmail # #QMAIL_DIR Maildir MAIL_DIR /var/spool/mail #MAIL_FILE .mail # Password aging controls: # # PASS_MAX_DAYS Maximum number of days a password may be used. # PASS_MIN_DAYS Minimum number of days allowed between password changes. # PASS_MIN_LEN Minimum acceptable password length. # PASS_WARN_AGE Number of days warning given before a password expires. # PASS_MAX_DAYS 99999 PASS_MIN_DAYS 0 PASS_MIN_LEN 5 PASS_WARN_AGE 7 # # Min/max values for automatic uid selection in useradd # UID_MIN 1000 UID_MAX 60000 # System accounts SYS_UID_MIN 201 SYS_UID_MAX 999 # # Min/max values for automatic gid selection in groupadd # GID_MIN 1000 GID_MAX 60000 # System accounts SYS_GID_MIN 201 SYS_GID_MAX 999 # # If defined, this command is run when removing a user. # It should remove any at/cron/print jobs etc. owned by # the user to be removed (passed as the first argument). # #USERDEL_CMD /usr/sbin/userdel_local # # If useradd should create home directories for users by default # On RH systems, we do. This option is overridden with the -m flag on # useradd command line. # CREATE_HOME yes # The permission mask is initialized to this value. If not specified, # the permission mask will be initialized to 022. UMASK 077 # This enables userdel to remove user groups if no members exist. # USERGROUPS_ENAB yes # Use SHA512 to encrypt password. ENCRYPT_METHOD SHA512 CHFN_RESTRICT no","title":"PAM"},{"location":"pam/#pam","text":"Conjunto de librerias que permiten la autenticaci\u00f3n de aplicaciones en el sistema. Dan una API para dar ciertos privilegios a programas para su autenticacaci\u00f3n. Los clientes de PAM son las aplicaciones que necesitan la autenticaci\u00f3n. API cuando se proporcionan preguntas y te retorna respuestas. APP Pam Aware es una aplicaci\u00f3n construida para utilizar pam a traves de su API. Ir\u00e1 a mirar el fichero de /etc/pam.d Modulos plugables en /usr/lib64/security Linea de un file PAM: type - control - module_path - module_arguments Types: auth, account, password, session. Control: required, requisite, sufficient, optional, include, substack. pam_echo: siempre da SUCCESS pam_permit: permite cambiar chfn sin poner paasswd pam_deny: niega que puedas cambiar pam_unix: permite si es un user valid ldd /usr/lib/chfn ver dependencias Calidad de un password: /etc/pam.d/passwd // /etc/security/pwquality.conf Time: /etc/security/time.conf Definimos volumen a compartir en: /etc/security/pam_mount.conf.xml Comprobamos conexion con LDAP con getent passwd user/ getent group group Ficheros importantes de nscd.conf, nslcd.conf y nsswitch.conf","title":"PAM"},{"location":"pam/#instalacion","text":"Dockerfile: # hostpam FROM fedora:27 LABEL version=\"1.0\" LABEL author=\"Miguel Amor\u00f3s\" LABEL subject=\"hostpam:scratch\" RUN dnf install -y vim util-linux-user-2.30.2-3.fc27.x86_64 finger passwd pam_mount nss-pam-ldapd authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/startup.sh WORKDIR /opt/docker Startup.sh: #! /bin/bash bash /opt/docker/install.sh /sbin/nscd /sbin/nslcd -d Install.sh: #! /bin/bash useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 cp /opt/docker/login.defs /etc/login.defs cp /opt/docker/nslcd.conf /etc/nslcd.conf cp /opt/docker/nslcd.conf /etc/nscd.conf cp /opt/docker/nsswitch.conf /etc/nsswitch.conf authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall #bash /opt/docker/auth.sh","title":"INSTALACION"},{"location":"pam/#ficheros","text":"nsswitch.conf: # # /etc/nsswitch.conf # # An example Name Service Switch config file. This file should be # sorted with the most-used services at the beginning. # # The entry '[NOTFOUND=return]' means that the search for an # entry should stop if the search in the previous entry turned # up nothing. Note that if the search failed due to some other reason # (like no NIS server responding) then the search continues with the # next entry. # # Valid entries include: # # nisplus Use NIS+ (NIS version 3) # nis Use NIS (NIS version 2), also called YP # dns Use DNS (Domain Name Service) # files Use the local files # db Use the local database (.db) files # compat Use NIS on compat mode # hesiod Use Hesiod for user lookups # [NOTFOUND=return] Stop searching if not found so far # # To use db, put the \"db\" in front of \"files\" for entries you want to be # looked up first in the databases # # Example: #passwd: db files nisplus nis #shadow: db files nisplus nis #group: db files nisplus nis passwd: files ldap systemd shadow: files ldap group: files ldap systemd #hosts: db files nisplus nis dns hosts: files dns myhostname # Example - obey only what nisplus tells us... #services: nisplus [NOTFOUND=return] files #networks: nisplus [NOTFOUND=return] files #protocols: nisplus [NOTFOUND=return] files #rpc: nisplus [NOTFOUND=return] files #ethers: nisplus [NOTFOUND=return] files #netmasks: nisplus [NOTFOUND=return] files bootparams: nisplus [NOTFOUND=return] files ethers: files netmasks: files networks: files protocols: files rpc: files services: files sss netgroup: nisplus sss publickey: nisplus automount: files nisplus aliases: files nisplus nslcd.conf: # This is the configuration file for the LDAP nameservice # switch library's nslcd daemon. It configures the mapping # between NSS names (see /etc/nsswitch.conf) and LDAP # information in the directory. # See the manual page nslcd.conf(5) for more information. # The user and group nslcd should run as. uid nslcd gid ldap # The uri pointing to the LDAP server to use for name lookups. # Multiple entries may be specified. The address that is used # here should be resolvable without using LDAP (obviously). #uri ldap://127.0.0.1/ #uri ldaps://127.0.0.1/ #uri ldapi://%2fvar%2frun%2fldapi_sock/ # Note: %2f encodes the '/' used as directory separator uri ldap://ldapserver # The LDAP version to use (defaults to 3 # if supported by client library) #ldap_version 3 # The distinguished name of the search base. base dc=edt,dc=org nscd.conf no hacemos nada Login.defs.conf: # # Please note that the parameters in this configuration file control the # behavior of the tools from the shadow-utils component. None of these # tools uses the PAM mechanism, and the utilities that use PAM (such as the # passwd command) should therefore be configured elsewhere. Refer to # /etc/pam.d/system-auth for more information. # # *REQUIRED* # Directory where mailboxes reside, _or_ name of file, relative to the # home directory. If you _do_ define both, MAIL_DIR takes precedence. # QMAIL_DIR is for Qmail # #QMAIL_DIR Maildir MAIL_DIR /var/spool/mail #MAIL_FILE .mail # Password aging controls: # # PASS_MAX_DAYS Maximum number of days a password may be used. # PASS_MIN_DAYS Minimum number of days allowed between password changes. # PASS_MIN_LEN Minimum acceptable password length. # PASS_WARN_AGE Number of days warning given before a password expires. # PASS_MAX_DAYS 99999 PASS_MIN_DAYS 0 PASS_MIN_LEN 5 PASS_WARN_AGE 7 # # Min/max values for automatic uid selection in useradd # UID_MIN 1000 UID_MAX 60000 # System accounts SYS_UID_MIN 201 SYS_UID_MAX 999 # # Min/max values for automatic gid selection in groupadd # GID_MIN 1000 GID_MAX 60000 # System accounts SYS_GID_MIN 201 SYS_GID_MAX 999 # # If defined, this command is run when removing a user. # It should remove any at/cron/print jobs etc. owned by # the user to be removed (passed as the first argument). # #USERDEL_CMD /usr/sbin/userdel_local # # If useradd should create home directories for users by default # On RH systems, we do. This option is overridden with the -m flag on # useradd command line. # CREATE_HOME yes # The permission mask is initialized to this value. If not specified, # the permission mask will be initialized to 022. UMASK 077 # This enables userdel to remove user groups if no members exist. # USERGROUPS_ENAB yes # Use SHA512 to encrypt password. ENCRYPT_METHOD SHA512 CHFN_RESTRICT no","title":"FICHEROS"},{"location":"powershell/","text":"COMANDOS POWERSHELL WINDOWS DOC Get-Help + COMANDO Especialmente muy \u00fatil para usuarios novatos en el uso de Powershell, este comando presenta una ayuda b\u00e1sica para conocer m\u00e1s acerca de los cmdlets y sus funciones Get-Help ls . ls -Force directorio Para listar directorios, -Force para archivos ocultos. dir // Get-Childitem lista el directorio. cd directorio Para cambiar de directorios mkdir directorio Para crear directorio history Para ver el historial de comandos Cp file dir/file1 // cp dir1 dir/dir1 -Recurse -Verbose Para copiar archivos y directorios con contenido. mv file1 dir/file1 Para mover archivos o cambiar nombres. rm file o rm -Force file o rm dir1 - Recurse Para borrar archivos o directorios. cat file1.txt // cat file1.txt -Head/Tail 10 Ver contenido de un archivo start notepad++ file2.txt editar un archivo. Get-Alias ls ver de donde proviene un comando. Select-String palabra dir/file.txt // ls dir/ -Recurse -Filter *.exe Para buscar palabras dentro de contenido de archivos. Se ha de activar la opci\u00f3n de indexing options en windows para buscar en ellos. cat words.txt | Select-String st > st_words.txt Redireccionamientos de comandos. Get-LocalUser // Get-LocalGroup // Get-LocalGroupMember namegroup ir a Computer Management > Users/groups en gr\u00e1fico. net user miguel \"contrase\u00f1a\" // net user miguel * // net user miguel /logonpasswordchg:yes Cambiar la contrase\u00f1a de un usuario. El * para escribirlo de manera oculta. /logonpasswordchg:yes para que cambie la contrase\u00f1a en el proximo inicio de sesion. net user miguel */contrase\u00f1a /add /logonpasswordchg:yes A\u00f1adir usuario y que cambie al proximo inicio de sesion. net user miguel /del // Remove-LocalUser miguel Borrar un usuario. icacls dir/file ver los permisos de un fichero o archivo. En interfaz, se va a propiedades del fichero o carpeta y se pueden ver/editar. icacls dir/file /grant 'Everyone:(OI)(CI)(R)/Everyone:(OI)(CI)(IO)(R)' Cambiamos permisos de directorio(3) y ficheros(4) en windows. Ayuda de icacls /? Compress-Archive -Path /dir/* /dir/file.zip Comprimir archivos a formato .zip Register-PackageSource -Name chocolatey -ProviderName Chocolatey -Location http://chocolatey.org/api/v2 Instalar repositorio para encontrar softwares y dependencias. Get-PackageSource Ver las fuentes de repositorios Find-Package sysinternals -IncluseDependencies Buscar un paquete Install-Package/Uninstall-Package sysinternals Instalar o borrar un paquete Get-Package -name sysinternals Ver un paquete si est\u00e1 instalado o su info. tasklist / Get-Process|out-gridview // Get-Process | Sort CPU -descending | Select -first 3 - Property ID,RAM,CPU Para ver los procesos del sistema. DOC . Get-WindowsFeature Ver caracteristicas y roles de windows server Install-WindowsFeature ad-domain-services, dns, dhcpserver, dhcp, rsat-dhcp -IncludeAllSubFeature Instalar caracteristicas Get-Command -module dhcpserver Ver los comandos de los modulos instalados. Import-Module / Import-Module addsdeployment Importar modulos. Enable-WindowsOptionalFeature -Online - FeatureName:Microsift-Hyper-V -All Activamos Hyper-V en windows. bcedit /set hypervisorlaunchtype off/auto habilita vmware o hyperV Set-VMProcessor -VMName DC01 -ExposeVirtualizationExtensions $true Para poder tener HyperV dentro de una maquina virtual (virtualizacion anidada) se ha de indicar en la maquina fisica. Get-Host Con la ejecuci\u00f3n de este comando se obtiene la versi\u00f3n de Windows PowerShell que est\u00e1 usando el sistema. Get-History Con este comando se obtiene un historial de todos los comandos que se ejecutaron bajo una sesi\u00f3n de PowerShell y que actualmente se encuentran ejecut\u00e1ndose. Get-LocalUser // Get-LocalUser -Name Miguel|fl Ver usuarios o user concreto en formato lista. Get-LocalGroup // Get-LocalGroup -Name Miguel|fl Ver grupos o grupo concreto en formato lista. Get-SmbShare Ver recursos compartidos. Get-disk // Get-disk -number 1 ver info de discos Get-Partition -Disknumber 0 ver info de una particion de discos Get-NetAdapter // Get-NetIpInterface Ver los adaptadores de red. Get-printer ver info de impresoras. get-eventlog -list // get-eventlog -logname system -index 4095|fl ver registros. Get-CimInstance. ver info del equipo. NetIpAddress ver las ips del ordenador. Get-NetRoute Muestra las rutas de enrutamiento. Get-WinHomeLocation Te dice la ubicacion de tu maquina. Get-Scheduledtask muestra las tareas en funcionamiento. Get-Location nos dice donde estamos. Set-Location dir_name nos hace un cd del directorio. Stop-Computer // Stop-Computer -computername \"dc=miguel\" Para maquinas locales y remotas. Get-Random Ejecutando este comando se obtiene un n\u00famero aleatorio entre 0 y 2.147.483.646. Get-Service Ser\u00e1 necesario saber qu\u00e9 servicios se instalaron en el sistema, para lo que se puede usar el comando Get-Service, que brindar\u00e1 informaci\u00f3n acerca de los servicios que se est\u00e1n ejecutando y los que ya fueron detenidos. Get-Command Windows PowerShell permite descubrir sus comandos y caracter\u00edsticas mediante Get-Command. Muestra la lista de comandos de una funci\u00f3n espec\u00edfica o para un prop\u00f3sito espec\u00edfico basado en tu par\u00e1metro de b\u00fasqueda (Get-Command -service ) Get-Date Para saber de una forma r\u00e1pida qu\u00e9 d\u00eda fue en una determinada fecha del pasado, usando este comando se obtendr\u00e1 el d\u00eda exacto. Copy-Item Con este comando se pueden copiar carpetas o archivos (Copy-Item \"C:\\Proyectos.htm\" -Destination \"C:\\MyData\\Proyectos.txt\".) Invoke-Command En el momento en que quieras ejecutar un script o un comando PowerShell (de forma local o remota, en uno o varios ordenadores), \u00abInvoke-Command\u00bb va a ser tu mejor opci\u00f3n. Es simple de utilizar y te ayudar\u00e1 a gestionar ordenadores por lotes.(Invoke-Command -ScriptBlock {Get-EventLog system -Newest 50} -ComputerName Server01) Invoke-WebRequest A trav\u00e9s de este cmdlet, similar a cURL en Linux, se puede hacer un inicio de sesi\u00f3n, un scraping y la descarga de informaci\u00f3n relacionada a servicios y p\u00e1ginas web, mientras se trabaja desde la interfaz de PowerShell haciendo el monitoreo de alg\u00fan sitio web del que se desee obtener esta informaci\u00f3n ((Invoke-WebRequest \u2013Uri \u2018https://wwww.ebay.com\u2019).Links) Get-Item En caso de que est\u00e9s buscando informaci\u00f3n acerca de un elemento con una ubicaci\u00f3n concreta, como podr\u00eda ser un directorio en el disco duro, el comando Get-Item resulta el indicado para esta tarea (Get-Item file.txt /Home/Documents) Remove-Item En caso de que desees borrar elementos como carpetas, archivos, funciones y variables y claves del registro, Remove-Item ser\u00e1 el mejor cmdlet. Lo importante es que ofrece par\u00e1metros para introducir y expulsar elementos (Remove-Item \"C:\\MyData\\Finanzas.txt\") Get-Content Cuando necesites todo lo que incluye en cuanto a contenido un archivo de texto en una ruta concreta, \u00e1brelo y l\u00e9elo utilizando un editor de textos como el Bloc de Notas (Get-Content \"C:\\Proyectos.htm\" -TotalCount 20) Set-Content Con este cmdlet es posible almacenar texto en un archivo, algo parecido a lo que se puede hacer con \u00abecho\u00bb en el Bash. Si se usa en combinaci\u00f3n con el cmdlet Get-Content, se puede ver primero qu\u00e9 es lo que contiene un determinado archivo para posteriormente hacer la copia a otro archivo a trav\u00e9s de Set-Content (Get-Content \"C:\\Proyectos.htm\" -TotalCount 30 | Set-Content \"Ejemplo.txt\") Get-Variable Si est\u00e1s en PowerShell tratando de utilizar variables, esto podr\u00e1 ser hecho con el cmdlet Get-Variable, con el que vas a poder visualizar dichos valores (Set-Variable -Name \"descuento\" -Value \"Aqu\u00ed se fija el valor\") Start-Process Con este cmdlet, Windows PowerShell hace que sea mucho m\u00e1s f\u00e1cil ejecutar procesos en el equipo (Start-Process -FilePath \u201ccalc\u201d \u2013Verb) Start-Service Si necesitas comenzar un servicio en el PC, el cmdlet Start-Service es el indicado en este caso, sirviendo de igual modo aunque dicho servicio est\u00e9 deshabilitado en el PC (Start-Service -Name \"WSearch\"). Copy-Item Si necesitas copiar archivos y directorios en tu disco de almacenamiento o entradas y claves de registro, puedes usar Copy-Item ( Copy-Item \"Geek.htm\" -Destination \"D:\\BLOG\\Geeks.txt\") ConvertTo-HTML PowerShell puede proporcionar informaci\u00f3n asombrosa sobre tu sistema. Sin embargo, lo presenta principalmente en un formato \u2018indigerible\u2019, por eso puedes usar ConvertTo-HTML para crear y formatear un informe y analizarlo o enviarlo a alguien (Get-Service | ConvertTo-HTML -Property Name, Status > C:\\Users\\Alex\\Desk top\\Servicios.htm) Scripts Los archivos se guardan en formato .ps1 Para hacer un echo: Write-Host \"Hola Mundo\"","title":"PowerShell/CMD"},{"location":"powershell/#comandos-powershell-windows","text":"DOC Get-Help + COMANDO Especialmente muy \u00fatil para usuarios novatos en el uso de Powershell, este comando presenta una ayuda b\u00e1sica para conocer m\u00e1s acerca de los cmdlets y sus funciones Get-Help ls . ls -Force directorio Para listar directorios, -Force para archivos ocultos. dir // Get-Childitem lista el directorio. cd directorio Para cambiar de directorios mkdir directorio Para crear directorio history Para ver el historial de comandos Cp file dir/file1 // cp dir1 dir/dir1 -Recurse -Verbose Para copiar archivos y directorios con contenido. mv file1 dir/file1 Para mover archivos o cambiar nombres. rm file o rm -Force file o rm dir1 - Recurse Para borrar archivos o directorios. cat file1.txt // cat file1.txt -Head/Tail 10 Ver contenido de un archivo start notepad++ file2.txt editar un archivo. Get-Alias ls ver de donde proviene un comando. Select-String palabra dir/file.txt // ls dir/ -Recurse -Filter *.exe Para buscar palabras dentro de contenido de archivos. Se ha de activar la opci\u00f3n de indexing options en windows para buscar en ellos. cat words.txt | Select-String st > st_words.txt Redireccionamientos de comandos. Get-LocalUser // Get-LocalGroup // Get-LocalGroupMember namegroup ir a Computer Management > Users/groups en gr\u00e1fico. net user miguel \"contrase\u00f1a\" // net user miguel * // net user miguel /logonpasswordchg:yes Cambiar la contrase\u00f1a de un usuario. El * para escribirlo de manera oculta. /logonpasswordchg:yes para que cambie la contrase\u00f1a en el proximo inicio de sesion. net user miguel */contrase\u00f1a /add /logonpasswordchg:yes A\u00f1adir usuario y que cambie al proximo inicio de sesion. net user miguel /del // Remove-LocalUser miguel Borrar un usuario. icacls dir/file ver los permisos de un fichero o archivo. En interfaz, se va a propiedades del fichero o carpeta y se pueden ver/editar. icacls dir/file /grant 'Everyone:(OI)(CI)(R)/Everyone:(OI)(CI)(IO)(R)' Cambiamos permisos de directorio(3) y ficheros(4) en windows. Ayuda de icacls /? Compress-Archive -Path /dir/* /dir/file.zip Comprimir archivos a formato .zip Register-PackageSource -Name chocolatey -ProviderName Chocolatey -Location http://chocolatey.org/api/v2 Instalar repositorio para encontrar softwares y dependencias. Get-PackageSource Ver las fuentes de repositorios Find-Package sysinternals -IncluseDependencies Buscar un paquete Install-Package/Uninstall-Package sysinternals Instalar o borrar un paquete Get-Package -name sysinternals Ver un paquete si est\u00e1 instalado o su info. tasklist / Get-Process|out-gridview // Get-Process | Sort CPU -descending | Select -first 3 - Property ID,RAM,CPU Para ver los procesos del sistema. DOC . Get-WindowsFeature Ver caracteristicas y roles de windows server Install-WindowsFeature ad-domain-services, dns, dhcpserver, dhcp, rsat-dhcp -IncludeAllSubFeature Instalar caracteristicas Get-Command -module dhcpserver Ver los comandos de los modulos instalados. Import-Module / Import-Module addsdeployment Importar modulos. Enable-WindowsOptionalFeature -Online - FeatureName:Microsift-Hyper-V -All Activamos Hyper-V en windows. bcedit /set hypervisorlaunchtype off/auto habilita vmware o hyperV Set-VMProcessor -VMName DC01 -ExposeVirtualizationExtensions $true Para poder tener HyperV dentro de una maquina virtual (virtualizacion anidada) se ha de indicar en la maquina fisica. Get-Host Con la ejecuci\u00f3n de este comando se obtiene la versi\u00f3n de Windows PowerShell que est\u00e1 usando el sistema. Get-History Con este comando se obtiene un historial de todos los comandos que se ejecutaron bajo una sesi\u00f3n de PowerShell y que actualmente se encuentran ejecut\u00e1ndose. Get-LocalUser // Get-LocalUser -Name Miguel|fl Ver usuarios o user concreto en formato lista. Get-LocalGroup // Get-LocalGroup -Name Miguel|fl Ver grupos o grupo concreto en formato lista. Get-SmbShare Ver recursos compartidos. Get-disk // Get-disk -number 1 ver info de discos Get-Partition -Disknumber 0 ver info de una particion de discos Get-NetAdapter // Get-NetIpInterface Ver los adaptadores de red. Get-printer ver info de impresoras. get-eventlog -list // get-eventlog -logname system -index 4095|fl ver registros. Get-CimInstance. ver info del equipo. NetIpAddress ver las ips del ordenador. Get-NetRoute Muestra las rutas de enrutamiento. Get-WinHomeLocation Te dice la ubicacion de tu maquina. Get-Scheduledtask muestra las tareas en funcionamiento. Get-Location nos dice donde estamos. Set-Location dir_name nos hace un cd del directorio. Stop-Computer // Stop-Computer -computername \"dc=miguel\" Para maquinas locales y remotas. Get-Random Ejecutando este comando se obtiene un n\u00famero aleatorio entre 0 y 2.147.483.646. Get-Service Ser\u00e1 necesario saber qu\u00e9 servicios se instalaron en el sistema, para lo que se puede usar el comando Get-Service, que brindar\u00e1 informaci\u00f3n acerca de los servicios que se est\u00e1n ejecutando y los que ya fueron detenidos. Get-Command Windows PowerShell permite descubrir sus comandos y caracter\u00edsticas mediante Get-Command. Muestra la lista de comandos de una funci\u00f3n espec\u00edfica o para un prop\u00f3sito espec\u00edfico basado en tu par\u00e1metro de b\u00fasqueda (Get-Command -service ) Get-Date Para saber de una forma r\u00e1pida qu\u00e9 d\u00eda fue en una determinada fecha del pasado, usando este comando se obtendr\u00e1 el d\u00eda exacto. Copy-Item Con este comando se pueden copiar carpetas o archivos (Copy-Item \"C:\\Proyectos.htm\" -Destination \"C:\\MyData\\Proyectos.txt\".) Invoke-Command En el momento en que quieras ejecutar un script o un comando PowerShell (de forma local o remota, en uno o varios ordenadores), \u00abInvoke-Command\u00bb va a ser tu mejor opci\u00f3n. Es simple de utilizar y te ayudar\u00e1 a gestionar ordenadores por lotes.(Invoke-Command -ScriptBlock {Get-EventLog system -Newest 50} -ComputerName Server01) Invoke-WebRequest A trav\u00e9s de este cmdlet, similar a cURL en Linux, se puede hacer un inicio de sesi\u00f3n, un scraping y la descarga de informaci\u00f3n relacionada a servicios y p\u00e1ginas web, mientras se trabaja desde la interfaz de PowerShell haciendo el monitoreo de alg\u00fan sitio web del que se desee obtener esta informaci\u00f3n ((Invoke-WebRequest \u2013Uri \u2018https://wwww.ebay.com\u2019).Links) Get-Item En caso de que est\u00e9s buscando informaci\u00f3n acerca de un elemento con una ubicaci\u00f3n concreta, como podr\u00eda ser un directorio en el disco duro, el comando Get-Item resulta el indicado para esta tarea (Get-Item file.txt /Home/Documents) Remove-Item En caso de que desees borrar elementos como carpetas, archivos, funciones y variables y claves del registro, Remove-Item ser\u00e1 el mejor cmdlet. Lo importante es que ofrece par\u00e1metros para introducir y expulsar elementos (Remove-Item \"C:\\MyData\\Finanzas.txt\") Get-Content Cuando necesites todo lo que incluye en cuanto a contenido un archivo de texto en una ruta concreta, \u00e1brelo y l\u00e9elo utilizando un editor de textos como el Bloc de Notas (Get-Content \"C:\\Proyectos.htm\" -TotalCount 20) Set-Content Con este cmdlet es posible almacenar texto en un archivo, algo parecido a lo que se puede hacer con \u00abecho\u00bb en el Bash. Si se usa en combinaci\u00f3n con el cmdlet Get-Content, se puede ver primero qu\u00e9 es lo que contiene un determinado archivo para posteriormente hacer la copia a otro archivo a trav\u00e9s de Set-Content (Get-Content \"C:\\Proyectos.htm\" -TotalCount 30 | Set-Content \"Ejemplo.txt\") Get-Variable Si est\u00e1s en PowerShell tratando de utilizar variables, esto podr\u00e1 ser hecho con el cmdlet Get-Variable, con el que vas a poder visualizar dichos valores (Set-Variable -Name \"descuento\" -Value \"Aqu\u00ed se fija el valor\") Start-Process Con este cmdlet, Windows PowerShell hace que sea mucho m\u00e1s f\u00e1cil ejecutar procesos en el equipo (Start-Process -FilePath \u201ccalc\u201d \u2013Verb) Start-Service Si necesitas comenzar un servicio en el PC, el cmdlet Start-Service es el indicado en este caso, sirviendo de igual modo aunque dicho servicio est\u00e9 deshabilitado en el PC (Start-Service -Name \"WSearch\"). Copy-Item Si necesitas copiar archivos y directorios en tu disco de almacenamiento o entradas y claves de registro, puedes usar Copy-Item ( Copy-Item \"Geek.htm\" -Destination \"D:\\BLOG\\Geeks.txt\") ConvertTo-HTML PowerShell puede proporcionar informaci\u00f3n asombrosa sobre tu sistema. Sin embargo, lo presenta principalmente en un formato \u2018indigerible\u2019, por eso puedes usar ConvertTo-HTML para crear y formatear un informe y analizarlo o enviarlo a alguien (Get-Service | ConvertTo-HTML -Property Name, Status > C:\\Users\\Alex\\Desk top\\Servicios.htm)","title":"COMANDOS POWERSHELL WINDOWS"},{"location":"powershell/#scripts","text":"Los archivos se guardan en formato .ps1 Para hacer un echo: Write-Host \"Hola Mundo\"","title":"Scripts"},{"location":"python/","text":"Intro Programaci\u00f3n PYTHON Shebang # !/usr/bin/python3 # -*-coding: utf-8-*- Server Python python -m SimpleHTTPDServer 80 Mostrar algo print(\"Uso windows\") Mostrar algo con formato print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}') Importar librerias import platform from xxxx import xx Variables x=100 y=True z=\"Miguel\" Condicional x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\") Bucle for (iterar elementos) food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i) Bucle while food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1 Funciones def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2 Test main if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x) Objetos class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2() Mayusculas, minusculas, letra capital mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize() Ver tipo de dato print(type(w)) #float Listas x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print() Tuplas t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e) Diccionarios dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}') Rangos r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e) List Comprension # de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2) Len len(*args/lista) Objetos # definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\")) Ficheros Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo M\u00f3dulos import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day) AVANZADO PYTHON Todos los COMMANDOS interesantes de python y utilidades que se integran con \u00e9l. import LIBRERIA from MODULO import FUNCION FUNCIONES Definir una funcion: ## FUNCION TABLA MULTIPLICAR def TablaMultiplicar(numero, rango): for i in range(rango+1): print(f'{numero} X {i} = {numero+i}') TablaMultiplicar(7,2) TablaMultiplicar(10,3) Print se pone al llamar la funcion para que te de un return, si no se pone nada, te dar\u00e1 los mensajes print de dentro de la funcion. Variables global dentro de una funcion, se puede utilizar dentro y fuera de esa funcion. Si como argumento ponemos (*argumento), devuelve una tupla. Funcion type para saber que tipo de valor es type(valor) Funcion float, int Transforma el valor en un decimal o entero float(numeroint) Funcion MATH De la liberia math(from match import sqrt,pow). hace una subida exponencial pow(5,2)==5**2 Hace la raiz cuadrada sqrt(100) Valor absoluto de un numero abs(-5) Funciones strings texto.lower(), texto.upper(), texto.capitalize(), texto.title(), texto.swapcase(), texto.strip(), texto.split(' '), texto.replace(' ', 'r'), len(texto). Funciones booleanos text.isupper(),texto.islower(), texto.startswith('a'), texto.endswith('z'). Si es mayusculas, minisculas, si empieza o termina en tal. Funciones listas lista.remove,insert,pop,append,sort,reverse,count,index,max,min,average. lista[1:-3]. Funciones diccionarios dict.pop,update,get,setdefault,copy,items,values,keys. Funciones conjuntos y tuplas conjunto.add, update, remove, discard, update, pop, clear. tupla[posicion] MODULOS Es un fichero con conjunto de funciones que podemos importar en otros ficheros. Todas enteras import modulo.py En concreto from modulo.py import funcion1.py GESTION ERRORES while True: try: edad = int(input(\"Dime tu edad: \")) print(\"Tu edad es:\", edad) break except ZeroDivisionError: print(\"No se puede dividir entre zero\") except ValueError: print(\"Numero incorrecto\") except KeyboardInterrupt: print(\"Has cancelado la ejecucion\") break finally: print(\"codigo ejecutado correctamente\") Hay varios except como divisionzero, keyboardinterrupt, error value... GESTION DE FICHEROS Leer un fichero: #!/usr/bin/python3 # leer un fichero en modo read y texto y lo guardamos en una variable fichero = open(\"./texto.txt\", \"rt\") # ver los datos datos_fichero = fichero.read() print(datos_fichero) Crear un fichero poniendo texto: #!/usr/bin/python3 # creamos un fichero y metodo write y texto para que escriba fichero = open(\"./texto2.txt\", \"wt\") # escribir los datos texto = \"Esto es un ejemplo para\\nescribir en un fichero\\nque vamos a grabar.\\n\" fichero.write(texto) # cerramos dichero fichero.close() # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. A\u00f1adir texto a un fichero: #!/usr/bin/python3 # creamos un fichero y metodo append y texto para que a\u00f1ada cosas al file fichero = open(\"./texto3.txt\", \"at\") # escribir los datos texto = \"\\nEsto es un ejemplo para\\na\u00f1adir texto\\n\" fichero.write(texto) # cerramos dichero fichero.close() # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. Borrar un fichero: #!/usr/bin/python3 # importamos un modulo para usar el metodo de borrar fichero del sistema import os # creamos un fichero y metodo append y texto para que a\u00f1ada cosas al file os.remove(\"./texto3.txt\") # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. Creamos un modulo de objeto Fichero y tratamos estas funciones en un programa: #!/usr/bin/python3 class Fichero: def __init__(self,nombre): self.nombre = nombre def grabar_fichero(self,texto): fichero = open(self.nombre, \"wt\") fichero.write(texto) fichero.close() def incluir_fichero(self,texto): fichero = open(self.nombre, \"at\") fichero.write(texto) fichero.close() def leer_fichero(self): fichero = open(self.nombre, \"rt\") texto = fichero.read() return texto #!/usr/bin/python3 # importamos el modulo import ficheros_modulo # creamos nombre del fichero nombre_fichero = \"fichero1.txt\" fichero = ficheros_modulo.Fichero(nombre_fichero) # en el fichero que vamos a crear, tiene este texto texto = \"Esta es la linea para a\u00f1adir.\\nEsta la segunda linea.\\n\" fichero.grabar_fichero(texto) # a\u00f1adimos al fichero este texto texto = \"Esta es la linea para a\u00f1adir.\\n\" fichero.incluir_fichero(texto) # leemos lo que tenemos de fichero leer_texto = fichero.leer_fichero() print(leer_texto) Tratamiento de binarios en python: #!/usr/bin/python3 # importamos el modulo para ficheros binarios import pickle # leer en modo binarios fichero = open(\"binarios_colores.pckl\", \"rb\") lista_colores = pickle.load(fichero) print(lista_colores) #!/usr/bin/python3 # importamos el modulo para ficheros binarios import pickle # a\u00f1adimos a un fichero una lista de colores en modo binarios colores = [\"azul\", \"amarillo\", \"verde\"] fichero = open(\"binarios_colores.pckl\", \"wb\") pickle.dump(colores, fichero) fichero.close() EXPRESIONES REGULARES Expresiones regulares con el modulo RE: #!/usr/bin/python3 # importamos el modulo de expresiones regulares import re texto = \"Hola me llamo miguel\" # SEARCH # buscamos ese patron dentro del texto resultado = re.search(\"llamo\",texto) # que acabe en miguel$, que empiece en ^miguel # que haya algo entre me.*miguel if resultado: print(\"OK\") else: print(\"No encontado\") # FINDALL texto2 = ''' el coche de miguel es rojo el coche de natalia es blanco ''' resultado = re.findall(\"coche.*rojo\", texto2) if resultado: print(\"OK\") else: print(\"No encontado\") # SPLIT texto3 = \"La silla es blanca y vale 80\" resultado = re.split(\"\\s\", texto3) if resultado: print(resultado) else: print(\"No encontado\") # SUB texto4 = \"La silla es blanca y vale 80\" resultado = re.sub(\"blanca\", \"roja\", texto4) if resultado: print(resultado) else: print(\"No encontado\") CONVERT JSON #!/usr/bin/python3 # importamos el modulo de json import _json producto1 = {\"nombre\":\"miguel\", \"apellido\":\"amoros\"} estructura_json = _json.dumps(producto1) print(estructura_json) producto2 = _json.loads(estructura_json) print(producto2) FECHA Y HORA #!/usr/bin/python3 # importamos el modulo de json from datetime import datetime fechayhora = datetime.now() print(fechayhora) a\u00f1o = fechayhora.year mes = fechayhora.month dia = fechayhora.day hora = fechayhora.hour minutos = fechayhora.minute segundos = fechayhora.second microsegundos = fechayhora.microsecond print(f\"La hora es {hora} : {minutos}\") print(f\"La fecha es {dia} {mes} {a\u00f1o}\") SQLite Crear base de datos con python con el modulo sqlite: TUTORIAL Podemos descargar la herramienta para visualizar las cosas y manejar la bd. Ejemplo de creacion de bbdd: #!/usr/bin/python3 # importamos el modulo de bd sqlite import sqlite3 # creamos/existente bbdd conexion = sqlite3.connect(\"bbdd1.db\") # sirve para poder hacer sentencias sql dentro cursor = conexion.cursor() # creamos una tabla cursor.execute(\"CREATE TABLE PERSONAS (nombre TEXT, apellido1 TEXT, apellido2 TEXT, edad INTEGER)\") # creamos fila cursor.execute(\"INSERT INTO PERSONAS VALUES ('Antonio', 'Perez', 'Gomez', 35)\") # creamos varias filas lista_personas = [('Miguel', 'Amoros', 'Moret', 28), ('Natalia', 'Sendra', 'Soler', 26)] cursor.executemany(\"INSERT INTO PERSONAS VALUES (?,?,?,?)\", lista_personas) # consulta de datos cursor.execute(\"SELECT * FROM PERSONAS\") personas = cursor.fetchall() for persona in personas: print(persona) # consulta datos con WHERE cursor.execute(\"SELECT * FROM PERSONAS WHERE edad > 28\") personas_edad = cursor.fetchall() for persona in personas_edad: print(persona) # consulta datos con WHERE y ordenado cursor.execute(\"SELECT * FROM PERSONAS WHERE edad <= 28 ORDER BY edad DESC\") personas_edad = cursor.fetchall() for persona in personas_edad: print(persona) # borrar datos cursor.execute(\"DELETE FROM PERSONAS WHERE nombre = 'Antonio'\") # actualizar datos cursor.execute(\"UPDATE PERSONAS SET nombre = 'Miguelito' where edad = 28\") # mantener el registro guardado conexion.commit() # cerramos bdd conexion.close() TKINTER TKINTER Es un modulo de python que sirve para crear una interfaz grafica. Ejemplo de lo que se puede hacer: #!/usr/bin/python3 # tkinter - componente raiz import tkinter from tkinter import filedialog # definimos la ventana de la aplicacion raiz = tkinter.Tk() raiz.title(\"Mi programa\") # definimos aspecto de la ventana frame = tkinter.Frame(raiz) frame.config(fg=\"blue\",width=400,height=300) frame.pack() # construimos un label texto = \"Hola mundo\" label = tkinter.Label(raiz,text=texto) label.config(fg=\"green\",bg=\"grey\",font=(\"Cortana\",30)) label.pack() # construimos una entrada por teclado entrada = tkinter.Entry(raiz) entrada.config(justify=\"center\", show=\"*\") entrada.pack() # construimos una box de texto box = tkinter.Text(raiz) box.config(width=20,height=10,font=(\"Verdana\",15),padx=10,pady=10,fg=\"green\",selectbackground=\"yellow\") box.pack() # construimos una boton que salte mensaje def accion(): print(\"Hola mundo\") boton = tkinter.Button(raiz,text=\"Ejecutar\",command=accion) boton.config(fg=\"green\") boton.pack() # construimos un multiseleccion def accion(): print(f\"La opcion es {opcion.get()}\") seleccion1 = tkinter.Radiobutton(raiz,text=\"Opcion 1\",variable=opcion,command=accion) seleccion1.pack() seleccion2 = tkinter.Radiobutton(raiz,text=\"Opcion 2\",variable=opcion,command=accion) seleccion2.pack() # construimos un checkbutton def verificar(): valor = check1.get() if valor == 1: print(\"La opcion est\u00e1 activada\") else: print(\"La opcion est\u00e1 activada\") check1 = tkinter.IntVar() boton1 = tkinter.Checkbutton(raiz,text=\"Opcion 1\",variable=check1,onvalue=1,offvalue=0,command=verificar) boton1.pack() # construimos boton con popup def avisar(): tkinter.messagebox.showinfo(\"Titulo\",\"Mensaje con la info\") boton = tkinter.Button(raiz,text=\"Pulsar para aviso\",command=avisar) boton.pack() # construimos boton con popup para responder si o no def preguntar(): tkinter.messagebox.askquestion(\"Titulo\",\"Quieres borrar la info?\") boton = tkinter.Button(raiz,text=\"Pulsar para preguntar\",command=preguntar) boton.pack() # construimos boton para a\u00f1adir fichero from tkinter import filedialog def abrirfichero(): rutafichero = filedialog.askopenfilename(title=\"Abrir un fichero\") print(rutafichero) boton = tkinter.Button(raiz,text=\"Pulsar para abrir fichero\",command=abrirfichero) boton.pack() # para que siga ejecutandose raiz.mainloop() PYDOC Es un comando que sirve para generar documentacion de una clase y sus funciones: pydoc /ruta/programa.py pydoc -w /ruta/programa.py para tenerla en html. DOCTEST Sirve para hacer pruebas en los docstrings de las funciones: # DOCTEST PARA PODER HACER PRUEBAS EN EL DOCSTRING def sumar(num1,num2): \"\"\" Esto es una funcion para sumar numeros >>> sumar(4,3) 7 >>> sumar(4,3) 8 >>> sumar(2,3) 5 \"\"\" return num1+num2 print(sumar(2,2)) # se importa al final la libreria para hacer las pruebas import doctest doctest.testmod() 4 ********************************************************************** File \"/home/miguel/Documents/curso_python2022/prueba_doctest.py\", line 9, in __main__.sumar Failed example: sumar(4,3) Expected: 8 Got: 7 ********************************************************************** 1 items had failures: 1 of 3 in __main__.sumar ***Test Failed*** 1 failures. UNISTEST Modulo que sirve para hacer test . #!/usr/bin/python3 # DOCTEST PARA PODER HACER PRUEBAS EN EL DOCSTRING def sumar(num1,num2): \"\"\" Esto es una funcion para sumar numeros \"\"\" return num1+num2 print(sumar(2,2)) # se importa al final la libreria para hacer las pruebas import unittest class pruebas(unittest.TestCase): def test(self): self.assertEqual(sumar(4,5),9) self.assertEqual(sumar(4,5),19) # prueba if __name__ == '__main__': unittest.main() 4 . ---------------------------------------------------------------------- Ran 1 test in 0.000s OK NUMPY NUMPY s una librer\u00eda de Python especializada en el c\u00e1lculo num\u00e9rico y el an\u00e1lisis de datos, especialmente para un gran volumen de datos. Incorpora una nueva clase de objetos llamados arrays que permite representar colecciones de datos de un mismo tipo en varias dimensiones, y funciones muy eficientes para su manipulaci\u00f3n. Instalamos libreria con pip3 install numpy Ejemplos de uso: #!/usr/bin/python3 # importamos la liberia import numpy as np # creamos arrays lista1 = [1,2,3,4] lista2 = [5,6,7,8] print(np.zeros(4)) # crea array de 4 ceros print(np.ones(5)) # crea array de 5 unos np.arange(5) # crea array de 0 a 4 np.arange(2,10) array1 = np.array(lista1) array2 = np.array([1,2,3,4,5,6]) print(array1) # array de dos dimensiones lista_doble = (lista1,lista2,lista2,lista1) print(lista_doble) array_doble = np.array(lista_doble) print(array_doble) # formas que tiene el array print(array_doble.shape) # 4 filas y 4 columnas print(array_doble.dtype) # numeros int # operaciones con arrays print(array1 + 4) # multiplica todo lo de dentro por 4 pero no queda asignado array_multiplicado = array1 * 3 print(array_multiplicado) # indexacion / slicing array3 = np.arange(10) print(array3) print(array3[0:3]) array4 = np.copy(array3) array4[0:4] = 20 print(array3,array4) # matrices array5 = np.arange(15).reshape(3,5) # 3 filas y 5 columnas print(array5) print(array5[1][1]) array6 = array5.T # cambio el orden de filas y columnas print(array6) # entrada / salida de arrays array7 = np.arange(6) np.save(\"array7\", array7) # guarda ese array con ese nombre print(np.load(\"array7.npy\")) # carga el archivo con el nombre del array array8 = np.arange(8) np.savez(\"arrayxy\", x=array7, y=array8) # guarda en tipo coordenadas x y array_xy = np.load(\"arrayxy.npz\") print(array_xy) print(array_xy[\"x\"]) print(array_xy[\"y\"]) np.savetxt(\"mifilearray.txt\", array8, delimiter=\",\") # guarda en un file txt print(np.loadtxt(\"mifilearray.txt\", delimiter=\",\")) # funciones print(np.sqrt(array1)) #raiz cuadrada print(np.random(5)) # 5 numeros aleatorios print(np.add(array1, array2)) # suma las dos print(np.maxium(array1,array2)) # elije el maximo de entre las dos arrays # funcion par e impar def pares(inicio,fin): if inicio % 2 == 0: array10 = np.arange(inicio,fin,2) else: inicio +=1 array10 = np.arange(inicio,fin) return array10 print(pares(0,21)) PANDAS PANDAS es una librer\u00eda de Python especializada en el manejo y an\u00e1lisis de estructuras de datos. Las principales caracter\u00edsticas de esta librer\u00eda son: Define nuevas estructuras de datos basadas en los arrays de la librer\u00eda NumPy pero con nuevas funcionalidades. Permite leer y escribir f\u00e1cilmente ficheros en formato CSV, Excel y bases de datos SQL. Permite acceder a los datos mediante \u00edndices o nombres para filas y columnas. Ofrece m\u00e9todos para reordenar, dividir y combinar conjuntos de datos. Permite trabajar con series temporales. Realiza todas estas operaciones de manera muy eficiente. PARA QUE QUEDE EL CAMBIO HECHO SIEMPRE HAY QUE ASIGNAR LA FUNCION QUE SE HAGA A LA SERIE O DATAFRAME DEL CUAL SE HACE ALGO. Metodo SERIES para crear indices: #!/usr/bin/python3 # importamos la liberia from logging.config import dictConfig import pandas as pd ## INDICES # te crea una serie de indices #0 1 serie1 = pd.Series([1,5,7]) #1 5 print(serie1) # ver un indice print(serie1[1]) #5 # crear un indice asignaturas = [\"mates\", \"sociales\", \"fisica\"] notas = [8,9,5] serie_notas_miguel = pd.Series(notas,asignaturas) print(serie_notas_miguel) print(serie_notas_miguel[\"mates\"]) #condiciones print(serie_notas_miguel[serie_notas_miguel>7]) #asignamos nombres serie_notas_miguel.name = \"Notas de Miguel\" serie_notas_miguel.index.name = \"Asignaturas de Bachiller\" print(serie_notas_miguel) # convertir la serie en un diccionario diccionario = serie_notas_miguel.to_dict() print(diccionario) serie = pd.Series(diccionario) print(serie) #operaciones con indices con las series notas2 = [1,5,3] serie_notas_natalia = pd.Series(notas2,asignaturas) print(serie_notas_natalia) notas_medias = (serie_notas_miguel+serie_notas_natalia)/2 print(notas_medias) Metodo DATAFRAMES: #!/usr/bin/python3 # importamos la liberia import pandas as pd import webbrowser # sirve para paginas web ## DATAFRAMES # sirve para abrir esta web website = \"https://es.wikipedia.org/wiki/Anexo:Campeones_de_la_NBA\" webbrowser.open(website) # copiamos algo al portapapeles de esa web y lo asignamos a un dataframe dataframe_nba = pd.read_clipboard() print(dataframe_nba) # te dice el nombre de las columnas print(dataframe_nba.columns) print(dataframe_nba.columns[\"Campe\u00f3n del Oeste\"]) # ver la info por el indice print(dataframe_nba.loc[5]) # ver los 5 primeros con head o especificos print(dataframe_nba.head()) print(dataframe_nba.head(7)) print(dataframe_nba.tail()) print(dataframe_nba.tail(4)) # dataframe con un diccionario asignaturas = [\"Mates\", \"Reli\", \"Sociales\", \"Natus\"] notas = [8,5,7,8] diccionario = { \"Asignaturas\": asignaturas, \"Notas\": notas} print(diccionario) # a raiz del diccionario te crea un dataframe con los nombres, notas e indices dataframe_notas = pd.DataFrame(diccionario) print(dataframe_notas) Metodo INDICES: #!/usr/bin/python3 # importamos la liberia import pandas as pd # INDICES lista_valores = [1,2,3] lista_indices = [\"a\", \"b\", \"c\"] serie = pd.Series(lista_valores,index=lista_indices) print(serie) # a 1 b 2 c 3 print(serie.index) # a b c print(serie.index[0]) # a lista_valores2 = [[1,2,3], [8,9,10], [3,5,6]] lista_indices2 = [\"Mates\", \"Natus\", \"Fisica\"] lista_nombres = [\"Miguel\",\"Natalia\", \"Cristina\"] # creamos el dataframe indicando indices, columnas y datos dataframe = pd.DataFrame(lista_valores2, index=lista_indices2, columns=lista_nombres) print(dataframe) print(dataframe.index) print(dataframe.index[2]) Si se crea mas indices que valores, se crean subindices y si usamos serie.unstack() convierte los otros indices en columnas. Para deshacer o el proceso inverso seria dataframe.stack() . Metodo BORRAR DATOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # ELIMINAR DATOS ## INDICES print(np.arange(4)) # creamos una serie a partir de un array de numpy y ponemos los indices serie = pd.Series(np.arange(4), index=[\"a\",\"b\",\"c\",\"d\"]) print(serie) # eliminamos indices serie.drop(\"c\") ## DATAFRAMES print(np.arange(9).reshape(3,3)) lista_valores = np.arange(9).reshape(3,3) lista_indices = [\"a\",\"b\",\"c\"] lista_columnas = [\"c1\",\"c2\",\"c3\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices,columns=lista_columnas) print(dataframe) dataframe.drop(\"b\") dataframe.drop(\"c2\", axis=1) Metodo SELECCIONAR DATOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # SELECCIONAR DATOS # en series lista_valores = np.arange(3) # 0 1 2 lista_indices = [\"i1\",\"i2\",\"i3\"] serie = pd.Series(lista_valores,index=lista_indices) print(serie) # multiplica los datos serie = serie *2 print(serie) # seleccionar datos print(serie[\"i1\"]) print(serie[1]) print(serie[0:2]) print(serie[serie>2]) serie[serie>2] = 6 print(serie) # en datafranes lista_valores2 = np.arange(25).reshape(5,5) lista_indices2 = [\"i1\",\"i2\",\"i3\",\"i4\",\"i5\"] lista_columnas2 = [\"c1\",\"c2\",\"c3\",\"c4\",\"c5\"] dataframe = pd.DataFrame(lista_valores2,index=lista_indices2,columns=lista_columnas2) print(dataframe) print(dataframe[\"c2\"]) # todo lo de la c2 print(dataframe[\"c2\"][\"i2\"]) # valor concreto print(dataframe[dataframe[\"c2\"]>15]) # los datos de donde la columnas2 print(dataframe>20) # muestra cada valor si es true o false print(dataframe.loc[\"i2\"]) # fila i2 Metodo ORGANIZAR Y CLASIFICAR ESTADISTICAS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # ORDENAR Y CLASIFICAR lista_valores = range(4) lista_indices = list(\"bcad\") lista_columnas = list(\"wxyz\") serie = pd.Series(lista_valores,index=lista_indices) print(serie) print(serie.sort_index()) print(serie.sort_values()) print(serie.rank()) # ordena ppor ranking de clasificaciones # esto crea una sserie con 10 datos randoms serie2 = pd.Series(np.random.randn(10)) print(serie2) print(serie2.rank()) # CLASIFICAR DATOS lista_indices2 = list(\"abc\") lista_columnas2 = list(\"xyz\") dataframe = pd.DataFrame(np.arange(9).reshape(3,3),index=lista_indices2, columns=lista_columnas2) print(dataframe) print(dataframe.describe()) print(dataframe.sum()) print(dataframe.max()) print(dataframe.max(axis=1)) print(dataframe.min()) print(dataframe.idxmin()) Metodo para ver los VALORES NULOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # VALORES NULOS lista_valores = [1,2,np.nan,4] lista_indices = list(\"abcd\") serie = pd.Series(lista_valores,index=lista_indices) print(serie) print(serie.isnull()) # ver booleanos los nulos print(serie.dropna()) # borrar los nulos lista_valores2 = [[1,2,1],[1,np.nan,2],[1,2,np.nan]] lista_indices2 = list(\"123\") lista_columnas2 = list(\"abc\") dataframe = pd.DataFrame(lista_valores2,index=lista_indices2,columns=lista_columnas2) print(dataframe) print(dataframe.isnull()) print(dataframe.dropna()) print(dataframe.fillna(0)) # rellena los nulos con 0 Metodo en HTML: #!/usr/bin/python3 import pandas as pd # TRATAMIENTO DE DATOS EN URL HTML url = \"https://es.wikipedia.org/wiki/Anexo:Finales_de_la_Copa_Mundial_de_F%C3%BAtbol\" # cogemos los datos de la url dataframe_futbol = pd.io.html.read_html(url) # cambiamos los campos de columnas por los que queremos print(dataframe_futbol.loc[0]) # creamos un diccionario con los nombres de las columnas diccionario = dict(dataframe_futbol.loc[0]) print(diccionario) # lo asignamos al nuevo dataframe dataframe_futbol = dataframe_futbol.rename(columns=diccionario) # ahora borramos la fila 1 que se repite y borramos la columna notas dataframe_futbol = dataframe_futbol.drop(0) dataframe_futbol = dataframe_futbol.drop(\"Notas\",axis=1) print(dataframe_futbol) Metodo en EXCEL: #!/usr/bin/python3 import pandas as pd # TRATAMIENTO DE DATOS EN FILE EXCELL # cogemos los datos deL FICHERO file_excell = pd.ExcelFile(\"/home/miguel/Documents/curso_python2022/pandas/poblacion.xlsx\") # en csv seria: # file_csv = pd.read_csv(\"/home/miguel/Documents/curso_python2022/pandas/poblacion.xlsx\") # parseamos en que hoja est\u00e1 dataframe = file_excell.parse(\"Hoja1\") print(dataframe) TRATAMIENTO DATOS DATAFRAMES UNION DE DATAFRAMES: #!/usr/bin/python3 import pandas as pd dataframe1 = pd.DataFrame({\"c1\" : [\"1\",\"2\",\"3\"], \"clave\" : [\"a\",\"b\",\"c\"]}) dataframe2 = pd.DataFrame({\"c2\" : [\"4\",\"5\",\"6\"], \"clave\" : [\"c\",\"b\",\"e\"]}) # hacer una union de datafames segun columna. Une por claves iguales dataframe3 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\") print(dataframe3) # une igual pero manteniendo los datos del de la izquierda dataframe4 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\",how=\"left\") print(dataframe4) # une igual pero manteniendo los datos del de la derecha dataframe5 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\",how=\"right\") print(dataframe5) CONCATENAR Y COMBINAR DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np # concatenamos los dos arrays array1 = np.arange(9).reshape(3,3) array_concatenado = np.concatenate([array1,array1]) print(array_concatenado) # concatenamos series poniendo los campos de serie serie1 = pd.Series([1,2,3], index=[\"a\",\"b\",\"c\"]) serie2 = pd.Series([4,5,6], index=[\"d\",\"e\",\"f\"]) serie_concatenada = pd.concat([serie1,serie2], keys=[\"serie1\",\"serie2\"]) print(serie_concatenada) # concatenamos datafranes dataframe1 = pd.DataFrame(np.random.rand(3,3), columns=[\"a\",\"b\",\"c\"]) #3 filas 3 columnas dataframe2 = pd.DataFrame(np.random.rand(2,3), columns=[\"a\",\"b\",\"c\"]) dataframe3 = pd.concat([dataframe1,dataframe2],keys=[\"dataframe1\",\"dataframe2\"]) print(dataframe3) # combinar serie y dataframes serie_combinada = serie1.combine_first(serie2) print(serie_combinada) dataframe_combinado = dataframe1.combine_first(dataframe2) print(dataframe_combinado) DUPLICAR DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = [[1,2],[1,2],[5,6],[5,8]] lista_indices = list(\"mnop\") lista_columnas = [\"columna1\",\"columna2\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices, columns=lista_columnas) print(dataframe) # BORRAR LOS DUPLICADOS dataframe_duplicado = dataframe.drop_duplicates() print(dataframe_duplicado) # BORRAR LOS DUPLICADOS PARA SOLO VALORES UNICOS EN UNA COLUMNA Y MANTENER EL ULTIMO VALOR dataframe_duplicado_columna = dataframe_duplicado.drop_duplicates([\"columna1\"], keep=\"last\") print(dataframe_duplicado_columna) METODO REEMPLAZAR SERIES: #!/usr/bin/python3 import pandas as pd import numpy as np # REEMPLAZAR SERIES serie = pd.Series([1,2,3,4,5,6]) print(serie) # reemplaza el 1 por el 10 serie1 = serie.replace(1,10) print(serie1) serie2 = serie.replace({1:10,2:20}) print(serie2) METODO PARA RENOMBRAR INDICES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = np.arange(9).reshape(3,3) lista_indices = list(\"abc\") lista_columnas = [\"columna1\",\"columna2\",\"columna3\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices, columns=lista_columnas) print(dataframe) # REEMPLAZAMOS LOS INDICES # POR MAYUSCULAS nuevos_indices = dataframe.index.map(str.upper) dataframe.index = nuevos_indices print(dataframe) # POR RENAME dataframe = dataframe.rename(index=str.lower) print(dataframe) # NUEVOS INDICES indices_nuevos = {\"a\":\"f\",\"b\":\"w\",\"c\":\"z\"} dataframe = dataframe.rename(index=indices_nuevos) print(dataframe) # O SOLO UNO DE LOS QUE HAY indices_uno = {\"f\":\"xxx\"} dataframe = dataframe.rename(index=indices_uno) print(dataframe) METODO PARA AGRUPAR CATEGORIAS: #!/usr/bin/python3 import pandas as pd import numpy as np precio = [42,50,45,23,5,21,88,34,26] rango = [10,20,30,40,50,60,70,80,90,100] # AGRUPAR CATEGORIAS # te sale en que rango est\u00e1 cada precio de la lista de rangos precio_con_rango = pd.cut(precio,rango) print(precio_con_rango) # cuenta cuantos hay para cada rango de precios print(pd.value_counts(precio_con_rango)) METODO PARA FILTRAR INDICES: #!/usr/bin/python3 import pandas as pd import numpy as np # 10 filas por 3 columnas lista_valores = np.random.rand(10,3) # creamos un dataframe dataframe = pd.DataFrame(lista_valores) print(dataframe) # filtramos por columna y valor columna0 = dataframe[0] print(columna0) print(columna0[columna0>0.40]) # tmb por dataframe se puede print(dataframe[dataframe>0.40]) METODO PARA COMBINAR ELEMENTOS: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = np.arange(25).reshape(5,5) dataframe = pd.DataFrame(lista_valores) # cambiamos los indices por una combinacion aleatoria combinacion = np.random.permutation(5) print(combinacion) # ahora ordena los valores segun el indice de la combinacion print(dataframe.take(combinacion)) METODO PARA AGRUPAR POR COLUMNAS GROUPBY: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = {\"clave1\": [\"x\",\"x\",\"y\",\"y\",\"z\"],\"clave2\": [\"a\",\"b\",\"a\",\"b\",\"a\"], \"datos1\": np.random.rand(5), \"datos2\": np.random.rand(5)} dataframe = pd.DataFrame(lista_valores) print(dataframe) # queremos agrupar datos1 con clave1 agrupacion = dataframe[\"datos1\"].groupby(dataframe[\"clave1\"]) print(agrupacion) # te hace una media de x y z print(agrupacion.mean()) METODO PARA AGREGACIONES EN DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = [[1,2,3],[4,5,6],[7,8,9],[np.nan,np.nan,np.nan]] lista_columnas = list(\"abc\") dataframe = pd.DataFrame(lista_valores,columns=lista_columnas) print(dataframe) # agregaciones de suma, minimo sobre los datos print(dataframe.agg([\"sum\",\"min\"])) # por fila print(dataframe.agg(\"sum\",axis=1)) SEABORN SEABORN es una librer\u00eda para Python que permite generar f\u00e1cilmente elegantes gr\u00e1ficos. Seaborn esta basada en matplotlib y proporciona una interfaz de alto nivel que es realmente sencilla de aprender. Dada su gran popularidad se encuentra instalada por defecto en la distribuci\u00f3n Anaconda. La representaci\u00f3n de datos es una tarea clave del an\u00e1lisis de datos. La utilizaci\u00f3n de una gr\u00e1fica adecuada puede hacer que los resultados y conclusiones se comuniquen de una forma adecuada o no. Conocer y manejar diferentes herramientas es clave para poder seleccionar la gr\u00e1fica adecua en cada ocasi\u00f3n. En esta entrada se va a repasar b\u00e1sicamente las funciones que ofrece la librer\u00eda Seaborn. INSTALACION pip3 install seaborn SELENIUM WEB DRIVER Es una herramienta que sirve para automatizar pruebas de testing en navegadores web. No se instala de manera nativa, sino en un IDE se corre como PyCharm. Multiplataforma. No tiene soporte, solo es para navegadores web. PyCharm PyCharm es este famoso IDE que adem\u00e1s cuenta con una versi\u00f3n para las distribuciones Gnu/Linux, lo que hace que sea m\u00e1s sencillo a\u00fan su utilizaci\u00f3n y creaci\u00f3n de programas con este lenguaje de programaci\u00f3n.PyCharm es un IDE, es decir, no solo es un editor de c\u00f3digo sino que tambi\u00e9n tiene un depurador, un interprete y otras herramientas que nos ayudar\u00e1n a crear y exportar los programas que creemos. PyCharm tiene un interprete en el editor de c\u00f3digo que nos ayudar\u00e1 a saber o conocer los posibles errores del c\u00f3digo en tiempo real, algo que ha hecho que Python y PyCharm sean elegidos por muchos usuarios que comienzan a programar. IDE: Un entorno de desarrollo integrado\u200b\u200b o entorno de desarrollo interactivo, en ingl\u00e9s Integrated Development Environment, es una aplicaci\u00f3n inform\u00e1tica que proporciona servicios integrales para facilitarle al desarrollador o programador el desarrollo de software. Instalamos pycharm desde la web indicada, extraemos el tar e inicimos desde el directorio bin con ./pycharm.sh . Selenium Para poder usar Selenium en pycharm tenemos que instalarlo en la web selenium y luego instalar la version de python. Instalamos con pip install selenium dentro de la terminal de pycharm. Ahora necesitamos instalar drivers de selenium . En este caso vamos a instalar el del navegador de google chrome pero todos se instalan del mismo modo. Una vez nos bajamos el zip de chrome, descomprimimos, copiamos el chromedriver.exe, vamos a pycharm y boton derecho a nuestro proyecto y creamos un python file de prueba. Despues de nuevo creamos un package file de nombre drivers y dentro de el, boton derecho y pegamos el driver. Ya solo tendremos que descargar los navegadores que queramos y peguemos en esta carpeta los ficheros ejecutables de drivers. Ahora hay que hacer un peque\u00f1o script para ver que todo esto funcione de pycharm con selenium en el navegador: # importamos la implementacion que crea una instancia para conectarse a un navegador from selenium import webdriver from selenium.webdriver.common.by import By # importamos libreria time import time # llamamos al driver y su path para abrir chrome (mejor llamar desde ruta original) #driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver\") controlador = webdriver.Chrome(executable_path=r\"/home/miguel/Downloads/chromedriver_linux64/chromedriver\") # maximiza la ventana total del navegador controlador.maximize_window() # metodo get nos indica la url a la que conectarnos controlador.get(\"https://www.udemy.com/join/login-popup/?locale=es_ES&response_type=html&next=https%3A%2F%2Fwww.udemy.com%2Fjoin%2Flogin-popup%2F%3Flocale%3Des_ES%26response_type%3Dhtml%26next%3Dhttps%253A%252F%252Fwww.udemy.com%252Fes%252F%253Futm_source%253Dadwords-brand%2526utm_medium%253Dudemyads%2526utm_campaign%253DNEW-AW-PROS-Branded-Search-SP-SPA_._ci__._sl_SPA_._vi__._sd_All_._la_SP_._%2526tabei%253D7%2526utm_term%253D_._ag_53604040718_._ad_254061738916_._de_c_._dm__._pl__._ti_kwd-357002749620_._li_1005424_._pd__._%2526gclid%253DCj0KCQjw-uH6BRDQARIsAI3I-Ud3hC1QNzFFLCPuZ6H6BbB4sNh5StLf3qvjF1S-mVR0WaM8fs7gOeEaAr_HEALw_wcB%2526persist_locale%253D%2526locale%253Des_ES\") # inspeccionamos el codigo fuente del udemy y vamos a la seccion de usuario/password y cppiamos el id usuario = controlador.find_element(By.ID, \"email--1\") password = controlador.find_element(By.ID, \"id_password\") # ingresaremos los datos directamente. Ponemos un tiempo para ver resultados. usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) password.send_keys(\"12345678910\") time.sleep(1) # ahora inspeccionamos el boton de iniciar sesion y su id. Despues clicamos boton = controlador.find_element(By.ID, \"submit-id-submit\") boton.click() time.sleep(3) # cerramos controlador.quit() XPATH Sirve para extraer informacion xhtml en las webs a trav\u00e9s de componentes y etiquetas segun su ubicacion. A\u00f1adimos la extension CROPATH Ejemplos (//etiqueta[@atributo=\"valor\"]) //div[@data-purpose='lecture-title'] Ejemplo con selenium en pycharm ruta relativa: from selenium import webdriver from selenium.webdriver.common.by import By import time driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver.exe\") driver.get(\"https://www.udemy.com/join/login-popup/?skip_suggest=1&locale=es_ES&next=https%3A%2F%2Fwww.udemy.com%2Fmobile%2Fipad%2F&response_type=html\") time.sleep(1) usuario = driver.find_element(By.XPATH,\"//input[@id='email--1']) usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) clave = driver.find_element(By.XPATH,\"//input[@name='password']\") clave.send_keys(\"12345678910\") time.sleep(1) boton = driver.find_element(By.XPATH,\"//input[@name='submit']\") boton.click() time.sleep(5) driver.quit() Ejemplo con selenium en pycharm ruta absoluta: from selenium import webdriver from selenium.webdriver.common.by import By import time driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver.exe\") driver.get(\"https://www.udemy.com/join/login-popup/?skip_suggest=1&locale=es_ES&next=https%3A%2F%2Fwww.udemy.com%2Fmobile%2Fipad%2F&response_type=html\") time.sleep(1) usuario = driver.find_element(By.XPATH,\"html[1]/body[1]/div[1]/div[2]/div[1]/div[3]/form[1]/div[1]/div[1]/div[1]/input[1]\") usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) clave = driver.find_element(By.XPATH,\"/html[1]/body[1]/div[1]/div[2]/div[1]/div[3]/form[1]/div[1]/div[2]/div[1]/input[1]\") clave.send_keys(\"12345678910\") time.sleep(1) boton = driver.find_element(By.XPATH,\"//input[@name='submit']\") boton.click() time.sleep(5) driver.quit() SELECTORES DE CSS(MODO CSS EN INSPECCIONAR ELEMENTOS): NORMALES clave = driver.find_element(By.CSS_SELECTOR,\"input[@name='password']\") ID boton = driver.find_element(By.CSS_SELECTOR,\"input#submit-id-submit\") CLASS boton = driver.find_element(By.CSS_SELECTOR,\"input.btn-primary\") TURTLE Es un modulo de Python utilizado para ense\u00f1ar programacion a trav\u00e9s de coordenadas relativas(X,Y). El objeto a programar recibe el nombre de TORTUGA. Comandos basicos: #!/usr/bin/python # importamos la libreria turtle import turtle # creamos la pantalla s = turtle.Screen() # color de la pantalla s.bgcolor(\"red\") # nombre de la pesta\u00f1a s.title(\"Proyecto basicos turtle\") # necesitamos el objeto, la tortuga a dibujar t = turtle.Turtle() # personalizamos la tortuga forma,color,tinta,etc t.shape(\"turtle\") # arrow, triangle, classic, circle t.shapesize(2,2,1) t.fillcolor(\"orange\") t.pencolor(\"white\") t.color(\"green\",\"blue\") # borde y relleno t.pensize(5) # rellenar figuras t.begin_fill() t.color(\"white\",\"blue\") # borde/tinta y relleno t.circle(100) t.end_fill() # dar velocidad a la tortuga (1-10) t.speed(1) # dar movimientos a la tortuga t.backward(100) t.right(90) t.forward(100) t.left(90) t.forward(100) # dar movimiento sin pintar t.penup() t.forward(50) t.pendown() t.forward(50) # hacer un retroceso t.undo() # limpiar pantalla y resetear posicion t.clear() t.reset() # dejar una marca como sello y seguir t.forward(100) t.stamp() t.forward(100) # movimiento perpendiculares t.goto(100,100) t.goto(-100,100) t.goto(0,0) # == t.home()) # movimientos de formas t.circle(50) #circulo diametro t.dot(30) #punto y diametro # esconder y mostrar de nuevo la tortuga dibujando t.hideturtle() t.circle(50) t.showturtle() t.circle(30) # movilizar la tortuga t.setx(100) t.sety(-10) # para que se quede la pantalla todo el rato turtle.done() # dar movimientos a la tortuga con un cuadrado t.forward(100) t.right(90) t.forward(100) t.right(90) t.forward(100) t.right(90) t.forward(100) # cuadrado automatizado t.color(\"red\",\"blue\") for i in range(4): t.forward(100) t.right(90) # dar movimientos con un circulo t.color(\"blue\",\"yellow\") t.circle(100) t.circle(80) t.circle(60) t.circle(40) t.circle(20) # circulo automatizado resultado = input(\"Quieres dibujar?: \") t.color(\"red\",\"blue\") if resultado == \"si\": while i<=100: t.circle(i) i+=20 else: print(\"No quieres dibujar...:(\")","title":"Python"},{"location":"python/#intro-programacion-python","text":"","title":"Intro Programaci\u00f3n PYTHON"},{"location":"python/#shebang","text":"# !/usr/bin/python3 # -*-coding: utf-8-*-","title":"Shebang"},{"location":"python/#server-python","text":"python -m SimpleHTTPDServer 80","title":"Server Python"},{"location":"python/#mostrar-algo","text":"print(\"Uso windows\")","title":"Mostrar algo"},{"location":"python/#mostrar-algo-con-formato","text":"print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}')","title":"Mostrar algo con formato"},{"location":"python/#importar-librerias","text":"import platform from xxxx import xx","title":"Importar librerias"},{"location":"python/#variables","text":"x=100 y=True z=\"Miguel\"","title":"Variables"},{"location":"python/#condicional","text":"x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\")","title":"Condicional"},{"location":"python/#bucle-for-iterar-elementos","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i)","title":"Bucle for (iterar elementos)"},{"location":"python/#bucle-while","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1","title":"Bucle while"},{"location":"python/#funciones","text":"def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2","title":"Funciones"},{"location":"python/#test-main","text":"if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x)","title":"Test main"},{"location":"python/#objetos","text":"class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2()","title":"Objetos"},{"location":"python/#mayusculas-minusculas-letra-capital","text":"mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize()","title":"Mayusculas, minusculas, letra capital"},{"location":"python/#ver-tipo-de-dato","text":"print(type(w)) #float","title":"Ver tipo de dato"},{"location":"python/#listas","text":"x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print()","title":"Listas"},{"location":"python/#tuplas","text":"t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e)","title":"Tuplas"},{"location":"python/#diccionarios","text":"dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}')","title":"Diccionarios"},{"location":"python/#rangos","text":"r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e)","title":"Rangos"},{"location":"python/#list-comprension","text":"# de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2)","title":"List Comprension"},{"location":"python/#len","text":"len(*args/lista)","title":"Len"},{"location":"python/#objetos_1","text":"# definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\"))","title":"Objetos"},{"location":"python/#ficheros","text":"Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo","title":"Ficheros"},{"location":"python/#modulos","text":"import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day)","title":"M\u00f3dulos"},{"location":"python/#avanzado-python","text":"Todos los COMMANDOS interesantes de python y utilidades que se integran con \u00e9l. import LIBRERIA from MODULO import FUNCION","title":"AVANZADO PYTHON"},{"location":"python/#funciones_1","text":"Definir una funcion: ## FUNCION TABLA MULTIPLICAR def TablaMultiplicar(numero, rango): for i in range(rango+1): print(f'{numero} X {i} = {numero+i}') TablaMultiplicar(7,2) TablaMultiplicar(10,3) Print se pone al llamar la funcion para que te de un return, si no se pone nada, te dar\u00e1 los mensajes print de dentro de la funcion. Variables global dentro de una funcion, se puede utilizar dentro y fuera de esa funcion. Si como argumento ponemos (*argumento), devuelve una tupla.","title":"FUNCIONES"},{"location":"python/#funcion-type","text":"para saber que tipo de valor es type(valor)","title":"Funcion type"},{"location":"python/#funcion-float-int","text":"Transforma el valor en un decimal o entero float(numeroint)","title":"Funcion float, int"},{"location":"python/#funcion-math","text":"De la liberia math(from match import sqrt,pow). hace una subida exponencial pow(5,2)==5**2 Hace la raiz cuadrada sqrt(100) Valor absoluto de un numero abs(-5)","title":"Funcion MATH"},{"location":"python/#funciones-strings","text":"texto.lower(), texto.upper(), texto.capitalize(), texto.title(), texto.swapcase(), texto.strip(), texto.split(' '), texto.replace(' ', 'r'), len(texto).","title":"Funciones strings"},{"location":"python/#funciones-booleanos","text":"text.isupper(),texto.islower(), texto.startswith('a'), texto.endswith('z'). Si es mayusculas, minisculas, si empieza o termina en tal.","title":"Funciones booleanos"},{"location":"python/#funciones-listas","text":"lista.remove,insert,pop,append,sort,reverse,count,index,max,min,average. lista[1:-3].","title":"Funciones listas"},{"location":"python/#funciones-diccionarios","text":"dict.pop,update,get,setdefault,copy,items,values,keys.","title":"Funciones diccionarios"},{"location":"python/#funciones-conjuntos-y-tuplas","text":"conjunto.add, update, remove, discard, update, pop, clear. tupla[posicion]","title":"Funciones conjuntos y tuplas"},{"location":"python/#modulos_1","text":"Es un fichero con conjunto de funciones que podemos importar en otros ficheros. Todas enteras import modulo.py En concreto from modulo.py import funcion1.py","title":"MODULOS"},{"location":"python/#gestion-errores","text":"while True: try: edad = int(input(\"Dime tu edad: \")) print(\"Tu edad es:\", edad) break except ZeroDivisionError: print(\"No se puede dividir entre zero\") except ValueError: print(\"Numero incorrecto\") except KeyboardInterrupt: print(\"Has cancelado la ejecucion\") break finally: print(\"codigo ejecutado correctamente\") Hay varios except como divisionzero, keyboardinterrupt, error value...","title":"GESTION ERRORES"},{"location":"python/#gestion-de-ficheros","text":"Leer un fichero: #!/usr/bin/python3 # leer un fichero en modo read y texto y lo guardamos en una variable fichero = open(\"./texto.txt\", \"rt\") # ver los datos datos_fichero = fichero.read() print(datos_fichero) Crear un fichero poniendo texto: #!/usr/bin/python3 # creamos un fichero y metodo write y texto para que escriba fichero = open(\"./texto2.txt\", \"wt\") # escribir los datos texto = \"Esto es un ejemplo para\\nescribir en un fichero\\nque vamos a grabar.\\n\" fichero.write(texto) # cerramos dichero fichero.close() # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. A\u00f1adir texto a un fichero: #!/usr/bin/python3 # creamos un fichero y metodo append y texto para que a\u00f1ada cosas al file fichero = open(\"./texto3.txt\", \"at\") # escribir los datos texto = \"\\nEsto es un ejemplo para\\na\u00f1adir texto\\n\" fichero.write(texto) # cerramos dichero fichero.close() # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. Borrar un fichero: #!/usr/bin/python3 # importamos un modulo para usar el metodo de borrar fichero del sistema import os # creamos un fichero y metodo append y texto para que a\u00f1ada cosas al file os.remove(\"./texto3.txt\") # al ejecutarlo se nos crea un fichero con ese nombre y ese texto. Creamos un modulo de objeto Fichero y tratamos estas funciones en un programa: #!/usr/bin/python3 class Fichero: def __init__(self,nombre): self.nombre = nombre def grabar_fichero(self,texto): fichero = open(self.nombre, \"wt\") fichero.write(texto) fichero.close() def incluir_fichero(self,texto): fichero = open(self.nombre, \"at\") fichero.write(texto) fichero.close() def leer_fichero(self): fichero = open(self.nombre, \"rt\") texto = fichero.read() return texto #!/usr/bin/python3 # importamos el modulo import ficheros_modulo # creamos nombre del fichero nombre_fichero = \"fichero1.txt\" fichero = ficheros_modulo.Fichero(nombre_fichero) # en el fichero que vamos a crear, tiene este texto texto = \"Esta es la linea para a\u00f1adir.\\nEsta la segunda linea.\\n\" fichero.grabar_fichero(texto) # a\u00f1adimos al fichero este texto texto = \"Esta es la linea para a\u00f1adir.\\n\" fichero.incluir_fichero(texto) # leemos lo que tenemos de fichero leer_texto = fichero.leer_fichero() print(leer_texto) Tratamiento de binarios en python: #!/usr/bin/python3 # importamos el modulo para ficheros binarios import pickle # leer en modo binarios fichero = open(\"binarios_colores.pckl\", \"rb\") lista_colores = pickle.load(fichero) print(lista_colores) #!/usr/bin/python3 # importamos el modulo para ficheros binarios import pickle # a\u00f1adimos a un fichero una lista de colores en modo binarios colores = [\"azul\", \"amarillo\", \"verde\"] fichero = open(\"binarios_colores.pckl\", \"wb\") pickle.dump(colores, fichero) fichero.close()","title":"GESTION DE FICHEROS"},{"location":"python/#expresiones-regulares","text":"Expresiones regulares con el modulo RE: #!/usr/bin/python3 # importamos el modulo de expresiones regulares import re texto = \"Hola me llamo miguel\" # SEARCH # buscamos ese patron dentro del texto resultado = re.search(\"llamo\",texto) # que acabe en miguel$, que empiece en ^miguel # que haya algo entre me.*miguel if resultado: print(\"OK\") else: print(\"No encontado\") # FINDALL texto2 = ''' el coche de miguel es rojo el coche de natalia es blanco ''' resultado = re.findall(\"coche.*rojo\", texto2) if resultado: print(\"OK\") else: print(\"No encontado\") # SPLIT texto3 = \"La silla es blanca y vale 80\" resultado = re.split(\"\\s\", texto3) if resultado: print(resultado) else: print(\"No encontado\") # SUB texto4 = \"La silla es blanca y vale 80\" resultado = re.sub(\"blanca\", \"roja\", texto4) if resultado: print(resultado) else: print(\"No encontado\")","title":"EXPRESIONES REGULARES"},{"location":"python/#convert-json","text":"#!/usr/bin/python3 # importamos el modulo de json import _json producto1 = {\"nombre\":\"miguel\", \"apellido\":\"amoros\"} estructura_json = _json.dumps(producto1) print(estructura_json) producto2 = _json.loads(estructura_json) print(producto2)","title":"CONVERT JSON"},{"location":"python/#fecha-y-hora","text":"#!/usr/bin/python3 # importamos el modulo de json from datetime import datetime fechayhora = datetime.now() print(fechayhora) a\u00f1o = fechayhora.year mes = fechayhora.month dia = fechayhora.day hora = fechayhora.hour minutos = fechayhora.minute segundos = fechayhora.second microsegundos = fechayhora.microsecond print(f\"La hora es {hora} : {minutos}\") print(f\"La fecha es {dia} {mes} {a\u00f1o}\")","title":"FECHA Y HORA"},{"location":"python/#sqlite","text":"Crear base de datos con python con el modulo sqlite: TUTORIAL Podemos descargar la herramienta para visualizar las cosas y manejar la bd. Ejemplo de creacion de bbdd: #!/usr/bin/python3 # importamos el modulo de bd sqlite import sqlite3 # creamos/existente bbdd conexion = sqlite3.connect(\"bbdd1.db\") # sirve para poder hacer sentencias sql dentro cursor = conexion.cursor() # creamos una tabla cursor.execute(\"CREATE TABLE PERSONAS (nombre TEXT, apellido1 TEXT, apellido2 TEXT, edad INTEGER)\") # creamos fila cursor.execute(\"INSERT INTO PERSONAS VALUES ('Antonio', 'Perez', 'Gomez', 35)\") # creamos varias filas lista_personas = [('Miguel', 'Amoros', 'Moret', 28), ('Natalia', 'Sendra', 'Soler', 26)] cursor.executemany(\"INSERT INTO PERSONAS VALUES (?,?,?,?)\", lista_personas) # consulta de datos cursor.execute(\"SELECT * FROM PERSONAS\") personas = cursor.fetchall() for persona in personas: print(persona) # consulta datos con WHERE cursor.execute(\"SELECT * FROM PERSONAS WHERE edad > 28\") personas_edad = cursor.fetchall() for persona in personas_edad: print(persona) # consulta datos con WHERE y ordenado cursor.execute(\"SELECT * FROM PERSONAS WHERE edad <= 28 ORDER BY edad DESC\") personas_edad = cursor.fetchall() for persona in personas_edad: print(persona) # borrar datos cursor.execute(\"DELETE FROM PERSONAS WHERE nombre = 'Antonio'\") # actualizar datos cursor.execute(\"UPDATE PERSONAS SET nombre = 'Miguelito' where edad = 28\") # mantener el registro guardado conexion.commit() # cerramos bdd conexion.close()","title":"SQLite"},{"location":"python/#tkinter","text":"TKINTER Es un modulo de python que sirve para crear una interfaz grafica. Ejemplo de lo que se puede hacer: #!/usr/bin/python3 # tkinter - componente raiz import tkinter from tkinter import filedialog # definimos la ventana de la aplicacion raiz = tkinter.Tk() raiz.title(\"Mi programa\") # definimos aspecto de la ventana frame = tkinter.Frame(raiz) frame.config(fg=\"blue\",width=400,height=300) frame.pack() # construimos un label texto = \"Hola mundo\" label = tkinter.Label(raiz,text=texto) label.config(fg=\"green\",bg=\"grey\",font=(\"Cortana\",30)) label.pack() # construimos una entrada por teclado entrada = tkinter.Entry(raiz) entrada.config(justify=\"center\", show=\"*\") entrada.pack() # construimos una box de texto box = tkinter.Text(raiz) box.config(width=20,height=10,font=(\"Verdana\",15),padx=10,pady=10,fg=\"green\",selectbackground=\"yellow\") box.pack() # construimos una boton que salte mensaje def accion(): print(\"Hola mundo\") boton = tkinter.Button(raiz,text=\"Ejecutar\",command=accion) boton.config(fg=\"green\") boton.pack() # construimos un multiseleccion def accion(): print(f\"La opcion es {opcion.get()}\") seleccion1 = tkinter.Radiobutton(raiz,text=\"Opcion 1\",variable=opcion,command=accion) seleccion1.pack() seleccion2 = tkinter.Radiobutton(raiz,text=\"Opcion 2\",variable=opcion,command=accion) seleccion2.pack() # construimos un checkbutton def verificar(): valor = check1.get() if valor == 1: print(\"La opcion est\u00e1 activada\") else: print(\"La opcion est\u00e1 activada\") check1 = tkinter.IntVar() boton1 = tkinter.Checkbutton(raiz,text=\"Opcion 1\",variable=check1,onvalue=1,offvalue=0,command=verificar) boton1.pack() # construimos boton con popup def avisar(): tkinter.messagebox.showinfo(\"Titulo\",\"Mensaje con la info\") boton = tkinter.Button(raiz,text=\"Pulsar para aviso\",command=avisar) boton.pack() # construimos boton con popup para responder si o no def preguntar(): tkinter.messagebox.askquestion(\"Titulo\",\"Quieres borrar la info?\") boton = tkinter.Button(raiz,text=\"Pulsar para preguntar\",command=preguntar) boton.pack() # construimos boton para a\u00f1adir fichero from tkinter import filedialog def abrirfichero(): rutafichero = filedialog.askopenfilename(title=\"Abrir un fichero\") print(rutafichero) boton = tkinter.Button(raiz,text=\"Pulsar para abrir fichero\",command=abrirfichero) boton.pack() # para que siga ejecutandose raiz.mainloop()","title":"TKINTER"},{"location":"python/#pydoc","text":"Es un comando que sirve para generar documentacion de una clase y sus funciones: pydoc /ruta/programa.py pydoc -w /ruta/programa.py para tenerla en html.","title":"PYDOC"},{"location":"python/#doctest","text":"Sirve para hacer pruebas en los docstrings de las funciones: # DOCTEST PARA PODER HACER PRUEBAS EN EL DOCSTRING def sumar(num1,num2): \"\"\" Esto es una funcion para sumar numeros >>> sumar(4,3) 7 >>> sumar(4,3) 8 >>> sumar(2,3) 5 \"\"\" return num1+num2 print(sumar(2,2)) # se importa al final la libreria para hacer las pruebas import doctest doctest.testmod() 4 ********************************************************************** File \"/home/miguel/Documents/curso_python2022/prueba_doctest.py\", line 9, in __main__.sumar Failed example: sumar(4,3) Expected: 8 Got: 7 ********************************************************************** 1 items had failures: 1 of 3 in __main__.sumar ***Test Failed*** 1 failures.","title":"DOCTEST"},{"location":"python/#unistest","text":"Modulo que sirve para hacer test . #!/usr/bin/python3 # DOCTEST PARA PODER HACER PRUEBAS EN EL DOCSTRING def sumar(num1,num2): \"\"\" Esto es una funcion para sumar numeros \"\"\" return num1+num2 print(sumar(2,2)) # se importa al final la libreria para hacer las pruebas import unittest class pruebas(unittest.TestCase): def test(self): self.assertEqual(sumar(4,5),9) self.assertEqual(sumar(4,5),19) # prueba if __name__ == '__main__': unittest.main() 4 . ---------------------------------------------------------------------- Ran 1 test in 0.000s OK","title":"UNISTEST"},{"location":"python/#numpy","text":"NUMPY s una librer\u00eda de Python especializada en el c\u00e1lculo num\u00e9rico y el an\u00e1lisis de datos, especialmente para un gran volumen de datos. Incorpora una nueva clase de objetos llamados arrays que permite representar colecciones de datos de un mismo tipo en varias dimensiones, y funciones muy eficientes para su manipulaci\u00f3n. Instalamos libreria con pip3 install numpy Ejemplos de uso: #!/usr/bin/python3 # importamos la liberia import numpy as np # creamos arrays lista1 = [1,2,3,4] lista2 = [5,6,7,8] print(np.zeros(4)) # crea array de 4 ceros print(np.ones(5)) # crea array de 5 unos np.arange(5) # crea array de 0 a 4 np.arange(2,10) array1 = np.array(lista1) array2 = np.array([1,2,3,4,5,6]) print(array1) # array de dos dimensiones lista_doble = (lista1,lista2,lista2,lista1) print(lista_doble) array_doble = np.array(lista_doble) print(array_doble) # formas que tiene el array print(array_doble.shape) # 4 filas y 4 columnas print(array_doble.dtype) # numeros int # operaciones con arrays print(array1 + 4) # multiplica todo lo de dentro por 4 pero no queda asignado array_multiplicado = array1 * 3 print(array_multiplicado) # indexacion / slicing array3 = np.arange(10) print(array3) print(array3[0:3]) array4 = np.copy(array3) array4[0:4] = 20 print(array3,array4) # matrices array5 = np.arange(15).reshape(3,5) # 3 filas y 5 columnas print(array5) print(array5[1][1]) array6 = array5.T # cambio el orden de filas y columnas print(array6) # entrada / salida de arrays array7 = np.arange(6) np.save(\"array7\", array7) # guarda ese array con ese nombre print(np.load(\"array7.npy\")) # carga el archivo con el nombre del array array8 = np.arange(8) np.savez(\"arrayxy\", x=array7, y=array8) # guarda en tipo coordenadas x y array_xy = np.load(\"arrayxy.npz\") print(array_xy) print(array_xy[\"x\"]) print(array_xy[\"y\"]) np.savetxt(\"mifilearray.txt\", array8, delimiter=\",\") # guarda en un file txt print(np.loadtxt(\"mifilearray.txt\", delimiter=\",\")) # funciones print(np.sqrt(array1)) #raiz cuadrada print(np.random(5)) # 5 numeros aleatorios print(np.add(array1, array2)) # suma las dos print(np.maxium(array1,array2)) # elije el maximo de entre las dos arrays # funcion par e impar def pares(inicio,fin): if inicio % 2 == 0: array10 = np.arange(inicio,fin,2) else: inicio +=1 array10 = np.arange(inicio,fin) return array10 print(pares(0,21))","title":"NUMPY"},{"location":"python/#pandas","text":"PANDAS es una librer\u00eda de Python especializada en el manejo y an\u00e1lisis de estructuras de datos. Las principales caracter\u00edsticas de esta librer\u00eda son: Define nuevas estructuras de datos basadas en los arrays de la librer\u00eda NumPy pero con nuevas funcionalidades. Permite leer y escribir f\u00e1cilmente ficheros en formato CSV, Excel y bases de datos SQL. Permite acceder a los datos mediante \u00edndices o nombres para filas y columnas. Ofrece m\u00e9todos para reordenar, dividir y combinar conjuntos de datos. Permite trabajar con series temporales. Realiza todas estas operaciones de manera muy eficiente. PARA QUE QUEDE EL CAMBIO HECHO SIEMPRE HAY QUE ASIGNAR LA FUNCION QUE SE HAGA A LA SERIE O DATAFRAME DEL CUAL SE HACE ALGO. Metodo SERIES para crear indices: #!/usr/bin/python3 # importamos la liberia from logging.config import dictConfig import pandas as pd ## INDICES # te crea una serie de indices #0 1 serie1 = pd.Series([1,5,7]) #1 5 print(serie1) # ver un indice print(serie1[1]) #5 # crear un indice asignaturas = [\"mates\", \"sociales\", \"fisica\"] notas = [8,9,5] serie_notas_miguel = pd.Series(notas,asignaturas) print(serie_notas_miguel) print(serie_notas_miguel[\"mates\"]) #condiciones print(serie_notas_miguel[serie_notas_miguel>7]) #asignamos nombres serie_notas_miguel.name = \"Notas de Miguel\" serie_notas_miguel.index.name = \"Asignaturas de Bachiller\" print(serie_notas_miguel) # convertir la serie en un diccionario diccionario = serie_notas_miguel.to_dict() print(diccionario) serie = pd.Series(diccionario) print(serie) #operaciones con indices con las series notas2 = [1,5,3] serie_notas_natalia = pd.Series(notas2,asignaturas) print(serie_notas_natalia) notas_medias = (serie_notas_miguel+serie_notas_natalia)/2 print(notas_medias) Metodo DATAFRAMES: #!/usr/bin/python3 # importamos la liberia import pandas as pd import webbrowser # sirve para paginas web ## DATAFRAMES # sirve para abrir esta web website = \"https://es.wikipedia.org/wiki/Anexo:Campeones_de_la_NBA\" webbrowser.open(website) # copiamos algo al portapapeles de esa web y lo asignamos a un dataframe dataframe_nba = pd.read_clipboard() print(dataframe_nba) # te dice el nombre de las columnas print(dataframe_nba.columns) print(dataframe_nba.columns[\"Campe\u00f3n del Oeste\"]) # ver la info por el indice print(dataframe_nba.loc[5]) # ver los 5 primeros con head o especificos print(dataframe_nba.head()) print(dataframe_nba.head(7)) print(dataframe_nba.tail()) print(dataframe_nba.tail(4)) # dataframe con un diccionario asignaturas = [\"Mates\", \"Reli\", \"Sociales\", \"Natus\"] notas = [8,5,7,8] diccionario = { \"Asignaturas\": asignaturas, \"Notas\": notas} print(diccionario) # a raiz del diccionario te crea un dataframe con los nombres, notas e indices dataframe_notas = pd.DataFrame(diccionario) print(dataframe_notas) Metodo INDICES: #!/usr/bin/python3 # importamos la liberia import pandas as pd # INDICES lista_valores = [1,2,3] lista_indices = [\"a\", \"b\", \"c\"] serie = pd.Series(lista_valores,index=lista_indices) print(serie) # a 1 b 2 c 3 print(serie.index) # a b c print(serie.index[0]) # a lista_valores2 = [[1,2,3], [8,9,10], [3,5,6]] lista_indices2 = [\"Mates\", \"Natus\", \"Fisica\"] lista_nombres = [\"Miguel\",\"Natalia\", \"Cristina\"] # creamos el dataframe indicando indices, columnas y datos dataframe = pd.DataFrame(lista_valores2, index=lista_indices2, columns=lista_nombres) print(dataframe) print(dataframe.index) print(dataframe.index[2]) Si se crea mas indices que valores, se crean subindices y si usamos serie.unstack() convierte los otros indices en columnas. Para deshacer o el proceso inverso seria dataframe.stack() . Metodo BORRAR DATOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # ELIMINAR DATOS ## INDICES print(np.arange(4)) # creamos una serie a partir de un array de numpy y ponemos los indices serie = pd.Series(np.arange(4), index=[\"a\",\"b\",\"c\",\"d\"]) print(serie) # eliminamos indices serie.drop(\"c\") ## DATAFRAMES print(np.arange(9).reshape(3,3)) lista_valores = np.arange(9).reshape(3,3) lista_indices = [\"a\",\"b\",\"c\"] lista_columnas = [\"c1\",\"c2\",\"c3\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices,columns=lista_columnas) print(dataframe) dataframe.drop(\"b\") dataframe.drop(\"c2\", axis=1) Metodo SELECCIONAR DATOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # SELECCIONAR DATOS # en series lista_valores = np.arange(3) # 0 1 2 lista_indices = [\"i1\",\"i2\",\"i3\"] serie = pd.Series(lista_valores,index=lista_indices) print(serie) # multiplica los datos serie = serie *2 print(serie) # seleccionar datos print(serie[\"i1\"]) print(serie[1]) print(serie[0:2]) print(serie[serie>2]) serie[serie>2] = 6 print(serie) # en datafranes lista_valores2 = np.arange(25).reshape(5,5) lista_indices2 = [\"i1\",\"i2\",\"i3\",\"i4\",\"i5\"] lista_columnas2 = [\"c1\",\"c2\",\"c3\",\"c4\",\"c5\"] dataframe = pd.DataFrame(lista_valores2,index=lista_indices2,columns=lista_columnas2) print(dataframe) print(dataframe[\"c2\"]) # todo lo de la c2 print(dataframe[\"c2\"][\"i2\"]) # valor concreto print(dataframe[dataframe[\"c2\"]>15]) # los datos de donde la columnas2 print(dataframe>20) # muestra cada valor si es true o false print(dataframe.loc[\"i2\"]) # fila i2 Metodo ORGANIZAR Y CLASIFICAR ESTADISTICAS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # ORDENAR Y CLASIFICAR lista_valores = range(4) lista_indices = list(\"bcad\") lista_columnas = list(\"wxyz\") serie = pd.Series(lista_valores,index=lista_indices) print(serie) print(serie.sort_index()) print(serie.sort_values()) print(serie.rank()) # ordena ppor ranking de clasificaciones # esto crea una sserie con 10 datos randoms serie2 = pd.Series(np.random.randn(10)) print(serie2) print(serie2.rank()) # CLASIFICAR DATOS lista_indices2 = list(\"abc\") lista_columnas2 = list(\"xyz\") dataframe = pd.DataFrame(np.arange(9).reshape(3,3),index=lista_indices2, columns=lista_columnas2) print(dataframe) print(dataframe.describe()) print(dataframe.sum()) print(dataframe.max()) print(dataframe.max(axis=1)) print(dataframe.min()) print(dataframe.idxmin()) Metodo para ver los VALORES NULOS: #!/usr/bin/python3 # importamos la liberia import pandas as pd import numpy as np # VALORES NULOS lista_valores = [1,2,np.nan,4] lista_indices = list(\"abcd\") serie = pd.Series(lista_valores,index=lista_indices) print(serie) print(serie.isnull()) # ver booleanos los nulos print(serie.dropna()) # borrar los nulos lista_valores2 = [[1,2,1],[1,np.nan,2],[1,2,np.nan]] lista_indices2 = list(\"123\") lista_columnas2 = list(\"abc\") dataframe = pd.DataFrame(lista_valores2,index=lista_indices2,columns=lista_columnas2) print(dataframe) print(dataframe.isnull()) print(dataframe.dropna()) print(dataframe.fillna(0)) # rellena los nulos con 0 Metodo en HTML: #!/usr/bin/python3 import pandas as pd # TRATAMIENTO DE DATOS EN URL HTML url = \"https://es.wikipedia.org/wiki/Anexo:Finales_de_la_Copa_Mundial_de_F%C3%BAtbol\" # cogemos los datos de la url dataframe_futbol = pd.io.html.read_html(url) # cambiamos los campos de columnas por los que queremos print(dataframe_futbol.loc[0]) # creamos un diccionario con los nombres de las columnas diccionario = dict(dataframe_futbol.loc[0]) print(diccionario) # lo asignamos al nuevo dataframe dataframe_futbol = dataframe_futbol.rename(columns=diccionario) # ahora borramos la fila 1 que se repite y borramos la columna notas dataframe_futbol = dataframe_futbol.drop(0) dataframe_futbol = dataframe_futbol.drop(\"Notas\",axis=1) print(dataframe_futbol) Metodo en EXCEL: #!/usr/bin/python3 import pandas as pd # TRATAMIENTO DE DATOS EN FILE EXCELL # cogemos los datos deL FICHERO file_excell = pd.ExcelFile(\"/home/miguel/Documents/curso_python2022/pandas/poblacion.xlsx\") # en csv seria: # file_csv = pd.read_csv(\"/home/miguel/Documents/curso_python2022/pandas/poblacion.xlsx\") # parseamos en que hoja est\u00e1 dataframe = file_excell.parse(\"Hoja1\") print(dataframe)","title":"PANDAS"},{"location":"python/#tratamiento-datos-dataframes","text":"UNION DE DATAFRAMES: #!/usr/bin/python3 import pandas as pd dataframe1 = pd.DataFrame({\"c1\" : [\"1\",\"2\",\"3\"], \"clave\" : [\"a\",\"b\",\"c\"]}) dataframe2 = pd.DataFrame({\"c2\" : [\"4\",\"5\",\"6\"], \"clave\" : [\"c\",\"b\",\"e\"]}) # hacer una union de datafames segun columna. Une por claves iguales dataframe3 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\") print(dataframe3) # une igual pero manteniendo los datos del de la izquierda dataframe4 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\",how=\"left\") print(dataframe4) # une igual pero manteniendo los datos del de la derecha dataframe5 = pd.DataFrame.merge(dataframe1,dataframe2,on=\"clave\",how=\"right\") print(dataframe5) CONCATENAR Y COMBINAR DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np # concatenamos los dos arrays array1 = np.arange(9).reshape(3,3) array_concatenado = np.concatenate([array1,array1]) print(array_concatenado) # concatenamos series poniendo los campos de serie serie1 = pd.Series([1,2,3], index=[\"a\",\"b\",\"c\"]) serie2 = pd.Series([4,5,6], index=[\"d\",\"e\",\"f\"]) serie_concatenada = pd.concat([serie1,serie2], keys=[\"serie1\",\"serie2\"]) print(serie_concatenada) # concatenamos datafranes dataframe1 = pd.DataFrame(np.random.rand(3,3), columns=[\"a\",\"b\",\"c\"]) #3 filas 3 columnas dataframe2 = pd.DataFrame(np.random.rand(2,3), columns=[\"a\",\"b\",\"c\"]) dataframe3 = pd.concat([dataframe1,dataframe2],keys=[\"dataframe1\",\"dataframe2\"]) print(dataframe3) # combinar serie y dataframes serie_combinada = serie1.combine_first(serie2) print(serie_combinada) dataframe_combinado = dataframe1.combine_first(dataframe2) print(dataframe_combinado) DUPLICAR DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = [[1,2],[1,2],[5,6],[5,8]] lista_indices = list(\"mnop\") lista_columnas = [\"columna1\",\"columna2\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices, columns=lista_columnas) print(dataframe) # BORRAR LOS DUPLICADOS dataframe_duplicado = dataframe.drop_duplicates() print(dataframe_duplicado) # BORRAR LOS DUPLICADOS PARA SOLO VALORES UNICOS EN UNA COLUMNA Y MANTENER EL ULTIMO VALOR dataframe_duplicado_columna = dataframe_duplicado.drop_duplicates([\"columna1\"], keep=\"last\") print(dataframe_duplicado_columna) METODO REEMPLAZAR SERIES: #!/usr/bin/python3 import pandas as pd import numpy as np # REEMPLAZAR SERIES serie = pd.Series([1,2,3,4,5,6]) print(serie) # reemplaza el 1 por el 10 serie1 = serie.replace(1,10) print(serie1) serie2 = serie.replace({1:10,2:20}) print(serie2) METODO PARA RENOMBRAR INDICES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = np.arange(9).reshape(3,3) lista_indices = list(\"abc\") lista_columnas = [\"columna1\",\"columna2\",\"columna3\"] dataframe = pd.DataFrame(lista_valores, index=lista_indices, columns=lista_columnas) print(dataframe) # REEMPLAZAMOS LOS INDICES # POR MAYUSCULAS nuevos_indices = dataframe.index.map(str.upper) dataframe.index = nuevos_indices print(dataframe) # POR RENAME dataframe = dataframe.rename(index=str.lower) print(dataframe) # NUEVOS INDICES indices_nuevos = {\"a\":\"f\",\"b\":\"w\",\"c\":\"z\"} dataframe = dataframe.rename(index=indices_nuevos) print(dataframe) # O SOLO UNO DE LOS QUE HAY indices_uno = {\"f\":\"xxx\"} dataframe = dataframe.rename(index=indices_uno) print(dataframe) METODO PARA AGRUPAR CATEGORIAS: #!/usr/bin/python3 import pandas as pd import numpy as np precio = [42,50,45,23,5,21,88,34,26] rango = [10,20,30,40,50,60,70,80,90,100] # AGRUPAR CATEGORIAS # te sale en que rango est\u00e1 cada precio de la lista de rangos precio_con_rango = pd.cut(precio,rango) print(precio_con_rango) # cuenta cuantos hay para cada rango de precios print(pd.value_counts(precio_con_rango)) METODO PARA FILTRAR INDICES: #!/usr/bin/python3 import pandas as pd import numpy as np # 10 filas por 3 columnas lista_valores = np.random.rand(10,3) # creamos un dataframe dataframe = pd.DataFrame(lista_valores) print(dataframe) # filtramos por columna y valor columna0 = dataframe[0] print(columna0) print(columna0[columna0>0.40]) # tmb por dataframe se puede print(dataframe[dataframe>0.40]) METODO PARA COMBINAR ELEMENTOS: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = np.arange(25).reshape(5,5) dataframe = pd.DataFrame(lista_valores) # cambiamos los indices por una combinacion aleatoria combinacion = np.random.permutation(5) print(combinacion) # ahora ordena los valores segun el indice de la combinacion print(dataframe.take(combinacion)) METODO PARA AGRUPAR POR COLUMNAS GROUPBY: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = {\"clave1\": [\"x\",\"x\",\"y\",\"y\",\"z\"],\"clave2\": [\"a\",\"b\",\"a\",\"b\",\"a\"], \"datos1\": np.random.rand(5), \"datos2\": np.random.rand(5)} dataframe = pd.DataFrame(lista_valores) print(dataframe) # queremos agrupar datos1 con clave1 agrupacion = dataframe[\"datos1\"].groupby(dataframe[\"clave1\"]) print(agrupacion) # te hace una media de x y z print(agrupacion.mean()) METODO PARA AGREGACIONES EN DATAFRAMES: #!/usr/bin/python3 import pandas as pd import numpy as np lista_valores = [[1,2,3],[4,5,6],[7,8,9],[np.nan,np.nan,np.nan]] lista_columnas = list(\"abc\") dataframe = pd.DataFrame(lista_valores,columns=lista_columnas) print(dataframe) # agregaciones de suma, minimo sobre los datos print(dataframe.agg([\"sum\",\"min\"])) # por fila print(dataframe.agg(\"sum\",axis=1))","title":"TRATAMIENTO DATOS DATAFRAMES"},{"location":"python/#seaborn","text":"SEABORN es una librer\u00eda para Python que permite generar f\u00e1cilmente elegantes gr\u00e1ficos. Seaborn esta basada en matplotlib y proporciona una interfaz de alto nivel que es realmente sencilla de aprender. Dada su gran popularidad se encuentra instalada por defecto en la distribuci\u00f3n Anaconda. La representaci\u00f3n de datos es una tarea clave del an\u00e1lisis de datos. La utilizaci\u00f3n de una gr\u00e1fica adecuada puede hacer que los resultados y conclusiones se comuniquen de una forma adecuada o no. Conocer y manejar diferentes herramientas es clave para poder seleccionar la gr\u00e1fica adecua en cada ocasi\u00f3n. En esta entrada se va a repasar b\u00e1sicamente las funciones que ofrece la librer\u00eda Seaborn. INSTALACION pip3 install seaborn","title":"SEABORN"},{"location":"python/#selenium-web-driver","text":"Es una herramienta que sirve para automatizar pruebas de testing en navegadores web. No se instala de manera nativa, sino en un IDE se corre como PyCharm. Multiplataforma. No tiene soporte, solo es para navegadores web.","title":"SELENIUM WEB DRIVER"},{"location":"python/#pycharm","text":"PyCharm es este famoso IDE que adem\u00e1s cuenta con una versi\u00f3n para las distribuciones Gnu/Linux, lo que hace que sea m\u00e1s sencillo a\u00fan su utilizaci\u00f3n y creaci\u00f3n de programas con este lenguaje de programaci\u00f3n.PyCharm es un IDE, es decir, no solo es un editor de c\u00f3digo sino que tambi\u00e9n tiene un depurador, un interprete y otras herramientas que nos ayudar\u00e1n a crear y exportar los programas que creemos. PyCharm tiene un interprete en el editor de c\u00f3digo que nos ayudar\u00e1 a saber o conocer los posibles errores del c\u00f3digo en tiempo real, algo que ha hecho que Python y PyCharm sean elegidos por muchos usuarios que comienzan a programar. IDE: Un entorno de desarrollo integrado\u200b\u200b o entorno de desarrollo interactivo, en ingl\u00e9s Integrated Development Environment, es una aplicaci\u00f3n inform\u00e1tica que proporciona servicios integrales para facilitarle al desarrollador o programador el desarrollo de software. Instalamos pycharm desde la web indicada, extraemos el tar e inicimos desde el directorio bin con ./pycharm.sh .","title":"PyCharm"},{"location":"python/#selenium","text":"Para poder usar Selenium en pycharm tenemos que instalarlo en la web selenium y luego instalar la version de python. Instalamos con pip install selenium dentro de la terminal de pycharm. Ahora necesitamos instalar drivers de selenium . En este caso vamos a instalar el del navegador de google chrome pero todos se instalan del mismo modo. Una vez nos bajamos el zip de chrome, descomprimimos, copiamos el chromedriver.exe, vamos a pycharm y boton derecho a nuestro proyecto y creamos un python file de prueba. Despues de nuevo creamos un package file de nombre drivers y dentro de el, boton derecho y pegamos el driver. Ya solo tendremos que descargar los navegadores que queramos y peguemos en esta carpeta los ficheros ejecutables de drivers. Ahora hay que hacer un peque\u00f1o script para ver que todo esto funcione de pycharm con selenium en el navegador: # importamos la implementacion que crea una instancia para conectarse a un navegador from selenium import webdriver from selenium.webdriver.common.by import By # importamos libreria time import time # llamamos al driver y su path para abrir chrome (mejor llamar desde ruta original) #driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver\") controlador = webdriver.Chrome(executable_path=r\"/home/miguel/Downloads/chromedriver_linux64/chromedriver\") # maximiza la ventana total del navegador controlador.maximize_window() # metodo get nos indica la url a la que conectarnos controlador.get(\"https://www.udemy.com/join/login-popup/?locale=es_ES&response_type=html&next=https%3A%2F%2Fwww.udemy.com%2Fjoin%2Flogin-popup%2F%3Flocale%3Des_ES%26response_type%3Dhtml%26next%3Dhttps%253A%252F%252Fwww.udemy.com%252Fes%252F%253Futm_source%253Dadwords-brand%2526utm_medium%253Dudemyads%2526utm_campaign%253DNEW-AW-PROS-Branded-Search-SP-SPA_._ci__._sl_SPA_._vi__._sd_All_._la_SP_._%2526tabei%253D7%2526utm_term%253D_._ag_53604040718_._ad_254061738916_._de_c_._dm__._pl__._ti_kwd-357002749620_._li_1005424_._pd__._%2526gclid%253DCj0KCQjw-uH6BRDQARIsAI3I-Ud3hC1QNzFFLCPuZ6H6BbB4sNh5StLf3qvjF1S-mVR0WaM8fs7gOeEaAr_HEALw_wcB%2526persist_locale%253D%2526locale%253Des_ES\") # inspeccionamos el codigo fuente del udemy y vamos a la seccion de usuario/password y cppiamos el id usuario = controlador.find_element(By.ID, \"email--1\") password = controlador.find_element(By.ID, \"id_password\") # ingresaremos los datos directamente. Ponemos un tiempo para ver resultados. usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) password.send_keys(\"12345678910\") time.sleep(1) # ahora inspeccionamos el boton de iniciar sesion y su id. Despues clicamos boton = controlador.find_element(By.ID, \"submit-id-submit\") boton.click() time.sleep(3) # cerramos controlador.quit()","title":"Selenium"},{"location":"python/#xpath","text":"Sirve para extraer informacion xhtml en las webs a trav\u00e9s de componentes y etiquetas segun su ubicacion. A\u00f1adimos la extension CROPATH Ejemplos (//etiqueta[@atributo=\"valor\"]) //div[@data-purpose='lecture-title'] Ejemplo con selenium en pycharm ruta relativa: from selenium import webdriver from selenium.webdriver.common.by import By import time driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver.exe\") driver.get(\"https://www.udemy.com/join/login-popup/?skip_suggest=1&locale=es_ES&next=https%3A%2F%2Fwww.udemy.com%2Fmobile%2Fipad%2F&response_type=html\") time.sleep(1) usuario = driver.find_element(By.XPATH,\"//input[@id='email--1']) usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) clave = driver.find_element(By.XPATH,\"//input[@name='password']\") clave.send_keys(\"12345678910\") time.sleep(1) boton = driver.find_element(By.XPATH,\"//input[@name='submit']\") boton.click() time.sleep(5) driver.quit() Ejemplo con selenium en pycharm ruta absoluta: from selenium import webdriver from selenium.webdriver.common.by import By import time driver = webdriver.Chrome(executable_path=\"Drivers/chromedriver.exe\") driver.get(\"https://www.udemy.com/join/login-popup/?skip_suggest=1&locale=es_ES&next=https%3A%2F%2Fwww.udemy.com%2Fmobile%2Fipad%2F&response_type=html\") time.sleep(1) usuario = driver.find_element(By.XPATH,\"html[1]/body[1]/div[1]/div[2]/div[1]/div[3]/form[1]/div[1]/div[1]/div[1]/input[1]\") usuario.send_keys(\"dfdflujogramas@gmail.com\") time.sleep(1) clave = driver.find_element(By.XPATH,\"/html[1]/body[1]/div[1]/div[2]/div[1]/div[3]/form[1]/div[1]/div[2]/div[1]/input[1]\") clave.send_keys(\"12345678910\") time.sleep(1) boton = driver.find_element(By.XPATH,\"//input[@name='submit']\") boton.click() time.sleep(5) driver.quit() SELECTORES DE CSS(MODO CSS EN INSPECCIONAR ELEMENTOS): NORMALES clave = driver.find_element(By.CSS_SELECTOR,\"input[@name='password']\") ID boton = driver.find_element(By.CSS_SELECTOR,\"input#submit-id-submit\") CLASS boton = driver.find_element(By.CSS_SELECTOR,\"input.btn-primary\")","title":"XPATH"},{"location":"python/#turtle","text":"Es un modulo de Python utilizado para ense\u00f1ar programacion a trav\u00e9s de coordenadas relativas(X,Y). El objeto a programar recibe el nombre de TORTUGA.","title":"TURTLE"},{"location":"python/#comandos-basicos","text":"#!/usr/bin/python # importamos la libreria turtle import turtle # creamos la pantalla s = turtle.Screen() # color de la pantalla s.bgcolor(\"red\") # nombre de la pesta\u00f1a s.title(\"Proyecto basicos turtle\") # necesitamos el objeto, la tortuga a dibujar t = turtle.Turtle() # personalizamos la tortuga forma,color,tinta,etc t.shape(\"turtle\") # arrow, triangle, classic, circle t.shapesize(2,2,1) t.fillcolor(\"orange\") t.pencolor(\"white\") t.color(\"green\",\"blue\") # borde y relleno t.pensize(5) # rellenar figuras t.begin_fill() t.color(\"white\",\"blue\") # borde/tinta y relleno t.circle(100) t.end_fill() # dar velocidad a la tortuga (1-10) t.speed(1) # dar movimientos a la tortuga t.backward(100) t.right(90) t.forward(100) t.left(90) t.forward(100) # dar movimiento sin pintar t.penup() t.forward(50) t.pendown() t.forward(50) # hacer un retroceso t.undo() # limpiar pantalla y resetear posicion t.clear() t.reset() # dejar una marca como sello y seguir t.forward(100) t.stamp() t.forward(100) # movimiento perpendiculares t.goto(100,100) t.goto(-100,100) t.goto(0,0) # == t.home()) # movimientos de formas t.circle(50) #circulo diametro t.dot(30) #punto y diametro # esconder y mostrar de nuevo la tortuga dibujando t.hideturtle() t.circle(50) t.showturtle() t.circle(30) # movilizar la tortuga t.setx(100) t.sety(-10) # para que se quede la pantalla todo el rato turtle.done() # dar movimientos a la tortuga con un cuadrado t.forward(100) t.right(90) t.forward(100) t.right(90) t.forward(100) t.right(90) t.forward(100) # cuadrado automatizado t.color(\"red\",\"blue\") for i in range(4): t.forward(100) t.right(90) # dar movimientos con un circulo t.color(\"blue\",\"yellow\") t.circle(100) t.circle(80) t.circle(60) t.circle(40) t.circle(20) # circulo automatizado resultado = input(\"Quieres dibujar?: \") t.color(\"red\",\"blue\") if resultado == \"si\": while i<=100: t.circle(i) i+=20 else: print(\"No quieres dibujar...:(\")","title":"Comandos basicos:"},{"location":"routing/","text":"ROUTING DATOS REDES Switch: permite conectar muchos PCs en una misma red Local. Router: permite conectar a otras redes Patch Panel: ayuda al montaje de los switch LAN: Local Area Network, red individual o local WAN: Wide Area Network, conecta dos o m\u00e1s LAN con TSP Internet: interconexion de redes por ISP Modelo TCP/IP: capa acceso a la red(MAC), capa internet(IP), capa transporte(puertos O y D - TCP/UDP) y capa de aplicaci\u00f3n(-/HTTP/FTP) ip a / ip r ifconfig /etc/resolv.conf // nslookup ip/web netstat -putano nc -l 2000 // nc ip/host 2000 traceroute ip/dominio Entre router cables serials y entre router y pc fast ethernet. IPS PUBLICAS/PRIVADAS Estas direcciones IP privadas son: 10.0.0.0 \u2013 10.255.255.255 172.16.0.0 \u2013 172.31.255.255 192.168.0.0 \u2013 192.168.255.255 169.254.0.0 \u2013 169.254.255.255 M\u00e1scaras: 255.0.0.0 / 255.255.0.0 / 255.255.255.0 Cuando una m\u00e1quina con una direcci\u00f3n IP privada quiere conectarse a Internet, deber\u00e1 sustituir esa direcci\u00f3n IP privada en una IP p\u00fablica. Este proceso es conocido como NAT (Network Address Translation). Si tenemos una red con muchos dispositivos con direcciones IP privadas, los routers o los firewalls se encargan de hacer salir a todos esos dispositivos que lo requieran por la misma direcci\u00f3n IP p\u00fablica (a veces puede ser un pool). Cuando el tr\u00e1fico vuelve, estos son capaces de deshacer el cambio de manera que pueden mantenerse todas las comunicaciones. Cuando un dispositivo de red quiere conectarse a trav\u00e9s de Internet normalmente tendr\u00e1 un router para salir a Internet. Este router tiene una direcci\u00f3n IP y a esta direcci\u00f3n IP del router (o firewall) se le denomina \u00abDefault Gateway\u00bb (Puerta de enlace Predeterminada). CALCULO DE IP DE RED, BITS DE HOSTS Y BITS DE RED C\u00e1lculo es IP / MASCARA = RED Se corta el bit para la red hasta que es diferente el bit de mascara y de la ip. Se pone 1 cuando hay 1 arriba y abajo, 0 si no coindicen Ejemplo: # red 10.20.192.7/19 10 . 20.110|00000.00000111 255.255.111|00000.00000000 -------------------------- 10.255.11000000.00000000 10.255.192.0 / 19 --------------------------- primer host 10.255.192.1 /19 10.255.110|00000.00000001 /19 ultimo host 10.255.223.254 /19 10.255.110|11111.11111110 /19 red 10.255.192.0/19 broadcast 10.255.223.255/19 SUBNETTING Numero de dispositivos por red: 2**bitsHosts - 2 Numero de bits para redes: 2**x = numero de subredes Ejemplo: # red 192.168.100.32/20 se quiere 3 subredes(2**2=4) # se necesitaran 2 bits mas de redes para hacer las subredes 192.168.0110|01 00.00100000 /20 192.168.0110|00|00.00000000 /22 >> 192.168.96.0 /22 192.168.0110|01|00.00000000 /22 >> 192.168.100.0 /22 192.168.0110|10|00.00000000 /22 >> 192.168.104.0 /22 192.168.0110|11|00.00000000 /22 >> 192.168.108.0 /22 VLSM - VLANS Mascaras de subred de tama\u00f1o variable. Division de subredes con diferentes dispositivos por cada subred. A partir de una red madre, se va dividiendo seguidamente Ejemplo: # red 192.168.224.0 /20 para dispositivos de 700,200,200,50,2,2 700 >> 2**10-2 200 >> 2**8-2 200 >> 2**8-2 50 >> 2**6-1 2 >> 2**2-2 2 >> 2**2-2 --700-- 192.168.1110|00 00.00000000 /20 192.168.1110 00|00.00000000 /22 >> 192.168.224.0 /22 red 192.168.1110 00|11.11111111 /22 >> 192.168.227.255 /22 broadcast --200-- 192.168.11100100.|00000000 /24 >> 192.168.228.0 /24 red 192.168.11100100.|11111111 /24 >> 192.168.228.255 /24 broadcast --200-- 192.168.11100101.|00000000 /24 >> 192.168.229.0 /24 red 192.168.11100101.|11111111 /24 >> 192.168.229.255 /24 broadcast --50-- 192.168.11100110.00|000000 /26 >> 192.168.230.0 /26 red 192.168.11100110.00|111111 /26 >> 192.168.230.63 /26 broadcast --2-- 192.168.230.010000|00 /30 >> 192.168.230.64 /30 red 192.168.230.010000|11 /30 >> 192.168.230.67 /30 broadcast --2-- 192.168.230.010001|00 /30 >> 192.168.230.68 /30 red 192.168.230.010001|11 /30 >> 192.168.230.71 /30 broadcast CONF ROUTER De una red, ponemos la .1 para el router y la .2 para el primer PC. De una red entre 2 routers, la .1 para uno la .2 para otro. Para configurar la ruta de un router a otro, se indica la red y mascara de la red de destino y la ip del siguiente router por el que tiene que pasar en su entrada de este router. CONFIGURACION ROUTER **CONFIGURACION DE LAS INTERFACES** # //PUERTO SERIAL ENTRE ROUTERS\\\\ Router>enable Router#configure terminal Router(config)#interface serial 0/1/0 (1/0 tambien) Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # //PUERTO FAST ETHERNET ROUTERS-PCS\\\\ Router>enable Router#configure terminal Router(config)#interface fa0/0 Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # ##PARA MANTENER GRABADA LA INFO DEL ROUTER## Router#copy running-config startup-config # # **CONFIGURACION DHCP PARA IP PCS** Router>enable Router#configure terminal Router(config)#ip dhcp excluded-address 172.16.3.1 Router(config)#ip dhcp pool xarxa1 Router(dhcp-config)#network 172.16.3.0 255.255.255.0 Router(dhcp-config)#default-router 172.16.3.1 Router#copy running-config startup-config # # **VER TABLAS DE ENRUTAMIENTO DEL ROUTER** Router#show ip route **CREAR LAS TABLAS DE ENRUTAMIENTO** Router(config)#ip route 172.16.3.0 255.255.255.0 172.16.2.1 (RED DE DESTINO, MASCARA DESTINO, ROUTER POR EL QUE PASAR) # # **COMPROBACION CONEXIONES** ping + ip destino, desde pc origen **VER INTERFACES CONFIGURADAS** router1#show interfaces fastEthernet 0/0 router1#show interfaces serial 0/0/0 Pautas: -VLSM(NUMERO HOSTS 2N-2 O SUBXARXES) Y RUTES RESUM - CONFIGURACION DE SWITCHOS CERRANDO PUERTOS Y HABILITANDO LOS QUE SE USAN PARA PC/ROUTER A TRAVES DE LAS MACS CREANDO VLAN POR CADA PUERTO EN CONCRETO - CONECTAMOS TODOS LOS CABLES SIGUIENDO EL ESQUEMA DE LOS SWITCHOS - PONEMOS LAS IPS DE CADA INTERFAZ Y DHCP EN LOS PCS, NO SERVIDOR. - TABLAS DE ENRUTAMIENTO ESTATICAS( TODAS LAS REDES POR ROUTER Y POR DEFECTO) DINAMICAS(RIP/RIP2) PARA CADA LO QUE TIENE CONECTADA A CADA EXTREMO ROUTER / PC / DHCP Ejemplo: ROUTER 7: PC0: Router>enable Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa1/0 Router(config-if)#ip address 192.168.12.129 255.255.255.192 Router(config-if)#no shutdown router(config-if)# %LINK-5-CHANGED: Interface FastEthernet1/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet1/0, changed state to up # PC1: Router(config)#interface fa6/0 %Invalid interface type and number Router(config)#interface ethernet 6/0 Router(config-if)#ip address 192.168.0.1 255.255.248.0 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface Ethernet6/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet6/0, changed state to up # WAN2: Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa0/0 Router(config-if)#ip address 172.16.240.5 255.255.255.252 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up # WAN3: Router(config-if)#interface serial 2/0 Router(config-if)#ip address 172.16.240.9 255.255.255.252 Router(config-if)#no shutdown %LINK-5-CHANGED: Interface Serial2/0, changed state to down # DHCP: DEP.CF Router(config)#ip dhcp excluded-address 192.168.12.129 Router(config)#ip dhcp pool depcf Router(dhcp-config)#network 192.168.12.128 255.255.255.192 Router(dhcp-config)#default-router 192.168.12.129 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK] # AULESCF Router(config)#ip dhcp excluded-address 192.168.0.1 Router(config)#ip dhcp pool aulescf Router(dhcp-config)#network 192.168.0.0 255.255.248.0 Router(dhcp-config)#default-router 192.168.0.1 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK]","title":"Routing"},{"location":"routing/#routing","text":"","title":"ROUTING"},{"location":"routing/#datos-redes","text":"Switch: permite conectar muchos PCs en una misma red Local. Router: permite conectar a otras redes Patch Panel: ayuda al montaje de los switch LAN: Local Area Network, red individual o local WAN: Wide Area Network, conecta dos o m\u00e1s LAN con TSP Internet: interconexion de redes por ISP Modelo TCP/IP: capa acceso a la red(MAC), capa internet(IP), capa transporte(puertos O y D - TCP/UDP) y capa de aplicaci\u00f3n(-/HTTP/FTP) ip a / ip r ifconfig /etc/resolv.conf // nslookup ip/web netstat -putano nc -l 2000 // nc ip/host 2000 traceroute ip/dominio Entre router cables serials y entre router y pc fast ethernet.","title":"DATOS REDES"},{"location":"routing/#ips-publicasprivadas","text":"Estas direcciones IP privadas son: 10.0.0.0 \u2013 10.255.255.255 172.16.0.0 \u2013 172.31.255.255 192.168.0.0 \u2013 192.168.255.255 169.254.0.0 \u2013 169.254.255.255 M\u00e1scaras: 255.0.0.0 / 255.255.0.0 / 255.255.255.0 Cuando una m\u00e1quina con una direcci\u00f3n IP privada quiere conectarse a Internet, deber\u00e1 sustituir esa direcci\u00f3n IP privada en una IP p\u00fablica. Este proceso es conocido como NAT (Network Address Translation). Si tenemos una red con muchos dispositivos con direcciones IP privadas, los routers o los firewalls se encargan de hacer salir a todos esos dispositivos que lo requieran por la misma direcci\u00f3n IP p\u00fablica (a veces puede ser un pool). Cuando el tr\u00e1fico vuelve, estos son capaces de deshacer el cambio de manera que pueden mantenerse todas las comunicaciones. Cuando un dispositivo de red quiere conectarse a trav\u00e9s de Internet normalmente tendr\u00e1 un router para salir a Internet. Este router tiene una direcci\u00f3n IP y a esta direcci\u00f3n IP del router (o firewall) se le denomina \u00abDefault Gateway\u00bb (Puerta de enlace Predeterminada).","title":"IPS PUBLICAS/PRIVADAS"},{"location":"routing/#calculo-de-ip-de-red-bits-de-hosts-y-bits-de-red","text":"C\u00e1lculo es IP / MASCARA = RED Se corta el bit para la red hasta que es diferente el bit de mascara y de la ip. Se pone 1 cuando hay 1 arriba y abajo, 0 si no coindicen Ejemplo: # red 10.20.192.7/19 10 . 20.110|00000.00000111 255.255.111|00000.00000000 -------------------------- 10.255.11000000.00000000 10.255.192.0 / 19 --------------------------- primer host 10.255.192.1 /19 10.255.110|00000.00000001 /19 ultimo host 10.255.223.254 /19 10.255.110|11111.11111110 /19 red 10.255.192.0/19 broadcast 10.255.223.255/19","title":"CALCULO DE IP DE RED, BITS DE HOSTS Y BITS DE RED"},{"location":"routing/#subnetting","text":"Numero de dispositivos por red: 2**bitsHosts - 2 Numero de bits para redes: 2**x = numero de subredes Ejemplo: # red 192.168.100.32/20 se quiere 3 subredes(2**2=4) # se necesitaran 2 bits mas de redes para hacer las subredes 192.168.0110|01 00.00100000 /20 192.168.0110|00|00.00000000 /22 >> 192.168.96.0 /22 192.168.0110|01|00.00000000 /22 >> 192.168.100.0 /22 192.168.0110|10|00.00000000 /22 >> 192.168.104.0 /22 192.168.0110|11|00.00000000 /22 >> 192.168.108.0 /22","title":"SUBNETTING"},{"location":"routing/#vlsm-vlans","text":"Mascaras de subred de tama\u00f1o variable. Division de subredes con diferentes dispositivos por cada subred. A partir de una red madre, se va dividiendo seguidamente Ejemplo: # red 192.168.224.0 /20 para dispositivos de 700,200,200,50,2,2 700 >> 2**10-2 200 >> 2**8-2 200 >> 2**8-2 50 >> 2**6-1 2 >> 2**2-2 2 >> 2**2-2 --700-- 192.168.1110|00 00.00000000 /20 192.168.1110 00|00.00000000 /22 >> 192.168.224.0 /22 red 192.168.1110 00|11.11111111 /22 >> 192.168.227.255 /22 broadcast --200-- 192.168.11100100.|00000000 /24 >> 192.168.228.0 /24 red 192.168.11100100.|11111111 /24 >> 192.168.228.255 /24 broadcast --200-- 192.168.11100101.|00000000 /24 >> 192.168.229.0 /24 red 192.168.11100101.|11111111 /24 >> 192.168.229.255 /24 broadcast --50-- 192.168.11100110.00|000000 /26 >> 192.168.230.0 /26 red 192.168.11100110.00|111111 /26 >> 192.168.230.63 /26 broadcast --2-- 192.168.230.010000|00 /30 >> 192.168.230.64 /30 red 192.168.230.010000|11 /30 >> 192.168.230.67 /30 broadcast --2-- 192.168.230.010001|00 /30 >> 192.168.230.68 /30 red 192.168.230.010001|11 /30 >> 192.168.230.71 /30 broadcast","title":"VLSM - VLANS"},{"location":"routing/#conf-router","text":"De una red, ponemos la .1 para el router y la .2 para el primer PC. De una red entre 2 routers, la .1 para uno la .2 para otro. Para configurar la ruta de un router a otro, se indica la red y mascara de la red de destino y la ip del siguiente router por el que tiene que pasar en su entrada de este router. CONFIGURACION ROUTER **CONFIGURACION DE LAS INTERFACES** # //PUERTO SERIAL ENTRE ROUTERS\\\\ Router>enable Router#configure terminal Router(config)#interface serial 0/1/0 (1/0 tambien) Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # //PUERTO FAST ETHERNET ROUTERS-PCS\\\\ Router>enable Router#configure terminal Router(config)#interface fa0/0 Router(config-if)#ip address 192.168.1.2 255.255.255.0 Router(config-if)#no shutdown # # ##PARA MANTENER GRABADA LA INFO DEL ROUTER## Router#copy running-config startup-config # # **CONFIGURACION DHCP PARA IP PCS** Router>enable Router#configure terminal Router(config)#ip dhcp excluded-address 172.16.3.1 Router(config)#ip dhcp pool xarxa1 Router(dhcp-config)#network 172.16.3.0 255.255.255.0 Router(dhcp-config)#default-router 172.16.3.1 Router#copy running-config startup-config # # **VER TABLAS DE ENRUTAMIENTO DEL ROUTER** Router#show ip route **CREAR LAS TABLAS DE ENRUTAMIENTO** Router(config)#ip route 172.16.3.0 255.255.255.0 172.16.2.1 (RED DE DESTINO, MASCARA DESTINO, ROUTER POR EL QUE PASAR) # # **COMPROBACION CONEXIONES** ping + ip destino, desde pc origen **VER INTERFACES CONFIGURADAS** router1#show interfaces fastEthernet 0/0 router1#show interfaces serial 0/0/0 Pautas: -VLSM(NUMERO HOSTS 2N-2 O SUBXARXES) Y RUTES RESUM - CONFIGURACION DE SWITCHOS CERRANDO PUERTOS Y HABILITANDO LOS QUE SE USAN PARA PC/ROUTER A TRAVES DE LAS MACS CREANDO VLAN POR CADA PUERTO EN CONCRETO - CONECTAMOS TODOS LOS CABLES SIGUIENDO EL ESQUEMA DE LOS SWITCHOS - PONEMOS LAS IPS DE CADA INTERFAZ Y DHCP EN LOS PCS, NO SERVIDOR. - TABLAS DE ENRUTAMIENTO ESTATICAS( TODAS LAS REDES POR ROUTER Y POR DEFECTO) DINAMICAS(RIP/RIP2) PARA CADA LO QUE TIENE CONECTADA A CADA EXTREMO","title":"CONF ROUTER"},{"location":"routing/#router-pc-dhcp","text":"Ejemplo: ROUTER 7: PC0: Router>enable Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa1/0 Router(config-if)#ip address 192.168.12.129 255.255.255.192 Router(config-if)#no shutdown router(config-if)# %LINK-5-CHANGED: Interface FastEthernet1/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet1/0, changed state to up # PC1: Router(config)#interface fa6/0 %Invalid interface type and number Router(config)#interface ethernet 6/0 Router(config-if)#ip address 192.168.0.1 255.255.248.0 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface Ethernet6/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface Ethernet6/0, changed state to up # WAN2: Router#configure terminal Enter configuration commands, one per line. End with CNTL/Z. Router(config)#interface fa0/0 Router(config-if)#ip address 172.16.240.5 255.255.255.252 Router(config-if)#no shutdown Router(config-if)# %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up # WAN3: Router(config-if)#interface serial 2/0 Router(config-if)#ip address 172.16.240.9 255.255.255.252 Router(config-if)#no shutdown %LINK-5-CHANGED: Interface Serial2/0, changed state to down # DHCP: DEP.CF Router(config)#ip dhcp excluded-address 192.168.12.129 Router(config)#ip dhcp pool depcf Router(dhcp-config)#network 192.168.12.128 255.255.255.192 Router(dhcp-config)#default-router 192.168.12.129 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK] # AULESCF Router(config)#ip dhcp excluded-address 192.168.0.1 Router(config)#ip dhcp pool aulescf Router(dhcp-config)#network 192.168.0.0 255.255.248.0 Router(dhcp-config)#default-router 192.168.0.1 Router(dhcp-config)#exit Router(config)#exit Router# %SYS-5-CONFIG_I: Configured from console by console Router#copy running-config startup-config Destination filename [startup-config]? Building configuration... [OK]","title":"ROUTER / PC / DHCP"},{"location":"samba/","text":"SAMBA Dos demonios: smbd y nmbd. Fichero de conf smb.conf. Topologia de red Peer to Peer: una red entre iguales, a su bola. Cliente/servidor. Puertos 139(netbios-ssn) y 445(microsoft-ds) testparm test de configuracion de samba smbtree hace un mensaje de broadcaste y salen los que contesten. Forma: \\\\server\\recurs INSTALACION Dockerfile: # Version: 0.0.1 # @edt M06 2019-2020 # samba # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"SAMBA server 2019-2020 - PAM\" RUN dnf -y install procps samba samba-client nss-pam-ldapd cifs-utils passwd pam_mount authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] Authconfig.sh: #! /bin/bash authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall Install.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # instal.lacio # ------------------------------------- # creacio usuaris locals useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 # Configuraci\u00f3 client autenticaci\u00f3 ldap bash /opt/docker/auth.sh # SAMBA ## Configuracio shares Samba mkdir /var/lib/samba/public chmod 777 /var/lib/samba/public cp /opt/docker/* /var/lib/samba/public/. mkdir /var/lib/samba/privat #chmod 777 /var/lib/samba/privat cp /opt/docker/*.md /var/lib/samba/privat/. cp /opt/docker/smb.conf /etc/samba/smb.conf ## creacio usuaris unix locals i samba locals useradd lila useradd roc useradd patipla useradd pla echo -e \"lila\\nlila\" | smbpasswd -a lila echo -e \"roc\\nroc\" | smbpasswd -a roc echo -e \"patipla\\npatipla\" | smbpasswd -a patipla echo -e \"pla\\npla\" | smbpasswd -a pla Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis ldap /sbin/nscd && echo \"nscd Ok\" /sbin/nslcd && echo \"nslcd Ok\" # activar els serveis samba /usr/sbin/smbd && echo \"smb Ok\" # creacion de users samba dels users ldap, creando sus cuentas y directorios bash /opt/docker/usersSambaUnixLdap.sh # servei per a detached /usr/sbin/nmbd -F usersldap.sh: llistaUsers=\"pere marta anna pau pere jordi\" for user in $llistaUsers do echo -e \"$user\\n$user\" | smbpasswd -a $user line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir && echo \"directori home creat\" cp -ra /etc/skel/. $homedir && echo \"skel copiado\" chown -R $uid.$gid $homedir && echo \"chown ok\" fi done for user in user{01..05} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done for user in user{06..10} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done FICHEROS smb.conf: [global] workgroup = MYGROUP server string = Samba Server Version %v log file = /var/log/samba/log.%m max log size = 50 security = user passdb backend = tdbsam load printers = yes cups options = raw [homes] comment = Home Directories browseable = no writable = yes ; valid users = %S ; valid users = MYDOMAIN\\%S [printers] comment = All Printers path = /var/spool/samba browseable = no guest ok = no writable = no printable = yes [documentation] comment = Documentaci\u00f3 doc del container path = /usr/share/doc public = yes browseable = yes writable = no printable = no guest ok = yes [manpages] comment = Documentaci\u00f3 man del container path = /usr/share/man public = yes browseable = yes writable = no printable = no guest ok = yes [public] comment = Share de contingut public path = /var/lib/samba/public public = yes browseable = yes writable = yes printable = no guest ok = yes [privat] comment = Share d'acc\u00e9s privat path = /var/lib/samba/privat public = no browseable = no writable = yes printable = no guest ok = yes ORDENES smbtree smbtree -D smbtree -S smbclient (-N) //server/recurs smbclient //server/recurs -U user%password smbget -R smb://server/recurs/file mount -t cifs //server/share / mnt -o guest /etc/samba/smb.conf /etc/samba/lmhosts useradd lila -> smbpasswd -a lila borrar: smbpasswd -x lila smbclient -d0 //samba/public -u lila pdbedit // pdbedit -Lv","title":"SAMBA"},{"location":"samba/#samba","text":"Dos demonios: smbd y nmbd. Fichero de conf smb.conf. Topologia de red Peer to Peer: una red entre iguales, a su bola. Cliente/servidor. Puertos 139(netbios-ssn) y 445(microsoft-ds) testparm test de configuracion de samba smbtree hace un mensaje de broadcaste y salen los que contesten. Forma: \\\\server\\recurs","title":"SAMBA"},{"location":"samba/#instalacion","text":"Dockerfile: # Version: 0.0.1 # @edt M06 2019-2020 # samba # ------------------------------------- FROM fedora:27 LABEL author=\"Miguel Amoros\" LABEL description=\"SAMBA server 2019-2020 - PAM\" RUN dnf -y install procps samba samba-client nss-pam-ldapd cifs-utils passwd pam_mount authconfig RUN mkdir /opt/docker COPY * /opt/docker/ RUN chmod +x /opt/docker/install.sh /opt/docker/startup.sh WORKDIR /opt/docker CMD [\"/opt/docker/startup.sh\"] Authconfig.sh: #! /bin/bash authconfig --enableshadow --enablelocauthorize --enableldap --enableldapauth --enablemkhomedir --ldapserver='ldapserver' --ldapbase='dc=edt,dc=org' --updateall Install.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # instal.lacio # ------------------------------------- # creacio usuaris locals useradd local1 useradd local2 useradd local3 echo \"local1\" | passwd --stdin local1 echo \"local2\" | passwd --stdin local2 echo \"local3\" | passwd --stdin local3 # Configuraci\u00f3 client autenticaci\u00f3 ldap bash /opt/docker/auth.sh # SAMBA ## Configuracio shares Samba mkdir /var/lib/samba/public chmod 777 /var/lib/samba/public cp /opt/docker/* /var/lib/samba/public/. mkdir /var/lib/samba/privat #chmod 777 /var/lib/samba/privat cp /opt/docker/*.md /var/lib/samba/privat/. cp /opt/docker/smb.conf /etc/samba/smb.conf ## creacio usuaris unix locals i samba locals useradd lila useradd roc useradd patipla useradd pla echo -e \"lila\\nlila\" | smbpasswd -a lila echo -e \"roc\\nroc\" | smbpasswd -a roc echo -e \"patipla\\npatipla\" | smbpasswd -a patipla echo -e \"pla\\npla\" | smbpasswd -a pla Startup.sh: #! /bin/bash # @edt ASIX M06 2019-2020 # startup.sh # ------------------------------------- #instalacio / preparacio /opt/docker/install.sh && echo \"Install Ok\" # activar els serveis ldap /sbin/nscd && echo \"nscd Ok\" /sbin/nslcd && echo \"nslcd Ok\" # activar els serveis samba /usr/sbin/smbd && echo \"smb Ok\" # creacion de users samba dels users ldap, creando sus cuentas y directorios bash /opt/docker/usersSambaUnixLdap.sh # servei per a detached /usr/sbin/nmbd -F usersldap.sh: llistaUsers=\"pere marta anna pau pere jordi\" for user in $llistaUsers do echo -e \"$user\\n$user\" | smbpasswd -a $user line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir && echo \"directori home creat\" cp -ra /etc/skel/. $homedir && echo \"skel copiado\" chown -R $uid.$gid $homedir && echo \"chown ok\" fi done for user in user{01..05} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done for user in user{06..10} do echo -e \"jupiter\\njupiter\" | smbpasswd -a $user && echo \"usuari $user creat\" line=$(getent passwd $user) uid=$(echo $line | cut -d: -f3) gid=$(echo $line | cut -d: -f4) homedir=$(echo $line | cut -d: -f6) echo \"$user $uid $gid $homedir\" if [ ! -d $homedir ]; then mkdir -p $homedir cp -ra /etc/skel/. $homedir chown -R $uid.$gid $homedir fi done","title":"INSTALACION"},{"location":"samba/#ficheros","text":"smb.conf: [global] workgroup = MYGROUP server string = Samba Server Version %v log file = /var/log/samba/log.%m max log size = 50 security = user passdb backend = tdbsam load printers = yes cups options = raw [homes] comment = Home Directories browseable = no writable = yes ; valid users = %S ; valid users = MYDOMAIN\\%S [printers] comment = All Printers path = /var/spool/samba browseable = no guest ok = no writable = no printable = yes [documentation] comment = Documentaci\u00f3 doc del container path = /usr/share/doc public = yes browseable = yes writable = no printable = no guest ok = yes [manpages] comment = Documentaci\u00f3 man del container path = /usr/share/man public = yes browseable = yes writable = no printable = no guest ok = yes [public] comment = Share de contingut public path = /var/lib/samba/public public = yes browseable = yes writable = yes printable = no guest ok = yes [privat] comment = Share d'acc\u00e9s privat path = /var/lib/samba/privat public = no browseable = no writable = yes printable = no guest ok = yes","title":"FICHEROS"},{"location":"samba/#ordenes","text":"smbtree smbtree -D smbtree -S smbclient (-N) //server/recurs smbclient //server/recurs -U user%password smbget -R smb://server/recurs/file mount -t cifs //server/share / mnt -o guest /etc/samba/smb.conf /etc/samba/lmhosts useradd lila -> smbpasswd -a lila borrar: smbpasswd -x lila smbclient -d0 //samba/public -u lila pdbedit // pdbedit -Lv","title":"ORDENES"},{"location":"sap/","text":"SAP CURSO Systeme Anwendungen und Produkte que significa en espa\u00f1ol 'sistemas, aplicaciones y productos'. SAP ERP\u200b es un software de planificaci\u00f3n de recursos empresariales desarrollado por la compa\u00f1\u00eda alemana SAP SE. SAP ERP incorpora las funciones empresariales claves de una organizaci\u00f3n. As\u00ed, si tuvi\u00e9ramos que definir qu\u00e9 es SAP dir\u00edamos que es un software ERP (Enterprise Resource Planning), que permite planificar y gestionar los recursos de todas las \u00e1reas de la empresa: desde log\u00edstica a contabilidad, pasando por el departamento comercial y de m\u00e1rketing, finanzas, producci\u00f3n, gesti\u00f3n de proyectos, de la calidad, mantenimiento o direcci\u00f3n y administraci\u00f3n general. ERP Enterprise Resource Planning, planificaci\u00f3n de recursos empresariales. Es un conjunto de programas integrados que apoya las principales actividades organizacionales tales como finanza, contabilidad, log\u00edstica, producci\u00f3n, ventas y recursos humanos. Caracteristicas: Procesos estandatizados En linea Bases de datos unica Integrar todos los datos y procesos en un sistema \u00fanico. Configurable Modular SAP System Applications and Products in data processing. Sistema, aplicaciones y productos en el procesamiento de datos. ARQUITECTURA Estructura cliente - servidor Estrucutra de ambiente de desarrollo - calidad - producci\u00f3n. SAP GUI es el programa que se instala el ordenador para porder acceder en remoto al servidor en una empresa. SAP LOGON es el programa que se utiliza para entrar o loguearse en SAP. Es como el iconito al clicar dos veces para conectarse y elegir a que servidor conectarse. MODULOS SAP Formado por varios modulos interconectados entre ellos y que si hay datos en uno se referencian en los otros automaticamente. Modulos: FI (Finanzas) CO (controling de costes) MM (Materiales) SD (venta y distribucion) PP ( Produccion) PM (mantemiento planta) QM (calidad) PS (proyectos) HR (rrhh) ABAP (programacion y desarrollo) CONCEPTOS BASICOS Customizing: configuraci\u00f3n que representa la estructura legal y los procesos de negocio de la empresa. Unidad Organizativa: representa la estructura jerarquica de la empresa en el sistema. Datos maestros: datos requeridos para realizar transacciones del proceso en el sistema(clientes, materiales, proveedores...) Batch Input: para la entrada de grandes cantidades de registros. Documentos: cada transacci\u00f3n registra datos en la bbdd, crea un documento con id unico. Transacci\u00f3n: procesos de negocio en el sistema SAP. Sistemas de informaci\u00f3n: transacciones guardadas. Z: programas desarrollados aparte. MODO SAP: cada pantalla del SAP. WORKFLOW: flujo de trabajo para optimizar procesos. DATOS MAESTROS: info que cambia poco y son campos obligatorios para rellenar(clientes, precios, proveedores..) ESTRUCTURA ORGANIZATIVA CONEXION A SAP PANTALLA EASSY SAP FAVORITOS BARRA HERRAMIENTAS BARRA DE MENU MODOS COMANDOS DE USO FRECUENTE En el cuadro de busqueda: /N - termina la transacci\u00f3n /O - ver modos abiertos /I - cierra la transaccion abierta /Ntransacci\u00f3n - lleva a la trans indicada y cierra la actual. /NEND - salir del sistema pero avisa de cosas abiertas. /NEX - cierra totalmente CAMPOS DIFERENTES","title":"SAP"},{"location":"sap/#sap-curso","text":"Systeme Anwendungen und Produkte que significa en espa\u00f1ol 'sistemas, aplicaciones y productos'. SAP ERP\u200b es un software de planificaci\u00f3n de recursos empresariales desarrollado por la compa\u00f1\u00eda alemana SAP SE. SAP ERP incorpora las funciones empresariales claves de una organizaci\u00f3n. As\u00ed, si tuvi\u00e9ramos que definir qu\u00e9 es SAP dir\u00edamos que es un software ERP (Enterprise Resource Planning), que permite planificar y gestionar los recursos de todas las \u00e1reas de la empresa: desde log\u00edstica a contabilidad, pasando por el departamento comercial y de m\u00e1rketing, finanzas, producci\u00f3n, gesti\u00f3n de proyectos, de la calidad, mantenimiento o direcci\u00f3n y administraci\u00f3n general.","title":"SAP CURSO"},{"location":"sap/#erp","text":"Enterprise Resource Planning, planificaci\u00f3n de recursos empresariales. Es un conjunto de programas integrados que apoya las principales actividades organizacionales tales como finanza, contabilidad, log\u00edstica, producci\u00f3n, ventas y recursos humanos. Caracteristicas: Procesos estandatizados En linea Bases de datos unica Integrar todos los datos y procesos en un sistema \u00fanico. Configurable Modular","title":"ERP"},{"location":"sap/#sap","text":"System Applications and Products in data processing. Sistema, aplicaciones y productos en el procesamiento de datos.","title":"SAP"},{"location":"sap/#arquitectura","text":"Estructura cliente - servidor Estrucutra de ambiente de desarrollo - calidad - producci\u00f3n. SAP GUI es el programa que se instala el ordenador para porder acceder en remoto al servidor en una empresa. SAP LOGON es el programa que se utiliza para entrar o loguearse en SAP. Es como el iconito al clicar dos veces para conectarse y elegir a que servidor conectarse.","title":"ARQUITECTURA"},{"location":"sap/#modulos-sap","text":"Formado por varios modulos interconectados entre ellos y que si hay datos en uno se referencian en los otros automaticamente. Modulos: FI (Finanzas) CO (controling de costes) MM (Materiales) SD (venta y distribucion) PP ( Produccion) PM (mantemiento planta) QM (calidad) PS (proyectos) HR (rrhh) ABAP (programacion y desarrollo)","title":"MODULOS SAP"},{"location":"sap/#conceptos-basicos","text":"Customizing: configuraci\u00f3n que representa la estructura legal y los procesos de negocio de la empresa. Unidad Organizativa: representa la estructura jerarquica de la empresa en el sistema. Datos maestros: datos requeridos para realizar transacciones del proceso en el sistema(clientes, materiales, proveedores...) Batch Input: para la entrada de grandes cantidades de registros. Documentos: cada transacci\u00f3n registra datos en la bbdd, crea un documento con id unico. Transacci\u00f3n: procesos de negocio en el sistema SAP. Sistemas de informaci\u00f3n: transacciones guardadas. Z: programas desarrollados aparte. MODO SAP: cada pantalla del SAP. WORKFLOW: flujo de trabajo para optimizar procesos. DATOS MAESTROS: info que cambia poco y son campos obligatorios para rellenar(clientes, precios, proveedores..)","title":"CONCEPTOS BASICOS"},{"location":"sap/#estructura-organizativa","text":"","title":"ESTRUCTURA ORGANIZATIVA"},{"location":"sap/#conexion-a-sap","text":"","title":"CONEXION A SAP"},{"location":"sap/#pantalla-eassy-sap","text":"","title":"PANTALLA EASSY SAP"},{"location":"sap/#favoritos","text":"","title":"FAVORITOS"},{"location":"sap/#barra-herramientas","text":"","title":"BARRA HERRAMIENTAS"},{"location":"sap/#barra-de-menu","text":"","title":"BARRA DE MENU"},{"location":"sap/#modos","text":"","title":"MODOS"},{"location":"sap/#comandos-de-uso-frecuente","text":"En el cuadro de busqueda: /N - termina la transacci\u00f3n /O - ver modos abiertos /I - cierra la transaccion abierta /Ntransacci\u00f3n - lleva a la trans indicada y cierra la actual. /NEND - salir del sistema pero avisa de cosas abiertas. /NEX - cierra totalmente","title":"COMANDOS DE USO FRECUENTE"},{"location":"sap/#campos-diferentes","text":"","title":"CAMPOS DIFERENTES"},{"location":"soporte/","text":"CURSO GOOGLE SOPORTE TI 1.ASPECTOS BASICOS ASISTENCIA TECNICA HACER OVERCLOOKING para aumentar la frecuencia de reloj de las CPU para que vaya mas rapido en segun que tareas. HERRAMIENTA BOOTEABLE USB Conectarse por ssh a una maquina remota LINUX: Linux: descargamos las ssh keys, le ponemos permisos 600 y nos conectamos con ssh -i user@ip. Windows: descargamos las prk keys, abrimos PUTTY e indicamos puerto 22 y user e IP. En la lista Category, expanda SSH. Haga clic en Auth (sin expandir). En el cuadro Private key file for authentication, busque el archivo PPK que descarg\u00f3 y haga clic en \u00e9l. Haga clic en el bot\u00f3n Open. Conectarse por ssh a una maquina remota WINDOWS: Windows: Para abrir Remote Desktop Connection, haga clic en el bot\u00f3n Start. En el cuadro de b\u00fasqueda, escriba Remote Desktop Connection y, luego, en la lista de resultados, haga clic en Remote Desktop Connection. Ingrese la direcci\u00f3n IP externa de la instancia a la que desea conectarse en el campo Computer. Busque la direcci\u00f3n IP externa de su instancia en el panel \"Connection Details\" a la izquierda. Haga clic en Connect. Cambie el nombre de usuario a student y use la contrase\u00f1a que se menciona en el panel \"Connection Details\" a la izquierda. Haga clic en OK. Haga clic en Yes para aceptar el certificado. Linux: Abra Remmina. Escriba la direcci\u00f3n IP externa de la instancia a la que desea conectarse. Busque la direcci\u00f3n IP externa de su instancia en el panel \"Connection Details\" a la izquierda. Haga clic en Connect. Aseg\u00farese de que el protocolo de conexi\u00f3n est\u00e9 establecido en RDP, como se muestra en la siguiente imagen: Aparecer\u00e1 una ventana para que acepte el certificado. Haga clic en Ok para continuar. Deje el campo de dominio vac\u00edo. Cambie el nombre de usuario a student y use la contrase\u00f1a que se menciona en el panel \"Connection Details\" a la izquierda para el campo Password. Haga clic en Ok para continuar. Para verificar programas instalados: Windows: en programas y caracteristicas. Linux: comando dpkg -s firefox . Actualizar paquetes apt-get install -f . 2. BIT Y BITES: REDES INFORM\u00c1TICAS. ARP es un protocolo de comunicaciones de la capa de enlace de datos,1\u200b responsable de encontrar la direcci\u00f3n de hardware (Ethernet MAC) que corresponde a una determinada direcci\u00f3n IP. Para ello se env\u00eda un paquete (ARP request) a la direcci\u00f3n de difusi\u00f3n de la red (broadcast, MAC = FF FF FF FF FF FF) que contiene la direcci\u00f3n IP por la que se pregunta, y se espera a que esa m\u00e1quina (u otra) responda (ARP reply) con la direcci\u00f3n Ethernet que le corresponde. Cada m\u00e1quina mantiene una cach\u00e9 con las direcciones traducidas para reducir el retardo y la carga. ARP permite a la direcci\u00f3n de Internet ser independiente de la direcci\u00f3n Ethernet, pero esto solo funciona si todas las m\u00e1quinas lo soportan. De manera sencilla de explicar, el objetivo del protocolo ARP es permitir a un dispositivo conectado a una red LAN obtener la direcci\u00f3n MAC de otro dispositivo conectado a la misma red LAN cuya direcci\u00f3n IP es conocida. ARP se utiliza en cuatro casos referentes a la comunicaci\u00f3n entre dos hosts: Cuando dos hosts est\u00e1n en la misma red y uno quiere enviar un paquete a otro. Cuando dos hosts est\u00e1n sobre redes diferentes y deben usar un gateway o router para alcanzar otro host. Cuando un router necesita enviar un paquete a un host a trav\u00e9s de otro router. Cuando un router necesita enviar un paquete a un host de la misma red. IP PRIVADAS: Clase A: 10.0.0.0 a 10.255.255.255/24 Clase B: 172.16.0.0 a 172.31.255.255/16 Clase C: 192.168.0.0 a 192.168.255.255/16 EJEMPLO PROTOCOLOS DE ENRUTAMIENTO PUERTOS: El puerto 0 no se usa para el tr\u00e1fico de red, pero a veces se usa en las comunicaciones que tienen lugar entre diferentes programas en la misma computadora. Los puertos 1-1023 se conocen como puertos del sistema o, en ocasiones, como \"puertos conocidos\". Estos puertos representan los puertos oficiales de los servicios de red m\u00e1s conocidos. En un video anterior, hablamos sobre c\u00f3mo HTTP normalmente se comunica a trav\u00e9s del puerto 80, mientras que FTP generalmente se comunica a trav\u00e9s del puerto 21. En la mayor\u00eda de los sistemas operativos, se necesita acceso a nivel de administrador para iniciar un programa que escucha en un puerto del sistema. Los puertos 1024-49151 se conocen como puertos registrados. Estos puertos se usan para muchos otros servicios de red que pueden no ser tan comunes como los que est\u00e1n en los puertos del sistema. Un buen ejemplo de un puerto registrado es 3306, que es el puerto que muchas bases de datos escuchan. Los puertos registrados a veces est\u00e1n registrados y reconocidos oficialmente por la IANA, pero no siempre. En la mayor\u00eda de los sistemas operativos, cualquier usuario de cualquier nivel de acceso puede iniciar un programa escuchando en un puerto registrado. Finalmente, tenemos los puertos 49152-65535. Estos son conocidos como puertos privados o ef\u00edmeros. Los puertos ef\u00edmeros no se pueden registrar con la IANA y generalmente se utilizan para establecer conexiones salientes. Debe recordar que todo el tr\u00e1fico TCP utiliza un puerto de destino y un puerto de origen. Cuando un cliente desea comunicarse con un servidor, se le asignar\u00e1 un puerto ef\u00edmero para que se use solo para esa conexi\u00f3n, mientras el servidor escucha en un sistema est\u00e1tico o un puerto registrado. DNS Para la gesti\u00f3n de un dominio existen una serie de par\u00e1metros que permiten configurar el dominio para diferentes tareas llamados registros DNS. Por ejemplo, con estos par\u00e1metros permiten que un mismo dominio pueda funcionar en varios servidores, crear subdominios y que cada uno apunte a diferentes IPs o distintos alojamientos. Entre los tipos de registro DNS m\u00e1s utilizados se encuentran los siguientes: A (Address), Registro de direcci\u00f3n. Devuelve una direcci\u00f3n IP. Este registro sirve para resolver nombres de alojamientos a un n\u00famero IPv4, teniendo en cuenta si la IP es din\u00e1mica o fija. Por ejemplo, para apuntar nuestro nombre de dominio a un servidor. AAAA (Address), Registro de direcci\u00f3n IPv6. Los registros AAAA son muy parecidos a los A, es decir, ambos devuelven una direcci\u00f3n IP. En el caso de los AAAA, las IPs que se almacenan son IPv6. Este tipo de registro de DNS, al igual que A, sirve para apuntar nuestro dominio a un determinado servidor. CAA (Certification Authority Authorization), Autorizaci\u00f3n de la Autoridad de Certificaci\u00f3n. Este par\u00e1metro de DNS es un mecanismo de seguridad que permite limitar las autoridades certificadoras v\u00e1lidas para un dominio. Es decir, indicar a qu\u00e9 autoridades certificadoras permitimos emitir certificados de seguridad (SSL) para el dominio. CNAME (Canonical Name), Registro de nombre can\u00f3nico. Este registro se suele utilizar para crear alias de un nombre. CNAME es una forma de hacer que el dominio apunte a otro dominio diferente o a un subdominio. Tambi\u00e9n puede usarse cuando distintos servicios est\u00e1n utilizando una misma IP, de forma que cada servicio tenga su propia entrada DNS. MX (Mail eXchange), Registro de intercambio del correo. Los registros MX apuntar al servidor de correo del dominio y es posible establecer tantos como sean necesarios. En relaci\u00f3n con estos registros ten en cuenta que, de forma autom\u00e1tica, se establecen prioridades. Es decir, el primer registro MX que introduzcas tendr\u00e1 prioridad sobre los siguientes. PTR (Pointer), Registro de puntero. O registro inverso, ya que funciona de manera opuesta a A, se encarga de traducir IPs a nombres de dominio. Generalmente PTR se utiliza en el archivo de configuraci\u00f3n de la zona DNS inversa. SRV (Service record), Localizador de servicios. Es un registro para servicios especiales que proporciona informaci\u00f3n relacionada con los servicios disponibles para un determinado dominio. Es habitual su uso con XMPP, LDAP o SIP. TXT (Text), Registro de texto. Registro para insertar el texto que desees. Suele utilizarse para verificar la autoridad del dominio o para evitar usos incorrectos de las direcciones de correo. Adem\u00e1s, TXT permite la creaci\u00f3n de registros especiales y Domain-Keys. Est\u00e1n los HOSTS FILES para indicar IP+dominio para especificar o nombras hosts de la red o direcciones ejemplo 192.168.1.5 pc-miguel Prueba de conectividad de puerto: NETCAT nc (-zv) google.com 80 TEST-NETCONNECTION Test-NetConnection google.com Comando PING para probar conexi\u00f3n a un host o direcci\u00f3n. TRACEROUTE o TRACERT para probar los saltos de nodos para llegar a una direcci\u00f3n: traceroute google.com o en windows tracert google.com Comando NSLOOKUP para ver los dns de un sitio. nslookup google.com . Sin nombre es modo interactivo. Se pueden poner server 8.8.8.8 y cambia de dns server o set type MX para que te diga esos registros. 3.SISTEMAS OPERATIVOS COMANDOS BASICOS LINUX COMANDOS CMD WINDOWS DOC Ayuda de comandos: Win: Get-Help comando o comando /? Lin: comando --help o man comando Listar directorios: Win: ls C:\\ o ls -Force C:\\ Lin: ls directorio o ls -la dir Cambiar directorios: cd Crear directorios: mkdir dir o mkdir dir \"el direcorio nuevo\" Historial de comandos: history o CONTROL+R texto Copiar archivos: cp file1 dir/file1 Copiar directorio: Win: cp dir1 /dir/dir1 -Recurse -Verbose Lin: cp -r dir1 /dir/dir1 Mover/renombrar archivos: mv file1 dir/filr1 Borrar: Win: rm file o rm -Force file o rm dir1 - Recurse Lin: rm file o rm -f file Ver contenido: cat file Head/tail: Win: cat file.txt -Head/Tail 10 Lin: head/tail -n5 file.txt Modificar archivos: Win: start notepad++ file2.txt Lin: vim file1.txt Donde se encuentra el comando: Win: Get-Alias ls Lin: whereis/which ls Buscar palabras en un texto/dir: Win: Select-String palabra dir/file.txt Se ha de activar indexing options de windows para buscar palabras entre contenido de ficheros. Win: ls dir/ -Recurse -Filter *.exe Lin: grep palabra dir/*.txt Redirecionamientos(>/>>/ ): Win: cat words.txt | Select-String st > st_words.txt Lin: cat words.txt | grep st > st_words.txt Ver usuarios y grupos: Win: ir a Computer Management > Users/groups Win: Get-LocalUser // Get-LocalGroup // Get-LocalGroupMember namegroup Linux: cat /etc/sudoers // cat /etc/passwd // cat /etc/group Cambiar contrase\u00f1a usuario: Win: net user miguel \"contrase\u00f1a\" // net user miguel * // net user miguel /logonpasswordchg:yes Lin: passwd miguel //passwd -e miguel A\u00f1adir usuario: Win: net user miguel */contrase\u00f1a /add /logonpasswordchg:yes Lin: useradd miguel Borrar usuario: Win: net user miguel /del // Remove-LocalUser miguel Lin: userdel miguel Ver permisos: Win : icacls dir/file Lin: ls -l dir/file Cambiar permisos: Win: icacls dir/file /grant 'Everyone:(OI)(CI)(R)' Lin: chmod u+x/650 dir/file En windows tenemos el permiso especial de Owner Propietary y en linux tenemos el SETUID(4), SETGID(2) Y STICKYBIT(1). Ejecutar paquetes software: Win: /path/file.exe Lin: dkpg -i/-r/-l file.deb // apt-get install file.rpm Comprimir archivos: Win : Compress-Archive -Path /dir/* /dir/file.zip Lin: tar -cvf file.tar file1 file2 file3 Dependencias de paquetes: Win: Register-PackageSource -Name chocolatey -ProviderName Chocolatey -Location http://chocolatey.org/api/v2 Win: Get-PackageSource Win: Find-Package sysinternals -IncluseDependencies Win: Install/Uninstall-Package sysinternals Lin: sudo apt install -f Sysinternals package es un tipo de herramienta/repositorio junto a Chocolatey para poder encontrar paquetes de dependencias de otros paquetes de software. Instalar paquetes: Win: Install-Package sysinternals // Get-Package paquete Lin: apt install paquete -y / apt remove/update/upgrade repos en /etc/apt/source.list Ver dispositivos: Win: device manager Lin: /dev Actualizaciones: Win: windows update Lin: uname -r // apt update / apt full-upgrade Ver procesos: Win : task manager / tasklist / Get-Process | Sort CPU -descending | Select -first 3 - Property ID,RAM,CPU Linux: ps / ps -ax / ps -ef / uptime / lsof / top Top para linux y process explorer para windows. Terminar procesos: Win: taskkill // taskkill /F /pid 586 Lin: kill 586 / kill -KILL/TSTP 856 Nos podemos conectar por SSH a otra maquina. Se ha de tener instalado el cliente ssh y en el otro el servidor ssh. Utilizamos herramientas en windows como putty.exe y pscp.exe y en linux via comandos: ssh -p puerto (-x) usuario:ip_host ssh-keygen -t rsa scp-copy-id ruta/fichero user@ip_host:ruta /etc/ssh.d/sshd_config Podemos conectarnos tambi\u00e9n a maquinas virtuales y poner un SO y utilizan recursos fisicos de nuestra maquina. Podemos ver registros de seguridad, servicios, autenticaci\u00f3n o del sistema en Event viewer de windows o en /var/log o /var/log/syslog en linux y buscar los posibles errores filtrando en su busqueda. Podemos clonar discos duros o sistemas operativos con herramienta como clonezilla o Ghost y en linux con el comando DD : dd if=/dev/sda1 of=/dev/disco_extraible/imagen.img bz=100M La ruta de un nuevo disco es conectarlo. Asignarla creando una particion y despues crearle un sistema de fichero para poder usar el espacio nuevo asignado. Formatero en Windows : en administrador de discos o en consola diskpart ->select disk1 -> clean -> create partition primary -> select partition 1 -> active -> format FS=NTFS label=my-disk quick Formatero en Linux: podemos usar la herramienta parted -l o fdisk . Para formatear un sistema de ficheros se usa mkfs -t ext4 /dev/sda2 una vez que hemos creado una partici\u00f3n con espacio. Luego se puede montar como por ejemplo el device nuevo o un usb con la opci\u00f3n mount /dev/sdb1 /opt/dir y para desmontar se usa umount dir/device . Para que se haga automaticamente en el sistema al iniciarse se pone en el fstab y blkid para ver el ui de la unidad. La swap es un memoria virtual de intercambio para acciones rapidas que se estan ejecutando. Para cambiarla en windows se va a panel de control -> seguridad ->sistema -> systema avanzados - > settings -> advanced y cambiarlo. En linux se asigna al principio al instalar el so o se puede crear una particion de tipo swap con `mkswap /dev/sdb1. Creaci\u00f3n de simbolyc link o hards donde en los hards se mantiene la chicha. Win: mklink (/H) /path/link_name file Lin: ln (-s) /path/file /path/link_name en linux se ve con ls -li los inodos y cuantos hard links tiene este archivo. Uso de disco: Win: en admin d discos o du Lin: du -h df -h Reparaci\u00f3n de sistemas de archivos: Win: fsutil repair query C: o chkdsk /F C: Lin: fsck /dev/sda1 PRACTICA PERMISOS Queremos quitar el permiso de Kara de WX y solo queremos que lea R. Le quitamos todos los permisos: ICACLS C:\\Users\\Qwiklab\\Documents\\important_document /remove \"Kara\" Le ponemos todos de nuevo solo con R: ICACLS C:\\Users\\Qwiklab\\Documents\\important_document /grant \"Kara:(r)\" Queremos que otro usuario tambien tenga permisos a esa carpeta y a kara se le sume RW. Le a\u00f1adimos a Phoebe permisos de lectura: ICACLS C:\\Users\\Qwiklab\\Secret\\ /grant \"Phoebe:(r)\" Le a\u00f1adimos a Kara el de escritura ya que la R ya la tiene: ICACLS C:\\Users\\Qwiklab\\Secret\\ /grant \"Kara:(w)\" Su objetivo en este ejemplo es cambiar los permisos de esta carpeta para que el grupo \"Everyone\" solo tenga permiso de lectura (no de escritura). Quitamos todos los permisos y luego a\u00f1adimos la R: ICACLS C:\\Users\\Qwiklab\\Music\\ /remove \"Everyone\" En este ejemplo, necesita modificar los permisos de ese archivo, de manera que el grupo llamado \"Authenticated Users\" tenga acceso de escritura. El grupo \"Authenticated Users\" contiene usuarios que se han autenticado en el dominio o en un dominio que es confiable por la computadora. A\u00f1adimos ese tipo de usuarios con el permiso: ICACLS C:\\Users\\Qwiklab\\Documents\\not_so_important_document /grant \"Authenticated Users:(w)\" En este ejemplo cambiar\u00e1 los permisos de otro archivo de la carpeta \"Documents\". El archivo llamado \"public_document\" tiene que estar disponible para lectura p\u00fablica, de manera que todas las personas del sistema puedan leerlo. La manera m\u00e1s sencilla de asegurarse de que todos los usuarios del sistema tengan permiso de lectura es agregar ese permiso al grupo \"Everyone\". ICACLS C:\\Users\\Qwiklab\\Documents\\public_document /grant \"Everyone:(r)\" CREAR PARTICIONES WINDOWS : Haga clic en el bot\u00f3n Start y seleccione Control Panel para abrir el panel de control. All\u00ed, navegue a System and Security y, luego, a Administrative Tools. En la ventana \"Administrative Tools\", haga doble clic en Computer Management. Como lo que nos interesa es administrar discos, en el panel izquierdo, debajo de Storage, seleccione Disk Management. En el panel de control, ver\u00e1 un di\u00e1logo donde deber\u00e1 ingresar el tama\u00f1o para reducir el disco. Escriba \"20,480MB\" para dividir el disco en dos particiones de 30 GB y 20 GB respectivamente. Haga clic en Shrink. El disco se reducir\u00e1 y el espacio adicional de 20 GB se mostrar\u00e1 como sin asignar. En este espacio sin asignar, crear\u00e1 una nueva partici\u00f3n de 20 GB. Haga clic con el bot\u00f3n derecho del mouse en el espacio y seleccione New Simple Volume. En la siguiente secci\u00f3n del asistente, aseg\u00farese de que la letra de la unidad sea E y haga clic en Next. A continuaci\u00f3n, formatear\u00e1 una partici\u00f3n para asignarle otro formato de archivo. El formateo de particiones es destructivo y borra todos los datos de la partici\u00f3n, lo que no es muy bueno. Recuerde siempre hacer una copia de seguridad de sus datos antes de modificar particiones en un sistema activo. Para formatear la partici\u00f3n \"E:\" y asignarle un formato de archivo distinto, haga clic con el bot\u00f3n derecho del mouse en la partici\u00f3n y seleccione Format. En el panel de control, ver\u00e1 un di\u00e1logo para formatear el sistema de archivos. En la lista desplegable de formatos de archivos, seleccione FAT32 y haga clic en OK. LINUX : En Linux, puede ver los dispositivos de bloques y los sistemas de archivos adjuntados a su sistema con el comando lsblk, que recopila informaci\u00f3n acerca de todos los dispositivos que se encuentran adjuntados al sistema y los imprime con una estructura de \u00e1rbol. Para ver los dispositivos que se adjuntaron a su VM, use el comando lsblk . De manera opcional, puede ver los discos activados en el sistema con el comando df . student-02-9da601a39b43@linux-instance:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10G 0 disk \u2514\u2500sda1 8:1 0 10G 0 part sdb 8:16 0 10G 0 disk \u2514\u2500sdb1 8:17 0 10G 0 part / Para enumerar todas las particiones que contiene /dev/sdb, pase /dev/sdb al comando fdisk sudo fdisk -l /dev/sdb . La activaci\u00f3n y desactivaci\u00f3n son los procesos mediante los cuales un dispositivo est\u00e1 disponible o deja de estarlo en un sistema de archivos Linux. Esto se logra con los comandos mount y umount. Antes de modificar un disco, primero debe desactivarlo del sistema con el comando \"umount\". Cuando haya terminado de modificar el disco, debe activarlo de nuevo en el sistema. m para ver las opciones del menu , p para ver las particiones d para borrar particion. Use el control de comando d para borrar la partici\u00f3n predeterminada. Cuando emita el control de comando d, fdisk le pedir\u00e1 que ingrese la cantidad de particiones que quiera borrar. Como tiene una sola partici\u00f3n, la predeterminada, fdisk la seleccionar\u00e1 autom\u00e1ticamente y la borrar\u00e1 para continuar. Ahora podr\u00e1 crear particiones nuevas. Para ello, ingrese el control de comando n. la primera cambiar\u00e1 al tipo de intercambio de Linux. Ingrese el control de comando t para cambiar el tipo de partici\u00f3n y seleccione la primera partici\u00f3n. Comando w para guardar cambios. A continuaci\u00f3n, crear\u00e1 diferentes sistemas de archivos en las particiones que acaba de generar. Para ello, usar\u00e1 el comando mkfs en Linux. Existen varios tipos de archivos. Es importante que los conozca a todos, junto con las funciones para las que son m\u00e1s adecuados. En este lab, formatear\u00e1 la segunda partici\u00f3n en \"ext4\", el tipo de sistema de archivos de Linux m\u00e1s usado sudo mkfs -t ext4 /dev/sda2 . Ahora, puede activar /dev/sda2 en una ubicaci\u00f3n del sistema de archivos para comenzar a acceder a los archivos que se encuentran en \u00e9l. Act\u00edvela en el directorio /home/my_drive. sudo mount /dev/sda2 /home/my_drive 4.ADMINISTRACI\u00d3N DE SISTEMAS Y SERVICIOS DE INFRAESTRUCTURA DE TI Software DNS Software DHCP Bind y POWER DNS OpenSSH Remote Clients Configurar DNS con apt install dnsmasq . Pruebas de dig www.google.com @localhost Parar servicio sudo service dnsmasq stop Depurar configuraciones sudo dnsmasq -d -q Cargar lista de host/ip sudo dnsmasq -d -q -H myshosts.txt DHCP con dhcp.conf: sudo dnsmasq -d -q -C dhcp.conf sudo dhclient -i interface -v Los servicios escriben registros en sudo tail /var/log/syslog Ver servicios con errores o running: sudo systemctl --state=failed/running En windows podemos ir al apartado servicios y buscar el servicio para parar, reiniciar, etc. Con powershell: Get-Service Get-Service wuauserv Get-Service wuauserv | Format-List * Stop-Service wuauserv Start-Service wuauserv Set-Service ScardSvr -StartupType Manual (habilitar servicios inhabilitados) Install-WindowsFeature Web-WebServer,Web-Mgmt-Tools -IncludeAllSubFeature (habilitar funciones adicionales). (En IIS cambiaremos la web de exemplo como si fuera index.html) CLIENTES DE CHAT CONFIGURAR CLIENTE EMAIL HERRAMIENTA DE CHROME DEV TOOLS LISTA ERRORES HTTP KERBEROS LDAP OPENLDAP Role-based access control CONFIGURATION MANAGER RSYNC COPIAS SEGURIDAD PRACTICA DNS Y DHCP Caso hipot\u00e9tico En la empresa donde trabaja, se configur\u00f3 dnsmasq para administrar las necesidades de - DNS y DHCP de la red. Actualmente, se usa casi todo el rango de DHCP para entregar IP din\u00e1micas. Se - agregar\u00e1n varios servidores a la red, que deben configurarse con direcciones IP - conocidas. Su tarea en este lab es modificar esa configuraci\u00f3n de dnsmasq para que los - servidores siempre tengan las mismas direcciones IP. Para ello, deber\u00e1 otorgar a los - servidores las IP necesarias y reducir el rango para las IP din\u00e1micas. Configuraci\u00f3n de red Debido a que sigue la regla de nunca realizar pruebas en producci\u00f3n, experimentar\u00e1 - con los cambios necesarios en una m\u00e1quina que simule la red. Debe hacerlo de esta - manera, en lugar de probar los comandos en el servidor DNS real. En la vida real, despu\u00e9s de terminar de experimentar, aplicar\u00eda los mismos cambios - que realiz\u00f3 en la etapa de prueba a la instancia de producci\u00f3n que ejecuta dnsmasq en - la red. Analicemos esta simulaci\u00f3n de configuraci\u00f3n de red. Para la instancia de pruebas, configuramos una interfaz de red virtual (llamada eth_srv) en la que escuchar\u00e1 el servidor DNS y DHCP. Podemos ver el estado de esa Interfaz con el comando ip. Con el siguiente comando se mostrar\u00e1 informaci\u00f3n sobre la configuraci\u00f3n de la red: Vemos nuestra interface: ip address show eth_srv Observamos que la interfaz est\u00e1 configurada para tener la direcci\u00f3n IPv4 192.168.1.1 en una red con una m\u00e1scara de red /24 o 255.255.255.0. Adem\u00e1s, tenemos otra interfaz de red virtual que usaremos para simular a un cliente que interact\u00faa con el servidor y solicita tr\u00e1fico DNS o DHCP. Esta interfaz se denomina eth_cli. Podemos ver el estado si usamos un comando equivalente como el que se mencion\u00f3 m\u00e1s arriba: student-04-aafdf6c7167d@linux-instance:~$ ip address show eth_srv 4: eth_srv@eth_cli: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 92:14:77:2e:2a:24 brd ff:ff:ff:ff:ff:ff inet 192.168.1.1/24 scope global eth_srv valid_lft forever preferred_lft forever inet6 fe80::9014:77ff:fe2e:2a24/64 scope link valid_lft forever preferred_lft forever student-04-aafdf6c7167d@linux-instance:~$ ip address show eth_cli 3: eth_cli@eth_srv: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 72:45:9a:57:5c:6e brd ff:ff:ff:ff:ff:ff inet6 fe80::7045:9aff:fe57:5c6e/64 scope link valid_lft forever preferred_lft forever En este caso, podemos ver que la interfaz no tiene una direcci\u00f3n IPv4 (todav\u00eda). Cuando se experimenta con cambios en un servicio, es recomendable habilitar la salida de depuraci\u00f3n para poder entender qu\u00e9 sucede y por qu\u00e9. Actualmente, dnsmasq se ejecuta como un demonio en segundo plano. Podemos consultar el estado con el comando de servicio que aprendimos en la lecci\u00f3n anterior: A fin de visualizar la salida de depuraci\u00f3n, vamos a detener el servicio en ejecuci\u00f3n y a iniciarlo manualmente como proceso en segundo plano. Primero debe detenerlo: sudo service dnsmasq stop Luego, debe iniciarlo manualmente con indicadores de depuraci\u00f3n: sudo dnsmasq -d -q Si esos par\u00e1metros le generan curiosidad, el indicador -d significa \"no demonio\", es decir, el servicio se ejecuta en primer plano en lugar de en segundo plano; el indicador -q significa \"registrar consultas\", es decir, se mostrar\u00e1n las interacciones con los clientes. Como el servicio se est\u00e1 ejecutando en segundo plano, no podr\u00e1 ejecutar otros comandos en la terminal hasta que lo detenga manualmente con \"Ctrl + C\". No lo detenga ahora, ya que necesitaremos que siga activo para el paso siguiente. Se puede hacer pruebas de que responde el dnsmasq en dos terminales: sudo dnsmasq -d -q student-04-aafdf6c7167d@linux-instance:~$ dig example.local @localhost cat /etc/dnsmasq.d/mycompany.conf vemos la info de la configuraci\u00f3n de dnsmasq: interface es el nombre de la interfaz que se usar\u00e1 para escuchar las solicitudes DHCP y entregar las respuestas; lo establecemos en nuestra interfaz virtual eth_srv. bind-interfaces significa que dnsmasq operar\u00e1 solo en esa interfaz y se ignorar\u00e1n las dem\u00e1s. domain es el nombre de dominio utilizado en la red. dhcp-option nos permite brindar a los clientes DHCP informaci\u00f3n adicional opcional. En este caso, establecemos el router (tambi\u00e9n conocido como puerta de enlace predeterminada) y el dns-server. Cuando los clientes reciban la respuesta DHCP, tambi\u00e9n recibir\u00e1n y aplicar\u00e1n esta configuraci\u00f3n. dhcp-range indica el rango de IP que est\u00e1 disponible para utilizarse en la asignaci\u00f3n din\u00e1mica de IP y la extensi\u00f3n del tiempo asignado. En este caso, toda la red (excepto el servidor DHCP) est\u00e1 disponible actualmente como parte del rango din\u00e1mico. Sabemos que debemos modificarlo. La asignaci\u00f3n de tiempo est\u00e1 definida en 24 horas, lo que no es recomendable si nuestra red tiene muchos dispositivos que solo son visibles durante per\u00edodos breves. C\u00f3mo experimentar con un cliente DHCP: Ahora que sabemos que el DNS funciona correctamente, vamos a experimentar con la configuraci\u00f3n de DHCP. En la segunda terminal, ejecutaremos dhclient, que es el cliente DHCP m\u00e1s com\u00fan en Linux. Como se mencion\u00f3, lo ejecutaremos en la interfaz eth_cli. Adem\u00e1s, le indicaremos que se inicie en modo detallado y ejecute un script de depuraci\u00f3n que le brindamos: sudo dhclient -i eth_cli -v -sf /root/debug_dhcp.sh La secuencia de comandos de depuraci\u00f3n que pasamos con el par\u00e1metro -sf es para que, en lugar de modificar toda la configuraci\u00f3n de red en esta m\u00e1quina, podamos ver qu\u00e9 informaci\u00f3n se recibi\u00f3 del servidor. Podemos cambiar la info del dhcp sudo nano /etc/dnsmasq.d/mycompany.conf : # This is the interface on which the DHCP server will be listening to. interface=eth_srv # This tells this dnsmasq to only operate on that interface and not operate # on any other interfaces, so that it doesn't interfere with other running # dnsmasq processes. bind-interfaces # Domain name that will be sent to the DHCP clients domain=mycompany.local # Default gateway that will be sent to the DHCP clients dhcp-option=option:router,192.168.1.1 # DNS servers to announce to the DHCP clients dhcp-option=option:dns-server,192.168.1.1 # Dynamic range of IPs to use for DHCP and the lease time. dhcp-range=192.168.1.20,192.168.1.254,6h dhcp-host=aa:bb:cc:dd:ee:b2,192.168.1.2 dhcp-host=aa:bb:cc:dd:ee:c3,192.168.1.3 dhcp-host=aa:bb:cc:dd:ee:d4,192.168.1.4 comprobamos que est\u00e9 bien el archivo student-04-aafdf6c7167d@linux-instance:~$ sudo dnsmasq --test -C /etc/dnsmasq.d/mycompany.conf dnsmasq: syntax check OK. Vemos que ahora est\u00e1 utilizando el nuevo rango, que comienza a partir de 192.168.1.20, y que la asignaci\u00f3n de tiempo es de 6 horas en lugar de 24 horas. PRACTICA APACHE Tenemos un caso en que la web, todo su contenido est\u00e1 en otra ubicaci\u00f3n y se ha de colocar bien y con su configuraci\u00f3n y desactivar la otra predeterminada. Instalamos apache sudo apt install apache2 -y . Los sitios webs donde se publican las webs se encuentran en ls -l /etc/apache2/sites-available . Ejemplo de virtualhost: <VirtualHost *:80> ServerAdmin webmaster@localhost DocumentRoot /var/www/html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> Eso indica que el servicio estar\u00e1 escuchando en el puerto 80 para todas las IP. Luego, se detalla la direcci\u00f3n de correo electr\u00f3nico del administrador del servicio, la ruta principal del sitio web y las rutas de los archivos de registro de errores y acceso. Copiamos el contenido de la web a donde toca sudo mv /opt/devel/ourcompany /var/www/ourcompany . Creamos un documento nuevo para crear nuestro propio sitio y no el predeterminado cd /etc/apache2/sites-available . Copiamos y editamos el nuevo cambiando el DocumentRoot por el nuevo sitio de /var/www/ourcompany: sudo cp 000-default.conf 001-ourcompany.conf student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo vim 001-ourcompany.conf Ya agregamos un sitio que apunta a la ubicaci\u00f3n adecuada, pero que todav\u00eda no est\u00e1 habilitado. Por el momento, sigue estando habilitado el sitio predeterminado. Apache2 nos permite tener sitios que est\u00e1n disponibles, aunque no necesariamente habilitados, para evitar los cambios disruptivos. Los sitios habilitados se administran en /etc/apache2/sites-enabled. Veamos el contenido de ese directorio: ls -l /etc/apache2/sites-enabled Las flechas indican que este archivo es un v\u00ednculo simb\u00f3lico al archivo que se encuentra en el directorio sites-available. En otras palabras, para habilitar o inhabilitar un sitio en Apache2, simplemente se crea o se quita un v\u00ednculo simb\u00f3lico entre los directorios sites-available y sites-enabled. Para simplificar el proceso, hay dos comandos, a2ensite y a2dissite, que administran esos symlinks por nosotros (los nombres provienen de las palabras en ingl\u00e9s para habilitar, [\"enable\"], o inhabilitar [\"disable\"] el sitio en Apache2). Utilicemos esos comandos para habilitar nuestro sitio nuevo e inhabilitar el predeterminado: sudo a2ensite 001-ourcompany.conf sudo a2dissite 000-default.conf Y volvamos a consultar el contenido del directorio: ls -l /etc/apache2/sites-enabled Hasta que no se recarga el servicio no funcionar\u00e1: sudo service apache2 reload Hay modulos en apache que se encuentran en: ls /etc/apache2/mods-available . Activamos uno que es para que se active el pie de pagina y no se vea una linea discontinua: student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo a2enmod include Considering dependency mime for include: Module mime already enabled Enabling module include. To activate the new configuration, you need to run: service apache2 restart student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo service apache2 restart student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo vi /etc/apache2/sites-available/001-ourcompany.conf ---- a\u00f1adimos despues del documentroot <Directory /var/www/ourcompany> Options +Includes XBitHack on </Directory> ------ student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo service apache2 reload PRACTICA NUBE Crearemos instancias en la nube con Google Cloud. Creamos una instancia con ciertas caracteristicas: gcloud compute instances create linux-instance --zone=us-central1-f --machine-type=n1-standard-1 --subnet=default --tags=http-server --image=ubuntu-1604-xenial-v20190628 --image-project=ubuntu-os-cloud --boot-disk-size=10GB gcloud compute instances create windows-instance --zone=us-central1-f --machine-type=n1-standard-1 --subnet=default --image=windows-server-2016-dc-v20190709 --image-project=windows-cloud --boot-disk-size=50GB Creamos un firewall: Las reglas de firewall le permiten seleccionar si desea o no que se permita cierto tr\u00e1fico entrante. Es posible que desee habilitar muchos m\u00e1s puertos, lo que se puede hacer despu\u00e9s de crear la instancia. Los puertos que se muestran en la p\u00e1gina de creaci\u00f3n de la VM son los m\u00e1s comunes. En esta instancia, agregue una regla de firewall para permitir el tr\u00e1fico HTTP: gcloud compute firewall-rules create default-allow-http --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=tcp:80 --source-ranges=0.0.0.0/0 --target-tags=http-server Listamos instancias con gcloud compute instances list : student-04-a602c997bb07@master-instance:~$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS linux-instance us-central1-f n1-standard-1 10.128.0.3 34.71.55.155 RUNNING master-instance us-central1-f n1-standard-1 10.128.0.2 34.66.56.246 RUNNING windows-instance us-central1-f n1-standard-1 10.128.0.4 34.72.123.73 RUNNING Para conectarse a una instancia se usa: gcloud compute ssh linux-instance --zone us-central1-f Actualizamos paquetes con sudo apt update e instalamos nginx sudo apt install nginx . Comprobamos yendo a la ip externa en el navegador. Ahora creamos un disco adicional de 500gb para luego usarlo a la instancia de windows: gcloud compute disks create additional-disk --type=pd-standard --size=500GB --zone=us-central1-f La unimos a la instancia: gcloud compute instances attach-disk windows-instance --disk additional-disk El disco que se vincul\u00f3 est\u00e1 en blanco; ni siquiera tiene un sistema de archivos. Para poder usarlo, primero tenemos que formatearlo. Este proceso se explica en detalle en el lab sobre c\u00f3mo particionar y formatear una unidad de disco en Windows. Aqu\u00ed lo explicaremos brevemente. Ingrese el siguiente comando para crear un usuario nuevo, acceder a la windows-instance y restablecer su contrase\u00f1a. gcloud compute reset-windows-password windows-instance --user=student --zone=us-central1-f ip_address: 34.72.123.73 password: b3JBcc[>@}Pu{$a username: student Ahora podemos conectarnos remotamente con remina o windows y veremos las caracteristicas. Despu\u00e9s de acceder, haga clic en el \u00edcono de Windows. Luego, busque la aplicaci\u00f3n de Computer Management (en Windows Administrative Tools), que se usa para particionar discos. Sugerencia: Puede evitar pasar por el Control Panel y tener que hacer clic en los diferentes subelementos. Para ello, escriba \"Computer\" en el men\u00fa de Windows. En el men\u00fa Storage hay una entrada de Disk Management que se puede usar para administrar discos. Cuando hace clic en esa entrada, aparece una ventana emergente que le indica que formatee el disco nuevo. Seleccione GPT como el tipo de tabla particionada. Luego, tendr\u00e1 un volumen disponible, pero no se formatear\u00e1 con ning\u00fan sistema de archivos espec\u00edfico. Para formatearlo, haga clic con el bot\u00f3n derecho en el volumen, seleccione \"New Simple Volume\" y siga las instrucciones del asistente para formatearlo usando el sistema de archivos NTFS. El sistema operativo formatear\u00e1 el disco y lo dejar\u00e1 listo para usar. PRACTICA ACTIVE DIRECTORY Active Directory es una herramienta central para los administradores de sistemas que necesitan administrar m\u00e1quinas Windows. Active Directory permite administrar usuarios, grupos, m\u00e1quinas y las pol\u00edticas que se aplican a todos ellos de manera centralizada. En esta pr\u00e1ctica de lab, interactuar\u00e1s con Active Directory; lo usar\u00e1s para agregar usuarios y grupos; editar\u00e1s la membres\u00eda de los usuarios, y tambi\u00e9n crear\u00e1s un nuevo objeto de directiva de grupo (GPO). Primero instalamos active directory por comando powershell: C:\\Qwiklabs\\ADSetup\\active_directory_install.ps1 Configuramos con este comando: C:\\Qwiklabs\\ADSetup\\configure_active_directory.ps1 Abra el Active Directory Administrative Center (ADAC). Para encontrarlo, escriba active en el men\u00fa de inicio de Windows. El Active Directory Administrative Center le permite administrar su instalaci\u00f3n de Active Directory mediante la configuraci\u00f3n de usuarios, grupos, computadoras, etc. Si\u00e9ntase libre de navegar por los recursos ya existentes en el directorio. Para este lab, crearemos un usuario nuevo llamado Alex. Primero, haga clic en la entrada example (local), que corresponde al dominio que puede administrar con su cuenta. Luego, despl\u00e1cese hacia abajo y haga doble clic en la entrada Users para ver la lista de usuarios y grupos que existen actualmente. Para crear un nuevo usuario, observe la lista de tareas en la parte derecha. En la secci\u00f3n Users, hay una opci\u00f3n New para las entradas que abre un men\u00fa secundario donde puede elegir el tipo de entidad que quiere crear. Como en este caso vamos a crear un usuario nuevo, haga clic en User. Si hace clic en la cuenta que se cre\u00f3, ver\u00e1 que, donde aparece el nombre del usuario, el sistema muestra Alex (Disabled). El sistema no habilitar\u00e1 una cuenta que no tenga una contrase\u00f1a segura. En este caso, la contrase\u00f1a est\u00e1 vac\u00eda porque no la establecimos. Obviamente, una contrase\u00f1a vac\u00eda no es segura. Para establecer una contrase\u00f1a, use la opci\u00f3n Reset password del men\u00fa. Al tener seleccionada la opci\u00f3n User must change password at next logon, nos aseguramos de que el usuario cambiar\u00e1 la contrase\u00f1a cuando acceda. El objetivo de este paso es que el administrador del sistema no sepa la contrase\u00f1a nueva del usuario despu\u00e9s del primer acceso. Cuando haya establecido una contrase\u00f1a segura, vuelva a intentar habilitar la cuenta. Esta vez deber\u00eda funcionar. Ahora agreguemos un nuevo grupo. Si explora los grupos existentes, ver\u00e1 que hay uno llamado \"Developers\" y otro llamado \"Java Developers\". Vamos a agregar un grupo adicional llamado \"Python Developers\". Agregue el nuevo grupo a \"Developers\" y, luego, agregue la cuenta que acabamos de crear para Alex al grupo \"Python Developers\". Para crear un nuevo grupo, use el mismo men\u00fa que us\u00f3 cuando cre\u00f3 un usuario nuevo, pero esta vez seleccione la opci\u00f3n New > Group. Estamos creando un grupo llamado \"Python Developers\"; ese es el \u00fanico dato obligatorio. Si lo desea, puede agregar informaci\u00f3n adicional en Description y Notes. Cuando termine, haga clic en OK para crear el grupo. C\u00f3mo agregar entidades a grupos: Ya tenemos un grupo llamado \"Python Developers\", y ahora queremos agregarlo al grupo \"Developers\" existente. Para hacerlo, nos desplazamos hacia abajo hasta la entrada nueva, hacemos clic con el bot\u00f3n derecho en la entrada de la lista y seleccionamos Add to another group. Cuando haga clic en el bot\u00f3n OK, se agregar\u00e1 el grupo \"Python Developers\" a \"Developers\". Ahora, tambi\u00e9n agregaremos a Alex a \"Python Developers\", pero seguiremos un camino diferente. En este caso, haremos doble clic en la entrada \"Python Developers\" de la lista, lo que abrir\u00e1 una ventana de edici\u00f3n para el grupo. Como en este caso agregaremos a Alex al grupo, haga clic en el bot\u00f3n Add, escriba \"Alex\" en el campo de texto, seleccione OK para agregarlo y, luego, OK para guardar los cambios. As\u00ed agregamos correctamente un nuevo miembro al grupo: Alex. Por \u00faltimo, hay usuario existente, llamado Alosha, que pas\u00f3 de desarrollar en Java a desarrollar en Python. Vamos a quitarlo del grupo \"Java Developers\" para agregarlo al grupo \"Python Developers\". Para ello, busque al usuario Alosha en la lista y haga doble clic en la entrada. Se abrir\u00e1n las propiedades del usuario, que podr\u00e1 editar. Hay muchas opciones de configuraci\u00f3n para cada usuario; haga clic en la secci\u00f3n de la izquierda llamada Member Of. Observamos que Alosha es miembro de los grupos \"Domain Users\" (todos los usuarios del dominio son miembros de este grupo) y \"Java Developers\". Seleccione la entrada \"Java Developers\" y haga clic en el bot\u00f3n Remove para quitarlo de ese grupo. Luego haga clic en Add para agregar una nueva membres\u00eda. Para administrar las pol\u00edticas de grupo, necesitamos usar la aplicaci\u00f3n de Group Policy Management. Para encontrarla, escriba group en el men\u00fa de inicio de Windows. Esta aplicaci\u00f3n le permite establecer pol\u00edticas que administrar\u00e1n el comportamiento de las m\u00e1quinas en su dominio. Puede aplicarlas a todo el dominio o a Organizational Units (OU) independientes. En nuestro caso, agregaremos una pol\u00edtica nueva a la OU de Developers que ya existe en el dominio. Para hacerlo, expanda el \u00e1rbol hasta llegar al \u00e1rbol de dominio de example.com y localice la OU de Developers dentro de \u00e9l. Para crear una pol\u00edtica nueva y vincularla, haga clic con el bot\u00f3n derecho en la entrada y seleccione la primera opci\u00f3n del men\u00fa: Create a GPO in this domain, and Link it here. Queremos establecer un fondo de pantalla predeterminado para las m\u00e1quinas de la OU de Developers, por lo que nuestra pol\u00edtica se llamar\u00e1 New Wallpaper. Cuando la hayamos creado, editaremos la pol\u00edtica haciendo clic con el bot\u00f3n derecho en la entrada y seleccionando la primera entrada del men\u00fa: Edit. Se abrir\u00e1 una nueva aplicaci\u00f3n: Group Policy Management Editor, que le permitir\u00e1 explorar y configurar todas las opciones que se pueden establecer en una pol\u00edtica de grupo. Como queremos establecer el fondo de pantalla, navegaremos hasta esta opci\u00f3n desde User Configuration > Policies > Administrative Templates > Desktop > Desktop. Se abrir\u00e1 una lista de opciones que podemos configurar, incluida \"Desktop Wallpaper\". Para establecer un valor espec\u00edfico de fondo de pantalla, haga doble clic en la entrada Desktop Wallpaper. La ventana que se abre le permite establecer el valor del fondo de pantalla. Para hacerlo, primero haga clic en el bot\u00f3n Enabled y, luego, ingrese una ruta de acceso para el fondo de pantalla. Puede ser una ruta local en la m\u00e1quina o la ruta de una red en un servidor que comparte archivos. Para este lab, ingresar\u00e1 C:\\Qwiklabs\\wallpaper.jpg en la secci\u00f3n Wallpaper Name.Cuando haga clic en OK, se crear\u00e1 la pol\u00edtica de grupo con los valores que deseamos. Para verificarlo, regrese a la aplicaci\u00f3n de Group Policy Management y haga clic en la pesta\u00f1a Settings de la pol\u00edtica nueva. Al hacer clic en los v\u00ednculos show en la p\u00e1gina web, puede observar que se defini\u00f3 la pol\u00edtica y que la \u00fanica opci\u00f3n modificada corresponde a Desktop Wallpaper, que tiene el valor que establecimos anteriormente. PRACTICA BACKUPPC La gesti\u00f3n de copias de seguridad es una actividad de importancia cr\u00edtica con la que tendr\u00e1s que lidiar como administrador del sistema. Cuando hablamos de administrar copias de seguridad, nos referimos no solo a la configuraci\u00f3n inicial de cierto software de copia de seguridad, sino tambi\u00e9n a garantizar que las copias de seguridad funcionen correctamente y puedan restaurarse de manera confiable. En este lab, usaremos varios comandos de Linux, que ya se explicaron en este curso y en el anterior. A continuaci\u00f3n, repasaremos sus funciones: sudo : Ejecuta un comando con derechos de administrador. apt update: Actualiza la lista de paquetes disponibles para instalar. apt install package: Instala el paquete en el sistema. a2enmod : Habilita un m\u00f3dulo de Apache2. a2ensite : Habilita un sitio web de Apache2. nano : Abre un editor de texto para editar el archivo. service restart: Reinicia el servicio indicado. ls : Muestra la lista de los archivos que se encuentran en un directorio. cp : Crea una copia del archivo antiguo con el nombre nuevo. cat : Muestra todo el contenido de un archivo. grep : Filtra el texto de un archivo seg\u00fan el patr\u00f3n. tail : Muestra las \u00faltimas l\u00edneas de un archivo. Administrar las copias de seguridad es una actividad muy importante que deber\u00e1 llevar a cabo como administrador del sistema. Adem\u00e1s de comprender la configuraci\u00f3n inicial de cierto software para hacer copias de seguridad, la administraci\u00f3n de las copias de seguridad implica garantizar que estas funcionen correctamente y puedan restablecerse de manera confiable. En este lab, instalar\u00e1 y configurar\u00e1 la infraestructura para realizar copias de seguridad de modo que m\u00e1quinas locales y remotas administren las copias. Tambi\u00e9n comprobar\u00e1 que las copias de seguridad puedan restablecerse de manera correcta. En este lab, usaremos BackupPc. Esta soluci\u00f3n le permite realizar copias de seguridad de los datos en distintos sistemas operativos y posee una interfaz web pr\u00e1ctica para la configuraci\u00f3n. En este lab, configuraremos una m\u00e1quina llamada backup-server como servidor. El servidor usar\u00e1 BackupPc para realizar copias de seguridad de otras m\u00e1quinas. Veremos c\u00f3mo realizar copias de seguridad de datos de la m\u00e1quina local y de m\u00e1quinas remotas de Linux y Windows, llamadas linux-instance y windows-instance, respectivamente. Respecto del servidor de copia de seguridad, es importante realizar una copia de seguridad del directorio /etc donde se almacenan las configuraciones. En cuanto a las m\u00e1quinas remotas, hay que realizar la copia de seguridad de los directorios /home y Users, donde se almacenan los datos de los usuarios. Primero nos conectamos al backup-server e instalamos la herramienta backupPc: sudo apt install backuppc Cuando se le pregunte c\u00f3mo configurar el correo electr\u00f3nico, responda Local only. Cuando haya confirmado el resto de las respuestas, el paquete terminar\u00e1 de instalarse. Si analiza el resultado, ver\u00e1 que indica que es necesario reiniciar Apache2 (y se recomienda usar systemctl restart apache2, comando equivalente al que estuvimos usando). Haremos eso en un minuto, pero antes configuraremos Apache2 y BackupPc para que usen SSL. Como mencionamos antes, para encriptar conexiones, usamos HTTPS en lugar de HTTP. Vamos a usar el sitio web que proporciona BackupPc para administrar nuestras copias de seguridad. El sitio web debe estar encriptado para evitar que la contrase\u00f1a administrativa y cualquier otro dato que se ingrese se transmitan en texto sin formato. De lo contrario, toda persona que esp\u00ede nuestra conexi\u00f3n a Internet podr\u00e1 leer la contrase\u00f1a de administrador y cualquier otro dato que enviemos. Para habilitar HTTPS en nuestro sitio web, es necesario realizar tres pasos: Para habilitar el m\u00f3dulo SSL, usaremos el comando a2enmod: sudo a2enmod ssl Para habilitar el sitio SSL predeterminado, usaremos el comando a2ensite.: sudo a2ensite default-ssl.conf C\u00f3mo habilitar SSL en BackupPc y configurar la contrase\u00f1a de administrador sudo vi /etc/backuppc/apache.conf . En este archivo se incluyen todas las configuraciones necesarias para que BackupPc tenga un sitio web que funcione en Apache2. Si desea obtener m\u00e1s informaci\u00f3n sobre las opciones establecidas, puede buscarlas en la documentaci\u00f3n de Apache2 . Quitaremos el car\u00e1cter # que est\u00e1 antes de la opci\u00f3n SSLRequireSSL. Eso significa que habilitamos la opci\u00f3n. El \u00fanico paso que falta es establecer la contrase\u00f1a administrativa del usuario backuppc. Podemos hacerlo con el comando htpasswd, que se usa para establecer contrase\u00f1as para usuarios web: sudo htpasswd /etc/backuppc/htpasswd backuppc Reiniciamos el server apache para aplicar cambios sudo service apache2 restart Ahora puede acceder a su sitio web. Para ello, copie la direcci\u00f3n IP externa de backup-server y p\u00e9guela en una ventana nueva del navegador. https://34.133.119.159/backuppc el navegador le pedir\u00e1 su nombre de usuario y contrase\u00f1a. El nombre de usuario es backuppc y la contrase\u00f1a es la que ingres\u00f3 con el comando htpasswd. Primero, crearemos una copia de seguridad de los archivos almacenados en la m\u00e1quina local. Para ello, haga clic en el v\u00ednculo Host Summary. En la p\u00e1gina de resumen del host, se enumeran todos los hosts conocidos en el momento de la consulta y su estado. Por ahora, el \u00fanico host conocido para el sistema de copias de seguridad es localhost. Aparecer\u00e1 bajo Hosts with no Backups porque todav\u00eda no tiene copias de seguridad. Si hace clic en el v\u00ednculo localhost, se abrir\u00e1 la p\u00e1gina con la informaci\u00f3n de copias de seguridad correspondiente a la m\u00e1quina local. Esta p\u00e1gina indica que no se realiz\u00f3 ninguna copia de seguridad de la m\u00e1quina todav\u00eda. Para cambiar eso, haga clic en el bot\u00f3n Start Full Backup. El mensaje en la parte superior de la p\u00e1gina se\u00f1ala que la copia de seguridad fall\u00f3 y que se produjeron errores. En la secci\u00f3n de resumen, vemos que hubo un intento de realizar una copia de seguridad, pero dice que fue \"parcial\", lo que indica que no se pudo completar la copia de seguridad. Para analizar los errores, haga clic en el v\u00ednculo Errors en la l\u00ednea \"Backup 0\". Hay muchos errores, pero el mensaje dice siempre lo mismo: Permission denied. La causa de ese error es que la copia de seguridad se est\u00e1 ejecutando con el usuario backuppc, y ese usuario no tiene permiso para ver muchos de los archivos y directorios correspondientes al directorio /etc. Analicemos c\u00f3mo se realiza la copia de seguridad de esos archivos. Haga clic en el v\u00ednculo Edit Config de la secci\u00f3n localhost en el lado izquierdo. Luego, haga clic en la pesta\u00f1a Xfer (abreviatura de transferencia). Para solucionar el problema de permisos que surgi\u00f3, le ordenaremos a backuppc que use el comando sudo para la creaci\u00f3n de la copia de seguridad. Para ello, en la opci\u00f3n TarClientCmd, agregue sudo antes de $tarPath. De esa forma, el comando tar podr\u00e1 acceder a todos los archivos a los que accede root. Damos a SAVE. Esta vez deber\u00eda haberse mostrado el error sudo: no tty present and no askpass program specified. La causa es que indicamos que se usar\u00eda el comando sudo, pero el usuario backuppc todav\u00eda no tiene permiso para usarlo sudo visudo (backuppc ALL=(ALL:ALL) NOPASSWD: /bin/tar) De esa forma, el usuario backuppc deber\u00eda poder acceder a todos los archivos cuando cree la copia de seguridad. Vuelva a la p\u00e1gina principal de localhost y haga clic en el bot\u00f3n Start Full Backup de nuevo. Confirme y luego regrese a la p\u00e1gina principal. Puede ver el contenido de cualquier copia de seguridad generada haciendo clic en el v\u00ednculo de la entrada de la copia de seguridad (en este caso, 0). Tambi\u00e9n puede ir directamente a la \u00faltima copia de seguridad haciendo clic en Browse backups en el men\u00fa superior izquierdo. Para examinar el contenido de una copia de seguridad, puede descargar directamente cualquiera de los archivos de la lista haciendo clic en el nombre. Por ejemplo, puede hacer clic en apache.conf y aparecer\u00e1 un mensaje para que descargue el archivo en su m\u00e1quina. Luego, podr\u00e1 abrirlo y verificar que tenga el mismo contenido que vimos antes. Si necesita restablecer algo de una copia de seguridad, debe seleccionar los elementos haciendo clic en la casilla de verificaci\u00f3n ubicada a la izquierda del nombre y, luego, hacer clic en el bot\u00f3n Restore selected files. Esto le permitir\u00e1 seleccionar c\u00f3mo restablecer los archivos, ya sea directamente en la unidad o bien descargando un archivo Zip o Tar que los contenga. C\u00f3mo hacer una copia de seguridad de una m\u00e1quina de Linux remota: Para permitir que nuestro servidor se conecte a la instancia de Linux de la que queremos hacer una copia de seguridad, crearemos un par de claves SSH que se almacenar\u00e1 en las m\u00e1quinas. sudo su - backuppc y ssh-keygen Veamos el contenido del directorio en el que se generaron las claves. sudo ls -l /var/lib/backuppc/.ssh/ Para ello, copie el archivo a /tmp de modo que pueda accederse sin derechos de administrador: sudo cp /var/lib/backuppc/.ssh/id_rsa.pub /tmp/ Luego, c\u00f3pielo a linux-instance con el siguiente comando: gcloud compute scp /tmp/id_rsa.pub root@linux-instance:~ --zone=us-central1-a Vemos que se ha copiado en la otra maquina sudo find / -type f -name \"id_rsa.pub\" Lo copiamos al user de nuestra maquina: sudo mv /home/sa_109733481048430167514/id_rsa.pub /home/student-03-104cdaa5152a Lo movemos a authorized keys para que accesa sin credenciales cat id_rsa.pub | sudo tee -a /root/.ssh/authorized_keys Ahora desde el server nos conectamos a la instancia linux sudo su - backuppc y ssh root@linux-instance C\u00f3mo agregar la m\u00e1quina remota en BackupPc: Ahora que funciona la conexi\u00f3n remota entre m\u00e1quinas, puede configurar este cliente nuevo en BackupPc. Vuelva a la interfaz web administrativa y haga clic en Edit Hosts. Agregaremos linux-instance como host nuevo. Para hacerlo, haga clic en el bot\u00f3n Add a fin de agregar una l\u00ednea nueva y escriba linux-instance en el campo host. Luego, haga clic en el bot\u00f3n Save. As\u00ed agregamos el host, pero falta configurarlo. Para ello, haga clic en Host summary, que ahora muestra los dos hosts conocidos por el sistema (uno con copias de seguridad y el otro sin ellas), y luego en linux-instance para ir a la p\u00e1gina de configuraci\u00f3n de esta instancia. La parte superior de la barra izquierda ahora se refiere a este host que queremos configurar. Para ello, haga clic en Edit Config y luego en la pesta\u00f1a Xfer. Para esta instancia, seleccione rsync como XferMethod. rsync es una herramienta que se puede usar para sincronizar el contenido de dos \u00e1rboles de directorios sin transferir todo el contenido, ya que verifica lo que ya se transfiri\u00f3 y solo transfiere lo nuevo. En la ruta de la copia de seguridad (llamada RsyncShareName en la configuraci\u00f3n rsync), escriba /home. Este es el directorio remoto que queremos resguardar. Tambi\u00e9n se puede realizar una copia de seguridad de toda la m\u00e1quina ingresando /. Sin embargo, en nuestro escenario, solo nos interesa /home, que contiene los directorios principales de los usuarios. Despu\u00e9s de ingresar el directorio, haga clic en el bot\u00f3n Save para guardar los cambios. Luego, vaya al v\u00ednculo de la p\u00e1gina principal de linux-instance, que muestra el estado actual de las copias de seguridad de esta m\u00e1quina. En este caso, todav\u00eda no hay copias. Haga clic en el bot\u00f3n Start Full Backup para forzar el inicio de una copia de seguridad nueva de esta m\u00e1quina. Despu\u00e9s de confirmar la realizaci\u00f3n de la copia, vuelva a la p\u00e1gina principal. Si borramos de la instancia linux el .pub vamos a la copia y restore el fichero este y lo volvemos a tener. Ahora nos conectamos a windows: Por \u00faltimo, tambi\u00e9n queremos usar el servidor de copias de seguridad para resguardar los datos remotos de una m\u00e1quina de Windows. En este caso, el recurso del que queremos hacer una copia de seguridad es el contenido de la carpeta Users. Para eso, debemos crear un usuario backuppc que pueda conectarse a la carpeta Users como recurso compartido. Primero, nos conectaremos a windows-instance. Creamos un usuario: Para crear la cuenta nueva, debemos abrir la aplicaci\u00f3n User Accounts. Abra Control Panel, haga clic en User Accounts y luego otra vez en User Accounts. Haga clic en Manage another account para crear una cuenta nueva. Una vez que creamos el usuario, debemos compartir la carpeta de la que queremos crear una copia de seguridad. En este escenario, queremos resguardar la carpeta Users. Para compartirla, abra File Explorer, navegue a la carpeta C:\\Users, haga clic con el bot\u00f3n derecho en ella y luego haga clic en Share with > Advanced sharing y compartimos Users. Vuelva a la interfaz web administrativa de BackupPc para configurar el host. Al igual que antes, primero debe hacer clic en Edit Hosts, agregar una entrada para windows-instance y guardar los cambios. Despu\u00e9s, haga clic en Host Summary y luego en windows-instance. Desde la p\u00e1gina principal, haga clic en Edit Config para editar la configuraci\u00f3n y seleccione la pesta\u00f1a Xfer. En este caso, vamos a usar smb como XferMethod. Tambi\u00e9n se conoce como samba y es el nombre del protocolo que se usa para interactuar con las carpetas compartidas de Windows. Debemos configurar SmbShareName para que sea la carpeta de la que queremos hacer una copia de seguridad (Users) y SmbShareUserName y SmbSharePassword para que sean el nombre de usuario y la contrase\u00f1a que creamos en la instancia de Windows. Haremos un backupp pero dara error. cambiamos en la instancia windows el tipo de cuenta creado por admin y al hacerlo de nuevo, ir\u00e1 bien. \u00a1Muy bien! Logr\u00f3 configurar un servidor de copias de seguridad, realizar copias de seguridad incrementales y completas, solucionar errores relacionados con permisos, verificar que las copias de seguridad funcionan bien descargando los archivos y restableci\u00e9ndolos, y configurar un servidor para que realice copias de seguridad de archivos de configuraci\u00f3n almacenados localmente y de directorios de usuarios ubicados en m\u00e1quinas remotas de Linux y Windows. En el proceso, tambi\u00e9n aprendi\u00f3 a administrar la configuraci\u00f3n sudo, intercambiar claves SSH, conectarse con carpetas compartidas en m\u00e1quinas Windows y mucho m\u00e1s. 5.SEGURIDAD INFORM\u00c1TICA OPENVPN opci\u00f3n segura para conexiones entre redes. REGLAS FIREWALL , mejores pr\u00e1cticas para crear reglas de firewall iptables. HAPROXY NGINX APACHE El modo promiscuo es aquel en el que una computadora conectada a una red compartida, tanto la basada en cable de cobre como la basada en tecnolog\u00eda inal\u00e1mbrica, captura todo el tr\u00e1fico que circula por ella. Este modo est\u00e1 muy relacionado con los sniffers que se basan en este modo para realizar su tarea. netstat -i ifconfig eth0 promisc or ip link set eth0 promisc on Softwares para la red de Sistema de prevenci\u00f3n / detecci\u00f3n de intrusiones: SURICATA SNORT Herramientas de filtrado de la red: Wireshark TCPDUMP Rsyslog es una aplicaci\u00f3n que se encarga de implementar un protocolo de syslog b\u00e1sico de captura, procesamiento y registro de los mensajes. La implementaci\u00f3n de Rsyslog tiene como objetivo principal mejorar la gesti\u00f3n y el control de toda la informaci\u00f3n de la red. Permite la detecci\u00f3n de ca\u00eddas de red, la anticipaci\u00f3n a posibles problemas futuros y prevenir fugas de informaci\u00f3n, es decir comportamientos inadecuados que causen errores en la red. Adem\u00e1s, Wispcontrol permite la configuraci\u00f3n de alertas de log personalizadas. As\u00ed, en caso de ca\u00edda o fallo del sistema, usted podr\u00e1 saber qu\u00e9 est\u00e1 ocurriendo en su red inmediatamente, pudiendo obtener informaci\u00f3n sobre las amenazas potenciales. dm-crypt cifrado de discos duros. OPENVAS herramienta para escaneo de vulnerabilidades. Crear o inspeccionar pares de claves, encriptar o desencriptar, y firmar o verificar con OpenSSL Recuerde que un par de claves consiste en una clave p\u00fablica que puede poner a disposici\u00f3n de todos y una clave privada que debe mantener en secreto. Cuando alguien quiera enviarle datos y asegurarse de que nadie m\u00e1s pueda verlos, puede usar su clave p\u00fablica para encriptarlos. Los datos encriptados con su clave p\u00fablica solo se pueden desencriptar con su clave privada, de modo que solo usted pueda ver los datos originales. Por eso, es importante que nadie m\u00e1s conozca sus claves privadas. Si alguien m\u00e1s tiene una copia de su clave privada, podr\u00e1 desencriptar los datos que estaban destinados a usted, lo que no es muy bueno. Primero, generaremos una clave privada RSA de 2,048 bits y la analizaremos. Para generar la clave, ingrese el siguiente comando en la terminal: openssl genrsa -out private_key.pem 2048 A continuaci\u00f3n, generaremos la clave p\u00fablica a partir de la clave privada y tambi\u00e9n la inspeccionaremos. Ahora que tiene una clave privada, debe generar una clave p\u00fablica para completar el par. Puede proporcion\u00e1rsela a cualquier persona que quiera enviarle datos encriptados. Cuando se encriptan los datos con su clave p\u00fablica, nadie puede desencriptarlos, a menos que tenga su clave privada. Para crear una clave p\u00fablica basada en una clave privada, ingrese el comando a continuaci\u00f3n. Deber\u00eda ver el siguiente resultado: openssl rsa -in private_key.pem -outform PEM -pubout -out public_key.pem Crear\u00e1 un archivo de texto con informaci\u00f3n que desea encriptar para protegerla. A continuaci\u00f3n, la encriptar\u00e1 y la inspeccionar\u00e1. Para crear el archivo, ingrese el siguiente comando. Se crear\u00e1 un archivo de texto nuevo llamado \"secret.txt\" que solo incluye el siguiente texto: \"This is a secret message, for authorized parties only\" (Este es un mensaje secreto, solo para los usuarios autorizados). Puede cambiar este mensaje por el que desee. echo 'This is a secret message, for authorized parties only' > secret.txt A continuaci\u00f3n, ingrese el siguiente comando para encriptar el archivo con su clave p\u00fablica: openssl rsautl -encrypt -pubin -inkey public_key.pem -in secret.txt -out secret.enc Para desencriptar, se necesita que se tenga la llave privada, en este caso nosotros: openssl rsautl -decrypt -inkey private_key.pem -in secret.enc Ahora, crear\u00e1 un resumen de hash del mensaje y, luego, una firma digital de este resumen. Una vez que haya terminado, verificar\u00e1 la firma del resumen. De esta manera, podr\u00e1 asegurarse de que el mensaje no se haya modificado ni falsificado. Si se modific\u00f3 el mensaje, el hash ser\u00e1 diferente del firmado, y la verificaci\u00f3n no se completar\u00e1 correctamente. Para crear un resumen de hash del mensaje, ingrese el siguiente comando: openssl dgst -sha256 -sign private_key.pem -out secret.txt.sha256 secret.txt Se crear\u00e1 un archivo llamado \"secret.txt.sha256\" con su clave privada, que incluye el resumen de hash de su archivo de texto secreto. Con este archivo, cualquiera puede usar su clave p\u00fablica y el resumen de hash para verificar que el archivo no se haya modificado desde que lo cre\u00f3 y encript\u00f3. Para realizar esta verificaci\u00f3n, ingrese el siguiente comando: openssl dgst -sha256 -verify public_key.pem -signature secret.txt.sha256 secret.txt Se mostrar\u00e1 el siguiente resultado, que indica que la verificaci\u00f3n se realiz\u00f3 correctamente y que ning\u00fan tercero malicioso modific\u00f3 el archivo. C\u00f3mo usar hashes En este lab, practicar\u00e1 c\u00f3mo usar y verificar hashes con las herramientas \"md5sum\" y \"shasum\". md5sum es un programa de hashing que calcula y verifica hashes de MD5 de 128 bits. Al igual que con todos los algoritmos de hashing, en teor\u00eda, hay un n\u00famero ilimitado de archivos que tendr\u00e1 un hash de MD5 determinado. Puede usar \"md5sum\" para verificar la integridad de los archivos. Del mismo modo, shasum es un programa de encriptaci\u00f3n que calcula y verifica hashes de SHA. Tambi\u00e9n se usa para verificar la integridad de los archivos. Este comando crea un archivo de texto llamado \"file.txt\" que tiene una sola l\u00ednea de texto b\u00e1sico: echo 'This is some text in a file, just so we have some data' > file.txt Ahora, generar\u00e1 la suma MD5 del archivo y la almacenar\u00e1. Para generar la suma de su archivo nuevo, ingrese el siguiente comando \"md5sum\": md5sum file.txt > file.txt.md5 Sin embargo, lo m\u00e1s significativo es que tambi\u00e9n puede verificar que el hash sea correcto y que el archivo original no haya sido manipulado desde que se cre\u00f3 la suma. Para ello, escriba el siguiente comando y observe el resultado mostrado a continuaci\u00f3n, que indica que el hash es v\u00e1lido: md5sum -c file.txt.md5 SHA1 y SHA256 brindan m\u00e1s seguridad que MD5; y SHA256 es m\u00e1s seguro que SHA1. Esto significa que es m\u00e1s f\u00e1cil que un tercero malicioso ataque un sistema con MD5 que con SHA1. Adem\u00e1s, debido a que SHA256 es el hash m\u00e1s seguro de los tres, es el que m\u00e1s se usa actualmente. shasum file.txt > file.txt.sha1 A continuaci\u00f3n, use el siguiente comando para verificar el hash. (Al igual que antes, este proceso fallar\u00e1 si se modific\u00f3 el archivo original). shasum -c file.txt.sha1 Puede usar la misma herramienta para crear una suma SHA256. El marcador \"-a\" especifica el algoritmo que se desea usar y, si no especifica ning\u00fan valor, se seleccionar\u00e1 SHA1 como valor predeterminado. Para generar la suma SHA256, use el siguiente comando: shasum -a 256 file.txt > file.txt.sha256 La seguridad adicional de SHA256 se debe a que crea un hash m\u00e1s largo y dif\u00edcil de adivinar. Se puede observar que el contenido de este archivo es mucho m\u00e1s largo que el del archivo de SHA1. Por \u00faltimo, para verificar la suma SHA256, puede usar el mismo comando que antes: shasum -c file.txt.sha256 Pr\u00e1ctica TCPDUMP En este lab, presentaremos \"tcpdump\" y algunas de sus funciones. \"tcpdump\" es la herramienta de an\u00e1lisis de red m\u00e1s importante para los profesionales de redes y seguridad de la informaci\u00f3n. Como especialista en asistencia de TI, es fundamental que conozca bien esta aplicaci\u00f3n si desea entender TCP/IP. \"tcpdump\" lo ayudar\u00e1 a mostrar el tr\u00e1fico de red de una manera m\u00e1s f\u00e1cil para analizar y solucionar problemas. Para comenzar, ingresaremos \"tcpdump\" y lo ejecutaremos sin ninguna opci\u00f3n. Tenga en cuenta que, como tcpdump requiere privilegios de administrador para capturar el tr\u00e1fico, todos los comandos deben comenzar con sudo. Como m\u00ednimo, debe especificar una interfaz en la que buscar con la marca -i. Recomendamos usar ip link para verificar el nombre de la interfaz de red principal. En este caso, usaremos la interfaz eth0 para todos los ejemplos, aunque no es necesariamente la que usar\u00eda en su propia m\u00e1quina. Si desea usar tcpdump para comenzar a buscar los paquetes en la interfaz, ingrese el siguiente comando. sudo tcpdump -i eth0 Paramos con CONTROL + C Para habilitar un an\u00e1lisis m\u00e1s completo, use la marca -v a fin de activar el resultado detallado. De manera predeterminada, \"tcpdump\" tambi\u00e9n intentar\u00e1 realizar b\u00fasquedas de DNS inversas a fin de resolver las direcciones IP en nombres de host, adem\u00e1s de reemplazar los n\u00fameros de puerto con nombres de servicios com\u00fanmente asociados. Puede usar la marca -n para inhabilitar este comportamiento. Recomendamos usar esta marca para evitar generar tr\u00e1fico adicional a partir de las b\u00fasquedas de DNS y a fin de acelerar el an\u00e1lisis. Ingrese el siguiente comando para probarlo: sudo tcpdump -i eth0 -vn Sin la marca de resultado detallado, \"tcpdump\" solo proporciona lo siguiente: el protocolo de Layer-3, las direcciones y los puertos de origen y destino detalles de TCP, como marcas, secuencia y n\u00fameros de ack, el tama\u00f1o de la ventana y las opciones Si activa la marca de resultado detallado, tambi\u00e9n obtendr\u00e1 toda la informaci\u00f3n sobre el encabezado de IP, como el tiempo de vida, el n\u00famero de ID de IP, las opciones de IP y las marcas de IP. Filtrado : A continuaci\u00f3n, analizaremos brevemente el lenguaje de filtrado de tcpdump, junto con el an\u00e1lisis de protocolo. tcpdump admite un lenguaje potente para el filtrado de paquetes que le permite capturar solo el tr\u00e1fico que le interesa o que desea analizar. Las reglas de filtrado van al final del comando, despu\u00e9s de especificar el resto de las marcas. Solo usaremos el filtrado para capturar tr\u00e1fico de DNS a un servidor DNS espec\u00edfico. A continuaci\u00f3n, generaremos tr\u00e1fico de DNS a fin de demostrar la capacidad de \"tcpdump\" de interpretar consultas y respuestas de DNS. sudo tcpdump -i eth0 -vn host 8.8.8.8 and port 53 Analicemos c\u00f3mo est\u00e1 formado este filtro y qu\u00e9 hace exactamente. Host 8.8.8.8 especifica que solo queremos paquetes cuya direcci\u00f3n IP de origen o destino coincida con lo que indicamos (en este caso, 8.8.8.8). Si solo queremos tr\u00e1fico en una direcci\u00f3n, tambi\u00e9n podr\u00edamos agregar un calificador de direcci\u00f3n, como dst o src (para las direcciones IP de destino y origen respectivamente). Sin embargo, si no incluye el calificador, se captar\u00e1 el tr\u00e1fico en cualquier direcci\u00f3n. La parte port 53 indica que solo queremos ver paquetes cuyo puerto de origen o destino coincida con lo que especificamos (en este caso, DNS). Estos dos filtros se unen mediante el operador l\u00f3gico \"and\", lo que significa que ambas partes deben ser verdaderas para que nuestro filtro capture un paquete. En otra terminal dig @8.8.8.8 A example.com . Este comando usa la herramienta dig para consultar un servidor DNS espec\u00edfico (en este caso, 8.8.8.8) y le pide el A record del dominio especificado (en este caso, \"example.com\"). El primero es la consulta de DNS, que es nuestra pregunta (de la segunda terminal) que va al servidor. Tenga en cuenta que, en este caso, el tr\u00e1fico es UDP. El an\u00e1lisis que tcpdump hace de la consulta de DNS comienza inmediatamente despu\u00e9s del campo checksum de UDP. Empieza con el n\u00famero de ID de DNS, seguido por algunas opciones de UDP y, por \u00faltimo, el tipo de consulta (en este caso, A?, que indica que pedimos un A record). A continuaci\u00f3n, sigue el nombre de dominio que nos interesa (example.com). El segundo paquete es la respuesta del servidor, que incluye el mismo ID de DNS de la consulta original, seguido de la consulta original. A continuaci\u00f3n, se encuentra la respuesta a la consulta, que contiene la direcci\u00f3n IP asociada con el nombre de dominio. C\u00f3mo guardar paquetes capturados: sudo tcpdump -i eth0 port 80 -w http.pcap Especifique el puerto \"80\" para comenzar una captura en nuestra interfaz \"eth0\" que solo filtre el tr\u00e1fico de HTTP. La marca -w indica que queremos guardar los paquetes capturados en un archivo llamado http.pcap. Una vez que se est\u00e9 ejecutando, vuelva a la segunda terminal, en la que generar\u00e1 algo de tr\u00e1fico http que se capturar\u00e1 en la terminal original. Todav\u00eda no detenga la captura que comenz\u00f3 con el comando anterior (si ya lo hizo, puede reiniciar ahora): curl example.com Este comando obtiene el HTML de example.com y lo muestra en la pantalla. Deber\u00eda verse de la siguiente manera (tenga en cuenta que aqu\u00ed solo se muestra la primera parte del resultado). Tambi\u00e9n se habr\u00e1 creado un archivo binario, llamado http.pcap, que contiene los paquetes que acabamos de capturar. No intente mostrar el contenido de este archivo en la pantalla. Como es un archivo binario, se mostrar\u00e1 como texto confuso que no podr\u00e1 leer. Para leer bien el fichero binario usamos el comando: tcpdump -r http.pcap -nv PROYECTO DE SEGURIDAD DE EMPRESA Autenticaci\u00f3n La autenticaci\u00f3n ser\u00e1 manejada centralmente por un servidor LDAP e incorporar\u00e1 generadores de contrase\u00f1a de un solo uso como un segundo factor para la autenticaci\u00f3n. Sitio web externo El sitio web orientado al cliente se servir\u00e1 a trav\u00e9s de HTTPS, ya que servir\u00e1 un sitio de comercio electr\u00f3nico que permitir\u00e1 a los visitantes navegar y comprar productos, as\u00ed como crear e iniciar sesi\u00f3n en cuentas. Este sitio web ser\u00eda de acceso p\u00fablico. Sitio interno El sitio web interno de los empleados tambi\u00e9n se servir\u00e1 a trav\u00e9s de HTTPS, ya que requerir\u00e1 autenticaci\u00f3n para que los empleados accedan. Tambi\u00e9n solo ser\u00e1 accesible desde la red interna de la empresa y solo con una cuenta autenticada. Acceso remoto Dado que los ingenieros requieren acceso remoto a sitios web internos, as\u00ed como acceso de l\u00ednea de comando remoto a estaciones de trabajo, se necesitar\u00e1 una soluci\u00f3n VPN de nivel de red, como OpenVPN. Para facilitar el acceso al sitio web interno, se recomienda un proxy inverso, adem\u00e1s de VPN. Ambos confiar\u00edan en el servidor LDAP que se mencion\u00f3 anteriormente para la autenticaci\u00f3n y autorizaci\u00f3n. Firewall Se requerir\u00eda un dispositivo de Firewall basado en la red. Incluir\u00eda reglas para permitir el tr\u00e1fico para varios servicios, comenzando con una regla de denegaci\u00f3n impl\u00edcita y luego abriendo puertos de forma selectiva. Tambi\u00e9n se necesitar\u00e1n reglas para permitir el acceso p\u00fablico al sitio web externo, y para permitir el tr\u00e1fico al servidor proxy inverso y al servidor VPN. Inal\u00e1mbrico Para la seguridad inal\u00e1mbrica, se debe usar 802.1X con EAP-TLS. Esto requerir\u00eda el uso de certificados de cliente, que tambi\u00e9n se pueden usar para autenticar otros servicios, como VPN, proxy inverso y autenticaci\u00f3n interna de sitios web. 802.1X es m\u00e1s seguro y se administra m\u00e1s f\u00e1cilmente a medida que la empresa crece, lo que la convierte en una mejor opci\u00f3n que WPA2. VLANs Se recomienda incorporar VLAN en la estructura de la red como una forma de segmentaci\u00f3n de la red; esto facilitar\u00e1 el control del acceso a diversos servicios. Las VLAN se pueden crear para roles o funciones generales para dispositivos y servicios. Se puede utilizar una VLAN de ingenier\u00eda para colocar todas las estaciones de trabajo de ingenier\u00eda y los servicios de ingenier\u00eda. Se puede usar una VLAN de infraestructura para todos los dispositivos de infraestructura, como puntos de acceso inal\u00e1mbricos, dispositivos de red y servidores cr\u00edticos como la autenticaci\u00f3n. Una VLAN de ventas puede usarse para m\u00e1quinas que no sean de ingenier\u00eda y una VLAN invitada ser\u00eda \u00fatil para otros dispositivos que no se ajusten a las otras asignaciones de VLAN. Seguridad para laptop A medida que la compa\u00f1\u00eda maneja la informaci\u00f3n de pago y los datos del usuario, la privacidad es una gran preocupaci\u00f3n. Las laptops deben tener cifrado de disco completo (FDE) como un requisito, para protegerse contra el acceso no autorizado a los datos en caso de p\u00e9rdida o robo de un dispositivo. El software antivirus tambi\u00e9n se recomienda encarecidamente para evitar infecciones de malware com\u00fan. Para protegerse contra ataques poco comunes y amenazas desconocidas, se recomienda el software de lista blanca binario, adem\u00e1s del software antivirus. Pol\u00edtica de aplicaci\u00f3n Para mejorar a\u00fan m\u00e1s la seguridad de las m\u00e1quinas cliente, se debe implementar una pol\u00edtica de aplicaci\u00f3n para restringir la instalaci\u00f3n de software de terceros solo a las aplicaciones relacionadas con las funciones de trabajo. Espec\u00edficamente, las categor\u00edas de aplicaci\u00f3n riesgosas y legalmente cuestionables deben ser expl\u00edcitamente prohibidas. Esto incluir\u00eda cosas como software pirateado, generadores de claves de licencia y software descifrado. Adem\u00e1s de las pol\u00edticas que restringen algunas formas de software, tambi\u00e9n se debe incluir una pol\u00edtica que requiera la instalaci\u00f3n oportuna de los parches de software. \"Oportunamente\" en este caso se definir\u00e1 como 30 d\u00edas a partir de la amplia disponibilidad del parche. Pol\u00edtica de privacidad de datos del usuario A medida que la empresa toma la privacidad del usuario muy en serio, algunas pol\u00edticas s\u00f3lidas sobre el acceso a los datos del usuario son un requisito cr\u00edtico. Los datos de usuario solo deben accederse para fines de trabajo espec\u00edficos, relacionados con una tarea o un proyecto en particular. Se deben realizar solicitudes de datos espec\u00edficos, en lugar de solicitudes exploratorias demasiado amplias. Las solicitudes deben ser revisadas y aprobadas antes de otorgar el acceso. Solo despu\u00e9s de la revisi\u00f3n y aprobaci\u00f3n se otorgar\u00e1 a una persona el acceso a los datos espec\u00edficos del usuario solicitados. Las solicitudes de acceso a los datos del usuario tambi\u00e9n deben tener una fecha de finalizaci\u00f3n. Adem\u00e1s de acceder a los datos del usuario, tambi\u00e9n es importante definir las pol\u00edticas relacionadas con el manejo y almacenamiento de los datos del usuario. Esto ayudar\u00e1 a evitar que los datos del usuario se pierdan y caigan en las manos equivocadas. No se deben permitir los datos de usuario en dispositivos de almacenamiento port\u00e1tiles, como llaves USB o discos duros externos. Si es necesaria una excepci\u00f3n, se debe utilizar un disco duro port\u00e1til cifrado para transportar los datos del usuario. Los datos de usuario en reposo siempre deben estar contenidos en medios encriptados para protegerlos del acceso no autorizado. Pol\u00edtica de seguridad Para garantizar que se usen contrase\u00f1as seguras, se debe aplicar la siguiente pol\u00edtica de contrase\u00f1a: La contrase\u00f1a debe tener una longitud m\u00ednima de 8 caracteres La contrase\u00f1a debe incluir un m\u00ednimo de un car\u00e1cter especial o un signo de puntuaci\u00f3n La contrase\u00f1a debe cambiarse una vez cada 12 meses Adem\u00e1s de estos requisitos de contrase\u00f1a, todos los empleados deben completar una capacitaci\u00f3n de seguridad obligatoria una vez al a\u00f1o. Esto deber\u00eda cubrir escenarios comunes relacionados con la seguridad, como evitar ser v\u00edctimas de ataques de phishing, buenas pr\u00e1cticas para mantener a salvo tu laptop y nuevas amenazas que han surgido desde la \u00faltima vez que se tom\u00f3 el curso. Sistemas de detecci\u00f3n o prevenci\u00f3n de intrusos Se recomienda un sistema de detecci\u00f3n de intrusos en la red para observar la actividad de la red en busca de signos de un ataque o infecci\u00f3n de malware. Esto permitir\u00eda buenas capacidades de monitoreo sin incomodar a los usuarios de la red. Se recomienda un sistema de prevenci\u00f3n de intrusiones en la red (NIPS) para la red donde se encuentran los servidores que contienen datos de usuario; este contiene datos mucho m\u00e1s valiosos, que es m\u00e1s probable que sean atacados en un ataque. Adem\u00e1s de la Prevenci\u00f3n de intrusiones en la red, tambi\u00e9n se recomienda que se instale en estos servidores el software de detecci\u00f3n de intrusiones basado en host (HIDS) para mejorar la supervisi\u00f3n de estos importantes sistemas. REGLAS EJEMPLO FIREWALL Mostrando el estado de nuestro firewall: iptables -L -n -v -L : Muestra las reglas. -v : Muestra informaci\u00f3n detallada. -n : Muestra la direcci\u00f3n ip y puerto en formato num\u00e9rico. No usa DNS para resolver nombres. Esto acelera la lista. --line-numbers, numerar las lineas. Mostrar las reglas de cadena de entrada y salida: iptables -L INPUT -n -v iptables -L OUTPUT -n -v --line-numbers Parar / Iniciar / Reiniciar el firewall: service iptables stop/start/restart Tambi\u00e9n se puede usar propio comando iptables para detenerlo y borrar todas las reglas: iptables -F iptables -X iptables -t nat -F iptables -t nat -X iptables -t mangle -F iptables -t mangle -X iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT -F : Borra todas las reglas. -X : Borra cadenas. -t table_name : Selecciona una tabla y elimina reglas. -P : Establece la pol\u00edtica por defecto (como DROP, REJECT o ACCEPT). ACCEPT/DROP/REJECT: las decisiones que puede tomar un cada regla de un filtro de paquetes pueden ser, dejar pasar el paquete (ACCEPT), responderle al emisor educadamente que ese paquete no puede pasar (REJECT) o bien simplemente descartarlo como si no hubiera llegado (DROP o DENY). Observamos que la diferencia entre REJECT y DROP consiste en que mediante REJECT se le contesta que el servicio no est\u00e1 disponible (icmp destination port unrechable) evitando as\u00ed demoras en la conexi\u00f3n y mediante DROP no se le contesta nada por lo cual el sistema remoto no corta la conexi\u00f3n hasta que ha transcurrido el tiempo de espera de la contestaci\u00f3n con la consiguiente ralentizaci\u00f3n. Para la red local es aconsejable usar REJECT aunque cada administrador tiene que estudiar su situaci\u00f3n. Borrar reglas del firewall: iptables -L INPUT -n --line-numbers iptables -L OUTPUT -n --line-numbers iptables -L OUTPUT -n --line-numbers | less iptables -L OUTPUT -n --line-numbers | grep 202.54.1.1 Obtendrendremos la lista de IPs. Miramos el n\u00famero de la izquierda y lo usamos para borrarla. Por ejemplo para borrar la l\u00ednea 4: iptables -D INPUT 4 O para encontrar una ip de origen y borrarla de la regla iptables -D INPUT -s 202.54.1.1 -j DROP -D : Elimina una o m\u00e1s reglas de la cadena seleccionada. Insertar reglas: iptables -L INPUT -n --line-numbers iptables -I INPUT 2 -s 202.54.1.2 -j DROP Guardar reglas: service iptables save Restaurar reglas: iptables-restore < /root/my.active.firewall.rules service iptables restart Para borrar todo el tr\u00e1fico: iptables -P INPUT DROP iptables -P OUTPUT DROP iptables -P FORWARD DROP iptables -L -v -n Para borrar todos los paquetes entrantes / enviados pero permitir el tr\u00e1fico saliente: iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT ACCEPT iptables -A INPUT -m state --state NEW,ESTABLISHED -j ACCEPT iptables -L -v -n Borrar direcciones de red privadas en la interfaz p\u00fablica: iptables -A INPUT -i eth1 -s 192.168.0.0/24 -j DROP iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP IP Spoofing es nada m\u00e1s que para detener los siguientes rangos de direcciones IPv4 para redes privadas en sus interfaces p\u00fablicas. Los paquetes con direcciones de origen no enrutables deben rechazarse. Bloqueando una direci\u00f3n IP (BLOCK IP): iptables -A INPUT -s 1.2.3.4 -j DROP Bloquear peticiones entrantes de un puerto (BLOCK PORT): Para bloquear todas las solicitudes de servicio en el puerto 80: iptables -A INPUT -p tcp --dport 80 -j DROP iptables -A INPUT -i eth1 -p tcp --dport 80 -j DROP Para bloquear el puerto 80 para una ip: iptables -A INPUT -p tcp -s 1.2.3.4 --dport 80 -j DROP iptables -A INPUT -i eth1 -p tcp -s 192.168.1.0/24 --dport 80 -j DROP Bloquear el dominio facebook.com: Primero, encontrar la direcci\u00f3n ip de facebook.com host -t a www.facebook.com Salida: www.facebook.com has address 69.171.228.40 Buscar el CIDR para 69.171.228.40: whois 69.171.228.40 | grep CIDR Salida: CIDR: 69.171.224.0/19 Para prevenir el acceso externo a facebook.com: iptables -A OUTPUT -p tcp -d 69.171.224.0/19 -j DROP Podemos usar tambi\u00e9n nombres de dominio: iptables -A OUTPUT -p tcp -d www.facebook.com -j DROP iptables -A OUTPUT -p tcp -d facebook.com -j DROP Bloquear ips de salida: Para bloquear el tr\u00e1fico saliente a un host o dominio en concreto como por ejemplo cyberciti.biz: host -t a cyberciti.biz Salida: cyberciti.biz has address 75.126.153.206 Una vez conocida la direcci\u00f3n ip, bloqueamos todo el tr\u00e1fico saliente para dicha ip as\u00ed: iptables -A OUTPUT -d 75.126.153.206 -j DROP Se puede usar una subred como la siguiente: iptables -A OUTPUT -d 192.168.1.0/24 -j DROP iptables -A OUTPUT -o eth1 -d 192.168.1.0/24 -j DROP En el ejemplo con iptables, se ha pasado autilizar el conjunto de reglas FORWARD por la diferencia de significado del conjunto de reglas INPUT en la implementaci\u00f3n de netfilter. Esto tiene implicaciones; significa que ninguna de las reglas protege el 'host' mismo del cortafuegos. Para imitar con precisi\u00f3n el ejemplo con ipchains, se replicar\u00eda cada una de las reglas de la cadena INPUT. En aras de la claridad, en su lugar se ha decidido eliminar todos los datagramas entrantes provenientes desde el lado de fuera de la interfaz: #!/bin/bash ########################################################################## # VERSI\ufffdN PARA IPTABLES # Este configuraci\ufffdn est\ufffd pensada como ejemplo de configuraci\ufffdn de # un cortafuegos sobre un 'host' \ufffdnico que no hospede \ufffdl mismo ning\ufffdn # servicio ########################################################################## # SECCI\ufffdN CONFIGURABLE POR EL USUARIO # El nombre y la localizaci\ufffdn de la utilidad iptables. IPTABLES=iptables # Ruta del ejecutable de iptables. PATH=\"/sbin\" # El espacio de direcciones de nuestra red interna y el dispositivo # de red que la soporta. OURNET=\"172.29.16.0/24\" OURBCAST=\"172.29.16.255\" OURDEV=\"eth0\" # Las direcciones de fuera y el dispositivo de red que la soporta. ANYADDR=\"0/0\" ANYDEV=\"eth1\" # Los servicios de TCP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los puertos # nota: separados por espacios TCPIN=\"smtp,www\" TCPOUT=\"smtp,www,ftp,ftp-data,irc\" # Los servicios de UDP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los puertos # nota: separados por espacios UDPIN=\"domain\" UDPOUT=\"domain\" # Los servicios de ICMP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los tipos # referencia para los n\ufffdmeros de los tipos: /usr/include/netinet/ip_icmp.h # nota: separados por espacios ICMPIN=\"0,3,11\" ICMPOUT=\"8,3,11\" # Registro; descomente la siguiente l\ufffdnea para habilitar el registro # de los datagramas rechazados por el cortafuegos # LOGGING=1 # FIN DE LA SECCI\ufffdN CONFIGURABLE POR EL USUARIO ########################################################################### # Borra las reglas de la cadena de entrada $IPTABLES -F FORWARD # # Por defecto, queremos denegar el acceso a los intentos de entrada $IPTABLES -P FORWARD deny # Rechaza todos los datagramas destinados a este host y recibidos # desde fuera. $IPTABLES -A INPUT -i $ANYDEV -j DROP # SUPLANTACI\ufffdN DE IDENTIDAD # No se deber\ufffda aceptar ning\ufffdn datagrama proveniente de fuera con una # direccci\ufffdn de origen coincidente con una de las nuestras, por # eso las rechazamos. $IPTABLES -A FORWARD -s $OURNET -i $ANYDEV -j DROP # 'SMURF' # No se permiten difusiones dirigidas de ICMP a nuestra red para evitar # los ataques del estilo denominado 'Smurf'. $IPTABLES -A FORWARD -m multiport -p icmp -i $ANYDEV -d $OURNET -j DENY # Deber\ufffdamos aceptar fragmentos, esto se debe explicitar en iptables. $IPTABLES -A FORWARD -f -j ACCEPT # TCP # Aceptaremos todos los datagramas de TCP que pertenezcan a una # conexi\ufffdn ya existente (i.e. cuyo bit de ACK valga 1) # en el caso de los puertos de TCP que estamos permitiendo. # Esto deber\ufffda capturar m\ufffds del 95% de todos los paquetes v\ufffdlidos de TCP. $IPTABLES -A FORWARD -m multiport -p tcp -d $OURNET --dports $TCPIN / ! --tcp-flags SYN,ACK ACK -j ACCEPT $IPTABLES -A FORWARD -m multiport -p tcp -s $OURNET --sports $TCPIN / ! --tcp-flags SYN,ACK ACK -j ACCEPT # TCP - CONEXIONES ENTRANTES # Aceptaremos \ufffdnicamente las solicitudes de conexi\ufffdn desde # fuera en los puertos de TCP permitidos. $IPTABLES -A FORWARD -m multiport -p tcp -i $ANYDEV -d $OURNET $TCPIN / --syn -j ACCEPT ## TCP - CONEXIONES SALIENTES # Aceptaremos todas las conexiones salientes de TCP hacia los puertos # de TCP permitidos $IPTABLES -A FORWARD -m multiport -p tcp -i $OURDEV -d $ANYADDR / --dports $TCPOUT --syn -j ACCEPT # UDP - ENTRADA ## Aceptaremos la entrada y vuelta de los datagramas de UDP por puertos #permitidos. $IPTABLES -A FORWARD -m multiport -p udp -i $ANYDEV -d $OURNET / --dports $UDPIN -j ACCEPT $IPTABLES -A FORWARD -m multiport -p udp -i $ANYDEV -s $OURNET / --sports $UDPIN -j ACCEPT # UDP - SALIDA # Se aceptar\ufffdn la salida de los datagramas de UDP hacia los puertos permitidos y su vuelta. $IPTABLES -A FORWARD -m multiport -p udp -i $OURDEV -d $ANYADDR / --dports $UDPOUT -j ACCEPT $IPTABLES -A FORWARD -m multiport -p udp -i $OURDEV -s $ANYADDR / --sports $UDPOUT -j ACCEPT # ICMP - ENTRADA # Aceptaremos la entrada de los datagramas de ICMP de los tipos permitidos $IPTABLES -A FORWARD -m multiport -p icmp -i $ANYDEV -d $OURNET / --dports $ICMPIN -j ACCEPT # ICMP - SALIDA # Aceptaremos la salida de los datagramas de ICMP de los tipos permitidos. $IPTABLES -A FORWARD -m multiport -p icmp -i $OURDEV -d $ANYADDR / --dports $ICMPOUT -j ACCEPT # CASO POR DEFECTO y REGISTRO # Todos los restantes datagramas caen dentro de la regla por defecto # y son eliminados. Ser\ufffdn registrados si m\ufffds arriba se ha configurado # la variable LOGGING. # if [ \"$LOGGING\" ] then # Registra los paquetes de TCP descartados $IPTABLES -A FORWARD -m tcp -p tcp -j LOG # Registra los paquetes de UDP descartados $IPTABLES -A FORWARD -m udp -p udp -j LOG # Registra los paquetes de ICMP descartados $IPTABLES -A FORWARD -m udp -p icmp -j LOG fi # # fin. GESTION DE ACCESO REMOTO CREAR VPN Esquina abajo derecha - VPN: Agregar nueva conexion VPN, proveedor windows, nombre de conexion, nombre del servidor(vpn.ejemplo.com), tipo de vpn(automatico) y tipo de inicio de sesion(nombre/pass). Tambien podemos modificarla y en opciones avanzadas, boton derecho propiedades. Podemos crear otra de manera mas clasica dando a centro de redes y recursos compartidos, nueva conexion vpn y ponemos nombre e ip. En seguridad de propiedades avanzadas en ikev2 si est\u00e1 activa la opcion de MODALIDAD sirve para reconectar automaticamente si se pierde la conexion de esta vpn. ESCRITORIO REMOTO En simbolo de sistema poniendo mstsc o en la barra de inicio poniendo ESCRITORIO REMOTO nos sale la ventana para conectarnos. Tenemos que tener activada la opcion de escritorio remoto de nuestro pc. Con mstsc /? vemos todas las opciones que podemos hacer. Como ejemplo mstsc /v: ip . En mas opciones, podemos poner nombre, usuario, colores, tama\u00f1o, recursos, rendimiento...Tmabien si tenemos un fichero RDP de la maquina virtual u otro equipo a conectarse para conectarse directamente. mstsc /v:server :3389 para ver como estan los puertos o netstat -A . En windows defender tambien salen cositas para que no nos de problemas de conexion remotas. En reglas de salida, podemos poner nueva regla - puerto - tipo de conexion y puerto que queremos usar - permitir conexion - dominio/privado/publico - nombre de la regla. La herramienta RDC de windows te permite la conexion remota. Hay que descargar esta herramienta. Hay tipo de conexiones para empresas en conexion remoto: BranchCache(reduce el ancho de banda) y DirectAcess, podemos conectarnos desde casa como si fuesemos el admin de server. OPCIONES ENERGIA Panel de control - energia. Por comando como administrador ponemos en c powercfg /? En directivas de grupo en la barra de busqueda: conf equipo - plantillas admin - sistema - admin energia y ahi podemos activar o desactivas directivas. CENTRO DE CONEXION En el panel de control hay centro de sincronizacion para poder poner carpetas compartidas antes de usar sin conexion, por si falla algo poder acceder. Con el comando gpedit tenemos en usuario - plantillas admin - red - archivos de conexion, directivas de grupo para el tema de archivos de conexion. Como por ejemplo habilitar la sincronizaacion dd archivos en redes estimadas. Las carpetas de trabajo se hace el admin habilitando en las directivas de grupo local para que el cliente pueda conectarse a estas carpetas. ACTUALIZACION Y RECUPERACION En SISTEMA , propiedades del sistema, selecionamos la unidad C configurar y activamos la proteccion del sistema para poder hacer cambios, restaurar sistema, etc. En Sistema, propiedades - proteccion del sistema, podemos crear un punto de restauracion. Igualmente, windows crea automaticamente tambien puntos de restauraciones. Podemos eliminar los puntos de restauracion eliminando la proteccion del sistema o en la opcion mas abajo que pone eliminar. Tambien en la herramienta de liberacion de espacio en disco en la barra de inicio y seleccionamos la unidad y lo que queremos limpiar. Con la tecla SHIFT Y REINICIAR nos sale opciones diferentes de apagado y solucionar problemas para poder restaurar sistema, formatear, etc. RECOMENDACION DE DESACTIVAR LOS ANTIVIRUS AL HACER RESTAURACIONES. Cuando restablecemos equipo manteniendo los archivos nos saldr\u00e1 en la unidad C una carpeta de windows.old con lo viejo de la otra copia y en el escritorio un fichero con los programas desintalados. WINRE es un entorno de recuperacion de windows. Accedemos a este menu avanzado en Configuracion - actualizacion - recuperacion y reiniciar ahora en el avanzado. Tambien con SHIFT y reiniciar y solcionar problemas - opciones avanzadas. En opciones de recuperacion tambien podemos crear copias de seguridad o crear imagen del sistema para poder luego restaurar desde esa imagen desde inicio avanazado de windows. En copia de seguridad podemos agregar dispositivo externo por ejemplo, y seleccionarlo para meterlas ahi. En configuracion - seguridad - copias de seguridad la tenemos que tener activada y en m\u00e1s opciones, configurar cuando se hacen y personalizarla. Tambien esta recuperar archivos desde historial y vemos las diferents versiones de archivos en carpetas y poder restaurar el archivo. En panel de control - historial de archivos - excluir archivos, podemos poner aqui las carpetas que no queremos que se hagan en las copias de seguridad. En windows Update podemos configurar cuando instalar en opciones avanzadas. En cambiar las horas activas podemos configurar que horas no queremos que el sistema operativo no se reinicie para instalar nada. Podemos programar una hora para instalarlas. En el historial de actualizaciones podemos ver recomendaciones y desintalar actualizaciones. En opciones de entrega podemos ver el monitor de actividad para ver como va el consumo de las actualizaciones.","title":"Soporte TI"},{"location":"soporte/#curso-google-soporte-ti","text":"","title":"CURSO GOOGLE SOPORTE TI"},{"location":"soporte/#1aspectos-basicos-asistencia-tecnica","text":"HACER OVERCLOOKING para aumentar la frecuencia de reloj de las CPU para que vaya mas rapido en segun que tareas. HERRAMIENTA BOOTEABLE USB Conectarse por ssh a una maquina remota LINUX: Linux: descargamos las ssh keys, le ponemos permisos 600 y nos conectamos con ssh -i user@ip. Windows: descargamos las prk keys, abrimos PUTTY e indicamos puerto 22 y user e IP. En la lista Category, expanda SSH. Haga clic en Auth (sin expandir). En el cuadro Private key file for authentication, busque el archivo PPK que descarg\u00f3 y haga clic en \u00e9l. Haga clic en el bot\u00f3n Open. Conectarse por ssh a una maquina remota WINDOWS: Windows: Para abrir Remote Desktop Connection, haga clic en el bot\u00f3n Start. En el cuadro de b\u00fasqueda, escriba Remote Desktop Connection y, luego, en la lista de resultados, haga clic en Remote Desktop Connection. Ingrese la direcci\u00f3n IP externa de la instancia a la que desea conectarse en el campo Computer. Busque la direcci\u00f3n IP externa de su instancia en el panel \"Connection Details\" a la izquierda. Haga clic en Connect. Cambie el nombre de usuario a student y use la contrase\u00f1a que se menciona en el panel \"Connection Details\" a la izquierda. Haga clic en OK. Haga clic en Yes para aceptar el certificado. Linux: Abra Remmina. Escriba la direcci\u00f3n IP externa de la instancia a la que desea conectarse. Busque la direcci\u00f3n IP externa de su instancia en el panel \"Connection Details\" a la izquierda. Haga clic en Connect. Aseg\u00farese de que el protocolo de conexi\u00f3n est\u00e9 establecido en RDP, como se muestra en la siguiente imagen: Aparecer\u00e1 una ventana para que acepte el certificado. Haga clic en Ok para continuar. Deje el campo de dominio vac\u00edo. Cambie el nombre de usuario a student y use la contrase\u00f1a que se menciona en el panel \"Connection Details\" a la izquierda para el campo Password. Haga clic en Ok para continuar. Para verificar programas instalados: Windows: en programas y caracteristicas. Linux: comando dpkg -s firefox . Actualizar paquetes apt-get install -f .","title":"1.ASPECTOS BASICOS ASISTENCIA TECNICA"},{"location":"soporte/#2-bit-y-bites-redes-informaticas","text":"ARP es un protocolo de comunicaciones de la capa de enlace de datos,1\u200b responsable de encontrar la direcci\u00f3n de hardware (Ethernet MAC) que corresponde a una determinada direcci\u00f3n IP. Para ello se env\u00eda un paquete (ARP request) a la direcci\u00f3n de difusi\u00f3n de la red (broadcast, MAC = FF FF FF FF FF FF) que contiene la direcci\u00f3n IP por la que se pregunta, y se espera a que esa m\u00e1quina (u otra) responda (ARP reply) con la direcci\u00f3n Ethernet que le corresponde. Cada m\u00e1quina mantiene una cach\u00e9 con las direcciones traducidas para reducir el retardo y la carga. ARP permite a la direcci\u00f3n de Internet ser independiente de la direcci\u00f3n Ethernet, pero esto solo funciona si todas las m\u00e1quinas lo soportan. De manera sencilla de explicar, el objetivo del protocolo ARP es permitir a un dispositivo conectado a una red LAN obtener la direcci\u00f3n MAC de otro dispositivo conectado a la misma red LAN cuya direcci\u00f3n IP es conocida. ARP se utiliza en cuatro casos referentes a la comunicaci\u00f3n entre dos hosts: Cuando dos hosts est\u00e1n en la misma red y uno quiere enviar un paquete a otro. Cuando dos hosts est\u00e1n sobre redes diferentes y deben usar un gateway o router para alcanzar otro host. Cuando un router necesita enviar un paquete a un host a trav\u00e9s de otro router. Cuando un router necesita enviar un paquete a un host de la misma red. IP PRIVADAS: Clase A: 10.0.0.0 a 10.255.255.255/24 Clase B: 172.16.0.0 a 172.31.255.255/16 Clase C: 192.168.0.0 a 192.168.255.255/16 EJEMPLO PROTOCOLOS DE ENRUTAMIENTO PUERTOS: El puerto 0 no se usa para el tr\u00e1fico de red, pero a veces se usa en las comunicaciones que tienen lugar entre diferentes programas en la misma computadora. Los puertos 1-1023 se conocen como puertos del sistema o, en ocasiones, como \"puertos conocidos\". Estos puertos representan los puertos oficiales de los servicios de red m\u00e1s conocidos. En un video anterior, hablamos sobre c\u00f3mo HTTP normalmente se comunica a trav\u00e9s del puerto 80, mientras que FTP generalmente se comunica a trav\u00e9s del puerto 21. En la mayor\u00eda de los sistemas operativos, se necesita acceso a nivel de administrador para iniciar un programa que escucha en un puerto del sistema. Los puertos 1024-49151 se conocen como puertos registrados. Estos puertos se usan para muchos otros servicios de red que pueden no ser tan comunes como los que est\u00e1n en los puertos del sistema. Un buen ejemplo de un puerto registrado es 3306, que es el puerto que muchas bases de datos escuchan. Los puertos registrados a veces est\u00e1n registrados y reconocidos oficialmente por la IANA, pero no siempre. En la mayor\u00eda de los sistemas operativos, cualquier usuario de cualquier nivel de acceso puede iniciar un programa escuchando en un puerto registrado. Finalmente, tenemos los puertos 49152-65535. Estos son conocidos como puertos privados o ef\u00edmeros. Los puertos ef\u00edmeros no se pueden registrar con la IANA y generalmente se utilizan para establecer conexiones salientes. Debe recordar que todo el tr\u00e1fico TCP utiliza un puerto de destino y un puerto de origen. Cuando un cliente desea comunicarse con un servidor, se le asignar\u00e1 un puerto ef\u00edmero para que se use solo para esa conexi\u00f3n, mientras el servidor escucha en un sistema est\u00e1tico o un puerto registrado. DNS Para la gesti\u00f3n de un dominio existen una serie de par\u00e1metros que permiten configurar el dominio para diferentes tareas llamados registros DNS. Por ejemplo, con estos par\u00e1metros permiten que un mismo dominio pueda funcionar en varios servidores, crear subdominios y que cada uno apunte a diferentes IPs o distintos alojamientos. Entre los tipos de registro DNS m\u00e1s utilizados se encuentran los siguientes: A (Address), Registro de direcci\u00f3n. Devuelve una direcci\u00f3n IP. Este registro sirve para resolver nombres de alojamientos a un n\u00famero IPv4, teniendo en cuenta si la IP es din\u00e1mica o fija. Por ejemplo, para apuntar nuestro nombre de dominio a un servidor. AAAA (Address), Registro de direcci\u00f3n IPv6. Los registros AAAA son muy parecidos a los A, es decir, ambos devuelven una direcci\u00f3n IP. En el caso de los AAAA, las IPs que se almacenan son IPv6. Este tipo de registro de DNS, al igual que A, sirve para apuntar nuestro dominio a un determinado servidor. CAA (Certification Authority Authorization), Autorizaci\u00f3n de la Autoridad de Certificaci\u00f3n. Este par\u00e1metro de DNS es un mecanismo de seguridad que permite limitar las autoridades certificadoras v\u00e1lidas para un dominio. Es decir, indicar a qu\u00e9 autoridades certificadoras permitimos emitir certificados de seguridad (SSL) para el dominio. CNAME (Canonical Name), Registro de nombre can\u00f3nico. Este registro se suele utilizar para crear alias de un nombre. CNAME es una forma de hacer que el dominio apunte a otro dominio diferente o a un subdominio. Tambi\u00e9n puede usarse cuando distintos servicios est\u00e1n utilizando una misma IP, de forma que cada servicio tenga su propia entrada DNS. MX (Mail eXchange), Registro de intercambio del correo. Los registros MX apuntar al servidor de correo del dominio y es posible establecer tantos como sean necesarios. En relaci\u00f3n con estos registros ten en cuenta que, de forma autom\u00e1tica, se establecen prioridades. Es decir, el primer registro MX que introduzcas tendr\u00e1 prioridad sobre los siguientes. PTR (Pointer), Registro de puntero. O registro inverso, ya que funciona de manera opuesta a A, se encarga de traducir IPs a nombres de dominio. Generalmente PTR se utiliza en el archivo de configuraci\u00f3n de la zona DNS inversa. SRV (Service record), Localizador de servicios. Es un registro para servicios especiales que proporciona informaci\u00f3n relacionada con los servicios disponibles para un determinado dominio. Es habitual su uso con XMPP, LDAP o SIP. TXT (Text), Registro de texto. Registro para insertar el texto que desees. Suele utilizarse para verificar la autoridad del dominio o para evitar usos incorrectos de las direcciones de correo. Adem\u00e1s, TXT permite la creaci\u00f3n de registros especiales y Domain-Keys. Est\u00e1n los HOSTS FILES para indicar IP+dominio para especificar o nombras hosts de la red o direcciones ejemplo 192.168.1.5 pc-miguel Prueba de conectividad de puerto: NETCAT nc (-zv) google.com 80 TEST-NETCONNECTION Test-NetConnection google.com Comando PING para probar conexi\u00f3n a un host o direcci\u00f3n. TRACEROUTE o TRACERT para probar los saltos de nodos para llegar a una direcci\u00f3n: traceroute google.com o en windows tracert google.com Comando NSLOOKUP para ver los dns de un sitio. nslookup google.com . Sin nombre es modo interactivo. Se pueden poner server 8.8.8.8 y cambia de dns server o set type MX para que te diga esos registros.","title":"2. BIT Y BITES: REDES INFORM\u00c1TICAS."},{"location":"soporte/#3sistemas-operativos","text":"COMANDOS BASICOS LINUX COMANDOS CMD WINDOWS DOC Ayuda de comandos: Win: Get-Help comando o comando /? Lin: comando --help o man comando Listar directorios: Win: ls C:\\ o ls -Force C:\\ Lin: ls directorio o ls -la dir Cambiar directorios: cd Crear directorios: mkdir dir o mkdir dir \"el direcorio nuevo\" Historial de comandos: history o CONTROL+R texto Copiar archivos: cp file1 dir/file1 Copiar directorio: Win: cp dir1 /dir/dir1 -Recurse -Verbose Lin: cp -r dir1 /dir/dir1 Mover/renombrar archivos: mv file1 dir/filr1 Borrar: Win: rm file o rm -Force file o rm dir1 - Recurse Lin: rm file o rm -f file Ver contenido: cat file Head/tail: Win: cat file.txt -Head/Tail 10 Lin: head/tail -n5 file.txt Modificar archivos: Win: start notepad++ file2.txt Lin: vim file1.txt Donde se encuentra el comando: Win: Get-Alias ls Lin: whereis/which ls Buscar palabras en un texto/dir: Win: Select-String palabra dir/file.txt Se ha de activar indexing options de windows para buscar palabras entre contenido de ficheros. Win: ls dir/ -Recurse -Filter *.exe Lin: grep palabra dir/*.txt Redirecionamientos(>/>>/ ): Win: cat words.txt | Select-String st > st_words.txt Lin: cat words.txt | grep st > st_words.txt Ver usuarios y grupos: Win: ir a Computer Management > Users/groups Win: Get-LocalUser // Get-LocalGroup // Get-LocalGroupMember namegroup Linux: cat /etc/sudoers // cat /etc/passwd // cat /etc/group Cambiar contrase\u00f1a usuario: Win: net user miguel \"contrase\u00f1a\" // net user miguel * // net user miguel /logonpasswordchg:yes Lin: passwd miguel //passwd -e miguel A\u00f1adir usuario: Win: net user miguel */contrase\u00f1a /add /logonpasswordchg:yes Lin: useradd miguel Borrar usuario: Win: net user miguel /del // Remove-LocalUser miguel Lin: userdel miguel Ver permisos: Win : icacls dir/file Lin: ls -l dir/file Cambiar permisos: Win: icacls dir/file /grant 'Everyone:(OI)(CI)(R)' Lin: chmod u+x/650 dir/file En windows tenemos el permiso especial de Owner Propietary y en linux tenemos el SETUID(4), SETGID(2) Y STICKYBIT(1). Ejecutar paquetes software: Win: /path/file.exe Lin: dkpg -i/-r/-l file.deb // apt-get install file.rpm Comprimir archivos: Win : Compress-Archive -Path /dir/* /dir/file.zip Lin: tar -cvf file.tar file1 file2 file3 Dependencias de paquetes: Win: Register-PackageSource -Name chocolatey -ProviderName Chocolatey -Location http://chocolatey.org/api/v2 Win: Get-PackageSource Win: Find-Package sysinternals -IncluseDependencies Win: Install/Uninstall-Package sysinternals Lin: sudo apt install -f Sysinternals package es un tipo de herramienta/repositorio junto a Chocolatey para poder encontrar paquetes de dependencias de otros paquetes de software. Instalar paquetes: Win: Install-Package sysinternals // Get-Package paquete Lin: apt install paquete -y / apt remove/update/upgrade repos en /etc/apt/source.list Ver dispositivos: Win: device manager Lin: /dev Actualizaciones: Win: windows update Lin: uname -r // apt update / apt full-upgrade Ver procesos: Win : task manager / tasklist / Get-Process | Sort CPU -descending | Select -first 3 - Property ID,RAM,CPU Linux: ps / ps -ax / ps -ef / uptime / lsof / top Top para linux y process explorer para windows. Terminar procesos: Win: taskkill // taskkill /F /pid 586 Lin: kill 586 / kill -KILL/TSTP 856 Nos podemos conectar por SSH a otra maquina. Se ha de tener instalado el cliente ssh y en el otro el servidor ssh. Utilizamos herramientas en windows como putty.exe y pscp.exe y en linux via comandos: ssh -p puerto (-x) usuario:ip_host ssh-keygen -t rsa scp-copy-id ruta/fichero user@ip_host:ruta /etc/ssh.d/sshd_config Podemos conectarnos tambi\u00e9n a maquinas virtuales y poner un SO y utilizan recursos fisicos de nuestra maquina. Podemos ver registros de seguridad, servicios, autenticaci\u00f3n o del sistema en Event viewer de windows o en /var/log o /var/log/syslog en linux y buscar los posibles errores filtrando en su busqueda. Podemos clonar discos duros o sistemas operativos con herramienta como clonezilla o Ghost y en linux con el comando DD : dd if=/dev/sda1 of=/dev/disco_extraible/imagen.img bz=100M La ruta de un nuevo disco es conectarlo. Asignarla creando una particion y despues crearle un sistema de fichero para poder usar el espacio nuevo asignado. Formatero en Windows : en administrador de discos o en consola diskpart ->select disk1 -> clean -> create partition primary -> select partition 1 -> active -> format FS=NTFS label=my-disk quick Formatero en Linux: podemos usar la herramienta parted -l o fdisk . Para formatear un sistema de ficheros se usa mkfs -t ext4 /dev/sda2 una vez que hemos creado una partici\u00f3n con espacio. Luego se puede montar como por ejemplo el device nuevo o un usb con la opci\u00f3n mount /dev/sdb1 /opt/dir y para desmontar se usa umount dir/device . Para que se haga automaticamente en el sistema al iniciarse se pone en el fstab y blkid para ver el ui de la unidad. La swap es un memoria virtual de intercambio para acciones rapidas que se estan ejecutando. Para cambiarla en windows se va a panel de control -> seguridad ->sistema -> systema avanzados - > settings -> advanced y cambiarlo. En linux se asigna al principio al instalar el so o se puede crear una particion de tipo swap con `mkswap /dev/sdb1. Creaci\u00f3n de simbolyc link o hards donde en los hards se mantiene la chicha. Win: mklink (/H) /path/link_name file Lin: ln (-s) /path/file /path/link_name en linux se ve con ls -li los inodos y cuantos hard links tiene este archivo. Uso de disco: Win: en admin d discos o du Lin: du -h df -h Reparaci\u00f3n de sistemas de archivos: Win: fsutil repair query C: o chkdsk /F C: Lin: fsck /dev/sda1","title":"3.SISTEMAS OPERATIVOS"},{"location":"soporte/#practica-permisos","text":"Queremos quitar el permiso de Kara de WX y solo queremos que lea R. Le quitamos todos los permisos: ICACLS C:\\Users\\Qwiklab\\Documents\\important_document /remove \"Kara\" Le ponemos todos de nuevo solo con R: ICACLS C:\\Users\\Qwiklab\\Documents\\important_document /grant \"Kara:(r)\" Queremos que otro usuario tambien tenga permisos a esa carpeta y a kara se le sume RW. Le a\u00f1adimos a Phoebe permisos de lectura: ICACLS C:\\Users\\Qwiklab\\Secret\\ /grant \"Phoebe:(r)\" Le a\u00f1adimos a Kara el de escritura ya que la R ya la tiene: ICACLS C:\\Users\\Qwiklab\\Secret\\ /grant \"Kara:(w)\" Su objetivo en este ejemplo es cambiar los permisos de esta carpeta para que el grupo \"Everyone\" solo tenga permiso de lectura (no de escritura). Quitamos todos los permisos y luego a\u00f1adimos la R: ICACLS C:\\Users\\Qwiklab\\Music\\ /remove \"Everyone\" En este ejemplo, necesita modificar los permisos de ese archivo, de manera que el grupo llamado \"Authenticated Users\" tenga acceso de escritura. El grupo \"Authenticated Users\" contiene usuarios que se han autenticado en el dominio o en un dominio que es confiable por la computadora. A\u00f1adimos ese tipo de usuarios con el permiso: ICACLS C:\\Users\\Qwiklab\\Documents\\not_so_important_document /grant \"Authenticated Users:(w)\" En este ejemplo cambiar\u00e1 los permisos de otro archivo de la carpeta \"Documents\". El archivo llamado \"public_document\" tiene que estar disponible para lectura p\u00fablica, de manera que todas las personas del sistema puedan leerlo. La manera m\u00e1s sencilla de asegurarse de que todos los usuarios del sistema tengan permiso de lectura es agregar ese permiso al grupo \"Everyone\". ICACLS C:\\Users\\Qwiklab\\Documents\\public_document /grant \"Everyone:(r)\"","title":"PRACTICA PERMISOS"},{"location":"soporte/#crear-particiones","text":"WINDOWS : Haga clic en el bot\u00f3n Start y seleccione Control Panel para abrir el panel de control. All\u00ed, navegue a System and Security y, luego, a Administrative Tools. En la ventana \"Administrative Tools\", haga doble clic en Computer Management. Como lo que nos interesa es administrar discos, en el panel izquierdo, debajo de Storage, seleccione Disk Management. En el panel de control, ver\u00e1 un di\u00e1logo donde deber\u00e1 ingresar el tama\u00f1o para reducir el disco. Escriba \"20,480MB\" para dividir el disco en dos particiones de 30 GB y 20 GB respectivamente. Haga clic en Shrink. El disco se reducir\u00e1 y el espacio adicional de 20 GB se mostrar\u00e1 como sin asignar. En este espacio sin asignar, crear\u00e1 una nueva partici\u00f3n de 20 GB. Haga clic con el bot\u00f3n derecho del mouse en el espacio y seleccione New Simple Volume. En la siguiente secci\u00f3n del asistente, aseg\u00farese de que la letra de la unidad sea E y haga clic en Next. A continuaci\u00f3n, formatear\u00e1 una partici\u00f3n para asignarle otro formato de archivo. El formateo de particiones es destructivo y borra todos los datos de la partici\u00f3n, lo que no es muy bueno. Recuerde siempre hacer una copia de seguridad de sus datos antes de modificar particiones en un sistema activo. Para formatear la partici\u00f3n \"E:\" y asignarle un formato de archivo distinto, haga clic con el bot\u00f3n derecho del mouse en la partici\u00f3n y seleccione Format. En el panel de control, ver\u00e1 un di\u00e1logo para formatear el sistema de archivos. En la lista desplegable de formatos de archivos, seleccione FAT32 y haga clic en OK. LINUX : En Linux, puede ver los dispositivos de bloques y los sistemas de archivos adjuntados a su sistema con el comando lsblk, que recopila informaci\u00f3n acerca de todos los dispositivos que se encuentran adjuntados al sistema y los imprime con una estructura de \u00e1rbol. Para ver los dispositivos que se adjuntaron a su VM, use el comando lsblk . De manera opcional, puede ver los discos activados en el sistema con el comando df . student-02-9da601a39b43@linux-instance:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10G 0 disk \u2514\u2500sda1 8:1 0 10G 0 part sdb 8:16 0 10G 0 disk \u2514\u2500sdb1 8:17 0 10G 0 part / Para enumerar todas las particiones que contiene /dev/sdb, pase /dev/sdb al comando fdisk sudo fdisk -l /dev/sdb . La activaci\u00f3n y desactivaci\u00f3n son los procesos mediante los cuales un dispositivo est\u00e1 disponible o deja de estarlo en un sistema de archivos Linux. Esto se logra con los comandos mount y umount. Antes de modificar un disco, primero debe desactivarlo del sistema con el comando \"umount\". Cuando haya terminado de modificar el disco, debe activarlo de nuevo en el sistema. m para ver las opciones del menu , p para ver las particiones d para borrar particion. Use el control de comando d para borrar la partici\u00f3n predeterminada. Cuando emita el control de comando d, fdisk le pedir\u00e1 que ingrese la cantidad de particiones que quiera borrar. Como tiene una sola partici\u00f3n, la predeterminada, fdisk la seleccionar\u00e1 autom\u00e1ticamente y la borrar\u00e1 para continuar. Ahora podr\u00e1 crear particiones nuevas. Para ello, ingrese el control de comando n. la primera cambiar\u00e1 al tipo de intercambio de Linux. Ingrese el control de comando t para cambiar el tipo de partici\u00f3n y seleccione la primera partici\u00f3n. Comando w para guardar cambios. A continuaci\u00f3n, crear\u00e1 diferentes sistemas de archivos en las particiones que acaba de generar. Para ello, usar\u00e1 el comando mkfs en Linux. Existen varios tipos de archivos. Es importante que los conozca a todos, junto con las funciones para las que son m\u00e1s adecuados. En este lab, formatear\u00e1 la segunda partici\u00f3n en \"ext4\", el tipo de sistema de archivos de Linux m\u00e1s usado sudo mkfs -t ext4 /dev/sda2 . Ahora, puede activar /dev/sda2 en una ubicaci\u00f3n del sistema de archivos para comenzar a acceder a los archivos que se encuentran en \u00e9l. Act\u00edvela en el directorio /home/my_drive. sudo mount /dev/sda2 /home/my_drive","title":"CREAR PARTICIONES"},{"location":"soporte/#4administracion-de-sistemas-y-servicios-de-infraestructura-de-ti","text":"Software DNS Software DHCP Bind y POWER DNS OpenSSH Remote Clients Configurar DNS con apt install dnsmasq . Pruebas de dig www.google.com @localhost Parar servicio sudo service dnsmasq stop Depurar configuraciones sudo dnsmasq -d -q Cargar lista de host/ip sudo dnsmasq -d -q -H myshosts.txt DHCP con dhcp.conf: sudo dnsmasq -d -q -C dhcp.conf sudo dhclient -i interface -v Los servicios escriben registros en sudo tail /var/log/syslog Ver servicios con errores o running: sudo systemctl --state=failed/running En windows podemos ir al apartado servicios y buscar el servicio para parar, reiniciar, etc. Con powershell: Get-Service Get-Service wuauserv Get-Service wuauserv | Format-List * Stop-Service wuauserv Start-Service wuauserv Set-Service ScardSvr -StartupType Manual (habilitar servicios inhabilitados) Install-WindowsFeature Web-WebServer,Web-Mgmt-Tools -IncludeAllSubFeature (habilitar funciones adicionales). (En IIS cambiaremos la web de exemplo como si fuera index.html) CLIENTES DE CHAT CONFIGURAR CLIENTE EMAIL HERRAMIENTA DE CHROME DEV TOOLS LISTA ERRORES HTTP KERBEROS LDAP OPENLDAP Role-based access control CONFIGURATION MANAGER RSYNC COPIAS SEGURIDAD","title":"4.ADMINISTRACI\u00d3N DE SISTEMAS Y SERVICIOS DE INFRAESTRUCTURA DE TI"},{"location":"soporte/#practica-dns-y-dhcp","text":"Caso hipot\u00e9tico En la empresa donde trabaja, se configur\u00f3 dnsmasq para administrar las necesidades de - DNS y DHCP de la red. Actualmente, se usa casi todo el rango de DHCP para entregar IP din\u00e1micas. Se - agregar\u00e1n varios servidores a la red, que deben configurarse con direcciones IP - conocidas. Su tarea en este lab es modificar esa configuraci\u00f3n de dnsmasq para que los - servidores siempre tengan las mismas direcciones IP. Para ello, deber\u00e1 otorgar a los - servidores las IP necesarias y reducir el rango para las IP din\u00e1micas. Configuraci\u00f3n de red Debido a que sigue la regla de nunca realizar pruebas en producci\u00f3n, experimentar\u00e1 - con los cambios necesarios en una m\u00e1quina que simule la red. Debe hacerlo de esta - manera, en lugar de probar los comandos en el servidor DNS real. En la vida real, despu\u00e9s de terminar de experimentar, aplicar\u00eda los mismos cambios - que realiz\u00f3 en la etapa de prueba a la instancia de producci\u00f3n que ejecuta dnsmasq en - la red. Analicemos esta simulaci\u00f3n de configuraci\u00f3n de red. Para la instancia de pruebas, configuramos una interfaz de red virtual (llamada eth_srv) en la que escuchar\u00e1 el servidor DNS y DHCP. Podemos ver el estado de esa Interfaz con el comando ip. Con el siguiente comando se mostrar\u00e1 informaci\u00f3n sobre la configuraci\u00f3n de la red: Vemos nuestra interface: ip address show eth_srv Observamos que la interfaz est\u00e1 configurada para tener la direcci\u00f3n IPv4 192.168.1.1 en una red con una m\u00e1scara de red /24 o 255.255.255.0. Adem\u00e1s, tenemos otra interfaz de red virtual que usaremos para simular a un cliente que interact\u00faa con el servidor y solicita tr\u00e1fico DNS o DHCP. Esta interfaz se denomina eth_cli. Podemos ver el estado si usamos un comando equivalente como el que se mencion\u00f3 m\u00e1s arriba: student-04-aafdf6c7167d@linux-instance:~$ ip address show eth_srv 4: eth_srv@eth_cli: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 92:14:77:2e:2a:24 brd ff:ff:ff:ff:ff:ff inet 192.168.1.1/24 scope global eth_srv valid_lft forever preferred_lft forever inet6 fe80::9014:77ff:fe2e:2a24/64 scope link valid_lft forever preferred_lft forever student-04-aafdf6c7167d@linux-instance:~$ ip address show eth_cli 3: eth_cli@eth_srv: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 72:45:9a:57:5c:6e brd ff:ff:ff:ff:ff:ff inet6 fe80::7045:9aff:fe57:5c6e/64 scope link valid_lft forever preferred_lft forever En este caso, podemos ver que la interfaz no tiene una direcci\u00f3n IPv4 (todav\u00eda). Cuando se experimenta con cambios en un servicio, es recomendable habilitar la salida de depuraci\u00f3n para poder entender qu\u00e9 sucede y por qu\u00e9. Actualmente, dnsmasq se ejecuta como un demonio en segundo plano. Podemos consultar el estado con el comando de servicio que aprendimos en la lecci\u00f3n anterior: A fin de visualizar la salida de depuraci\u00f3n, vamos a detener el servicio en ejecuci\u00f3n y a iniciarlo manualmente como proceso en segundo plano. Primero debe detenerlo: sudo service dnsmasq stop Luego, debe iniciarlo manualmente con indicadores de depuraci\u00f3n: sudo dnsmasq -d -q Si esos par\u00e1metros le generan curiosidad, el indicador -d significa \"no demonio\", es decir, el servicio se ejecuta en primer plano en lugar de en segundo plano; el indicador -q significa \"registrar consultas\", es decir, se mostrar\u00e1n las interacciones con los clientes. Como el servicio se est\u00e1 ejecutando en segundo plano, no podr\u00e1 ejecutar otros comandos en la terminal hasta que lo detenga manualmente con \"Ctrl + C\". No lo detenga ahora, ya que necesitaremos que siga activo para el paso siguiente. Se puede hacer pruebas de que responde el dnsmasq en dos terminales: sudo dnsmasq -d -q student-04-aafdf6c7167d@linux-instance:~$ dig example.local @localhost cat /etc/dnsmasq.d/mycompany.conf vemos la info de la configuraci\u00f3n de dnsmasq: interface es el nombre de la interfaz que se usar\u00e1 para escuchar las solicitudes DHCP y entregar las respuestas; lo establecemos en nuestra interfaz virtual eth_srv. bind-interfaces significa que dnsmasq operar\u00e1 solo en esa interfaz y se ignorar\u00e1n las dem\u00e1s. domain es el nombre de dominio utilizado en la red. dhcp-option nos permite brindar a los clientes DHCP informaci\u00f3n adicional opcional. En este caso, establecemos el router (tambi\u00e9n conocido como puerta de enlace predeterminada) y el dns-server. Cuando los clientes reciban la respuesta DHCP, tambi\u00e9n recibir\u00e1n y aplicar\u00e1n esta configuraci\u00f3n. dhcp-range indica el rango de IP que est\u00e1 disponible para utilizarse en la asignaci\u00f3n din\u00e1mica de IP y la extensi\u00f3n del tiempo asignado. En este caso, toda la red (excepto el servidor DHCP) est\u00e1 disponible actualmente como parte del rango din\u00e1mico. Sabemos que debemos modificarlo. La asignaci\u00f3n de tiempo est\u00e1 definida en 24 horas, lo que no es recomendable si nuestra red tiene muchos dispositivos que solo son visibles durante per\u00edodos breves. C\u00f3mo experimentar con un cliente DHCP: Ahora que sabemos que el DNS funciona correctamente, vamos a experimentar con la configuraci\u00f3n de DHCP. En la segunda terminal, ejecutaremos dhclient, que es el cliente DHCP m\u00e1s com\u00fan en Linux. Como se mencion\u00f3, lo ejecutaremos en la interfaz eth_cli. Adem\u00e1s, le indicaremos que se inicie en modo detallado y ejecute un script de depuraci\u00f3n que le brindamos: sudo dhclient -i eth_cli -v -sf /root/debug_dhcp.sh La secuencia de comandos de depuraci\u00f3n que pasamos con el par\u00e1metro -sf es para que, en lugar de modificar toda la configuraci\u00f3n de red en esta m\u00e1quina, podamos ver qu\u00e9 informaci\u00f3n se recibi\u00f3 del servidor. Podemos cambiar la info del dhcp sudo nano /etc/dnsmasq.d/mycompany.conf : # This is the interface on which the DHCP server will be listening to. interface=eth_srv # This tells this dnsmasq to only operate on that interface and not operate # on any other interfaces, so that it doesn't interfere with other running # dnsmasq processes. bind-interfaces # Domain name that will be sent to the DHCP clients domain=mycompany.local # Default gateway that will be sent to the DHCP clients dhcp-option=option:router,192.168.1.1 # DNS servers to announce to the DHCP clients dhcp-option=option:dns-server,192.168.1.1 # Dynamic range of IPs to use for DHCP and the lease time. dhcp-range=192.168.1.20,192.168.1.254,6h dhcp-host=aa:bb:cc:dd:ee:b2,192.168.1.2 dhcp-host=aa:bb:cc:dd:ee:c3,192.168.1.3 dhcp-host=aa:bb:cc:dd:ee:d4,192.168.1.4 comprobamos que est\u00e9 bien el archivo student-04-aafdf6c7167d@linux-instance:~$ sudo dnsmasq --test -C /etc/dnsmasq.d/mycompany.conf dnsmasq: syntax check OK. Vemos que ahora est\u00e1 utilizando el nuevo rango, que comienza a partir de 192.168.1.20, y que la asignaci\u00f3n de tiempo es de 6 horas en lugar de 24 horas.","title":"PRACTICA DNS Y DHCP"},{"location":"soporte/#practica-apache","text":"Tenemos un caso en que la web, todo su contenido est\u00e1 en otra ubicaci\u00f3n y se ha de colocar bien y con su configuraci\u00f3n y desactivar la otra predeterminada. Instalamos apache sudo apt install apache2 -y . Los sitios webs donde se publican las webs se encuentran en ls -l /etc/apache2/sites-available . Ejemplo de virtualhost: <VirtualHost *:80> ServerAdmin webmaster@localhost DocumentRoot /var/www/html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined </VirtualHost> Eso indica que el servicio estar\u00e1 escuchando en el puerto 80 para todas las IP. Luego, se detalla la direcci\u00f3n de correo electr\u00f3nico del administrador del servicio, la ruta principal del sitio web y las rutas de los archivos de registro de errores y acceso. Copiamos el contenido de la web a donde toca sudo mv /opt/devel/ourcompany /var/www/ourcompany . Creamos un documento nuevo para crear nuestro propio sitio y no el predeterminado cd /etc/apache2/sites-available . Copiamos y editamos el nuevo cambiando el DocumentRoot por el nuevo sitio de /var/www/ourcompany: sudo cp 000-default.conf 001-ourcompany.conf student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo vim 001-ourcompany.conf Ya agregamos un sitio que apunta a la ubicaci\u00f3n adecuada, pero que todav\u00eda no est\u00e1 habilitado. Por el momento, sigue estando habilitado el sitio predeterminado. Apache2 nos permite tener sitios que est\u00e1n disponibles, aunque no necesariamente habilitados, para evitar los cambios disruptivos. Los sitios habilitados se administran en /etc/apache2/sites-enabled. Veamos el contenido de ese directorio: ls -l /etc/apache2/sites-enabled Las flechas indican que este archivo es un v\u00ednculo simb\u00f3lico al archivo que se encuentra en el directorio sites-available. En otras palabras, para habilitar o inhabilitar un sitio en Apache2, simplemente se crea o se quita un v\u00ednculo simb\u00f3lico entre los directorios sites-available y sites-enabled. Para simplificar el proceso, hay dos comandos, a2ensite y a2dissite, que administran esos symlinks por nosotros (los nombres provienen de las palabras en ingl\u00e9s para habilitar, [\"enable\"], o inhabilitar [\"disable\"] el sitio en Apache2). Utilicemos esos comandos para habilitar nuestro sitio nuevo e inhabilitar el predeterminado: sudo a2ensite 001-ourcompany.conf sudo a2dissite 000-default.conf Y volvamos a consultar el contenido del directorio: ls -l /etc/apache2/sites-enabled Hasta que no se recarga el servicio no funcionar\u00e1: sudo service apache2 reload Hay modulos en apache que se encuentran en: ls /etc/apache2/mods-available . Activamos uno que es para que se active el pie de pagina y no se vea una linea discontinua: student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo a2enmod include Considering dependency mime for include: Module mime already enabled Enabling module include. To activate the new configuration, you need to run: service apache2 restart student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo service apache2 restart student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo vi /etc/apache2/sites-available/001-ourcompany.conf ---- a\u00f1adimos despues del documentroot <Directory /var/www/ourcompany> Options +Includes XBitHack on </Directory> ------ student-00-e3d872882ea6@linux-instance:/etc/apache2/sites-available$ sudo service apache2 reload","title":"PRACTICA APACHE"},{"location":"soporte/#practica-nube","text":"Crearemos instancias en la nube con Google Cloud. Creamos una instancia con ciertas caracteristicas: gcloud compute instances create linux-instance --zone=us-central1-f --machine-type=n1-standard-1 --subnet=default --tags=http-server --image=ubuntu-1604-xenial-v20190628 --image-project=ubuntu-os-cloud --boot-disk-size=10GB gcloud compute instances create windows-instance --zone=us-central1-f --machine-type=n1-standard-1 --subnet=default --image=windows-server-2016-dc-v20190709 --image-project=windows-cloud --boot-disk-size=50GB Creamos un firewall: Las reglas de firewall le permiten seleccionar si desea o no que se permita cierto tr\u00e1fico entrante. Es posible que desee habilitar muchos m\u00e1s puertos, lo que se puede hacer despu\u00e9s de crear la instancia. Los puertos que se muestran en la p\u00e1gina de creaci\u00f3n de la VM son los m\u00e1s comunes. En esta instancia, agregue una regla de firewall para permitir el tr\u00e1fico HTTP: gcloud compute firewall-rules create default-allow-http --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=tcp:80 --source-ranges=0.0.0.0/0 --target-tags=http-server Listamos instancias con gcloud compute instances list : student-04-a602c997bb07@master-instance:~$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS linux-instance us-central1-f n1-standard-1 10.128.0.3 34.71.55.155 RUNNING master-instance us-central1-f n1-standard-1 10.128.0.2 34.66.56.246 RUNNING windows-instance us-central1-f n1-standard-1 10.128.0.4 34.72.123.73 RUNNING Para conectarse a una instancia se usa: gcloud compute ssh linux-instance --zone us-central1-f Actualizamos paquetes con sudo apt update e instalamos nginx sudo apt install nginx . Comprobamos yendo a la ip externa en el navegador. Ahora creamos un disco adicional de 500gb para luego usarlo a la instancia de windows: gcloud compute disks create additional-disk --type=pd-standard --size=500GB --zone=us-central1-f La unimos a la instancia: gcloud compute instances attach-disk windows-instance --disk additional-disk El disco que se vincul\u00f3 est\u00e1 en blanco; ni siquiera tiene un sistema de archivos. Para poder usarlo, primero tenemos que formatearlo. Este proceso se explica en detalle en el lab sobre c\u00f3mo particionar y formatear una unidad de disco en Windows. Aqu\u00ed lo explicaremos brevemente. Ingrese el siguiente comando para crear un usuario nuevo, acceder a la windows-instance y restablecer su contrase\u00f1a. gcloud compute reset-windows-password windows-instance --user=student --zone=us-central1-f ip_address: 34.72.123.73 password: b3JBcc[>@}Pu{$a username: student Ahora podemos conectarnos remotamente con remina o windows y veremos las caracteristicas. Despu\u00e9s de acceder, haga clic en el \u00edcono de Windows. Luego, busque la aplicaci\u00f3n de Computer Management (en Windows Administrative Tools), que se usa para particionar discos. Sugerencia: Puede evitar pasar por el Control Panel y tener que hacer clic en los diferentes subelementos. Para ello, escriba \"Computer\" en el men\u00fa de Windows. En el men\u00fa Storage hay una entrada de Disk Management que se puede usar para administrar discos. Cuando hace clic en esa entrada, aparece una ventana emergente que le indica que formatee el disco nuevo. Seleccione GPT como el tipo de tabla particionada. Luego, tendr\u00e1 un volumen disponible, pero no se formatear\u00e1 con ning\u00fan sistema de archivos espec\u00edfico. Para formatearlo, haga clic con el bot\u00f3n derecho en el volumen, seleccione \"New Simple Volume\" y siga las instrucciones del asistente para formatearlo usando el sistema de archivos NTFS. El sistema operativo formatear\u00e1 el disco y lo dejar\u00e1 listo para usar.","title":"PRACTICA NUBE"},{"location":"soporte/#practica-active-directory","text":"Active Directory es una herramienta central para los administradores de sistemas que necesitan administrar m\u00e1quinas Windows. Active Directory permite administrar usuarios, grupos, m\u00e1quinas y las pol\u00edticas que se aplican a todos ellos de manera centralizada. En esta pr\u00e1ctica de lab, interactuar\u00e1s con Active Directory; lo usar\u00e1s para agregar usuarios y grupos; editar\u00e1s la membres\u00eda de los usuarios, y tambi\u00e9n crear\u00e1s un nuevo objeto de directiva de grupo (GPO). Primero instalamos active directory por comando powershell: C:\\Qwiklabs\\ADSetup\\active_directory_install.ps1 Configuramos con este comando: C:\\Qwiklabs\\ADSetup\\configure_active_directory.ps1 Abra el Active Directory Administrative Center (ADAC). Para encontrarlo, escriba active en el men\u00fa de inicio de Windows. El Active Directory Administrative Center le permite administrar su instalaci\u00f3n de Active Directory mediante la configuraci\u00f3n de usuarios, grupos, computadoras, etc. Si\u00e9ntase libre de navegar por los recursos ya existentes en el directorio. Para este lab, crearemos un usuario nuevo llamado Alex. Primero, haga clic en la entrada example (local), que corresponde al dominio que puede administrar con su cuenta. Luego, despl\u00e1cese hacia abajo y haga doble clic en la entrada Users para ver la lista de usuarios y grupos que existen actualmente. Para crear un nuevo usuario, observe la lista de tareas en la parte derecha. En la secci\u00f3n Users, hay una opci\u00f3n New para las entradas que abre un men\u00fa secundario donde puede elegir el tipo de entidad que quiere crear. Como en este caso vamos a crear un usuario nuevo, haga clic en User. Si hace clic en la cuenta que se cre\u00f3, ver\u00e1 que, donde aparece el nombre del usuario, el sistema muestra Alex (Disabled). El sistema no habilitar\u00e1 una cuenta que no tenga una contrase\u00f1a segura. En este caso, la contrase\u00f1a est\u00e1 vac\u00eda porque no la establecimos. Obviamente, una contrase\u00f1a vac\u00eda no es segura. Para establecer una contrase\u00f1a, use la opci\u00f3n Reset password del men\u00fa. Al tener seleccionada la opci\u00f3n User must change password at next logon, nos aseguramos de que el usuario cambiar\u00e1 la contrase\u00f1a cuando acceda. El objetivo de este paso es que el administrador del sistema no sepa la contrase\u00f1a nueva del usuario despu\u00e9s del primer acceso. Cuando haya establecido una contrase\u00f1a segura, vuelva a intentar habilitar la cuenta. Esta vez deber\u00eda funcionar. Ahora agreguemos un nuevo grupo. Si explora los grupos existentes, ver\u00e1 que hay uno llamado \"Developers\" y otro llamado \"Java Developers\". Vamos a agregar un grupo adicional llamado \"Python Developers\". Agregue el nuevo grupo a \"Developers\" y, luego, agregue la cuenta que acabamos de crear para Alex al grupo \"Python Developers\". Para crear un nuevo grupo, use el mismo men\u00fa que us\u00f3 cuando cre\u00f3 un usuario nuevo, pero esta vez seleccione la opci\u00f3n New > Group. Estamos creando un grupo llamado \"Python Developers\"; ese es el \u00fanico dato obligatorio. Si lo desea, puede agregar informaci\u00f3n adicional en Description y Notes. Cuando termine, haga clic en OK para crear el grupo. C\u00f3mo agregar entidades a grupos: Ya tenemos un grupo llamado \"Python Developers\", y ahora queremos agregarlo al grupo \"Developers\" existente. Para hacerlo, nos desplazamos hacia abajo hasta la entrada nueva, hacemos clic con el bot\u00f3n derecho en la entrada de la lista y seleccionamos Add to another group. Cuando haga clic en el bot\u00f3n OK, se agregar\u00e1 el grupo \"Python Developers\" a \"Developers\". Ahora, tambi\u00e9n agregaremos a Alex a \"Python Developers\", pero seguiremos un camino diferente. En este caso, haremos doble clic en la entrada \"Python Developers\" de la lista, lo que abrir\u00e1 una ventana de edici\u00f3n para el grupo. Como en este caso agregaremos a Alex al grupo, haga clic en el bot\u00f3n Add, escriba \"Alex\" en el campo de texto, seleccione OK para agregarlo y, luego, OK para guardar los cambios. As\u00ed agregamos correctamente un nuevo miembro al grupo: Alex. Por \u00faltimo, hay usuario existente, llamado Alosha, que pas\u00f3 de desarrollar en Java a desarrollar en Python. Vamos a quitarlo del grupo \"Java Developers\" para agregarlo al grupo \"Python Developers\". Para ello, busque al usuario Alosha en la lista y haga doble clic en la entrada. Se abrir\u00e1n las propiedades del usuario, que podr\u00e1 editar. Hay muchas opciones de configuraci\u00f3n para cada usuario; haga clic en la secci\u00f3n de la izquierda llamada Member Of. Observamos que Alosha es miembro de los grupos \"Domain Users\" (todos los usuarios del dominio son miembros de este grupo) y \"Java Developers\". Seleccione la entrada \"Java Developers\" y haga clic en el bot\u00f3n Remove para quitarlo de ese grupo. Luego haga clic en Add para agregar una nueva membres\u00eda. Para administrar las pol\u00edticas de grupo, necesitamos usar la aplicaci\u00f3n de Group Policy Management. Para encontrarla, escriba group en el men\u00fa de inicio de Windows. Esta aplicaci\u00f3n le permite establecer pol\u00edticas que administrar\u00e1n el comportamiento de las m\u00e1quinas en su dominio. Puede aplicarlas a todo el dominio o a Organizational Units (OU) independientes. En nuestro caso, agregaremos una pol\u00edtica nueva a la OU de Developers que ya existe en el dominio. Para hacerlo, expanda el \u00e1rbol hasta llegar al \u00e1rbol de dominio de example.com y localice la OU de Developers dentro de \u00e9l. Para crear una pol\u00edtica nueva y vincularla, haga clic con el bot\u00f3n derecho en la entrada y seleccione la primera opci\u00f3n del men\u00fa: Create a GPO in this domain, and Link it here. Queremos establecer un fondo de pantalla predeterminado para las m\u00e1quinas de la OU de Developers, por lo que nuestra pol\u00edtica se llamar\u00e1 New Wallpaper. Cuando la hayamos creado, editaremos la pol\u00edtica haciendo clic con el bot\u00f3n derecho en la entrada y seleccionando la primera entrada del men\u00fa: Edit. Se abrir\u00e1 una nueva aplicaci\u00f3n: Group Policy Management Editor, que le permitir\u00e1 explorar y configurar todas las opciones que se pueden establecer en una pol\u00edtica de grupo. Como queremos establecer el fondo de pantalla, navegaremos hasta esta opci\u00f3n desde User Configuration > Policies > Administrative Templates > Desktop > Desktop. Se abrir\u00e1 una lista de opciones que podemos configurar, incluida \"Desktop Wallpaper\". Para establecer un valor espec\u00edfico de fondo de pantalla, haga doble clic en la entrada Desktop Wallpaper. La ventana que se abre le permite establecer el valor del fondo de pantalla. Para hacerlo, primero haga clic en el bot\u00f3n Enabled y, luego, ingrese una ruta de acceso para el fondo de pantalla. Puede ser una ruta local en la m\u00e1quina o la ruta de una red en un servidor que comparte archivos. Para este lab, ingresar\u00e1 C:\\Qwiklabs\\wallpaper.jpg en la secci\u00f3n Wallpaper Name.Cuando haga clic en OK, se crear\u00e1 la pol\u00edtica de grupo con los valores que deseamos. Para verificarlo, regrese a la aplicaci\u00f3n de Group Policy Management y haga clic en la pesta\u00f1a Settings de la pol\u00edtica nueva. Al hacer clic en los v\u00ednculos show en la p\u00e1gina web, puede observar que se defini\u00f3 la pol\u00edtica y que la \u00fanica opci\u00f3n modificada corresponde a Desktop Wallpaper, que tiene el valor que establecimos anteriormente.","title":"PRACTICA ACTIVE DIRECTORY"},{"location":"soporte/#practica-backuppc","text":"La gesti\u00f3n de copias de seguridad es una actividad de importancia cr\u00edtica con la que tendr\u00e1s que lidiar como administrador del sistema. Cuando hablamos de administrar copias de seguridad, nos referimos no solo a la configuraci\u00f3n inicial de cierto software de copia de seguridad, sino tambi\u00e9n a garantizar que las copias de seguridad funcionen correctamente y puedan restaurarse de manera confiable. En este lab, usaremos varios comandos de Linux, que ya se explicaron en este curso y en el anterior. A continuaci\u00f3n, repasaremos sus funciones: sudo : Ejecuta un comando con derechos de administrador. apt update: Actualiza la lista de paquetes disponibles para instalar. apt install package: Instala el paquete en el sistema. a2enmod : Habilita un m\u00f3dulo de Apache2. a2ensite : Habilita un sitio web de Apache2. nano : Abre un editor de texto para editar el archivo. service restart: Reinicia el servicio indicado. ls : Muestra la lista de los archivos que se encuentran en un directorio. cp : Crea una copia del archivo antiguo con el nombre nuevo. cat : Muestra todo el contenido de un archivo. grep : Filtra el texto de un archivo seg\u00fan el patr\u00f3n. tail : Muestra las \u00faltimas l\u00edneas de un archivo. Administrar las copias de seguridad es una actividad muy importante que deber\u00e1 llevar a cabo como administrador del sistema. Adem\u00e1s de comprender la configuraci\u00f3n inicial de cierto software para hacer copias de seguridad, la administraci\u00f3n de las copias de seguridad implica garantizar que estas funcionen correctamente y puedan restablecerse de manera confiable. En este lab, instalar\u00e1 y configurar\u00e1 la infraestructura para realizar copias de seguridad de modo que m\u00e1quinas locales y remotas administren las copias. Tambi\u00e9n comprobar\u00e1 que las copias de seguridad puedan restablecerse de manera correcta. En este lab, usaremos BackupPc. Esta soluci\u00f3n le permite realizar copias de seguridad de los datos en distintos sistemas operativos y posee una interfaz web pr\u00e1ctica para la configuraci\u00f3n. En este lab, configuraremos una m\u00e1quina llamada backup-server como servidor. El servidor usar\u00e1 BackupPc para realizar copias de seguridad de otras m\u00e1quinas. Veremos c\u00f3mo realizar copias de seguridad de datos de la m\u00e1quina local y de m\u00e1quinas remotas de Linux y Windows, llamadas linux-instance y windows-instance, respectivamente. Respecto del servidor de copia de seguridad, es importante realizar una copia de seguridad del directorio /etc donde se almacenan las configuraciones. En cuanto a las m\u00e1quinas remotas, hay que realizar la copia de seguridad de los directorios /home y Users, donde se almacenan los datos de los usuarios. Primero nos conectamos al backup-server e instalamos la herramienta backupPc: sudo apt install backuppc Cuando se le pregunte c\u00f3mo configurar el correo electr\u00f3nico, responda Local only. Cuando haya confirmado el resto de las respuestas, el paquete terminar\u00e1 de instalarse. Si analiza el resultado, ver\u00e1 que indica que es necesario reiniciar Apache2 (y se recomienda usar systemctl restart apache2, comando equivalente al que estuvimos usando). Haremos eso en un minuto, pero antes configuraremos Apache2 y BackupPc para que usen SSL. Como mencionamos antes, para encriptar conexiones, usamos HTTPS en lugar de HTTP. Vamos a usar el sitio web que proporciona BackupPc para administrar nuestras copias de seguridad. El sitio web debe estar encriptado para evitar que la contrase\u00f1a administrativa y cualquier otro dato que se ingrese se transmitan en texto sin formato. De lo contrario, toda persona que esp\u00ede nuestra conexi\u00f3n a Internet podr\u00e1 leer la contrase\u00f1a de administrador y cualquier otro dato que enviemos. Para habilitar HTTPS en nuestro sitio web, es necesario realizar tres pasos: Para habilitar el m\u00f3dulo SSL, usaremos el comando a2enmod: sudo a2enmod ssl Para habilitar el sitio SSL predeterminado, usaremos el comando a2ensite.: sudo a2ensite default-ssl.conf C\u00f3mo habilitar SSL en BackupPc y configurar la contrase\u00f1a de administrador sudo vi /etc/backuppc/apache.conf . En este archivo se incluyen todas las configuraciones necesarias para que BackupPc tenga un sitio web que funcione en Apache2. Si desea obtener m\u00e1s informaci\u00f3n sobre las opciones establecidas, puede buscarlas en la documentaci\u00f3n de Apache2 . Quitaremos el car\u00e1cter # que est\u00e1 antes de la opci\u00f3n SSLRequireSSL. Eso significa que habilitamos la opci\u00f3n. El \u00fanico paso que falta es establecer la contrase\u00f1a administrativa del usuario backuppc. Podemos hacerlo con el comando htpasswd, que se usa para establecer contrase\u00f1as para usuarios web: sudo htpasswd /etc/backuppc/htpasswd backuppc Reiniciamos el server apache para aplicar cambios sudo service apache2 restart Ahora puede acceder a su sitio web. Para ello, copie la direcci\u00f3n IP externa de backup-server y p\u00e9guela en una ventana nueva del navegador. https://34.133.119.159/backuppc el navegador le pedir\u00e1 su nombre de usuario y contrase\u00f1a. El nombre de usuario es backuppc y la contrase\u00f1a es la que ingres\u00f3 con el comando htpasswd. Primero, crearemos una copia de seguridad de los archivos almacenados en la m\u00e1quina local. Para ello, haga clic en el v\u00ednculo Host Summary. En la p\u00e1gina de resumen del host, se enumeran todos los hosts conocidos en el momento de la consulta y su estado. Por ahora, el \u00fanico host conocido para el sistema de copias de seguridad es localhost. Aparecer\u00e1 bajo Hosts with no Backups porque todav\u00eda no tiene copias de seguridad. Si hace clic en el v\u00ednculo localhost, se abrir\u00e1 la p\u00e1gina con la informaci\u00f3n de copias de seguridad correspondiente a la m\u00e1quina local. Esta p\u00e1gina indica que no se realiz\u00f3 ninguna copia de seguridad de la m\u00e1quina todav\u00eda. Para cambiar eso, haga clic en el bot\u00f3n Start Full Backup. El mensaje en la parte superior de la p\u00e1gina se\u00f1ala que la copia de seguridad fall\u00f3 y que se produjeron errores. En la secci\u00f3n de resumen, vemos que hubo un intento de realizar una copia de seguridad, pero dice que fue \"parcial\", lo que indica que no se pudo completar la copia de seguridad. Para analizar los errores, haga clic en el v\u00ednculo Errors en la l\u00ednea \"Backup 0\". Hay muchos errores, pero el mensaje dice siempre lo mismo: Permission denied. La causa de ese error es que la copia de seguridad se est\u00e1 ejecutando con el usuario backuppc, y ese usuario no tiene permiso para ver muchos de los archivos y directorios correspondientes al directorio /etc. Analicemos c\u00f3mo se realiza la copia de seguridad de esos archivos. Haga clic en el v\u00ednculo Edit Config de la secci\u00f3n localhost en el lado izquierdo. Luego, haga clic en la pesta\u00f1a Xfer (abreviatura de transferencia). Para solucionar el problema de permisos que surgi\u00f3, le ordenaremos a backuppc que use el comando sudo para la creaci\u00f3n de la copia de seguridad. Para ello, en la opci\u00f3n TarClientCmd, agregue sudo antes de $tarPath. De esa forma, el comando tar podr\u00e1 acceder a todos los archivos a los que accede root. Damos a SAVE. Esta vez deber\u00eda haberse mostrado el error sudo: no tty present and no askpass program specified. La causa es que indicamos que se usar\u00eda el comando sudo, pero el usuario backuppc todav\u00eda no tiene permiso para usarlo sudo visudo (backuppc ALL=(ALL:ALL) NOPASSWD: /bin/tar) De esa forma, el usuario backuppc deber\u00eda poder acceder a todos los archivos cuando cree la copia de seguridad. Vuelva a la p\u00e1gina principal de localhost y haga clic en el bot\u00f3n Start Full Backup de nuevo. Confirme y luego regrese a la p\u00e1gina principal. Puede ver el contenido de cualquier copia de seguridad generada haciendo clic en el v\u00ednculo de la entrada de la copia de seguridad (en este caso, 0). Tambi\u00e9n puede ir directamente a la \u00faltima copia de seguridad haciendo clic en Browse backups en el men\u00fa superior izquierdo. Para examinar el contenido de una copia de seguridad, puede descargar directamente cualquiera de los archivos de la lista haciendo clic en el nombre. Por ejemplo, puede hacer clic en apache.conf y aparecer\u00e1 un mensaje para que descargue el archivo en su m\u00e1quina. Luego, podr\u00e1 abrirlo y verificar que tenga el mismo contenido que vimos antes. Si necesita restablecer algo de una copia de seguridad, debe seleccionar los elementos haciendo clic en la casilla de verificaci\u00f3n ubicada a la izquierda del nombre y, luego, hacer clic en el bot\u00f3n Restore selected files. Esto le permitir\u00e1 seleccionar c\u00f3mo restablecer los archivos, ya sea directamente en la unidad o bien descargando un archivo Zip o Tar que los contenga. C\u00f3mo hacer una copia de seguridad de una m\u00e1quina de Linux remota: Para permitir que nuestro servidor se conecte a la instancia de Linux de la que queremos hacer una copia de seguridad, crearemos un par de claves SSH que se almacenar\u00e1 en las m\u00e1quinas. sudo su - backuppc y ssh-keygen Veamos el contenido del directorio en el que se generaron las claves. sudo ls -l /var/lib/backuppc/.ssh/ Para ello, copie el archivo a /tmp de modo que pueda accederse sin derechos de administrador: sudo cp /var/lib/backuppc/.ssh/id_rsa.pub /tmp/ Luego, c\u00f3pielo a linux-instance con el siguiente comando: gcloud compute scp /tmp/id_rsa.pub root@linux-instance:~ --zone=us-central1-a Vemos que se ha copiado en la otra maquina sudo find / -type f -name \"id_rsa.pub\" Lo copiamos al user de nuestra maquina: sudo mv /home/sa_109733481048430167514/id_rsa.pub /home/student-03-104cdaa5152a Lo movemos a authorized keys para que accesa sin credenciales cat id_rsa.pub | sudo tee -a /root/.ssh/authorized_keys Ahora desde el server nos conectamos a la instancia linux sudo su - backuppc y ssh root@linux-instance C\u00f3mo agregar la m\u00e1quina remota en BackupPc: Ahora que funciona la conexi\u00f3n remota entre m\u00e1quinas, puede configurar este cliente nuevo en BackupPc. Vuelva a la interfaz web administrativa y haga clic en Edit Hosts. Agregaremos linux-instance como host nuevo. Para hacerlo, haga clic en el bot\u00f3n Add a fin de agregar una l\u00ednea nueva y escriba linux-instance en el campo host. Luego, haga clic en el bot\u00f3n Save. As\u00ed agregamos el host, pero falta configurarlo. Para ello, haga clic en Host summary, que ahora muestra los dos hosts conocidos por el sistema (uno con copias de seguridad y el otro sin ellas), y luego en linux-instance para ir a la p\u00e1gina de configuraci\u00f3n de esta instancia. La parte superior de la barra izquierda ahora se refiere a este host que queremos configurar. Para ello, haga clic en Edit Config y luego en la pesta\u00f1a Xfer. Para esta instancia, seleccione rsync como XferMethod. rsync es una herramienta que se puede usar para sincronizar el contenido de dos \u00e1rboles de directorios sin transferir todo el contenido, ya que verifica lo que ya se transfiri\u00f3 y solo transfiere lo nuevo. En la ruta de la copia de seguridad (llamada RsyncShareName en la configuraci\u00f3n rsync), escriba /home. Este es el directorio remoto que queremos resguardar. Tambi\u00e9n se puede realizar una copia de seguridad de toda la m\u00e1quina ingresando /. Sin embargo, en nuestro escenario, solo nos interesa /home, que contiene los directorios principales de los usuarios. Despu\u00e9s de ingresar el directorio, haga clic en el bot\u00f3n Save para guardar los cambios. Luego, vaya al v\u00ednculo de la p\u00e1gina principal de linux-instance, que muestra el estado actual de las copias de seguridad de esta m\u00e1quina. En este caso, todav\u00eda no hay copias. Haga clic en el bot\u00f3n Start Full Backup para forzar el inicio de una copia de seguridad nueva de esta m\u00e1quina. Despu\u00e9s de confirmar la realizaci\u00f3n de la copia, vuelva a la p\u00e1gina principal. Si borramos de la instancia linux el .pub vamos a la copia y restore el fichero este y lo volvemos a tener. Ahora nos conectamos a windows: Por \u00faltimo, tambi\u00e9n queremos usar el servidor de copias de seguridad para resguardar los datos remotos de una m\u00e1quina de Windows. En este caso, el recurso del que queremos hacer una copia de seguridad es el contenido de la carpeta Users. Para eso, debemos crear un usuario backuppc que pueda conectarse a la carpeta Users como recurso compartido. Primero, nos conectaremos a windows-instance. Creamos un usuario: Para crear la cuenta nueva, debemos abrir la aplicaci\u00f3n User Accounts. Abra Control Panel, haga clic en User Accounts y luego otra vez en User Accounts. Haga clic en Manage another account para crear una cuenta nueva. Una vez que creamos el usuario, debemos compartir la carpeta de la que queremos crear una copia de seguridad. En este escenario, queremos resguardar la carpeta Users. Para compartirla, abra File Explorer, navegue a la carpeta C:\\Users, haga clic con el bot\u00f3n derecho en ella y luego haga clic en Share with > Advanced sharing y compartimos Users. Vuelva a la interfaz web administrativa de BackupPc para configurar el host. Al igual que antes, primero debe hacer clic en Edit Hosts, agregar una entrada para windows-instance y guardar los cambios. Despu\u00e9s, haga clic en Host Summary y luego en windows-instance. Desde la p\u00e1gina principal, haga clic en Edit Config para editar la configuraci\u00f3n y seleccione la pesta\u00f1a Xfer. En este caso, vamos a usar smb como XferMethod. Tambi\u00e9n se conoce como samba y es el nombre del protocolo que se usa para interactuar con las carpetas compartidas de Windows. Debemos configurar SmbShareName para que sea la carpeta de la que queremos hacer una copia de seguridad (Users) y SmbShareUserName y SmbSharePassword para que sean el nombre de usuario y la contrase\u00f1a que creamos en la instancia de Windows. Haremos un backupp pero dara error. cambiamos en la instancia windows el tipo de cuenta creado por admin y al hacerlo de nuevo, ir\u00e1 bien. \u00a1Muy bien! Logr\u00f3 configurar un servidor de copias de seguridad, realizar copias de seguridad incrementales y completas, solucionar errores relacionados con permisos, verificar que las copias de seguridad funcionan bien descargando los archivos y restableci\u00e9ndolos, y configurar un servidor para que realice copias de seguridad de archivos de configuraci\u00f3n almacenados localmente y de directorios de usuarios ubicados en m\u00e1quinas remotas de Linux y Windows. En el proceso, tambi\u00e9n aprendi\u00f3 a administrar la configuraci\u00f3n sudo, intercambiar claves SSH, conectarse con carpetas compartidas en m\u00e1quinas Windows y mucho m\u00e1s.","title":"PRACTICA BACKUPPC"},{"location":"soporte/#5seguridad-informatica","text":"OPENVPN opci\u00f3n segura para conexiones entre redes. REGLAS FIREWALL , mejores pr\u00e1cticas para crear reglas de firewall iptables. HAPROXY NGINX APACHE El modo promiscuo es aquel en el que una computadora conectada a una red compartida, tanto la basada en cable de cobre como la basada en tecnolog\u00eda inal\u00e1mbrica, captura todo el tr\u00e1fico que circula por ella. Este modo est\u00e1 muy relacionado con los sniffers que se basan en este modo para realizar su tarea. netstat -i ifconfig eth0 promisc or ip link set eth0 promisc on Softwares para la red de Sistema de prevenci\u00f3n / detecci\u00f3n de intrusiones: SURICATA SNORT Herramientas de filtrado de la red: Wireshark TCPDUMP Rsyslog es una aplicaci\u00f3n que se encarga de implementar un protocolo de syslog b\u00e1sico de captura, procesamiento y registro de los mensajes. La implementaci\u00f3n de Rsyslog tiene como objetivo principal mejorar la gesti\u00f3n y el control de toda la informaci\u00f3n de la red. Permite la detecci\u00f3n de ca\u00eddas de red, la anticipaci\u00f3n a posibles problemas futuros y prevenir fugas de informaci\u00f3n, es decir comportamientos inadecuados que causen errores en la red. Adem\u00e1s, Wispcontrol permite la configuraci\u00f3n de alertas de log personalizadas. As\u00ed, en caso de ca\u00edda o fallo del sistema, usted podr\u00e1 saber qu\u00e9 est\u00e1 ocurriendo en su red inmediatamente, pudiendo obtener informaci\u00f3n sobre las amenazas potenciales. dm-crypt cifrado de discos duros. OPENVAS herramienta para escaneo de vulnerabilidades.","title":"5.SEGURIDAD INFORM\u00c1TICA"},{"location":"soporte/#crear-o-inspeccionar-pares-de-claves-encriptar-o-desencriptar-y-firmar-o-verificar-con-openssl","text":"Recuerde que un par de claves consiste en una clave p\u00fablica que puede poner a disposici\u00f3n de todos y una clave privada que debe mantener en secreto. Cuando alguien quiera enviarle datos y asegurarse de que nadie m\u00e1s pueda verlos, puede usar su clave p\u00fablica para encriptarlos. Los datos encriptados con su clave p\u00fablica solo se pueden desencriptar con su clave privada, de modo que solo usted pueda ver los datos originales. Por eso, es importante que nadie m\u00e1s conozca sus claves privadas. Si alguien m\u00e1s tiene una copia de su clave privada, podr\u00e1 desencriptar los datos que estaban destinados a usted, lo que no es muy bueno. Primero, generaremos una clave privada RSA de 2,048 bits y la analizaremos. Para generar la clave, ingrese el siguiente comando en la terminal: openssl genrsa -out private_key.pem 2048 A continuaci\u00f3n, generaremos la clave p\u00fablica a partir de la clave privada y tambi\u00e9n la inspeccionaremos. Ahora que tiene una clave privada, debe generar una clave p\u00fablica para completar el par. Puede proporcion\u00e1rsela a cualquier persona que quiera enviarle datos encriptados. Cuando se encriptan los datos con su clave p\u00fablica, nadie puede desencriptarlos, a menos que tenga su clave privada. Para crear una clave p\u00fablica basada en una clave privada, ingrese el comando a continuaci\u00f3n. Deber\u00eda ver el siguiente resultado: openssl rsa -in private_key.pem -outform PEM -pubout -out public_key.pem Crear\u00e1 un archivo de texto con informaci\u00f3n que desea encriptar para protegerla. A continuaci\u00f3n, la encriptar\u00e1 y la inspeccionar\u00e1. Para crear el archivo, ingrese el siguiente comando. Se crear\u00e1 un archivo de texto nuevo llamado \"secret.txt\" que solo incluye el siguiente texto: \"This is a secret message, for authorized parties only\" (Este es un mensaje secreto, solo para los usuarios autorizados). Puede cambiar este mensaje por el que desee. echo 'This is a secret message, for authorized parties only' > secret.txt A continuaci\u00f3n, ingrese el siguiente comando para encriptar el archivo con su clave p\u00fablica: openssl rsautl -encrypt -pubin -inkey public_key.pem -in secret.txt -out secret.enc Para desencriptar, se necesita que se tenga la llave privada, en este caso nosotros: openssl rsautl -decrypt -inkey private_key.pem -in secret.enc Ahora, crear\u00e1 un resumen de hash del mensaje y, luego, una firma digital de este resumen. Una vez que haya terminado, verificar\u00e1 la firma del resumen. De esta manera, podr\u00e1 asegurarse de que el mensaje no se haya modificado ni falsificado. Si se modific\u00f3 el mensaje, el hash ser\u00e1 diferente del firmado, y la verificaci\u00f3n no se completar\u00e1 correctamente. Para crear un resumen de hash del mensaje, ingrese el siguiente comando: openssl dgst -sha256 -sign private_key.pem -out secret.txt.sha256 secret.txt Se crear\u00e1 un archivo llamado \"secret.txt.sha256\" con su clave privada, que incluye el resumen de hash de su archivo de texto secreto. Con este archivo, cualquiera puede usar su clave p\u00fablica y el resumen de hash para verificar que el archivo no se haya modificado desde que lo cre\u00f3 y encript\u00f3. Para realizar esta verificaci\u00f3n, ingrese el siguiente comando: openssl dgst -sha256 -verify public_key.pem -signature secret.txt.sha256 secret.txt Se mostrar\u00e1 el siguiente resultado, que indica que la verificaci\u00f3n se realiz\u00f3 correctamente y que ning\u00fan tercero malicioso modific\u00f3 el archivo.","title":"Crear o inspeccionar pares de claves, encriptar o desencriptar, y firmar o verificar con OpenSSL"},{"location":"soporte/#como-usar-hashes","text":"En este lab, practicar\u00e1 c\u00f3mo usar y verificar hashes con las herramientas \"md5sum\" y \"shasum\". md5sum es un programa de hashing que calcula y verifica hashes de MD5 de 128 bits. Al igual que con todos los algoritmos de hashing, en teor\u00eda, hay un n\u00famero ilimitado de archivos que tendr\u00e1 un hash de MD5 determinado. Puede usar \"md5sum\" para verificar la integridad de los archivos. Del mismo modo, shasum es un programa de encriptaci\u00f3n que calcula y verifica hashes de SHA. Tambi\u00e9n se usa para verificar la integridad de los archivos. Este comando crea un archivo de texto llamado \"file.txt\" que tiene una sola l\u00ednea de texto b\u00e1sico: echo 'This is some text in a file, just so we have some data' > file.txt Ahora, generar\u00e1 la suma MD5 del archivo y la almacenar\u00e1. Para generar la suma de su archivo nuevo, ingrese el siguiente comando \"md5sum\": md5sum file.txt > file.txt.md5 Sin embargo, lo m\u00e1s significativo es que tambi\u00e9n puede verificar que el hash sea correcto y que el archivo original no haya sido manipulado desde que se cre\u00f3 la suma. Para ello, escriba el siguiente comando y observe el resultado mostrado a continuaci\u00f3n, que indica que el hash es v\u00e1lido: md5sum -c file.txt.md5 SHA1 y SHA256 brindan m\u00e1s seguridad que MD5; y SHA256 es m\u00e1s seguro que SHA1. Esto significa que es m\u00e1s f\u00e1cil que un tercero malicioso ataque un sistema con MD5 que con SHA1. Adem\u00e1s, debido a que SHA256 es el hash m\u00e1s seguro de los tres, es el que m\u00e1s se usa actualmente. shasum file.txt > file.txt.sha1 A continuaci\u00f3n, use el siguiente comando para verificar el hash. (Al igual que antes, este proceso fallar\u00e1 si se modific\u00f3 el archivo original). shasum -c file.txt.sha1 Puede usar la misma herramienta para crear una suma SHA256. El marcador \"-a\" especifica el algoritmo que se desea usar y, si no especifica ning\u00fan valor, se seleccionar\u00e1 SHA1 como valor predeterminado. Para generar la suma SHA256, use el siguiente comando: shasum -a 256 file.txt > file.txt.sha256 La seguridad adicional de SHA256 se debe a que crea un hash m\u00e1s largo y dif\u00edcil de adivinar. Se puede observar que el contenido de este archivo es mucho m\u00e1s largo que el del archivo de SHA1. Por \u00faltimo, para verificar la suma SHA256, puede usar el mismo comando que antes: shasum -c file.txt.sha256","title":"C\u00f3mo usar hashes"},{"location":"soporte/#practica-tcpdump","text":"En este lab, presentaremos \"tcpdump\" y algunas de sus funciones. \"tcpdump\" es la herramienta de an\u00e1lisis de red m\u00e1s importante para los profesionales de redes y seguridad de la informaci\u00f3n. Como especialista en asistencia de TI, es fundamental que conozca bien esta aplicaci\u00f3n si desea entender TCP/IP. \"tcpdump\" lo ayudar\u00e1 a mostrar el tr\u00e1fico de red de una manera m\u00e1s f\u00e1cil para analizar y solucionar problemas. Para comenzar, ingresaremos \"tcpdump\" y lo ejecutaremos sin ninguna opci\u00f3n. Tenga en cuenta que, como tcpdump requiere privilegios de administrador para capturar el tr\u00e1fico, todos los comandos deben comenzar con sudo. Como m\u00ednimo, debe especificar una interfaz en la que buscar con la marca -i. Recomendamos usar ip link para verificar el nombre de la interfaz de red principal. En este caso, usaremos la interfaz eth0 para todos los ejemplos, aunque no es necesariamente la que usar\u00eda en su propia m\u00e1quina. Si desea usar tcpdump para comenzar a buscar los paquetes en la interfaz, ingrese el siguiente comando. sudo tcpdump -i eth0 Paramos con CONTROL + C Para habilitar un an\u00e1lisis m\u00e1s completo, use la marca -v a fin de activar el resultado detallado. De manera predeterminada, \"tcpdump\" tambi\u00e9n intentar\u00e1 realizar b\u00fasquedas de DNS inversas a fin de resolver las direcciones IP en nombres de host, adem\u00e1s de reemplazar los n\u00fameros de puerto con nombres de servicios com\u00fanmente asociados. Puede usar la marca -n para inhabilitar este comportamiento. Recomendamos usar esta marca para evitar generar tr\u00e1fico adicional a partir de las b\u00fasquedas de DNS y a fin de acelerar el an\u00e1lisis. Ingrese el siguiente comando para probarlo: sudo tcpdump -i eth0 -vn Sin la marca de resultado detallado, \"tcpdump\" solo proporciona lo siguiente: el protocolo de Layer-3, las direcciones y los puertos de origen y destino detalles de TCP, como marcas, secuencia y n\u00fameros de ack, el tama\u00f1o de la ventana y las opciones Si activa la marca de resultado detallado, tambi\u00e9n obtendr\u00e1 toda la informaci\u00f3n sobre el encabezado de IP, como el tiempo de vida, el n\u00famero de ID de IP, las opciones de IP y las marcas de IP. Filtrado : A continuaci\u00f3n, analizaremos brevemente el lenguaje de filtrado de tcpdump, junto con el an\u00e1lisis de protocolo. tcpdump admite un lenguaje potente para el filtrado de paquetes que le permite capturar solo el tr\u00e1fico que le interesa o que desea analizar. Las reglas de filtrado van al final del comando, despu\u00e9s de especificar el resto de las marcas. Solo usaremos el filtrado para capturar tr\u00e1fico de DNS a un servidor DNS espec\u00edfico. A continuaci\u00f3n, generaremos tr\u00e1fico de DNS a fin de demostrar la capacidad de \"tcpdump\" de interpretar consultas y respuestas de DNS. sudo tcpdump -i eth0 -vn host 8.8.8.8 and port 53 Analicemos c\u00f3mo est\u00e1 formado este filtro y qu\u00e9 hace exactamente. Host 8.8.8.8 especifica que solo queremos paquetes cuya direcci\u00f3n IP de origen o destino coincida con lo que indicamos (en este caso, 8.8.8.8). Si solo queremos tr\u00e1fico en una direcci\u00f3n, tambi\u00e9n podr\u00edamos agregar un calificador de direcci\u00f3n, como dst o src (para las direcciones IP de destino y origen respectivamente). Sin embargo, si no incluye el calificador, se captar\u00e1 el tr\u00e1fico en cualquier direcci\u00f3n. La parte port 53 indica que solo queremos ver paquetes cuyo puerto de origen o destino coincida con lo que especificamos (en este caso, DNS). Estos dos filtros se unen mediante el operador l\u00f3gico \"and\", lo que significa que ambas partes deben ser verdaderas para que nuestro filtro capture un paquete. En otra terminal dig @8.8.8.8 A example.com . Este comando usa la herramienta dig para consultar un servidor DNS espec\u00edfico (en este caso, 8.8.8.8) y le pide el A record del dominio especificado (en este caso, \"example.com\"). El primero es la consulta de DNS, que es nuestra pregunta (de la segunda terminal) que va al servidor. Tenga en cuenta que, en este caso, el tr\u00e1fico es UDP. El an\u00e1lisis que tcpdump hace de la consulta de DNS comienza inmediatamente despu\u00e9s del campo checksum de UDP. Empieza con el n\u00famero de ID de DNS, seguido por algunas opciones de UDP y, por \u00faltimo, el tipo de consulta (en este caso, A?, que indica que pedimos un A record). A continuaci\u00f3n, sigue el nombre de dominio que nos interesa (example.com). El segundo paquete es la respuesta del servidor, que incluye el mismo ID de DNS de la consulta original, seguido de la consulta original. A continuaci\u00f3n, se encuentra la respuesta a la consulta, que contiene la direcci\u00f3n IP asociada con el nombre de dominio. C\u00f3mo guardar paquetes capturados: sudo tcpdump -i eth0 port 80 -w http.pcap Especifique el puerto \"80\" para comenzar una captura en nuestra interfaz \"eth0\" que solo filtre el tr\u00e1fico de HTTP. La marca -w indica que queremos guardar los paquetes capturados en un archivo llamado http.pcap. Una vez que se est\u00e9 ejecutando, vuelva a la segunda terminal, en la que generar\u00e1 algo de tr\u00e1fico http que se capturar\u00e1 en la terminal original. Todav\u00eda no detenga la captura que comenz\u00f3 con el comando anterior (si ya lo hizo, puede reiniciar ahora): curl example.com Este comando obtiene el HTML de example.com y lo muestra en la pantalla. Deber\u00eda verse de la siguiente manera (tenga en cuenta que aqu\u00ed solo se muestra la primera parte del resultado). Tambi\u00e9n se habr\u00e1 creado un archivo binario, llamado http.pcap, que contiene los paquetes que acabamos de capturar. No intente mostrar el contenido de este archivo en la pantalla. Como es un archivo binario, se mostrar\u00e1 como texto confuso que no podr\u00e1 leer. Para leer bien el fichero binario usamos el comando: tcpdump -r http.pcap -nv","title":"Pr\u00e1ctica TCPDUMP"},{"location":"soporte/#proyecto-de-seguridad-de-empresa","text":"Autenticaci\u00f3n La autenticaci\u00f3n ser\u00e1 manejada centralmente por un servidor LDAP e incorporar\u00e1 generadores de contrase\u00f1a de un solo uso como un segundo factor para la autenticaci\u00f3n. Sitio web externo El sitio web orientado al cliente se servir\u00e1 a trav\u00e9s de HTTPS, ya que servir\u00e1 un sitio de comercio electr\u00f3nico que permitir\u00e1 a los visitantes navegar y comprar productos, as\u00ed como crear e iniciar sesi\u00f3n en cuentas. Este sitio web ser\u00eda de acceso p\u00fablico. Sitio interno El sitio web interno de los empleados tambi\u00e9n se servir\u00e1 a trav\u00e9s de HTTPS, ya que requerir\u00e1 autenticaci\u00f3n para que los empleados accedan. Tambi\u00e9n solo ser\u00e1 accesible desde la red interna de la empresa y solo con una cuenta autenticada. Acceso remoto Dado que los ingenieros requieren acceso remoto a sitios web internos, as\u00ed como acceso de l\u00ednea de comando remoto a estaciones de trabajo, se necesitar\u00e1 una soluci\u00f3n VPN de nivel de red, como OpenVPN. Para facilitar el acceso al sitio web interno, se recomienda un proxy inverso, adem\u00e1s de VPN. Ambos confiar\u00edan en el servidor LDAP que se mencion\u00f3 anteriormente para la autenticaci\u00f3n y autorizaci\u00f3n. Firewall Se requerir\u00eda un dispositivo de Firewall basado en la red. Incluir\u00eda reglas para permitir el tr\u00e1fico para varios servicios, comenzando con una regla de denegaci\u00f3n impl\u00edcita y luego abriendo puertos de forma selectiva. Tambi\u00e9n se necesitar\u00e1n reglas para permitir el acceso p\u00fablico al sitio web externo, y para permitir el tr\u00e1fico al servidor proxy inverso y al servidor VPN. Inal\u00e1mbrico Para la seguridad inal\u00e1mbrica, se debe usar 802.1X con EAP-TLS. Esto requerir\u00eda el uso de certificados de cliente, que tambi\u00e9n se pueden usar para autenticar otros servicios, como VPN, proxy inverso y autenticaci\u00f3n interna de sitios web. 802.1X es m\u00e1s seguro y se administra m\u00e1s f\u00e1cilmente a medida que la empresa crece, lo que la convierte en una mejor opci\u00f3n que WPA2. VLANs Se recomienda incorporar VLAN en la estructura de la red como una forma de segmentaci\u00f3n de la red; esto facilitar\u00e1 el control del acceso a diversos servicios. Las VLAN se pueden crear para roles o funciones generales para dispositivos y servicios. Se puede utilizar una VLAN de ingenier\u00eda para colocar todas las estaciones de trabajo de ingenier\u00eda y los servicios de ingenier\u00eda. Se puede usar una VLAN de infraestructura para todos los dispositivos de infraestructura, como puntos de acceso inal\u00e1mbricos, dispositivos de red y servidores cr\u00edticos como la autenticaci\u00f3n. Una VLAN de ventas puede usarse para m\u00e1quinas que no sean de ingenier\u00eda y una VLAN invitada ser\u00eda \u00fatil para otros dispositivos que no se ajusten a las otras asignaciones de VLAN. Seguridad para laptop A medida que la compa\u00f1\u00eda maneja la informaci\u00f3n de pago y los datos del usuario, la privacidad es una gran preocupaci\u00f3n. Las laptops deben tener cifrado de disco completo (FDE) como un requisito, para protegerse contra el acceso no autorizado a los datos en caso de p\u00e9rdida o robo de un dispositivo. El software antivirus tambi\u00e9n se recomienda encarecidamente para evitar infecciones de malware com\u00fan. Para protegerse contra ataques poco comunes y amenazas desconocidas, se recomienda el software de lista blanca binario, adem\u00e1s del software antivirus. Pol\u00edtica de aplicaci\u00f3n Para mejorar a\u00fan m\u00e1s la seguridad de las m\u00e1quinas cliente, se debe implementar una pol\u00edtica de aplicaci\u00f3n para restringir la instalaci\u00f3n de software de terceros solo a las aplicaciones relacionadas con las funciones de trabajo. Espec\u00edficamente, las categor\u00edas de aplicaci\u00f3n riesgosas y legalmente cuestionables deben ser expl\u00edcitamente prohibidas. Esto incluir\u00eda cosas como software pirateado, generadores de claves de licencia y software descifrado. Adem\u00e1s de las pol\u00edticas que restringen algunas formas de software, tambi\u00e9n se debe incluir una pol\u00edtica que requiera la instalaci\u00f3n oportuna de los parches de software. \"Oportunamente\" en este caso se definir\u00e1 como 30 d\u00edas a partir de la amplia disponibilidad del parche. Pol\u00edtica de privacidad de datos del usuario A medida que la empresa toma la privacidad del usuario muy en serio, algunas pol\u00edticas s\u00f3lidas sobre el acceso a los datos del usuario son un requisito cr\u00edtico. Los datos de usuario solo deben accederse para fines de trabajo espec\u00edficos, relacionados con una tarea o un proyecto en particular. Se deben realizar solicitudes de datos espec\u00edficos, en lugar de solicitudes exploratorias demasiado amplias. Las solicitudes deben ser revisadas y aprobadas antes de otorgar el acceso. Solo despu\u00e9s de la revisi\u00f3n y aprobaci\u00f3n se otorgar\u00e1 a una persona el acceso a los datos espec\u00edficos del usuario solicitados. Las solicitudes de acceso a los datos del usuario tambi\u00e9n deben tener una fecha de finalizaci\u00f3n. Adem\u00e1s de acceder a los datos del usuario, tambi\u00e9n es importante definir las pol\u00edticas relacionadas con el manejo y almacenamiento de los datos del usuario. Esto ayudar\u00e1 a evitar que los datos del usuario se pierdan y caigan en las manos equivocadas. No se deben permitir los datos de usuario en dispositivos de almacenamiento port\u00e1tiles, como llaves USB o discos duros externos. Si es necesaria una excepci\u00f3n, se debe utilizar un disco duro port\u00e1til cifrado para transportar los datos del usuario. Los datos de usuario en reposo siempre deben estar contenidos en medios encriptados para protegerlos del acceso no autorizado. Pol\u00edtica de seguridad Para garantizar que se usen contrase\u00f1as seguras, se debe aplicar la siguiente pol\u00edtica de contrase\u00f1a: La contrase\u00f1a debe tener una longitud m\u00ednima de 8 caracteres La contrase\u00f1a debe incluir un m\u00ednimo de un car\u00e1cter especial o un signo de puntuaci\u00f3n La contrase\u00f1a debe cambiarse una vez cada 12 meses Adem\u00e1s de estos requisitos de contrase\u00f1a, todos los empleados deben completar una capacitaci\u00f3n de seguridad obligatoria una vez al a\u00f1o. Esto deber\u00eda cubrir escenarios comunes relacionados con la seguridad, como evitar ser v\u00edctimas de ataques de phishing, buenas pr\u00e1cticas para mantener a salvo tu laptop y nuevas amenazas que han surgido desde la \u00faltima vez que se tom\u00f3 el curso. Sistemas de detecci\u00f3n o prevenci\u00f3n de intrusos Se recomienda un sistema de detecci\u00f3n de intrusos en la red para observar la actividad de la red en busca de signos de un ataque o infecci\u00f3n de malware. Esto permitir\u00eda buenas capacidades de monitoreo sin incomodar a los usuarios de la red. Se recomienda un sistema de prevenci\u00f3n de intrusiones en la red (NIPS) para la red donde se encuentran los servidores que contienen datos de usuario; este contiene datos mucho m\u00e1s valiosos, que es m\u00e1s probable que sean atacados en un ataque. Adem\u00e1s de la Prevenci\u00f3n de intrusiones en la red, tambi\u00e9n se recomienda que se instale en estos servidores el software de detecci\u00f3n de intrusiones basado en host (HIDS) para mejorar la supervisi\u00f3n de estos importantes sistemas.","title":"PROYECTO DE SEGURIDAD DE EMPRESA"},{"location":"soporte/#reglas-ejemplo-firewall","text":"Mostrando el estado de nuestro firewall: iptables -L -n -v -L : Muestra las reglas. -v : Muestra informaci\u00f3n detallada. -n : Muestra la direcci\u00f3n ip y puerto en formato num\u00e9rico. No usa DNS para resolver nombres. Esto acelera la lista. --line-numbers, numerar las lineas. Mostrar las reglas de cadena de entrada y salida: iptables -L INPUT -n -v iptables -L OUTPUT -n -v --line-numbers Parar / Iniciar / Reiniciar el firewall: service iptables stop/start/restart Tambi\u00e9n se puede usar propio comando iptables para detenerlo y borrar todas las reglas: iptables -F iptables -X iptables -t nat -F iptables -t nat -X iptables -t mangle -F iptables -t mangle -X iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT -F : Borra todas las reglas. -X : Borra cadenas. -t table_name : Selecciona una tabla y elimina reglas. -P : Establece la pol\u00edtica por defecto (como DROP, REJECT o ACCEPT). ACCEPT/DROP/REJECT: las decisiones que puede tomar un cada regla de un filtro de paquetes pueden ser, dejar pasar el paquete (ACCEPT), responderle al emisor educadamente que ese paquete no puede pasar (REJECT) o bien simplemente descartarlo como si no hubiera llegado (DROP o DENY). Observamos que la diferencia entre REJECT y DROP consiste en que mediante REJECT se le contesta que el servicio no est\u00e1 disponible (icmp destination port unrechable) evitando as\u00ed demoras en la conexi\u00f3n y mediante DROP no se le contesta nada por lo cual el sistema remoto no corta la conexi\u00f3n hasta que ha transcurrido el tiempo de espera de la contestaci\u00f3n con la consiguiente ralentizaci\u00f3n. Para la red local es aconsejable usar REJECT aunque cada administrador tiene que estudiar su situaci\u00f3n. Borrar reglas del firewall: iptables -L INPUT -n --line-numbers iptables -L OUTPUT -n --line-numbers iptables -L OUTPUT -n --line-numbers | less iptables -L OUTPUT -n --line-numbers | grep 202.54.1.1 Obtendrendremos la lista de IPs. Miramos el n\u00famero de la izquierda y lo usamos para borrarla. Por ejemplo para borrar la l\u00ednea 4: iptables -D INPUT 4 O para encontrar una ip de origen y borrarla de la regla iptables -D INPUT -s 202.54.1.1 -j DROP -D : Elimina una o m\u00e1s reglas de la cadena seleccionada. Insertar reglas: iptables -L INPUT -n --line-numbers iptables -I INPUT 2 -s 202.54.1.2 -j DROP Guardar reglas: service iptables save Restaurar reglas: iptables-restore < /root/my.active.firewall.rules service iptables restart Para borrar todo el tr\u00e1fico: iptables -P INPUT DROP iptables -P OUTPUT DROP iptables -P FORWARD DROP iptables -L -v -n Para borrar todos los paquetes entrantes / enviados pero permitir el tr\u00e1fico saliente: iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT ACCEPT iptables -A INPUT -m state --state NEW,ESTABLISHED -j ACCEPT iptables -L -v -n Borrar direcciones de red privadas en la interfaz p\u00fablica: iptables -A INPUT -i eth1 -s 192.168.0.0/24 -j DROP iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP IP Spoofing es nada m\u00e1s que para detener los siguientes rangos de direcciones IPv4 para redes privadas en sus interfaces p\u00fablicas. Los paquetes con direcciones de origen no enrutables deben rechazarse. Bloqueando una direci\u00f3n IP (BLOCK IP): iptables -A INPUT -s 1.2.3.4 -j DROP Bloquear peticiones entrantes de un puerto (BLOCK PORT): Para bloquear todas las solicitudes de servicio en el puerto 80: iptables -A INPUT -p tcp --dport 80 -j DROP iptables -A INPUT -i eth1 -p tcp --dport 80 -j DROP Para bloquear el puerto 80 para una ip: iptables -A INPUT -p tcp -s 1.2.3.4 --dport 80 -j DROP iptables -A INPUT -i eth1 -p tcp -s 192.168.1.0/24 --dport 80 -j DROP Bloquear el dominio facebook.com: Primero, encontrar la direcci\u00f3n ip de facebook.com host -t a www.facebook.com Salida: www.facebook.com has address 69.171.228.40 Buscar el CIDR para 69.171.228.40: whois 69.171.228.40 | grep CIDR Salida: CIDR: 69.171.224.0/19 Para prevenir el acceso externo a facebook.com: iptables -A OUTPUT -p tcp -d 69.171.224.0/19 -j DROP Podemos usar tambi\u00e9n nombres de dominio: iptables -A OUTPUT -p tcp -d www.facebook.com -j DROP iptables -A OUTPUT -p tcp -d facebook.com -j DROP Bloquear ips de salida: Para bloquear el tr\u00e1fico saliente a un host o dominio en concreto como por ejemplo cyberciti.biz: host -t a cyberciti.biz Salida: cyberciti.biz has address 75.126.153.206 Una vez conocida la direcci\u00f3n ip, bloqueamos todo el tr\u00e1fico saliente para dicha ip as\u00ed: iptables -A OUTPUT -d 75.126.153.206 -j DROP Se puede usar una subred como la siguiente: iptables -A OUTPUT -d 192.168.1.0/24 -j DROP iptables -A OUTPUT -o eth1 -d 192.168.1.0/24 -j DROP En el ejemplo con iptables, se ha pasado autilizar el conjunto de reglas FORWARD por la diferencia de significado del conjunto de reglas INPUT en la implementaci\u00f3n de netfilter. Esto tiene implicaciones; significa que ninguna de las reglas protege el 'host' mismo del cortafuegos. Para imitar con precisi\u00f3n el ejemplo con ipchains, se replicar\u00eda cada una de las reglas de la cadena INPUT. En aras de la claridad, en su lugar se ha decidido eliminar todos los datagramas entrantes provenientes desde el lado de fuera de la interfaz: #!/bin/bash ########################################################################## # VERSI\ufffdN PARA IPTABLES # Este configuraci\ufffdn est\ufffd pensada como ejemplo de configuraci\ufffdn de # un cortafuegos sobre un 'host' \ufffdnico que no hospede \ufffdl mismo ning\ufffdn # servicio ########################################################################## # SECCI\ufffdN CONFIGURABLE POR EL USUARIO # El nombre y la localizaci\ufffdn de la utilidad iptables. IPTABLES=iptables # Ruta del ejecutable de iptables. PATH=\"/sbin\" # El espacio de direcciones de nuestra red interna y el dispositivo # de red que la soporta. OURNET=\"172.29.16.0/24\" OURBCAST=\"172.29.16.255\" OURDEV=\"eth0\" # Las direcciones de fuera y el dispositivo de red que la soporta. ANYADDR=\"0/0\" ANYDEV=\"eth1\" # Los servicios de TCP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los puertos # nota: separados por espacios TCPIN=\"smtp,www\" TCPOUT=\"smtp,www,ftp,ftp-data,irc\" # Los servicios de UDP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los puertos # nota: separados por espacios UDPIN=\"domain\" UDPOUT=\"domain\" # Los servicios de ICMP que deseamos permitir que pasen - un \"\" vac\ufffdo # significa todos los tipos # referencia para los n\ufffdmeros de los tipos: /usr/include/netinet/ip_icmp.h # nota: separados por espacios ICMPIN=\"0,3,11\" ICMPOUT=\"8,3,11\" # Registro; descomente la siguiente l\ufffdnea para habilitar el registro # de los datagramas rechazados por el cortafuegos # LOGGING=1 # FIN DE LA SECCI\ufffdN CONFIGURABLE POR EL USUARIO ########################################################################### # Borra las reglas de la cadena de entrada $IPTABLES -F FORWARD # # Por defecto, queremos denegar el acceso a los intentos de entrada $IPTABLES -P FORWARD deny # Rechaza todos los datagramas destinados a este host y recibidos # desde fuera. $IPTABLES -A INPUT -i $ANYDEV -j DROP # SUPLANTACI\ufffdN DE IDENTIDAD # No se deber\ufffda aceptar ning\ufffdn datagrama proveniente de fuera con una # direccci\ufffdn de origen coincidente con una de las nuestras, por # eso las rechazamos. $IPTABLES -A FORWARD -s $OURNET -i $ANYDEV -j DROP # 'SMURF' # No se permiten difusiones dirigidas de ICMP a nuestra red para evitar # los ataques del estilo denominado 'Smurf'. $IPTABLES -A FORWARD -m multiport -p icmp -i $ANYDEV -d $OURNET -j DENY # Deber\ufffdamos aceptar fragmentos, esto se debe explicitar en iptables. $IPTABLES -A FORWARD -f -j ACCEPT # TCP # Aceptaremos todos los datagramas de TCP que pertenezcan a una # conexi\ufffdn ya existente (i.e. cuyo bit de ACK valga 1) # en el caso de los puertos de TCP que estamos permitiendo. # Esto deber\ufffda capturar m\ufffds del 95% de todos los paquetes v\ufffdlidos de TCP. $IPTABLES -A FORWARD -m multiport -p tcp -d $OURNET --dports $TCPIN / ! --tcp-flags SYN,ACK ACK -j ACCEPT $IPTABLES -A FORWARD -m multiport -p tcp -s $OURNET --sports $TCPIN / ! --tcp-flags SYN,ACK ACK -j ACCEPT # TCP - CONEXIONES ENTRANTES # Aceptaremos \ufffdnicamente las solicitudes de conexi\ufffdn desde # fuera en los puertos de TCP permitidos. $IPTABLES -A FORWARD -m multiport -p tcp -i $ANYDEV -d $OURNET $TCPIN / --syn -j ACCEPT ## TCP - CONEXIONES SALIENTES # Aceptaremos todas las conexiones salientes de TCP hacia los puertos # de TCP permitidos $IPTABLES -A FORWARD -m multiport -p tcp -i $OURDEV -d $ANYADDR / --dports $TCPOUT --syn -j ACCEPT # UDP - ENTRADA ## Aceptaremos la entrada y vuelta de los datagramas de UDP por puertos #permitidos. $IPTABLES -A FORWARD -m multiport -p udp -i $ANYDEV -d $OURNET / --dports $UDPIN -j ACCEPT $IPTABLES -A FORWARD -m multiport -p udp -i $ANYDEV -s $OURNET / --sports $UDPIN -j ACCEPT # UDP - SALIDA # Se aceptar\ufffdn la salida de los datagramas de UDP hacia los puertos permitidos y su vuelta. $IPTABLES -A FORWARD -m multiport -p udp -i $OURDEV -d $ANYADDR / --dports $UDPOUT -j ACCEPT $IPTABLES -A FORWARD -m multiport -p udp -i $OURDEV -s $ANYADDR / --sports $UDPOUT -j ACCEPT # ICMP - ENTRADA # Aceptaremos la entrada de los datagramas de ICMP de los tipos permitidos $IPTABLES -A FORWARD -m multiport -p icmp -i $ANYDEV -d $OURNET / --dports $ICMPIN -j ACCEPT # ICMP - SALIDA # Aceptaremos la salida de los datagramas de ICMP de los tipos permitidos. $IPTABLES -A FORWARD -m multiport -p icmp -i $OURDEV -d $ANYADDR / --dports $ICMPOUT -j ACCEPT # CASO POR DEFECTO y REGISTRO # Todos los restantes datagramas caen dentro de la regla por defecto # y son eliminados. Ser\ufffdn registrados si m\ufffds arriba se ha configurado # la variable LOGGING. # if [ \"$LOGGING\" ] then # Registra los paquetes de TCP descartados $IPTABLES -A FORWARD -m tcp -p tcp -j LOG # Registra los paquetes de UDP descartados $IPTABLES -A FORWARD -m udp -p udp -j LOG # Registra los paquetes de ICMP descartados $IPTABLES -A FORWARD -m udp -p icmp -j LOG fi # # fin.","title":"REGLAS EJEMPLO FIREWALL"},{"location":"soporte/#gestion-de-acceso-remoto","text":"","title":"GESTION DE ACCESO REMOTO"},{"location":"soporte/#crear-vpn","text":"Esquina abajo derecha - VPN: Agregar nueva conexion VPN, proveedor windows, nombre de conexion, nombre del servidor(vpn.ejemplo.com), tipo de vpn(automatico) y tipo de inicio de sesion(nombre/pass). Tambien podemos modificarla y en opciones avanzadas, boton derecho propiedades. Podemos crear otra de manera mas clasica dando a centro de redes y recursos compartidos, nueva conexion vpn y ponemos nombre e ip. En seguridad de propiedades avanzadas en ikev2 si est\u00e1 activa la opcion de MODALIDAD sirve para reconectar automaticamente si se pierde la conexion de esta vpn.","title":"CREAR VPN"},{"location":"soporte/#escritorio-remoto","text":"En simbolo de sistema poniendo mstsc o en la barra de inicio poniendo ESCRITORIO REMOTO nos sale la ventana para conectarnos. Tenemos que tener activada la opcion de escritorio remoto de nuestro pc. Con mstsc /? vemos todas las opciones que podemos hacer. Como ejemplo mstsc /v: ip . En mas opciones, podemos poner nombre, usuario, colores, tama\u00f1o, recursos, rendimiento...Tmabien si tenemos un fichero RDP de la maquina virtual u otro equipo a conectarse para conectarse directamente. mstsc /v:server :3389 para ver como estan los puertos o netstat -A . En windows defender tambien salen cositas para que no nos de problemas de conexion remotas. En reglas de salida, podemos poner nueva regla - puerto - tipo de conexion y puerto que queremos usar - permitir conexion - dominio/privado/publico - nombre de la regla. La herramienta RDC de windows te permite la conexion remota. Hay que descargar esta herramienta. Hay tipo de conexiones para empresas en conexion remoto: BranchCache(reduce el ancho de banda) y DirectAcess, podemos conectarnos desde casa como si fuesemos el admin de server.","title":"ESCRITORIO REMOTO"},{"location":"soporte/#opciones-energia","text":"Panel de control - energia. Por comando como administrador ponemos en c powercfg /? En directivas de grupo en la barra de busqueda: conf equipo - plantillas admin - sistema - admin energia y ahi podemos activar o desactivas directivas.","title":"OPCIONES ENERGIA"},{"location":"soporte/#centro-de-conexion","text":"En el panel de control hay centro de sincronizacion para poder poner carpetas compartidas antes de usar sin conexion, por si falla algo poder acceder. Con el comando gpedit tenemos en usuario - plantillas admin - red - archivos de conexion, directivas de grupo para el tema de archivos de conexion. Como por ejemplo habilitar la sincronizaacion dd archivos en redes estimadas. Las carpetas de trabajo se hace el admin habilitando en las directivas de grupo local para que el cliente pueda conectarse a estas carpetas.","title":"CENTRO DE CONEXION"},{"location":"soporte/#actualizacion-y-recuperacion","text":"En SISTEMA , propiedades del sistema, selecionamos la unidad C configurar y activamos la proteccion del sistema para poder hacer cambios, restaurar sistema, etc. En Sistema, propiedades - proteccion del sistema, podemos crear un punto de restauracion. Igualmente, windows crea automaticamente tambien puntos de restauraciones. Podemos eliminar los puntos de restauracion eliminando la proteccion del sistema o en la opcion mas abajo que pone eliminar. Tambien en la herramienta de liberacion de espacio en disco en la barra de inicio y seleccionamos la unidad y lo que queremos limpiar. Con la tecla SHIFT Y REINICIAR nos sale opciones diferentes de apagado y solucionar problemas para poder restaurar sistema, formatear, etc. RECOMENDACION DE DESACTIVAR LOS ANTIVIRUS AL HACER RESTAURACIONES. Cuando restablecemos equipo manteniendo los archivos nos saldr\u00e1 en la unidad C una carpeta de windows.old con lo viejo de la otra copia y en el escritorio un fichero con los programas desintalados. WINRE es un entorno de recuperacion de windows. Accedemos a este menu avanzado en Configuracion - actualizacion - recuperacion y reiniciar ahora en el avanzado. Tambien con SHIFT y reiniciar y solcionar problemas - opciones avanzadas. En opciones de recuperacion tambien podemos crear copias de seguridad o crear imagen del sistema para poder luego restaurar desde esa imagen desde inicio avanazado de windows. En copia de seguridad podemos agregar dispositivo externo por ejemplo, y seleccionarlo para meterlas ahi. En configuracion - seguridad - copias de seguridad la tenemos que tener activada y en m\u00e1s opciones, configurar cuando se hacen y personalizarla. Tambien esta recuperar archivos desde historial y vemos las diferents versiones de archivos en carpetas y poder restaurar el archivo. En panel de control - historial de archivos - excluir archivos, podemos poner aqui las carpetas que no queremos que se hagan en las copias de seguridad. En windows Update podemos configurar cuando instalar en opciones avanzadas. En cambiar las horas activas podemos configurar que horas no queremos que el sistema operativo no se reinicie para instalar nada. Podemos programar una hora para instalarlas. En el historial de actualizaciones podemos ver recomendaciones y desintalar actualizaciones. En opciones de entrega podemos ver el monitor de actividad para ver como va el consumo de las actualizaciones.","title":"ACTUALIZACION Y RECUPERACION"},{"location":"taller/","text":"lime y volability herramientas forense linux pfsense para controlar los firewalls y hacer de enrutador en una pyme. Entornos de pruebas virtuales con Docker + Jenkins + Selenoid DOCKER Tras instalarlo descargando el binario o a trav\u00e9s de repositorio v\u00eda Linux, podemos comenzar descargando la \u00faltima release estable de Jenkins y lanzando un contenedor: $ sudo docker pull jenkins/jenkins:lts $ sudo docker run -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkins/jenkins:lts detallando el comando: -p : : puerto, se indica que el contenedor tendr\u00e1 expuesto su puerto 8080 (derecha) y se enlazar\u00e1 con el puerto 8080 (izquierda). Es decir, sabiendo que vamos a levantar la aplicaci\u00f3n Jenkins dentro del contenedor, -v :: volumen, se indica que para el contenedor que estamos levantando, vamos a realizar la persistencia de la ruta indicada mediante el volumen cuyo nombre indicamos precedido de los dos puntos (:). Por tanto, todo lo que este contenedor albergue dentro de la ruta \u201c/var/jenkins_home\u201d, ser\u00e1 almacenado en el volumen de docker con nombre \u201cjenkins_home\u201c. Para consultar la lista de vol\u00famenes que tenemos activos podemos realizarlo mediante el comando: $ sudo docker volume ls jenkins/jenkins:lts : Nombre de la imagen de la que queremos construir un contenedor. Tras ejecutar el comando, se construir\u00e1 un contenedor de Jenkins y veremos el log de la ejecuci\u00f3n en todo momento. Habiendo realizado esta acci\u00f3n desde un terminal, \u00e9ste se quedar\u00e1 mostrando el log de todo lo que sucede en el contenedor. En el momento en el que apaguemos dicho contenedor, el terminal cerrar\u00e1 la tarea y nos devolver\u00e1 al prompt. Durante la construcci\u00f3n de Jenkins se mostrar\u00e1 una clave codificada que habremos de utilizar para identificarnos en Jenkins y comenzar a instalar plugins y preparar el servicio. Entre otras cosas podemos crear el usuario administrador (recomendable si no queremos tener que escribir esa contrase\u00f1a cifrada cada vez que queramos acceder). Podemos acceder a Jenkins introduciendo en el navegador la direcci\u00f3n \u201chttp://localhost:8080\u201c A partir de este momento, tras introducir la clave cifrada y configurar el usuario, podemos trabajar con Jenkins. JENKINS Se trata de un servidor de integraci\u00f3n contin\u00faa. No s\u00f3lo eso, tambi\u00e9n nos permite llevar a cabo distintas tareas de manera automatizada. Estas labores pueden llevarse a cabo de manera \u201csimple\u201d mediante un proyecto de tipo \u201cFreestyle\u201d o multiproyecto, o lo que, durante los \u00faltimos a\u00f1os ha cogido m\u00e1s fuerza, pipelines. El objetivo de este curso trata de generar un entorno simple de pruebas para que, a partir de este conocimiento, pod\u00e1is construir en base a algo m\u00e1s de investigaci\u00f3n, proyectos m\u00e1s robustos y complejos. Por lo que crearemos un proyecto de tipo Freestyle. Este tipo de proyectos se puede asemejar a un \u201cLego\u201d. En base a las piezas que podemos ir seleccionando (los distintos plugins que Jenkins incorpora), podemos importar c\u00f3digo desde un repositorio; lanzar un proyecto maven con ciertos par\u00e1metros; enviar un correo electr\u00f3nico con los resultados al finalizar la ejecuci\u00f3n\u2026 Todo ello de manera simple, introduciendo los par\u00e1metros pertinentes de acuerdo a la tarea que se desea llevar a cabo. SELENOID Escrito en Golang, nos permite desplegar un sistema de contenedores con Selenium en su interior. En s\u00ed mismo es un sistema donde podemos ejecutar nuestros proyectos de manera simple. S\u00f3lo indicando las caracter\u00edsticas del navegador que deseamos lanzar y enrutando correctamenta la direcci\u00f3n de ejecuci\u00f3n, podemos ver a tiempo real como se ejecutan las instancias de los navegadores con nuestras pruebas. Esto incluye grabaci\u00f3n de v\u00eddeo seg\u00fan lo configuremos. Para ello, debemos partir de un fichero de configuraci\u00f3n. En este caso implementaremos el siguiente fichero: browsers.json { \"chrome\": { \"default\": \"89.0\", \"versions\": { \"89.0\": { \"image\": \"selenoid/vnc:chrome_89.0\", \"port\": \"4444\", \"path\": \"/\" } } } } En su contenido estamos indicando a Selenoid que debe integrar un controlador (y navegador) de Chrome, versi\u00f3n 89. Selenoid descargar el contenedor indicado desde DockerHub y lo pondr\u00e1 en marcha cuando sea oportuno. Como se puede ver, la imagen correspondiente a este navegador es: selenoid/vnc:chrome_89.0 DOCKER-COMPOSE Para poder trabajar con m\u00faltiples contenedores de manera simult\u00e1nea, lo m\u00e1s c\u00f3modo es utilizar docker-compose. Esta herramienta nos permite definir un esquema de contenedores describi\u00e9ndolos en un fichero con formato YAML. Existen varios \u201cversionados\u201d de ficheros docker-compose.yml , en este caso vamos a usar la 3.3. Para m\u00e1s informaci\u00f3n pod\u00e9is consultar la documentaci\u00f3n oficial de docker: https://docs.docker.com/compose/compose-file/compose-versioning/ A continuaci\u00f3n, el fichero que nosotros utilizaremos durante el taller: docker-compose.yml version: \"3.3\" volumes: jenkins_home: external: name: jenkins_home services: jenkins: build: ./jenkins_dockerfile/ network_mode: bridge links: - selenoid volumes: - \"jenkins_home:/var/jenkins_home\" ports: [\"9000:8080\",\"50000:50000\"] selenoid: image: \"aerokube/selenoid:latest-release\" network_mode: bridge volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" - \"./config/:/etc/selenoid/:ro\" ports: [\"4444:4444\"] selenoid-ui: image: \"aerokube/selenoid-ui\" network_mode: bridge links: - selenoid ports: [\"8080:8080\"] command: [\"--selenoid-uri\", \"http://selenoid:4444\"] Explicaci\u00f3n: Muy importante mantener el indentado correctamente. En los campos ra\u00edz (sin indentar): \u201cvolumes\u201c y \u201cservices\u201d, se se\u00f1ala a qu\u00e9 corresponde el \u00e1rbol de datos que contienen cada uno. En primer lugar encontramos \u201cvolumes\u201d, donde se indica el nombre del volumen de persistencia de datos que vamos a utilizar junto a los contenedores descritos m\u00e1s abajo. En este caso: jenkins_home En segundo lugar encontramos \u201cservices\u201d, esta secci\u00f3n hacer referencia a los contenedores que vamos a desplegar al ejecutar este fichero de docker-compose. En este caso, ser\u00e1n \u201cjenkins\u201c, \u201cselenoid\u201c y \u201cselenoid-ui\u201d. Los nombres son irrelevantes. S\u00f3lo sirven para detectarse unos a otros (de ah\u00ed las referencias que podemos ver en los campos \u201clinks\u201c y \u201ccommand\u201c. Donde, en este \u00faltimo, se utiliza el nombre para hacer referencia el dominio en la URL, sin necesidad de conocer la IP que dispondr\u00e1 el contenedor a la hora de ser desplegado. El apartado \u201cimage\u201c hace referencia a la imagen que tomar\u00e1 el contenedor como base para ser desplegado. Aunque esto es obvio. El campo \u201cnetwork_mode\u201c indica la red que va a utilizar el contenedor en cuesti\u00f3n. En docker podemos crear redes de comunicaci\u00f3n entre los distintos contenedores. Tantas como necesitemos, y hacer que los contenedores sean visibles unos entre o otros (o no). Para el objetivo de este taller no necesitamos crear ning\u00fan tipo de red. Indicando el modo \u201cbridge\u201c hacemos referencia a que todos van a estar dentro de la red por defecto. En el campo \u201cvolumes\u201c informamos al contenedor que debe usar la persistencia de los contenedores que se indican. En el caso de jenkins se informa que debe usar el volumen \u201cjenkins_home\u201c (que declaramos al inicio del documento), mientras que en selenoid indicamos las rutas que se van a compartir entre el host (nuestra m\u00e1quina local) y el propio contenedor. Esto quiere decir, por ejemplo, que aquellos ficheros que situ\u00e1semos en nuestra ruta \u201c./config/\u201c ser\u00edan visibles DENTRO del contenedor de selenoid, en la ruta \u201c/etc/selenoid/\u201c. Indicar tambi\u00e9n que el \u201c:ro\u201c en el caso de selenoid hace referencia a que el path \u201c./config/\u201c se toma como path de S\u00d3LO LECTURA (Read Only). Al igual vimos antes al correr el contenedor de Jenkins, \u201cports\u201c expondr\u00e1 el puerto del contenedor (derecha) con el puerto local (izquierda). Es decir, si escribimos \u201c9000:8080\u201d, quiere decir que nuestro puerto 9000 en la m\u00e1quina local, ser\u00e1 el mismo que el 8080 del contenedor. En el caso, de Jenkins, si estamos exponiendo \u201c9000:8080\u201d, el ir en nuestro navegador a \u201chttp://localhost:9000\u201c ser\u00eda exactamente lo mismo que, estando DENTRO el contenedor, ir al \u201chttp://localhost:8080\u201c. Finalmente, el campo \u201cbuild\u201d que podemos encontrar en el servicio de Jenkins hace que, en lugar de partir de una imagen base (como hacemos en los casos de selenoid), partimos de un fichero \u201cDockerfile\u201d (literalmente, un fichero sin extensi\u00f3n y con nombre \u201cDockerfile\u201d), desde el que se construir\u00e1 el contenedor en cuesti\u00f3n de Jenkins. Este \u00faltimo apartado lo veremos con mayor profunidad a continuaci\u00f3n. DOCKERFILE Este fichero se utiliza para construir contenedores con alg\u00fan tipo de configuraci\u00f3n \u201cextra\u201d que no se dispone de serie. En nuestro caso, el objetivo es instalar Python y su herramienta Pip para poder utilizarlas a la hora de ejecutar el proyecto. Para ello, a continuaci\u00f3n lo describimos: Dockerfile FROM jenkins/jenkins:lts USER root RUN apt-get update && \\ apt-get -y install python3 && \\ apt-get -y install python3-pip && \\ pip3 install --no-cache-dir selenium && \\ Explicaci\u00f3n: Aunque es f\u00e1cil de interpretar, en este caso es un fichero muy sencillo con una serie de comandos muy b\u00e1sica. A la hora de construir el contenedor, a docker se le dice: FROM (a partir de la imagen) jenkins/jenkins:lts y USER (utilizando el usuario) root entonces RUN (ejecuta los siguientes comandos) apt-get update apt-get -y install python3 apt-get -y install python3-pip pip3 install --no-cache-dir selenium Estos ser\u00e1n ejecutados durante la creaci\u00f3n del contenedor. Dada la imagen de la que partimos, cuando termine de construir el contenedor pondr\u00e1 en marcha el servicio de Jenkins igual que si lo hubi\u00e9ramos hecho sin el uso del Dockerfile. EJECUCI\u00d3N . \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 browsers.json \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 jenkins_dockerfile \u2514\u2500\u2500 Dockerfile Una vez tenemos listo todo, y todo ubicado en las carpetas correspondientes (fijaos en las rutas que se indican dentro del fichero docker-compose.yml), procedemos a levantar los contenedores mediante el comando: $ sudo docker-compose up Si deseasemos ignorar el log, podr\u00edamos utilizar el par\u00e1metro \u201c-d\u201d (detached) y la consola volver\u00eda a estar disponible tras ejecutar el comando. Ya teniendo en marcha los contenedores, al haber ejecutado al inicio el contenedor de Jenkins de manera independiente Y habiendo persistido los datos mediante el volumen \u201cjenkins_home\u201d, podremos ver que en esta ocasi\u00f3n jenkins ya no nos pregunta por la configuraci\u00f3n. En su lugar ya aparece configurado y \u00fanicamente debemos ponernos manos a la obra para crear el proyecto de pruebas. Como dijimos, utilizaremos el Freestyle como tipo de proyecto. Una vez creado, agregaremos la opci\u00f3n de \u201cimportar c\u00f3digo fuente\u201d mediante Git, e indicaremos el repositorio donde hemos situado el archivo de pruebas (en nuestro caso un fichero python con una prueba b\u00e1sica de navegaci\u00f3n). En mi caso pedir\u00e1 las credenciales por ser un repositorio privado. En el vuestro pod\u00e9is usar el que mejor os venga. Una vez indicado el repositorio, el siguiente paso ser\u00e1 la ejecuci\u00f3n. Nuestro fichero es un script en python, por lo que \u00fanicamente necesitar\u00edamos ejecutar un comando en la shell: python3 PRUEBAselenium.py Una vez introducido, guardamos el proyecto y, antes de ejecutarlo, abrimos una nueva ventana del navegador. En este caso nos dirigiremos a \u201chttp://localhost:8080\u201c, donde dispondremos de la interfaz de Selenoid, como ya vimos al principio del taller. Una vez en el dashboard, podemos darle a \u201cEjecutar proyecto en Jenkins\u201d y volver de nuevo a Selenoid para ver como se genera una nueva sesi\u00f3n de Chrome, en vivo. Y, finalmente, tras acabar la prueba, recibir el resultado dentro del proyecto de Jenkins. COMENTARIOS FINALES El objetivo de este taller ha sido dar una pincelada superficial a las distintas herramientas disponibles y la gran versatilidad que ofrece con ahondar s\u00f3lo un poco m\u00e1s. Los m\u00e1s experimentados sabr\u00e1n que no es la forma correcta de construir una arquitectura de sistema donde ejecutar las pruebas de un proyecto, pero estar\u00e1n de acuerdo conmigo en que tiene un objetivo meramente did\u00e1ctico. Ahora os toca a vosotros investigar sobre contenedores docker, acoplamiento de esclavos Jenkins sobre los que ejecutar las Build. Agregar m\u00e1s navegadores a Selenoid, incluso activando el sistema de grabaci\u00f3n de v\u00eddeo que este ofrece. S\u00f3lo espero que os haya servido de gu\u00eda para empezar en todo este tema. Un saludo y gracias por vuestra atenci\u00f3n ;) PRUEBA SELENIUM UTILIZADA DURANTE LA PRUEBA: PRUEBAselenium.py import unittest from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import time class prueba_selenium(unittest.TestCase): def setUp(self): print(\"me ejecuto antes de cada test\") #self.driver = webdriver.Chrome() self.driver = webdriver.Remote(command_executor=\"http://selenoid:4444/wd/hub\", desired_capabilities={'browserName': 'chrome', 'browserVersion':'89.0', 'selenoid:options':{'enableVNC':True}}) def test_a(self): print(\"me ejecuto en cada test A\") #GOOGLE self.driver.get(\"https://www.google.com\") time.sleep(5) #aceptamos cookies self.driver.find_element(By.XPATH, \"//div[text()='Acepto']/ancestor::button\").click() time.sleep(5) #buscamos texto wikipedia search_bar = self.driver.find_element(By.NAME, \"q\") time.sleep(5) search_bar.send_keys(\"wikipedia\") time.sleep(5) search_bar.send_keys(Keys.ENTER) #BUSQUEDA DE GOOGLE self.driver.find_element(By.XPATH, \"(//div[@id='search']/descendant::div[@class='g'])[1]/descendant::a[1]\").click() #WIKIPEDIA title = self.driver.title self.assertEqual(\"Wikipe, la enciclopedia libre\", title) pass def tearDown(self): print(\"me ejecuto despu\u00e9s de cada test\") self.driver.quit() pass if name == \" main \": unittest.main()","title":"Taller"},{"location":"terraform/","text":"TERRAFORM Tutoriales INSTALL sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" sudo apt-get update && sudo apt-get install terraform terraform -help touch ~/.bashrc terraform -install-autocomplete Creamos un primer recurso de prueba en la carpeta con extensi\u00f3n main.tf provider \"aws\"{ region = \"us-east-1\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" } IAM USERS Podemos crear un usuario en IAM y podemos asignarle tags, roles como accessAdministrator, y nos dar\u00eda los tokens para conectarse. Luego en users, en credentials podemos modificar y obtener nuevos keys o descargar el que ten\u00edamos. TERRAFORM INIT Una vez tenemos el directorio donde trabajamos con por ejemplo el simple fichero de main.tf , hacemos un terraform init para crear como el directorio de trabajo, muy parecido al git init. Te crea el terraform provider y otros ficheros ocultos. Ejemplo main.tf: provider \"aws\"{ region = \"us-east-1\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" } TERRAFORM PLAN Esta orden terraform plan sirve como para ver si estar\u00eda bien el proceso ejecutado antes de hacer un terraform aply y las cosas que se crear\u00edan con nuestro fichero de codigo. Ejemplo main.tf: provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" } TERRAFORM APPLY Si estamos contentos con el terraform plan, hacemos un terraform aply para que se ejecute lo indicado en el fichero. Se puede usar la opci\u00f3n --auto-approve cuando ya sabemos lo que va a crear y saltar todos las lineas de creaci\u00f3n. Lo creado se crea en el apartado de VPC . Virtual Private Cloud TERRAFORM DESTROY Se puede eliminar todo lo creado manualmente o con la orden terraform destroy VARIABLES Se crea en el fichero main.tf Variable sring: variable \"mystringvar\" { type = string default = \"my first var\" } Variable n\u00famero: variable \"mynumvar\" { type = number default = 100 } Variable booleana: variable \"isenabled\" { default = false } Variable lista: variable \"mylistvar\" { type = list(string) default = [\"apple\",\"20\"] } Variable map: variable \"mymapvar\" { type = map default = { Key1 = \"value1\" Key2 = \"value2\" } } Variable tuple: variable \"mytuplevar\" { type = tuple([string,number,string]) default = [\"var1\",10,\"var2\"] } Variable input: variable \"myinputvar\" { type = string description = \"please enter a vpc name\" } Varibale objecto: variable \"myobjectvar\" { type = object({name = string, port = list(number)}) default = { name = \"myports\" port = [22,80,443] } } Output. DOC : output \"myoutput\" { value = aws_vpc.myVPC.id } Recurso con alguna llamada a la variable: resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mystringvar } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mylistvar[0] } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mymapvar[\"Key2\"] } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.myinputvar } } En ficheros Si creas una variable pero sin darle valor default te sale un input al hacer un apply: variable \"subnet_prefix\" { description = \"cidr block de la subnet\" #default } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix availability_zone = \"us-east-1a\" } Tambi\u00e9n en vez de un input, pasando por variable con terraform apply -var \"subnet_prefix=10.10.1.0/24\" O en un fichero terraform-tfvars : subnet_prefix=\"10.10.1.0/24\" Si lo creamos con otro nombre, por ejemplo ejemplo.tfvars luego lo pasamos con un terraform apply -var-file example.tfvars O de tipo string y fichero ( subnet_prefix=[\"10.10.1.0/24\"] ): variable \"subnet_prefix\" { description = \"cidr block de la subnet\" type = string } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix availability_zone = \"us-east-1a\" } Y en un fichero terraform.tfvars hacer un apply normal Tambien con un listado: subnet_prefix=[\"10.10.1.0/24\", \"10.0.0.5/14\"] variable \"subnet_prefix\" { description = \"cidr block de la subnet\" type = string } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[1] availability_zone = \"us-east-1a\" } CREAR INSTANCIA Documentacion de fichero main.tf . Aqui descubrimos modulos, recursos etc que se pueden indicar y crear en el fichero. En el fichero main.tf: provider \"aws\" { region = \"us-east-1\" access_key = \"xxxx\" secret_key = \"xxxx\" } resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } Despues hariamos un terraform init, plan y aply. EIP EIP provider \"aws\" { region = \"us-east-1\" access_key = \"xxxx\" secret_key = \"xxxx\" } resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } resource \"aws_eip\" \"myEIP\" { instance = aws_instance.EC2.id } ouput \"myOutEIP\" { value = aws_eip.myEIP.public_ip } SECURITY GROUP Crear un security group: resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" security_groups = [aws_security_group.mySG.name] } resource \"aws_security_group\" \"mySG\" { name = \"Allow HTTPS\" ingress { description = \"allow https from any where\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { description = \"allow https from any where\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } } USER Ejemplo de crear un usuario con ciertas directrices: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_iam_user\" \"myname\"{ name = \"test-user\" } resource \"aws_iam_policy\" \"myPolicy\"{ name = \"test-custom-policy\" policy = <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"elasticloadbalancing:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"cloudwatch:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"iam:CreateServiceLinkedRole\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"iam:AWSServiceName\": [ \"autoscaling.amazonaws.com\", \"ec2scheduled.amazonaws.com\", \"elasticloadbalancing.amazonaws.com\", \"spot.amazonaws.com\", \"spotfleet.amazonaws.com\", \"transitgateway.amazonaws.com\" ] } } } ] } EOF } resource \"aws_iam_policy_attachment\" \"attachMyPolicy\"{ name = \"attachment\" users = [aws_iam_user.myname.name] policy_arn = aws_iam_policy.myPolicy.myPolicy_arn } la politicas las copiamos de IAM-politicas RDS Crear una instancia de base de datos: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_db_instance\" \"myRDS\"{ name = \"myDB\" instance_class = \"db.t2.micro\" allocated_storage = 5 storage_type = \"gp2\" engine = \"mysql\" engine_version = \"5.7\" username = \"test\" password = \"jupiter\" parameter_group_name = \"default.mysql5.7\" skip_final_snapshot = true port = 3306 } BUCKET Los buckets son depositos que se pueden crear en s3 para guardar cosas permanentes. ejemplo: main.tf provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"test\"{ cidr_block = \"10.0.0.0/16\" } backend.tf terraform{ backend \"s3\"{ key = \"terraform/tfstate.tfstate\" bucket \"el_creado_en_s3\" egion = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } } SCALING Podemos crear varios recursos del mismo tipo con count : provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" count = 3 } IMPORT Cuando creas un VPC puedes importar la info de lo creado en un fichero: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"myVPC\"{ cidr_block = \"10.0.0.0/16\" } Despues en la web vemos que id tenemos y hacemos en consola terraform import aws_vpc.myVPC.idxxx DEPENDS ON Cuando queremos crear algo que primero dependa de una cosa creada: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } resource \"aws_instance\" \"web\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" depends_on = [aws_instance.DB] } VALIDATE Podemos usar la orden terraform validate para ver que est\u00e9 correcta la sintaxi del fichero de configuraci\u00f3n que se crea. FORMAT Podemos usar la orden terraform fmt para ver que est\u00e9 correcta la anidaci\u00f3n del fichero de configuraci\u00f3n que se crea. MULTIPLOS PROVIDERS Podemos crear diferentes recursos en diferentes zonas y provedores. Ejemplo: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" alias = west } provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" alias = east } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" provider = aws.west } resource \"aws_instance\" \"web\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" depends_on = [aws_instance.DB] } LOCAL PROVISIONER Para usar un comando que haga cosas en local mientras se crea el codigo del fichero. provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" provisioner \"local-exec\"{ command = \"echo ${aws_instance.DB.private_ip}\" >> private_ip.txt } } REMOTE PROVISIONER Para usar un comando que haga cosas en REMOTO mientras se crea el codigo del fichero. provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" key_name = \"tf_test\" provisioner \"remote-exec\"{ inline = [ \"sudo dnf install nginx -y\" \"sudo systemctl start nginx\" ] } connection { type = \"ssh\" user = \"ec2-user\" private_key = file(\"./test.pem\") host = sef.public_ip } } PLAN DESTROY Para borrar los planes se usa terraform plan -destroy TARGET Sirve para destruir y aplicar un solo recurso en concreto al que quieras eliminar/a\u00f1adir. Tambi\u00e9n arrastrar\u00e1 a todo lo que lleve relacionado: terraform destroy -target aws_instance.web-server-instance WORKSPACES Se usa como si utilizaras diferentes ramas: terraform workspace terraform workspace new test terraform workspace list terraform workspace show terraform workspace select test terraform workspace delete test STATE Se puede utilizar diferentes comandos para ver estados: terraform state terraform state list terraform state show aws_eip.one(el recurso creado) FUNCTIONS Se pueden usar funciones EJERCICIO PR\u00c1CTICO En este ejercicio vamos a crear una instancia con ubuntu y un webserver de apache. provider \"aws\" { region = \"us-east-1\" access_key = \"xxx\" secret_key = \"xxx\" } # 1. Create vpc resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" tags = { Name = \"production\" } } # 2. Create Internet Gateway resource \"aws_internet_gateway\" \"gw\" { vpc_id = aws_vpc.prod-vpc.id } # 3. Create Custom Route Table resource \"aws_route_table\" \"prod-route-table\" { vpc_id = aws_vpc.prod-vpc.id route { cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.gw.id } route { ipv6_cidr_block = \"::/0\" gateway_id = aws_internet_gateway.gw.id } tags = { Name = \"Prod\" } } # 4. Create a Subnet resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = \"10.0.1.0/24\" availability_zone = \"us-east-1a\" tags = { Name = \"prod-subnet\" } } # 5. Associate subnet with Route Table resource \"aws_route_table_association\" \"a\" { subnet_id = aws_subnet.subnet-1.id route_table_id = aws_route_table.prod-route-table.id } # 6. Create Security Group to allow port 22,80,443 resource \"aws_security_group\" \"allow_web\" { name = \"allow_web_traffic\" description = \"Allow Web inbound traffic\" vpc_id = aws_vpc.prod-vpc.id ingress { description = \"HTTPS\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } ingress { description = \"HTTP\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } ingress { description = \"SSH\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"allow_web\" } } # 7. Create a network interface with an ip in the subnet that was created in step 4 resource \"aws_network_interface\" \"web-server-nic\" { subnet_id = aws_subnet.subnet-1.id private_ips = [\"10.0.1.50\"] security_groups = [aws_security_group.allow_web.id] } # 8. Assign an elastic IP to the network interface created in step 7 resource \"aws_eip\" \"one\" { vpc = true network_interface = aws_network_interface.web-server-nic.id associate_with_private_ip = \"10.0.1.50\" depends_on = [aws_internet_gateway.gw] } output \"server_public_ip\" { value = aws_eip.one.public_ip } # 9. Create Ubuntu server and install/enable apache2 resource \"aws_instance\" \"web-server-instance\" { ami = \"ami-085925f297f89fce1\" instance_type = \"t2.micro\" availability_zone = \"us-east-1a\" key_name = \"main-key\" network_interface { device_index = 0 network_interface_id = aws_network_interface.web-server-nic.id } user_data = <<-EOF #!/bin/bash sudo apt update -y sudo apt install apache2 -y sudo systemctl start apache2 sudo bash -c 'echo your very first web server > /var/www/html/index.html' EOF tags = { Name = \"web-server\" } } output \"server_private_ip\" { value = aws_instance.web-server-instance.private_ip } output \"server_id\" { value = aws_instance.web-server-instance.id } resource \"<provider>_<resource_type>\" \"name\" { config options..... key = \"value\" key2 = \"another value\" } Con variables En este ejercicio practico se puede crear variables en otros ficheros para construir los recursos: provider \"aws\" { region = \"us-east-1\" access_key = \"AKIAJTTSSUF2PB6HDCCA\" secret_key = \"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\" } variable \"subnet_prefix\" { description = \"cidr block for the subnet\" } resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" tags = { Name = \"production\" } } resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[0].cidr_block availability_zone = \"us-east-1a\" tags = { Name = var.subnet_prefix[0].name } } resource \"aws_subnet\" \"subnet-2\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[1].cidr_block availability_zone = \"us-east-1a\" tags = { Name = var.subnet_prefix[1].name } } Un fichero creao llamado terraform-tfvars : subnet_prefix = [{ cidr_block = \"10.0.1.0/24\", name = \"prod_subnet\" }, { cidr_block = \"10.0.2.0/24\", name = \"dev_subnet\" }] Para construir este fichero se utiliza un terraform apply","title":"Terraform"},{"location":"terraform/#terraform","text":"Tutoriales","title":"TERRAFORM"},{"location":"terraform/#install","text":"sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" sudo apt-get update && sudo apt-get install terraform terraform -help touch ~/.bashrc terraform -install-autocomplete Creamos un primer recurso de prueba en la carpeta con extensi\u00f3n main.tf provider \"aws\"{ region = \"us-east-1\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" }","title":"INSTALL"},{"location":"terraform/#iam-users","text":"Podemos crear un usuario en IAM y podemos asignarle tags, roles como accessAdministrator, y nos dar\u00eda los tokens para conectarse. Luego en users, en credentials podemos modificar y obtener nuevos keys o descargar el que ten\u00edamos.","title":"IAM USERS"},{"location":"terraform/#terraform-init","text":"Una vez tenemos el directorio donde trabajamos con por ejemplo el simple fichero de main.tf , hacemos un terraform init para crear como el directorio de trabajo, muy parecido al git init. Te crea el terraform provider y otros ficheros ocultos. Ejemplo main.tf: provider \"aws\"{ region = \"us-east-1\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" }","title":"TERRAFORM INIT"},{"location":"terraform/#terraform-plan","text":"Esta orden terraform plan sirve como para ver si estar\u00eda bien el proceso ejecutado antes de hacer un terraform aply y las cosas que se crear\u00edan con nuestro fichero de codigo. Ejemplo main.tf: provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"myvpc\"{ cidr_block = \"10.0.0.0/16\" }","title":"TERRAFORM PLAN"},{"location":"terraform/#terraform-apply","text":"Si estamos contentos con el terraform plan, hacemos un terraform aply para que se ejecute lo indicado en el fichero. Se puede usar la opci\u00f3n --auto-approve cuando ya sabemos lo que va a crear y saltar todos las lineas de creaci\u00f3n. Lo creado se crea en el apartado de VPC . Virtual Private Cloud","title":"TERRAFORM APPLY"},{"location":"terraform/#terraform-destroy","text":"Se puede eliminar todo lo creado manualmente o con la orden terraform destroy","title":"TERRAFORM DESTROY"},{"location":"terraform/#variables","text":"Se crea en el fichero main.tf Variable sring: variable \"mystringvar\" { type = string default = \"my first var\" } Variable n\u00famero: variable \"mynumvar\" { type = number default = 100 } Variable booleana: variable \"isenabled\" { default = false } Variable lista: variable \"mylistvar\" { type = list(string) default = [\"apple\",\"20\"] } Variable map: variable \"mymapvar\" { type = map default = { Key1 = \"value1\" Key2 = \"value2\" } } Variable tuple: variable \"mytuplevar\" { type = tuple([string,number,string]) default = [\"var1\",10,\"var2\"] } Variable input: variable \"myinputvar\" { type = string description = \"please enter a vpc name\" } Varibale objecto: variable \"myobjectvar\" { type = object({name = string, port = list(number)}) default = { name = \"myports\" port = [22,80,443] } } Output. DOC : output \"myoutput\" { value = aws_vpc.myVPC.id } Recurso con alguna llamada a la variable: resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mystringvar } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mylistvar[0] } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.mymapvar[\"Key2\"] } } resource \"aws_vpc\" \"myVPC\" { cidr_block = \"10.10.10.0/24\" tags = { Name = var.myinputvar } }","title":"VARIABLES"},{"location":"terraform/#en-ficheros","text":"Si creas una variable pero sin darle valor default te sale un input al hacer un apply: variable \"subnet_prefix\" { description = \"cidr block de la subnet\" #default } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix availability_zone = \"us-east-1a\" } Tambi\u00e9n en vez de un input, pasando por variable con terraform apply -var \"subnet_prefix=10.10.1.0/24\" O en un fichero terraform-tfvars : subnet_prefix=\"10.10.1.0/24\" Si lo creamos con otro nombre, por ejemplo ejemplo.tfvars luego lo pasamos con un terraform apply -var-file example.tfvars O de tipo string y fichero ( subnet_prefix=[\"10.10.1.0/24\"] ): variable \"subnet_prefix\" { description = \"cidr block de la subnet\" type = string } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix availability_zone = \"us-east-1a\" } Y en un fichero terraform.tfvars hacer un apply normal Tambien con un listado: subnet_prefix=[\"10.10.1.0/24\", \"10.0.0.5/14\"] variable \"subnet_prefix\" { description = \"cidr block de la subnet\" type = string } resouce \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[1] availability_zone = \"us-east-1a\" }","title":"En ficheros"},{"location":"terraform/#crear-instancia","text":"Documentacion de fichero main.tf . Aqui descubrimos modulos, recursos etc que se pueden indicar y crear en el fichero. En el fichero main.tf: provider \"aws\" { region = \"us-east-1\" access_key = \"xxxx\" secret_key = \"xxxx\" } resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } Despues hariamos un terraform init, plan y aply.","title":"CREAR INSTANCIA"},{"location":"terraform/#eip","text":"EIP provider \"aws\" { region = \"us-east-1\" access_key = \"xxxx\" secret_key = \"xxxx\" } resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } resource \"aws_eip\" \"myEIP\" { instance = aws_instance.EC2.id } ouput \"myOutEIP\" { value = aws_eip.myEIP.public_ip }","title":"EIP"},{"location":"terraform/#security-group","text":"Crear un security group: resource \"aws_instance\" \"EC2\" { ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" security_groups = [aws_security_group.mySG.name] } resource \"aws_security_group\" \"mySG\" { name = \"Allow HTTPS\" ingress { description = \"allow https from any where\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { description = \"allow https from any where\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } }","title":"SECURITY GROUP"},{"location":"terraform/#user","text":"Ejemplo de crear un usuario con ciertas directrices: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_iam_user\" \"myname\"{ name = \"test-user\" } resource \"aws_iam_policy\" \"myPolicy\"{ name = \"test-custom-policy\" policy = <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"elasticloadbalancing:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"cloudwatch:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"iam:CreateServiceLinkedRole\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"iam:AWSServiceName\": [ \"autoscaling.amazonaws.com\", \"ec2scheduled.amazonaws.com\", \"elasticloadbalancing.amazonaws.com\", \"spot.amazonaws.com\", \"spotfleet.amazonaws.com\", \"transitgateway.amazonaws.com\" ] } } } ] } EOF } resource \"aws_iam_policy_attachment\" \"attachMyPolicy\"{ name = \"attachment\" users = [aws_iam_user.myname.name] policy_arn = aws_iam_policy.myPolicy.myPolicy_arn } la politicas las copiamos de IAM-politicas","title":"USER"},{"location":"terraform/#rds","text":"Crear una instancia de base de datos: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_db_instance\" \"myRDS\"{ name = \"myDB\" instance_class = \"db.t2.micro\" allocated_storage = 5 storage_type = \"gp2\" engine = \"mysql\" engine_version = \"5.7\" username = \"test\" password = \"jupiter\" parameter_group_name = \"default.mysql5.7\" skip_final_snapshot = true port = 3306 }","title":"RDS"},{"location":"terraform/#bucket","text":"Los buckets son depositos que se pueden crear en s3 para guardar cosas permanentes. ejemplo: main.tf provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"test\"{ cidr_block = \"10.0.0.0/16\" } backend.tf terraform{ backend \"s3\"{ key = \"terraform/tfstate.tfstate\" bucket \"el_creado_en_s3\" egion = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } }","title":"BUCKET"},{"location":"terraform/#scaling","text":"Podemos crear varios recursos del mismo tipo con count : provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" count = 3 }","title":"SCALING"},{"location":"terraform/#import","text":"Cuando creas un VPC puedes importar la info de lo creado en un fichero: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_vpc\" \"myVPC\"{ cidr_block = \"10.0.0.0/16\" } Despues en la web vemos que id tenemos y hacemos en consola terraform import aws_vpc.myVPC.idxxx","title":"IMPORT"},{"location":"terraform/#depends-on","text":"Cuando queremos crear algo que primero dependa de una cosa creada: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" } resource \"aws_instance\" \"web\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" depends_on = [aws_instance.DB] }","title":"DEPENDS ON"},{"location":"terraform/#validate","text":"Podemos usar la orden terraform validate para ver que est\u00e9 correcta la sintaxi del fichero de configuraci\u00f3n que se crea.","title":"VALIDATE"},{"location":"terraform/#format","text":"Podemos usar la orden terraform fmt para ver que est\u00e9 correcta la anidaci\u00f3n del fichero de configuraci\u00f3n que se crea.","title":"FORMAT"},{"location":"terraform/#multiplos-providers","text":"Podemos crear diferentes recursos en diferentes zonas y provedores. Ejemplo: provider \"aws\"{ region = \"us-west-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" alias = west } provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" alias = east } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" provider = aws.west } resource \"aws_instance\" \"web\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" depends_on = [aws_instance.DB] }","title":"MULTIPLOS PROVIDERS"},{"location":"terraform/#local-provisioner","text":"Para usar un comando que haga cosas en local mientras se crea el codigo del fichero. provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" provisioner \"local-exec\"{ command = \"echo ${aws_instance.DB.private_ip}\" >> private_ip.txt } }","title":"LOCAL PROVISIONER"},{"location":"terraform/#remote-provisioner","text":"Para usar un comando que haga cosas en REMOTO mientras se crea el codigo del fichero. provider \"aws\"{ region = \"us-east-1\" access_key = \"xxxxx\" secret_key = \"xxxxx\" } resource \"aws_instance\" \"DB\"{ ami = \"ami-047a51fa27710816e\" instance_type = \"t2.micro\" key_name = \"tf_test\" provisioner \"remote-exec\"{ inline = [ \"sudo dnf install nginx -y\" \"sudo systemctl start nginx\" ] } connection { type = \"ssh\" user = \"ec2-user\" private_key = file(\"./test.pem\") host = sef.public_ip } }","title":"REMOTE PROVISIONER"},{"location":"terraform/#plan-destroy","text":"Para borrar los planes se usa terraform plan -destroy","title":"PLAN DESTROY"},{"location":"terraform/#target","text":"Sirve para destruir y aplicar un solo recurso en concreto al que quieras eliminar/a\u00f1adir. Tambi\u00e9n arrastrar\u00e1 a todo lo que lleve relacionado: terraform destroy -target aws_instance.web-server-instance","title":"TARGET"},{"location":"terraform/#workspaces","text":"Se usa como si utilizaras diferentes ramas: terraform workspace terraform workspace new test terraform workspace list terraform workspace show terraform workspace select test terraform workspace delete test","title":"WORKSPACES"},{"location":"terraform/#state","text":"Se puede utilizar diferentes comandos para ver estados: terraform state terraform state list terraform state show aws_eip.one(el recurso creado)","title":"STATE"},{"location":"terraform/#functions","text":"Se pueden usar funciones","title":"FUNCTIONS"},{"location":"terraform/#ejercicio-practico","text":"En este ejercicio vamos a crear una instancia con ubuntu y un webserver de apache. provider \"aws\" { region = \"us-east-1\" access_key = \"xxx\" secret_key = \"xxx\" } # 1. Create vpc resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" tags = { Name = \"production\" } } # 2. Create Internet Gateway resource \"aws_internet_gateway\" \"gw\" { vpc_id = aws_vpc.prod-vpc.id } # 3. Create Custom Route Table resource \"aws_route_table\" \"prod-route-table\" { vpc_id = aws_vpc.prod-vpc.id route { cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.gw.id } route { ipv6_cidr_block = \"::/0\" gateway_id = aws_internet_gateway.gw.id } tags = { Name = \"Prod\" } } # 4. Create a Subnet resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = \"10.0.1.0/24\" availability_zone = \"us-east-1a\" tags = { Name = \"prod-subnet\" } } # 5. Associate subnet with Route Table resource \"aws_route_table_association\" \"a\" { subnet_id = aws_subnet.subnet-1.id route_table_id = aws_route_table.prod-route-table.id } # 6. Create Security Group to allow port 22,80,443 resource \"aws_security_group\" \"allow_web\" { name = \"allow_web_traffic\" description = \"Allow Web inbound traffic\" vpc_id = aws_vpc.prod-vpc.id ingress { description = \"HTTPS\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } ingress { description = \"HTTP\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } ingress { description = \"SSH\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"allow_web\" } } # 7. Create a network interface with an ip in the subnet that was created in step 4 resource \"aws_network_interface\" \"web-server-nic\" { subnet_id = aws_subnet.subnet-1.id private_ips = [\"10.0.1.50\"] security_groups = [aws_security_group.allow_web.id] } # 8. Assign an elastic IP to the network interface created in step 7 resource \"aws_eip\" \"one\" { vpc = true network_interface = aws_network_interface.web-server-nic.id associate_with_private_ip = \"10.0.1.50\" depends_on = [aws_internet_gateway.gw] } output \"server_public_ip\" { value = aws_eip.one.public_ip } # 9. Create Ubuntu server and install/enable apache2 resource \"aws_instance\" \"web-server-instance\" { ami = \"ami-085925f297f89fce1\" instance_type = \"t2.micro\" availability_zone = \"us-east-1a\" key_name = \"main-key\" network_interface { device_index = 0 network_interface_id = aws_network_interface.web-server-nic.id } user_data = <<-EOF #!/bin/bash sudo apt update -y sudo apt install apache2 -y sudo systemctl start apache2 sudo bash -c 'echo your very first web server > /var/www/html/index.html' EOF tags = { Name = \"web-server\" } } output \"server_private_ip\" { value = aws_instance.web-server-instance.private_ip } output \"server_id\" { value = aws_instance.web-server-instance.id } resource \"<provider>_<resource_type>\" \"name\" { config options..... key = \"value\" key2 = \"another value\" }","title":"EJERCICIO PR\u00c1CTICO"},{"location":"terraform/#con-variables","text":"En este ejercicio practico se puede crear variables en otros ficheros para construir los recursos: provider \"aws\" { region = \"us-east-1\" access_key = \"AKIAJTTSSUF2PB6HDCCA\" secret_key = \"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\" } variable \"subnet_prefix\" { description = \"cidr block for the subnet\" } resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" tags = { Name = \"production\" } } resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[0].cidr_block availability_zone = \"us-east-1a\" tags = { Name = var.subnet_prefix[0].name } } resource \"aws_subnet\" \"subnet-2\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = var.subnet_prefix[1].cidr_block availability_zone = \"us-east-1a\" tags = { Name = var.subnet_prefix[1].name } } Un fichero creao llamado terraform-tfvars : subnet_prefix = [{ cidr_block = \"10.0.1.0/24\", name = \"prod_subnet\" }, { cidr_block = \"10.0.2.0/24\", name = \"dev_subnet\" }] Para construir este fichero se utiliza un terraform apply","title":"Con variables"},{"location":"testing/","text":"QA Y TESTING HTTPIE HTTPie es un cliente HTTP de l\u00ednea de comandos que tiene como objetivo hacer que la interacci\u00f3n CLI con los servicios web sea lo m\u00e1s amigable posible para los humanos. HTTPie est\u00e1 dise\u00f1ado para probar, depurar y, en general, interactuar con API y servidores HTTP. INSTALACION dnf install httpie dnf upgrade httpie GET: http get <url.api/> http get <url.api/users> Content-Type:aplication/json Api-Token:xxx POST: http post <url.api/users> username=xxx password=xxx http post <url.api/> Content-Type:aplication/json Api-Token:xxx PUT: echo '{\"username\":\"miguel\", \"email\" : xxx...}' | http put <http:api.url/users> Content-Type:aplication/json Api-Token:xxx http put <http:api.url/users> Content-Type:aplication/json Api-Token:xxx @/path/file/modifications.json POSTMAN Postman es una aplicaci\u00f3n que nos permite realizar pruebas API. Es un cliente HTTP que nos da la posibilidad de testear 'HTTP requests' a trav\u00e9s de una interfaz gr\u00e1fica de usuario, por medio de la cual obtendremos diferentes tipos de respuesta que posteriormente deber\u00e1n ser validados. guia Los requests son peticiones a un servidor. Se pueden crear colecciones de peticiones sobre una API. Dentro de esta coleccion se puede crear diferentes ambientes como de test-local o producci\u00f3n. Dentro de cada ambiente se puede crear diferentes variables como la url de local y la url de producci\u00f3n para cambiar rapido una petici\u00f3n sin cambiar cosas. Para poner cosas en las peticiones, se puede poner en Body-raw-json. Como username, password, y en Headers content-type, api-token. Las peticiones mas usadas son GET, POST, PUT, PATCH, DELETE. La diferencia entre put y patch es que con put necesitas poner todos los campos para modificar algo mientras que patch solo el campo modificado. En la url de petici\u00f3n tambien se pueden pasar parametros debajo de la caja de peticiones como: GET http://url/users?limit=10&lang=es-es&role=admin El collection Runner hace que de un conjunto de colecci\u00f3n, se hagan las peticiones todas de golpe por orden. No obstante, tambien podrias crear un script para decir que vaya a tal petici\u00f3n si hace una, que salte otras, etc. Podemos crear diferentes workspaces para cada api a probar. Podemos importar/exportar colecciones a postman. As\u00ed como compartir colecci\u00f3n donde nos dar\u00e1 una url para un compa\u00f1ero de equipo. Podemos aprender Postman y sacar certificados con EXPLORE - Postman student programm . En la pesta\u00f1a TESTS podemos probar pruebas, tenemos snipets con ya pruebas utilizadas para poder hacer tests de nuestra api. Al hacer SEND nos saldr\u00e1 abajo el Test Result de la prueba. pm.test(\"Status code is 200\", function () { pm.response.to.have.status(200); }); Tambi\u00e9n est\u00e1 la pesta\u00f1a pre-request script para poner codigo que hacer antes de la petici\u00f3n. En la pesta\u00f1a general de postman de EXPLORE tenemos colecciones y apis ya creadas para probar funciones, tests,etc. Para usarlo, tenemos que hacer un FORK de las colecciones ya hechas. Podemos printar las respuestas de manera m\u00e1s bonitas con c\u00f3digo, para ello lo pondremos en la pesta\u00f1a test y lo vemos en el resultado en VISUALIZE. const template = <table bgcolor=\"ffff\"> <tr> <th>{{name}}</th> <th>{{species}}</th> </tr> {{#each response.results}} <tr> <td>{{name}}</td> <td>{{species}}</td> </tr> {{/each}} </table>; pm.visualizer.set(template, { response: pm.response.json() }) la info viene de un contenido RESULTS y sus campos name, species. Tambien se puede generar peticiones dinamicas como por ejemplo hacer que cada vez te vaya diciendo un id de tu pagina: const postId = Math.floor(Math.random() * 100) * 1); pm.globals.set(\"postId\", postId); console.log(\"PostId\", postId); Curso Experto tutorial GO Ejemplo de prueba testing en GO: package main import \"fmt\" func main() { suma := Suma(2, 2) fmt.Println(\"El resultado de la suma es: \", suma) resta := Resta(2, 2) fmt.Println(\"El resultado de la resta es: \", resta) multiplicacion := Multiplicacion(2, 2) fmt.Println(\"El resultado de la multiplicacion es: \", multiplicacion) division := Division(2, 2) fmt.Println(\"El resultado de la division es: \", division) } func Suma(numero1, numero2 int) (resultado int) { resultado = numero1 + numero2 return } func Resta(numero1, numero2 int) (resultado int) { resultado = numero1 - numero2 return } func Multiplicacion(numero1, numero2 int) (resultado int) { resultado = numero1 * numero2 return } func Division(numero1, numero2 int) (resultado int) { resultado = numero1 / numero2 return } Lo probamos con go run calculadora.go Creamos un fichero de pruebas con las funciones donde se debe llamar file_test.go: package main import \"testing\" import \"fmt\" func TestSuma(t *testing.T) { suma := Suma(7, 7) if suma != 14 { t.Error(\"Se esperaba 30 y se obtuvo\", suma) } fmt.Println(\"Prueba de suma exitosa\") } func TestResta(t *testing.T) { resta := Resta(7, 7) if resta != 0 { t.Error(\"Se esperaba 30 y se obtuvo\", resta) } fmt.Println(\"Prueba de resta exitosa\") } func TestMultiplicacion(t *testing.T) { multiplicacion := Multiplicacion(7, 7) if multiplicacion != 49 { t.Error(\"Se esperaba 30 y se obtuvo\", multiplicacion) } fmt.Println(\"Prueba de multiplicacion exitosa\") } func TestDivision(t *testing.T) { division := Division(7, 7) if division != 1 { t.Error(\"Se esperaba 30 y se obtuvo\", division) } fmt.Println(\"Prueba de division exitosa\") } Se prueba con go test/go test -cover Desarrollando Pruebas de Integraci\u00f3n: Script: package main import \"fmt\" func main() { suma := Suma(2, 2) fmt.Println(\"El resultado de la suma es: \", suma) resta := Resta(2, 2) fmt.Println(\"El resultado de la resta es: \", resta) multiplicacion := Multiplicacion(2, 2) fmt.Println(\"El resultado de la multiplicacion es: \", multiplicacion) division := Division(2, 2) fmt.Println(\"El resultado de la division es: \", division) promedio := Promedio(2, 2, 2, 2, 2) fmt.Println(\"El resultado del promedio es: \", promedio) } func Suma(numero1, numero2 int) (resultado int) { resultado = numero1 + numero2 return } func Resta(numero1, numero2 int) (resultado int) { resultado = numero1 - numero2 return } func Multiplicacion(numero1, numero2 int) (resultado int) { resultado = numero1 * numero2 return } func Division(numero1, numero2 int) (resultado int) { resultado = numero1 / numero2 return } func Promedio(numero1, numero2, numero3, numero4, numero5 int) (resultado int) { resultado = (numero1 + numero2 + numero3 + numero4 + numero5) / 5 return } Funciones de prueba: package main import \"testing\" import \"fmt\" func TestSuma(t *testing.T) { suma := Suma(7, 7) if suma != 14 { t.Error(\"Se esperaba 14 y se obtuvo\", suma) } fmt.Println(\"Prueba de suma exitosa\") } func TestResta(t *testing.T) { resta := Resta(7, 7) if resta != 0 { t.Error(\"Se esperaba 0 y se obtuvo\", resta) } fmt.Println(\"Prueba de resta exitosa\") } func TestMultiplicacion(t *testing.T) { multiplicacion := Multiplicacion(7, 7) if multiplicacion != 49 { t.Error(\"Se esperaba 49 y se obtuvo\", multiplicacion) } fmt.Println(\"Prueba de multiplicacion exitosa\") } func TestDivision(t *testing.T) { division := Division(7, 7) if division != 1 { t.Error(\"Se esperaba 1 y se obtuvo\", division) } fmt.Println(\"Prueba de division exitosa\") } func TestPromedio(t *testing.T) { promedio := Promedio(7, 7, 7, 7, 7) if promedio != 7 { t.Error(\"Se esperaba 7 y se obtuvo\", promedio) } fmt.Println(\"Prueba del promedio exitosa\") } Integracion de test: // +build integration package main import \"testing\" import \"fmt\" func TestIntegrationCalculadora(t *testing.T) { suma := Suma(8, 8) resta := Resta(8, 8) multiplicacion := Multiplicacion(8, 8) division := Division(8, 8) promedio := Promedio(8, 8, 8, 8, 8) promediototal := Promedio(suma, resta, multiplicacion, division, promedio) if promediototal != 17 { t.Error(\"Se esperaba 17 y se obtuvo\", promediototal) } fmt.Println(\"Prueba de integracion del promedio total fue exitosa\") } Probamos con go test -tags=integration Pruebas de rendimiento: Supongamos que deseamos almacenar el valor de un string que se va construyendo a trav\u00e9s de un loop. Para ello probaremos 3 formas diferentes de implementar dicha funcionalidad. A continuaci\u00f3n creamos el archivo \"longstring.go\", con el siguiente contenido: package longstring import ( \"bytes\" \"strings\" ) func MedianteConcatenacion(longitud int) string { var s string for i := 0; i < longitud; i++ { s += \"texto\" } return s } func MedianteArreglo(longitud int) string { s := []string{} for i := 0; i < longitud; i++ { s = append(s, \"texto\") } return strings.Join(s, \"\") } func MedianteBuffer(longitud int) string { var buffer bytes.Buffer for i := 0; i < longitud; i++ { buffer.WriteString(\"texto\") } return buffer.String() }","title":"Testing"},{"location":"testing/#qa-y-testing","text":"","title":"QA Y TESTING"},{"location":"testing/#httpie","text":"HTTPie es un cliente HTTP de l\u00ednea de comandos que tiene como objetivo hacer que la interacci\u00f3n CLI con los servicios web sea lo m\u00e1s amigable posible para los humanos. HTTPie est\u00e1 dise\u00f1ado para probar, depurar y, en general, interactuar con API y servidores HTTP. INSTALACION dnf install httpie dnf upgrade httpie GET: http get <url.api/> http get <url.api/users> Content-Type:aplication/json Api-Token:xxx POST: http post <url.api/users> username=xxx password=xxx http post <url.api/> Content-Type:aplication/json Api-Token:xxx PUT: echo '{\"username\":\"miguel\", \"email\" : xxx...}' | http put <http:api.url/users> Content-Type:aplication/json Api-Token:xxx http put <http:api.url/users> Content-Type:aplication/json Api-Token:xxx @/path/file/modifications.json","title":"HTTPIE"},{"location":"testing/#postman","text":"Postman es una aplicaci\u00f3n que nos permite realizar pruebas API. Es un cliente HTTP que nos da la posibilidad de testear 'HTTP requests' a trav\u00e9s de una interfaz gr\u00e1fica de usuario, por medio de la cual obtendremos diferentes tipos de respuesta que posteriormente deber\u00e1n ser validados. guia Los requests son peticiones a un servidor. Se pueden crear colecciones de peticiones sobre una API. Dentro de esta coleccion se puede crear diferentes ambientes como de test-local o producci\u00f3n. Dentro de cada ambiente se puede crear diferentes variables como la url de local y la url de producci\u00f3n para cambiar rapido una petici\u00f3n sin cambiar cosas. Para poner cosas en las peticiones, se puede poner en Body-raw-json. Como username, password, y en Headers content-type, api-token. Las peticiones mas usadas son GET, POST, PUT, PATCH, DELETE. La diferencia entre put y patch es que con put necesitas poner todos los campos para modificar algo mientras que patch solo el campo modificado. En la url de petici\u00f3n tambien se pueden pasar parametros debajo de la caja de peticiones como: GET http://url/users?limit=10&lang=es-es&role=admin El collection Runner hace que de un conjunto de colecci\u00f3n, se hagan las peticiones todas de golpe por orden. No obstante, tambien podrias crear un script para decir que vaya a tal petici\u00f3n si hace una, que salte otras, etc. Podemos crear diferentes workspaces para cada api a probar. Podemos importar/exportar colecciones a postman. As\u00ed como compartir colecci\u00f3n donde nos dar\u00e1 una url para un compa\u00f1ero de equipo. Podemos aprender Postman y sacar certificados con EXPLORE - Postman student programm . En la pesta\u00f1a TESTS podemos probar pruebas, tenemos snipets con ya pruebas utilizadas para poder hacer tests de nuestra api. Al hacer SEND nos saldr\u00e1 abajo el Test Result de la prueba. pm.test(\"Status code is 200\", function () { pm.response.to.have.status(200); }); Tambi\u00e9n est\u00e1 la pesta\u00f1a pre-request script para poner codigo que hacer antes de la petici\u00f3n. En la pesta\u00f1a general de postman de EXPLORE tenemos colecciones y apis ya creadas para probar funciones, tests,etc. Para usarlo, tenemos que hacer un FORK de las colecciones ya hechas. Podemos printar las respuestas de manera m\u00e1s bonitas con c\u00f3digo, para ello lo pondremos en la pesta\u00f1a test y lo vemos en el resultado en VISUALIZE. const template = <table bgcolor=\"ffff\"> <tr> <th>{{name}}</th> <th>{{species}}</th> </tr> {{#each response.results}} <tr> <td>{{name}}</td> <td>{{species}}</td> </tr> {{/each}} </table>; pm.visualizer.set(template, { response: pm.response.json() }) la info viene de un contenido RESULTS y sus campos name, species. Tambien se puede generar peticiones dinamicas como por ejemplo hacer que cada vez te vaya diciendo un id de tu pagina: const postId = Math.floor(Math.random() * 100) * 1); pm.globals.set(\"postId\", postId); console.log(\"PostId\", postId);","title":"POSTMAN"},{"location":"testing/#curso-experto","text":"tutorial","title":"Curso Experto"},{"location":"testing/#go","text":"Ejemplo de prueba testing en GO: package main import \"fmt\" func main() { suma := Suma(2, 2) fmt.Println(\"El resultado de la suma es: \", suma) resta := Resta(2, 2) fmt.Println(\"El resultado de la resta es: \", resta) multiplicacion := Multiplicacion(2, 2) fmt.Println(\"El resultado de la multiplicacion es: \", multiplicacion) division := Division(2, 2) fmt.Println(\"El resultado de la division es: \", division) } func Suma(numero1, numero2 int) (resultado int) { resultado = numero1 + numero2 return } func Resta(numero1, numero2 int) (resultado int) { resultado = numero1 - numero2 return } func Multiplicacion(numero1, numero2 int) (resultado int) { resultado = numero1 * numero2 return } func Division(numero1, numero2 int) (resultado int) { resultado = numero1 / numero2 return } Lo probamos con go run calculadora.go Creamos un fichero de pruebas con las funciones donde se debe llamar file_test.go: package main import \"testing\" import \"fmt\" func TestSuma(t *testing.T) { suma := Suma(7, 7) if suma != 14 { t.Error(\"Se esperaba 30 y se obtuvo\", suma) } fmt.Println(\"Prueba de suma exitosa\") } func TestResta(t *testing.T) { resta := Resta(7, 7) if resta != 0 { t.Error(\"Se esperaba 30 y se obtuvo\", resta) } fmt.Println(\"Prueba de resta exitosa\") } func TestMultiplicacion(t *testing.T) { multiplicacion := Multiplicacion(7, 7) if multiplicacion != 49 { t.Error(\"Se esperaba 30 y se obtuvo\", multiplicacion) } fmt.Println(\"Prueba de multiplicacion exitosa\") } func TestDivision(t *testing.T) { division := Division(7, 7) if division != 1 { t.Error(\"Se esperaba 30 y se obtuvo\", division) } fmt.Println(\"Prueba de division exitosa\") } Se prueba con go test/go test -cover Desarrollando Pruebas de Integraci\u00f3n: Script: package main import \"fmt\" func main() { suma := Suma(2, 2) fmt.Println(\"El resultado de la suma es: \", suma) resta := Resta(2, 2) fmt.Println(\"El resultado de la resta es: \", resta) multiplicacion := Multiplicacion(2, 2) fmt.Println(\"El resultado de la multiplicacion es: \", multiplicacion) division := Division(2, 2) fmt.Println(\"El resultado de la division es: \", division) promedio := Promedio(2, 2, 2, 2, 2) fmt.Println(\"El resultado del promedio es: \", promedio) } func Suma(numero1, numero2 int) (resultado int) { resultado = numero1 + numero2 return } func Resta(numero1, numero2 int) (resultado int) { resultado = numero1 - numero2 return } func Multiplicacion(numero1, numero2 int) (resultado int) { resultado = numero1 * numero2 return } func Division(numero1, numero2 int) (resultado int) { resultado = numero1 / numero2 return } func Promedio(numero1, numero2, numero3, numero4, numero5 int) (resultado int) { resultado = (numero1 + numero2 + numero3 + numero4 + numero5) / 5 return } Funciones de prueba: package main import \"testing\" import \"fmt\" func TestSuma(t *testing.T) { suma := Suma(7, 7) if suma != 14 { t.Error(\"Se esperaba 14 y se obtuvo\", suma) } fmt.Println(\"Prueba de suma exitosa\") } func TestResta(t *testing.T) { resta := Resta(7, 7) if resta != 0 { t.Error(\"Se esperaba 0 y se obtuvo\", resta) } fmt.Println(\"Prueba de resta exitosa\") } func TestMultiplicacion(t *testing.T) { multiplicacion := Multiplicacion(7, 7) if multiplicacion != 49 { t.Error(\"Se esperaba 49 y se obtuvo\", multiplicacion) } fmt.Println(\"Prueba de multiplicacion exitosa\") } func TestDivision(t *testing.T) { division := Division(7, 7) if division != 1 { t.Error(\"Se esperaba 1 y se obtuvo\", division) } fmt.Println(\"Prueba de division exitosa\") } func TestPromedio(t *testing.T) { promedio := Promedio(7, 7, 7, 7, 7) if promedio != 7 { t.Error(\"Se esperaba 7 y se obtuvo\", promedio) } fmt.Println(\"Prueba del promedio exitosa\") } Integracion de test: // +build integration package main import \"testing\" import \"fmt\" func TestIntegrationCalculadora(t *testing.T) { suma := Suma(8, 8) resta := Resta(8, 8) multiplicacion := Multiplicacion(8, 8) division := Division(8, 8) promedio := Promedio(8, 8, 8, 8, 8) promediototal := Promedio(suma, resta, multiplicacion, division, promedio) if promediototal != 17 { t.Error(\"Se esperaba 17 y se obtuvo\", promediototal) } fmt.Println(\"Prueba de integracion del promedio total fue exitosa\") } Probamos con go test -tags=integration Pruebas de rendimiento: Supongamos que deseamos almacenar el valor de un string que se va construyendo a trav\u00e9s de un loop. Para ello probaremos 3 formas diferentes de implementar dicha funcionalidad. A continuaci\u00f3n creamos el archivo \"longstring.go\", con el siguiente contenido: package longstring import ( \"bytes\" \"strings\" ) func MedianteConcatenacion(longitud int) string { var s string for i := 0; i < longitud; i++ { s += \"texto\" } return s } func MedianteArreglo(longitud int) string { s := []string{} for i := 0; i < longitud; i++ { s = append(s, \"texto\") } return strings.Join(s, \"\") } func MedianteBuffer(longitud int) string { var buffer bytes.Buffer for i := 0; i < longitud; i++ { buffer.WriteString(\"texto\") } return buffer.String() }","title":"GO"},{"location":"tomcat/","text":"APACHE TOMCAT DOCUMENTACI\u00d3N TOMCAT Instalaci\u00f3n: Seguimos los pasos de esta web : sudo apt update sudo apt install default-jdk sudo groupadd tomcat sudo useradd -s /bin/false -g tomcat -d /home/ubuntu/tomcat tomcat cd /tmp curl -O paste_the_copied_link_here sudo mkdir /home/ubuntu/tomcat sudo tar xzvf apache-tomcat-*tar.gz -C /home/ubuntu/tomcat --strip-components=1 ubuntu@ip-172-31-38-253:~/tomcat$ cd /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chgrp -R tomcat /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod -R g+r conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod g+x conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chown -R tomcat webapps/ work/ temp/ logs/ ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo update-java-alternatives -l java-1.11.0-openjdk-amd64 1111 /usr/lib/jvm/java-1.11.0-openjdk-amd64 ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vim /etc/systemd/system/tomcat.service ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl daemon-reload ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl start tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl status tomcat \u25cf tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2021-04-08 11:00:09 UTC; 4s ago Process: 4655 ExecStart=/home/ubuntu/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 4662 (java) Tasks: 30 (limit: 1160) Memory: 128.6M CGroup: /system.slice/tomcat.service \u2514\u25004662 /usr/lib/jvm/java-1.11.0-openjdk-amd64/bin/java -Djava.util.logging.config.file=/home/ubuntu/tomcat/conf/> Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Starting Apache Tomcat Web Application Container... Apr 08 11:00:09 ip-172-31-38-253 startup.sh[4655]: Tomcat started. Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Started Apache Tomcat Web Application Container. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo ufw allow 8080 Rules updated Rules updated (v6) ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat sudo systemctl enable tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl enable tomcat Created symlink /etc/systemd/system/multi-user.target.wants/tomcat.service \u2192 /etc/systemd/system/tomcat.service. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/conf/tomcat-users.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/host-manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat Podemos poner en -bashrc la variable JAVA_HOME de /usr/lib/lvm/java... para que tengamos la variable configurada. Levantamos servicios o despliegues con: ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo bin/catalina.sh // bin/startup.sh start Comprobamos que est\u00e1 funcionando con: root@ip-172-31-29-160:/home/ubuntu/tomcat/bin# ps ax | grep java DIRECTORIOS TOMCAT Tomcat directorios: /bin - Startup, shutdown, and other scripts. The *.sh files (for Unix systems) are functional duplicates of the *.bat files (for Windows systems). Since the Win32 command-line lacks certain functionality, there are some additional files in here. /conf - Configuration files and related DTDs. The most important file in here is server.xml. It is the main configuration file for the container. /logs - Log files are here by default. /webapps - This is where your webapps go. CATALINA_HOME and CATALINA_BASE: CATALINA_HOME: Represents the root of your Tomcat installation, for example /home/tomcat/apache-tomcat-9.0.10 or C:\\Program Files\\apache-tomcat-9.0.10. CATALINA_BASE: Represents the root of a runtime configuration of a specific Tomcat instance. If you want to have multiple Tomcat instances on one machine, use the CATALINA_BASE property. If you set the properties to different locations, the CATALINA_HOME location contains static sources, such as .jar files, or binary files. The CATALINA_BASE location contains configuration files, log files, deployed applications, and other runtime requirements. Ejemplo APP: Sample Application The example app has been packaged as a war file and can be downloaded here (Note: make sure your browser doesn't change file extension or append a new one). The easiest way to run this application is simply to move the war file to your CATALINA_BASE/webapps directory. A default Tomcat install will automatically expand and deploy the application for you. You can view it with the following URL (assuming that you're running tomcat on port 8080 which is the default): http://localhost:8080/sample If you just want to browse the contents, you can unpack the war file with the jar command. jar -xvf sample.war Note: CATALINA_BASE is usually the directory in which you unpacked the Tomcat distribution. For more information on CATALINA_HOME, CATALINA_BASE and the difference between them see RUNNING.txt in the directory you unpacked your Tomcat distribution. Copiamos el ejemplo de demo app: [isx46410800@miguel .ssh]$ scp -i mykeypair.pem /home/isx46410800/Documents/tomcat/DemoApp.tar.gz ubuntu@3.8.187.27:/home/ubuntu DemoApp.tar.gz 100% 0 0.0KB/s 00:00 Estructura: el cliente se conecta al servidor por diferentes puertos que hace peticiones como servicios. el engine se encarga de gestionar estas peticiones a trav\u00e9s del host que contiene todas las aplicaciones que hay dentro del contexto. El fichero server.xml contiene la configuraci\u00f3n global, central de tomcat. Catalina es el motor de los servlets, tomcat en s\u00ed. Catalina.properties se pueden configurar variables, cosas del sistema, tiene algunas librer\u00edas o paquetes. Se pueden cambiar o a\u00f1adir propiedades Con la orden catalina.sh (nos sale por poner la variable JAVA_HOME en el path): root@ip-172-31-29-160:/home/ubuntu/tomcat# catalina.sh version Using CATALINA_BASE: /home/ubuntu/tomcat Using CATALINA_HOME: /home/ubuntu/tomcat Using CATALINA_TMPDIR: /home/ubuntu/tomcat/temp Using JRE_HOME: /usr Using CLASSPATH: /home/ubuntu/tomcat/bin/bootstrap.jar:/home/ubuntu/tomcat/bin/tomcat-juli.jar Using CATALINA_OPTS: NOTE: Picked up JDK_JAVA_OPTIONS: --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED Server version: Apache Tomcat/9.0.45 Server built: Mar 30 2021 10:29:04 UTC Server number: 9.0.45.0 OS Name: Linux OS Version: 5.4.0-1038-aws Architecture: amd64 JVM Version: 11.0.10+9-Ubuntu-0ubuntu1.20.04 JVM Vendor: Ubuntu Catalina base nos dice cual es la intancia activa, si hubiera varias instancias se deber\u00eda poner cual es la activa. Tambien podemos poner la variable CATALINA_HOME en el bashrc para que nos pille ya el directorio de tomcat. export PATH=$PATH:/home/ubuntu/tomcat/bin export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ export CATALINA_HOME=/home/ubuntu/tomcat Podemos usar catalina.sh run para iniciar en foreground o start en background. PUERTO SHUTDOWN Podemos ver la doc del server.xml para diferentes configuraciones o objetos que se pueden poner. Unas buenas pr\u00e1cticas es desactivar el puerto de shutdown para que no se pueda hacer ni por tcp ni por remoto. Si recibimos una cadena de SHUTDOWN por cierto puerto, nos matan el tomcat. <Server port=\"8005\" shutdown=\"SHUTDOWN\"> --- root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# telnet localhost 8005 Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. SHUTDOWN Connection closed by foreign host. root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# ps ax | grep java 15743 pts/0 S+ 0:00 grep --color=auto java Para que no pase eso, segun la docu ponemos puerto -1 o cambiamos la string de shutdown por algo dificil de acertar: <Server port=\"-1\" shutdown=\"SHUTDOWN\"> JAVA_OPTS Y CATALINA_OPTS JAVA_OPTS es lo que coge java para todas las cosas java y CATALINA_OPTS son solo para cosas relacionadas con Tomcat. DESPLIEGUE APPS Siempre que desplegamos algo tenemos que hacerlos en el directorio webapps de tomcat. Podemos hacerlo copiando directamente el .war o el directorio con todo el contenido. Al rato veremos que se nos crea el directorio autodesplegado en webapps con el contenido del war copiado: [isx46410800@miguel tomcat]$ sudo cp ejemploWAR/ejemplo.war /home/tomcat/tomcat/webapps/. Con un directorio: [isx46410800@miguel tomcat]$ ll ejemploWARdir/web1/ total 20 -rw-r--r--. 1 isx46410800 isx46410800 309 Jul 5 2020 hello.jsp drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 images -rw-r--r--. 1 isx46410800 isx46410800 470 Jul 5 2020 index.html drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 META-INF drwxr-xr-x. 3 isx46410800 isx46410800 4096 Jul 5 2020 WEB-INF [isx46410800@miguel tomcat]$ sudo cp -r ejemploWARdir/web1/ /home/tomcat/tomcat/webapps/. Si cambiamos algo, modificamos en el c\u00f3digo fuente el index.html y hello.jsp. Luego tenemos que generar de nuevo el .jar en el directorio de todo el codigo fuente con jar cvf ejemplo.war * y esto lo copiamos en el webapps. Para quitar despliegues con solo eliminar el .war o el directorio, ya se hace undeploy. INTEGRACION ECLIPSE CON TOMCAT Instale Eclipse IDE en CentOS, RHEL y Fedora: Se requiere una versi\u00f3n de Java 9 o superior para instalar Eclipse IDE y la forma m\u00e1s sencilla de instalar Oracle Java JDK desde los repositorios predeterminados. yum install java-11-openjdk-devel java -version A continuaci\u00f3n, abra un navegador, navegue hasta la p\u00e1gina de descarga oficial de Eclipse y descargue la \u00faltima versi\u00f3n del paquete tar espec\u00edfico para su arquitectura de distribuci\u00f3n de Linux instalada. Alternativamente, tambi\u00e9n puede descargar el archivo de instalaci\u00f3n de Eclipse IDE en su sistema a trav\u00e9s de la utilidad wget, emitiendo el siguiente comando. wget http://ftp.yz.yamagata-u.ac.jp/pub/eclipse/oomph/epp/2020-06/R/eclipse-inst-linux64.tar.gz Una vez que se complete la descarga, navegue hasta el directorio donde se descarg\u00f3 el paquete de archivo y emita los siguientes comandos para comenzar a instalar Eclipse IDE. tar -xvf eclipse-inst-linux64.tar.gz cd eclipse-installer/ sudo ./eclipse-inst TOMCAT MANAGER A\u00f1adimos en el fichero conf/tomcat-users.xml un usuario con privilegios para poder entrar: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui\"/> Para entrar: http://localhost:8888/manager/html Para desplegar una app vamos a browse seleccionamos el fichero .war y deploy. Despues se puede parar, undeploy, expirar sesion etc. Tambien se puede desplegar un dir(con solo todo el codigo descomprimido) o war con un path diferente nombre y no coja el nombre del archivo war o del directorio. Para utilizar el modo comando se ha de a\u00f1adir en el fichero de conf/tomcat-users.xml: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui,manager-script\"/> Se entra en http://localhost:8888/manager/text/list Algunos comandos: - http://localhost:8888/manager/text/serverinfo - http://localhost:8888/manager/text/list - http://localhost:8888/manager/text/vminfo - http://localhost:8888/manager/text/sessions?path=/app1 - http://localhost:8888/manager/text/threaddump - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1&path=/app3 - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war&path=/app2 - http://localhost:8888/manager/text/stop?path=/app2 - http://localhost:8888/manager/text/start?path=/app2 - http://localhost:8888/manager/text/expire?path=/app1&iddle=1 - http://localhost:8888/manager/text/undeploy?path=/app3 LOGS Fichero clave: conf/logging.properties. Los handers son los indicadores de a donde enrutamos las entradas de los logs de tomcat. handlers = 1catalina.org.apache.juli.AsyncFileHandler, 2localhost.org.apache.juli.AsyncFileHandler, 3manager.org.apache.juli.AsyncFileHandler, 4host-manager.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # .handlers = 1catalina.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # ############################################################ # Handler specific properties. # Describes specific configuration info for Handlers. ############################################################ # 1catalina.org.apache.juli.AsyncFileHandler.level = FINE 1catalina.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina. 1catalina.org.apache.juli.AsyncFileHandler.maxDays = 90 1catalina.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 2localhost.org.apache.juli.AsyncFileHandler.level = FINE 2localhost.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 2localhost.org.apache.juli.AsyncFileHandler.prefix = localhost. 2localhost.org.apache.juli.AsyncFileHandler.maxDays = 90 2localhost.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 4host-manager.org.apache.juli.AsyncFileHandler.level = FINE 4host-manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager. 4host-manager.org.apache.juli.AsyncFileHandler.maxDays = 90 4host-manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter java.util.logging.ConsoleHandler.encoding = UTF-8 el .handlers es el que coge de predeterminado Documentaci\u00f3n paquete JULI Estos handers del archivo coinciden con lo que tenemos en /logs: [tomcat@miguel tomcat]$ ll logs/ total 100 -rw-r-----. 1 tomcat tomcat 16752 may 19 13:55 catalina.2021-05-19.log -rw-r-----. 1 tomcat tomcat 15599 may 20 20:53 catalina.2021-05-20.log -rw-r-----. 1 tomcat tomcat 32351 may 20 20:53 catalina.out -rw-r-----. 1 tomcat tomcat 175 may 19 13:28 host-manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 0 may 20 20:23 host-manager.2021-05-20.log -rw-r-----. 1 tomcat tomcat 1121 may 19 13:28 localhost.2021-05-19.log -rw-r-----. 1 tomcat tomcat 1122 may 20 20:37 localhost.2021-05-20.log -rw-r-----. 1 tomcat tomcat 2034 may 19 14:14 localhost_access_log.2021-05-19.txt -rw-r-----. 1 tomcat tomcat 3656 may 20 20:53 localhost_access_log.2021-05-20.txt -rw-r-----. 1 tomcat tomcat 0 may 19 13:25 manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 4328 may 20 20:53 manager.2021-05-20.log Podemos personaliar uno: 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs/mis-logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 3manager.org.apache.juli.AsyncFileHandler.formatter = java.util.logging.XMLFormatter Vemos que se genera contenido en el personalizado: [tomcat@miguel logs]$ ll mis-logs/ total 0 -rw-r-----. 1 tomcat tomcat 0 may 21 00:23 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/ cat: mis-logs/: Es un directorio [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE log SYSTEM \"logger.dtd\"> <log> <record> <date>2021-05-21T00:25:02</date> <millis>1621549502823</millis> <sequence>46</sequence> <logger>org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager]</logger> <level>INFO</level> <class>org.apache.catalina.core.ApplicationContext</class> <method>log</method> <thread>17</thread> <message>HTMLManager: init: Associated with Deployer 'Catalina:type=Deployer,host=localhost'</message> </record> VALVES Las valvulas son sistemas de seguridad para la hora de entrar a una aplicaci\u00f3n etc, segun en que capa de servicio est\u00e1 especificada. DOC VALVES Una personalizada: <Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --> <!-- <Valve className=\"org.apache.catalina.authenticator.SingleSignOn\" /> --> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs/mis-logs\" prefix=\"localhost_access_log\" suffix=\".log\" pattern=\"%h %l %u %t &quot;%r&quot; %s %b %B\" /> </Host> Vemos el contenido: [tomcat@miguel logs]$ ll mis-logs/ total 12 -rw-r-----. 1 tomcat tomcat 217 may 21 00:41 localhost_access_log.2021-05-21.log -rw-r-----. 1 tomcat tomcat 5539 may 21 00:41 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/localhost_access_log.2021-05-21.log 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:22 +0200] \"GET /manager/text/stop?path=/app2 HTTP/1.1\" 200 70 70 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:25 +0200] \"GET /manager/text/start?path=/app2 HTTP/1.1\" 200 73 73 Ahora hacemos un ejemplo que permita o no unas ips o puertos: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\"/> solo acceden los que tienen la ip de localhost 127.0.0.1. Si ponemos nuestra ip:8888 no nos deja porque no hemos entrado por la de localhost indicada en el valve. JDBC BBDD Instalamos la herramienta probe para ver de otra manera el contenido de tomcat como una consola. Instalamos mariadb como bbdd mysql. MariaDB [seguridad]> select * from usuarios; +----------+ | nombre | +----------+ | Miguel | | Cristina | | Isabel | +----------+ 3 rows in set (0.000 sec) MariaDB [seguridad]> create user 'desa'@'localhost' identified by 'desa'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> grant all on seguridad.* to 'desa'@'localhost'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> commit; Instalamos el driver jdbc de mariadb en nuestro caso en: [isx46410800@miguel tomcat]$ sudo cp ~/Downloads/mariadb-java-client-2.6.1-sources.jar /home/tomcat/lib Configuramos el recurso de bbdd en tomcat que utilice ese driver de jdbc: <Context path=\"/jdbc\" > <Resource name=\"jdbc/cursoDB\" auth=\"Container\" type=\"javax.sql.DataSource\" username=\"desa\" password=\"desa\" driverClassName=\"org.mariadb.jdbc.Driver\" url=\"jdbc:mariadb://localhost:3306/seguridad\" initialSize=\"4\" maxActive=\"15\" maxIdle=\"3\"/> </Context> REALM Son otras opciones de seguridad para en contra de Tomcat, proteger recursos, impedir accesos indeseados... <!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --> <Realm className=\"org.apache.catalina.realm.LockOutRealm\"> <!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key \"UserDatabase\". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --> <Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/> </Realm> Sirven para proteger recursos webs y se basa en usuarios y roles asociados a las aplicaciones. Creamos un realm: <Context path=\"/ejeRealm\"> <Realm className=\"org.apache.catalina.realm.MemoryRealm\" pathname=\"conf/usuarios.xml\"/> </Context> en usuarios.xml: <!xml version=\"1.0\" enconding=\"UTF-8\"?> <tomcat-users> <user username=\"usu1\" password=\"usu1\" roles=\"usuario\"/> </tomcat-users> Y con la app que pasamos, indicamos que solo podr\u00e1 entrar segun el usuario o roles indicados en la app. APACHE CON TOMCAT Descargamos el conector mod_jk Luego tenemos que instalar como root una serie de paquetes como el paquete de http de desarrollador, el framework runtime apr para trabajar como dev, el compilador de c... [root@miguel tomcat]# dnf install httpd-devel apr apr-devel apr-util apr-util-devel gcc make libtool Despues vamos como root al directorio para compilar un arhcivo y meterlo en el tomcat lo que necesitamos. 1- /home/isx46410800/Downloads/tomcat-connectors-1.2.48-src/native 2- Configure para configurar, preparar librerias, el codigo fuente 3- make para compilar 4- make-install para instalar 5- tendremos el fichero mod_jk.so para poder conectar el apache con el tomcat, y puedan entenderse entre ellos. [root@miguel native]# #./configure --with-apxs=/bin/apxs [root@miguel native]# whereis apxs apxs: /usr/bin/apxs /usr/share/man/man1/apxs.1.gz [root@miguel native]# make [root@miguel apache-2.0]# cd apache-2.0/ [root@miguel apache-2.0]# ll mod_jk.s* -rwxr-xr-x. 1 root root 1961464 may 22 15:49 mod_jk.so [root@miguel apache-2.0]# cp mod_jk.so /etc/httpd/modules/ Despues a\u00f1adimos un archivo a la configuracion de apache en /etc/httpd/conf/httpd.conf: IncludeOptional conf/mod_jk.conf [root@miguel conf]# cat mod_jk.conf # Cargamos el modulo LoadModule jk_module modules/mod_jk.so # Indicamos dinde esta el fichero workers.properties JkWorkersFile conf/workers.properties # Podemos generar ficheros de log JkShmFile logs/mod_jk.shm JkLogFile logs/mod_jk.log JkLogLevel info # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus <Location /jkmanager/> </Location> # Mapeamos las URL contra un conjunto de workers JkMount /* MisTomcat Creamos nuestros ficheros de workers.properties de lo que ser\u00e1 nuestros tomcats diferentes, clusters etc(conf/workers.properties). # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status #worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 #worker.worker2.type=ajp13 #worker.worker2.host=localhost #worker.worker2.port=9010 #worker.worker3.type=ajp13 #worker.worker3.host=localhost #worker.worker3.port=10011 # Workers en el Load Balancer worker.MisTomcat.balance_workers=worker1 #worker.MisTomcat.balance_workers=worker1,worker2,worker3 Por ahora comentamos los workers2 y 3 porque solo trabajamos con uno. La info de el puerto y tipo ajp13 la vemos en la info de conf/server.xml y descomentamos la linea de ajp, indicamos los datos y a\u00f1adimos un parametro para que no de fallo: <!-- Define an AJP 1.3 Connector on port 8009 --> <Connector protocol=\"AJP/1.3\" address=\"::1\" port=\"8009\" secretRequired=\"false\" redirectPort=\"8443\" /> Ahora vemos que al pasarle al localhost sin puerto algo, nos manda al tomcat: CLUSTERS Instalaremos dos tomcats m\u00e1s y en cada uno de ellos modificamos el puertos y el ajp correspondiente: [tomcat@miguel ~]$ vim tomcat1/conf/server.xml [tomcat@miguel ~]$ vim tomcat2/conf/server.xml Eliminamos la variable del .bashrc que apuntaba cual era el tomcat, ya que ahora al tener 3 no podemos indicar que sea el primero solo. Tambi\u00e9n podemos abrir tres terminales y exportar la variable con el home correspondiente: [tomcat@miguel tomcat1]$ export CATALINA_HOME=/home/tomcat/tomcat1 Despues arrancamos cada maquina desde su bin para que pille esa variable: [tomcat@miguel bin]$ ./catalina.sh run Ahora modificamos el worker.properties de apache para que haya 3 workers con su ajp correspondiente, en modo balanceador de carga y para que mantenga la sesion con el el sticky sesion: # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 worker.worker2.type=ajp13 worker.worker2.host=localhost worker.worker2.port=9009 worker.worker3.type=ajp13 worker.worker3.host=localhost worker.worker3.port=7009 # Workers en el Load Balancer #worker.MisTomcat.balance_workers=worker1 worker.MisTomcat.balance_workers=worker1,worker2,worker3 worker.MisTomcat.sticky_session=true Paramos y arrancamos de nuevo el apache. Para probar el entorno de cluster copiamos el war de cluster para saber en cual estoy en los 3 tomcats: [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat1/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat2/webapps/. Probamos: http://localhost/cluster/hello.jsp Probamos la parte del jkstatus como worker: # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus Del worker properties: # Activar los workers worker.jkstatus.type=status Comprobamos con http://localhost/jkmanager/ PERSISTENCIA EN TOMCAT Sirve para cuando estamos en una session alojada por ejemplo en tomcat 1, todo se quede alojado ahi y no recargue en otro tomcat y perdamos todo lo de la sesion. En el server.xml de cada uno a\u00f1adimos el jvmroute. Lo que hace es identificar cada sesion con un ID de sesion y lo asigna al worker que ya indicamos el el worker.properties: <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker1\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker2\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker3\"> Descomentamos tambi\u00e9n la opcion de cluster : <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/> Si vamos al navegador y ponemos ajustes - web developer -storage inspector, veremos que nos sale siempre el mismo id worker de la sesion. VIRTUALHOSTS Estos crea dentro de un mismo tomcat, diferentes dominios repartidos para que segun lo que sea, trabaja una cosa u otra. En server.xml ponemos: #Virtual host 1 <Host name=\"empresa1.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa1.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"example_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa1\" debug=\"0\" reloadable=\"true\"/> </Host> #Virtual host 2 <Host name=\"empresa2.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa2.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"mydomain_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa2\" debug=\"0\" reloadable=\"true\"/> </Host> el alias es a lo que responde la petici\u00f3n, con este nombre referido. Para probar estos alias tenemos que ponerlos en el etc/hosts, dns o en el hosting. 192.168.1.113 www.empresa1.com 192.168.1.113 www.empresa2.com Copiamos la app de prueba de virtual host y comprobamos que por los dos alias te responde segun lo indicado en el server.xml Tenemos una herramienta que es el VHOST MANAGER. Para ello necesitamos crear un usuario con los permisos en el tomcat-users.xml: <user username=\"admin1\" password=\"jupiter\" roles=\"admin-gui,admin-script\"/> Rebotamos y entramos desde inicio - host manager o http://localhost:8888/host-manager/html Aqu\u00ed podemos eliminar, a\u00f1adirlo con opciones indicadas y para que se guarde los cambios a final de pagina hemos de a\u00f1adir la siguiente directiz en server.xml. <Listener className=\"org.apache.catalina.storeconfig.StoreConfigLifecycleListener\"/>","title":"Tomcat"},{"location":"tomcat/#apache-tomcat","text":"DOCUMENTACI\u00d3N TOMCAT","title":"APACHE TOMCAT"},{"location":"tomcat/#instalacion","text":"Seguimos los pasos de esta web : sudo apt update sudo apt install default-jdk sudo groupadd tomcat sudo useradd -s /bin/false -g tomcat -d /home/ubuntu/tomcat tomcat cd /tmp curl -O paste_the_copied_link_here sudo mkdir /home/ubuntu/tomcat sudo tar xzvf apache-tomcat-*tar.gz -C /home/ubuntu/tomcat --strip-components=1 ubuntu@ip-172-31-38-253:~/tomcat$ cd /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chgrp -R tomcat /home/ubuntu/tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod -R g+r conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chmod g+x conf ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo chown -R tomcat webapps/ work/ temp/ logs/ ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo update-java-alternatives -l java-1.11.0-openjdk-amd64 1111 /usr/lib/jvm/java-1.11.0-openjdk-amd64 ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vim /etc/systemd/system/tomcat.service ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl daemon-reload ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl start tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl status tomcat \u25cf tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; vendor preset: enabled) Active: active (running) since Thu 2021-04-08 11:00:09 UTC; 4s ago Process: 4655 ExecStart=/home/ubuntu/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 4662 (java) Tasks: 30 (limit: 1160) Memory: 128.6M CGroup: /system.slice/tomcat.service \u2514\u25004662 /usr/lib/jvm/java-1.11.0-openjdk-amd64/bin/java -Djava.util.logging.config.file=/home/ubuntu/tomcat/conf/> Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Starting Apache Tomcat Web Application Container... Apr 08 11:00:09 ip-172-31-38-253 startup.sh[4655]: Tomcat started. Apr 08 11:00:09 ip-172-31-38-253 systemd[1]: Started Apache Tomcat Web Application Container. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo ufw allow 8080 Rules updated Rules updated (v6) ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat sudo systemctl enable tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl enable tomcat Created symlink /etc/systemd/system/multi-user.target.wants/tomcat.service \u2192 /etc/systemd/system/tomcat.service. ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/conf/tomcat-users.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo vi /home/ubuntu/tomcat/webapps/host-manager/META-INF/context.xml ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo systemctl restart tomcat Podemos poner en -bashrc la variable JAVA_HOME de /usr/lib/lvm/java... para que tengamos la variable configurada. Levantamos servicios o despliegues con: ubuntu@ip-172-31-38-253:/home/ubuntu/tomcat$ sudo bin/catalina.sh // bin/startup.sh start Comprobamos que est\u00e1 funcionando con: root@ip-172-31-29-160:/home/ubuntu/tomcat/bin# ps ax | grep java","title":"Instalaci\u00f3n:"},{"location":"tomcat/#directorios-tomcat","text":"Tomcat directorios: /bin - Startup, shutdown, and other scripts. The *.sh files (for Unix systems) are functional duplicates of the *.bat files (for Windows systems). Since the Win32 command-line lacks certain functionality, there are some additional files in here. /conf - Configuration files and related DTDs. The most important file in here is server.xml. It is the main configuration file for the container. /logs - Log files are here by default. /webapps - This is where your webapps go. CATALINA_HOME and CATALINA_BASE: CATALINA_HOME: Represents the root of your Tomcat installation, for example /home/tomcat/apache-tomcat-9.0.10 or C:\\Program Files\\apache-tomcat-9.0.10. CATALINA_BASE: Represents the root of a runtime configuration of a specific Tomcat instance. If you want to have multiple Tomcat instances on one machine, use the CATALINA_BASE property. If you set the properties to different locations, the CATALINA_HOME location contains static sources, such as .jar files, or binary files. The CATALINA_BASE location contains configuration files, log files, deployed applications, and other runtime requirements. Ejemplo APP: Sample Application The example app has been packaged as a war file and can be downloaded here (Note: make sure your browser doesn't change file extension or append a new one). The easiest way to run this application is simply to move the war file to your CATALINA_BASE/webapps directory. A default Tomcat install will automatically expand and deploy the application for you. You can view it with the following URL (assuming that you're running tomcat on port 8080 which is the default): http://localhost:8080/sample If you just want to browse the contents, you can unpack the war file with the jar command. jar -xvf sample.war Note: CATALINA_BASE is usually the directory in which you unpacked the Tomcat distribution. For more information on CATALINA_HOME, CATALINA_BASE and the difference between them see RUNNING.txt in the directory you unpacked your Tomcat distribution. Copiamos el ejemplo de demo app: [isx46410800@miguel .ssh]$ scp -i mykeypair.pem /home/isx46410800/Documents/tomcat/DemoApp.tar.gz ubuntu@3.8.187.27:/home/ubuntu DemoApp.tar.gz 100% 0 0.0KB/s 00:00 Estructura: el cliente se conecta al servidor por diferentes puertos que hace peticiones como servicios. el engine se encarga de gestionar estas peticiones a trav\u00e9s del host que contiene todas las aplicaciones que hay dentro del contexto. El fichero server.xml contiene la configuraci\u00f3n global, central de tomcat. Catalina es el motor de los servlets, tomcat en s\u00ed. Catalina.properties se pueden configurar variables, cosas del sistema, tiene algunas librer\u00edas o paquetes. Se pueden cambiar o a\u00f1adir propiedades Con la orden catalina.sh (nos sale por poner la variable JAVA_HOME en el path): root@ip-172-31-29-160:/home/ubuntu/tomcat# catalina.sh version Using CATALINA_BASE: /home/ubuntu/tomcat Using CATALINA_HOME: /home/ubuntu/tomcat Using CATALINA_TMPDIR: /home/ubuntu/tomcat/temp Using JRE_HOME: /usr Using CLASSPATH: /home/ubuntu/tomcat/bin/bootstrap.jar:/home/ubuntu/tomcat/bin/tomcat-juli.jar Using CATALINA_OPTS: NOTE: Picked up JDK_JAVA_OPTIONS: --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED Server version: Apache Tomcat/9.0.45 Server built: Mar 30 2021 10:29:04 UTC Server number: 9.0.45.0 OS Name: Linux OS Version: 5.4.0-1038-aws Architecture: amd64 JVM Version: 11.0.10+9-Ubuntu-0ubuntu1.20.04 JVM Vendor: Ubuntu Catalina base nos dice cual es la intancia activa, si hubiera varias instancias se deber\u00eda poner cual es la activa. Tambien podemos poner la variable CATALINA_HOME en el bashrc para que nos pille ya el directorio de tomcat. export PATH=$PATH:/home/ubuntu/tomcat/bin export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ export CATALINA_HOME=/home/ubuntu/tomcat Podemos usar catalina.sh run para iniciar en foreground o start en background.","title":"DIRECTORIOS TOMCAT"},{"location":"tomcat/#puerto-shutdown","text":"Podemos ver la doc del server.xml para diferentes configuraciones o objetos que se pueden poner. Unas buenas pr\u00e1cticas es desactivar el puerto de shutdown para que no se pueda hacer ni por tcp ni por remoto. Si recibimos una cadena de SHUTDOWN por cierto puerto, nos matan el tomcat. <Server port=\"8005\" shutdown=\"SHUTDOWN\"> --- root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# telnet localhost 8005 Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. SHUTDOWN Connection closed by foreign host. root@ip-172-31-29-160:/home/ubuntu/tomcat/conf# ps ax | grep java 15743 pts/0 S+ 0:00 grep --color=auto java Para que no pase eso, segun la docu ponemos puerto -1 o cambiamos la string de shutdown por algo dificil de acertar: <Server port=\"-1\" shutdown=\"SHUTDOWN\">","title":"PUERTO SHUTDOWN"},{"location":"tomcat/#java_opts-y-catalina_opts","text":"JAVA_OPTS es lo que coge java para todas las cosas java y CATALINA_OPTS son solo para cosas relacionadas con Tomcat.","title":"JAVA_OPTS Y CATALINA_OPTS"},{"location":"tomcat/#despliegue-apps","text":"Siempre que desplegamos algo tenemos que hacerlos en el directorio webapps de tomcat. Podemos hacerlo copiando directamente el .war o el directorio con todo el contenido. Al rato veremos que se nos crea el directorio autodesplegado en webapps con el contenido del war copiado: [isx46410800@miguel tomcat]$ sudo cp ejemploWAR/ejemplo.war /home/tomcat/tomcat/webapps/. Con un directorio: [isx46410800@miguel tomcat]$ ll ejemploWARdir/web1/ total 20 -rw-r--r--. 1 isx46410800 isx46410800 309 Jul 5 2020 hello.jsp drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 images -rw-r--r--. 1 isx46410800 isx46410800 470 Jul 5 2020 index.html drwxr-xr-x. 2 isx46410800 isx46410800 4096 Jul 5 2020 META-INF drwxr-xr-x. 3 isx46410800 isx46410800 4096 Jul 5 2020 WEB-INF [isx46410800@miguel tomcat]$ sudo cp -r ejemploWARdir/web1/ /home/tomcat/tomcat/webapps/. Si cambiamos algo, modificamos en el c\u00f3digo fuente el index.html y hello.jsp. Luego tenemos que generar de nuevo el .jar en el directorio de todo el codigo fuente con jar cvf ejemplo.war * y esto lo copiamos en el webapps. Para quitar despliegues con solo eliminar el .war o el directorio, ya se hace undeploy.","title":"DESPLIEGUE APPS"},{"location":"tomcat/#integracion-eclipse-con-tomcat","text":"Instale Eclipse IDE en CentOS, RHEL y Fedora: Se requiere una versi\u00f3n de Java 9 o superior para instalar Eclipse IDE y la forma m\u00e1s sencilla de instalar Oracle Java JDK desde los repositorios predeterminados. yum install java-11-openjdk-devel java -version A continuaci\u00f3n, abra un navegador, navegue hasta la p\u00e1gina de descarga oficial de Eclipse y descargue la \u00faltima versi\u00f3n del paquete tar espec\u00edfico para su arquitectura de distribuci\u00f3n de Linux instalada. Alternativamente, tambi\u00e9n puede descargar el archivo de instalaci\u00f3n de Eclipse IDE en su sistema a trav\u00e9s de la utilidad wget, emitiendo el siguiente comando. wget http://ftp.yz.yamagata-u.ac.jp/pub/eclipse/oomph/epp/2020-06/R/eclipse-inst-linux64.tar.gz Una vez que se complete la descarga, navegue hasta el directorio donde se descarg\u00f3 el paquete de archivo y emita los siguientes comandos para comenzar a instalar Eclipse IDE. tar -xvf eclipse-inst-linux64.tar.gz cd eclipse-installer/ sudo ./eclipse-inst","title":"INTEGRACION ECLIPSE CON TOMCAT"},{"location":"tomcat/#tomcat-manager","text":"A\u00f1adimos en el fichero conf/tomcat-users.xml un usuario con privilegios para poder entrar: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui\"/> Para entrar: http://localhost:8888/manager/html Para desplegar una app vamos a browse seleccionamos el fichero .war y deploy. Despues se puede parar, undeploy, expirar sesion etc. Tambien se puede desplegar un dir(con solo todo el codigo descomprimido) o war con un path diferente nombre y no coja el nombre del archivo war o del directorio. Para utilizar el modo comando se ha de a\u00f1adir en el fichero de conf/tomcat-users.xml: <user username=\"admin\" password=\"jupiter\" roles=\"manager-gui,admin-gui,manager-script\"/> Se entra en http://localhost:8888/manager/text/list Algunos comandos: - http://localhost:8888/manager/text/serverinfo - http://localhost:8888/manager/text/list - http://localhost:8888/manager/text/vminfo - http://localhost:8888/manager/text/sessions?path=/app1 - http://localhost:8888/manager/text/threaddump - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1&path=/app3 - http://localhost:8888/manager/text/deploy?war=file:/tmp/app1.war&path=/app2 - http://localhost:8888/manager/text/stop?path=/app2 - http://localhost:8888/manager/text/start?path=/app2 - http://localhost:8888/manager/text/expire?path=/app1&iddle=1 - http://localhost:8888/manager/text/undeploy?path=/app3","title":"TOMCAT MANAGER"},{"location":"tomcat/#logs","text":"Fichero clave: conf/logging.properties. Los handers son los indicadores de a donde enrutamos las entradas de los logs de tomcat. handlers = 1catalina.org.apache.juli.AsyncFileHandler, 2localhost.org.apache.juli.AsyncFileHandler, 3manager.org.apache.juli.AsyncFileHandler, 4host-manager.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # .handlers = 1catalina.org.apache.juli.AsyncFileHandler, java.util.logging.ConsoleHandler # ############################################################ # Handler specific properties. # Describes specific configuration info for Handlers. ############################################################ # 1catalina.org.apache.juli.AsyncFileHandler.level = FINE 1catalina.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina. 1catalina.org.apache.juli.AsyncFileHandler.maxDays = 90 1catalina.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 2localhost.org.apache.juli.AsyncFileHandler.level = FINE 2localhost.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 2localhost.org.apache.juli.AsyncFileHandler.prefix = localhost. 2localhost.org.apache.juli.AsyncFileHandler.maxDays = 90 2localhost.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # 4host-manager.org.apache.juli.AsyncFileHandler.level = FINE 4host-manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs 4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager. 4host-manager.org.apache.juli.AsyncFileHandler.maxDays = 90 4host-manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 # java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter java.util.logging.ConsoleHandler.encoding = UTF-8 el .handlers es el que coge de predeterminado Documentaci\u00f3n paquete JULI Estos handers del archivo coinciden con lo que tenemos en /logs: [tomcat@miguel tomcat]$ ll logs/ total 100 -rw-r-----. 1 tomcat tomcat 16752 may 19 13:55 catalina.2021-05-19.log -rw-r-----. 1 tomcat tomcat 15599 may 20 20:53 catalina.2021-05-20.log -rw-r-----. 1 tomcat tomcat 32351 may 20 20:53 catalina.out -rw-r-----. 1 tomcat tomcat 175 may 19 13:28 host-manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 0 may 20 20:23 host-manager.2021-05-20.log -rw-r-----. 1 tomcat tomcat 1121 may 19 13:28 localhost.2021-05-19.log -rw-r-----. 1 tomcat tomcat 1122 may 20 20:37 localhost.2021-05-20.log -rw-r-----. 1 tomcat tomcat 2034 may 19 14:14 localhost_access_log.2021-05-19.txt -rw-r-----. 1 tomcat tomcat 3656 may 20 20:53 localhost_access_log.2021-05-20.txt -rw-r-----. 1 tomcat tomcat 0 may 19 13:25 manager.2021-05-19.log -rw-r-----. 1 tomcat tomcat 4328 may 20 20:53 manager.2021-05-20.log Podemos personaliar uno: 3manager.org.apache.juli.AsyncFileHandler.level = FINE 3manager.org.apache.juli.AsyncFileHandler.directory = ${catalina.base}/logs/mis-logs 3manager.org.apache.juli.AsyncFileHandler.prefix = manager. 3manager.org.apache.juli.AsyncFileHandler.maxDays = 90 3manager.org.apache.juli.AsyncFileHandler.encoding = UTF-8 3manager.org.apache.juli.AsyncFileHandler.formatter = java.util.logging.XMLFormatter Vemos que se genera contenido en el personalizado: [tomcat@miguel logs]$ ll mis-logs/ total 0 -rw-r-----. 1 tomcat tomcat 0 may 21 00:23 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/ cat: mis-logs/: Es un directorio [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/manager.2021-05-21.log <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <!DOCTYPE log SYSTEM \"logger.dtd\"> <log> <record> <date>2021-05-21T00:25:02</date> <millis>1621549502823</millis> <sequence>46</sequence> <logger>org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager]</logger> <level>INFO</level> <class>org.apache.catalina.core.ApplicationContext</class> <method>log</method> <thread>17</thread> <message>HTMLManager: init: Associated with Deployer 'Catalina:type=Deployer,host=localhost'</message> </record>","title":"LOGS"},{"location":"tomcat/#valves","text":"Las valvulas son sistemas de seguridad para la hora de entrar a una aplicaci\u00f3n etc, segun en que capa de servicio est\u00e1 especificada. DOC VALVES Una personalizada: <Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --> <!-- <Valve className=\"org.apache.catalina.authenticator.SingleSignOn\" /> --> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs/mis-logs\" prefix=\"localhost_access_log\" suffix=\".log\" pattern=\"%h %l %u %t &quot;%r&quot; %s %b %B\" /> </Host> Vemos el contenido: [tomcat@miguel logs]$ ll mis-logs/ total 12 -rw-r-----. 1 tomcat tomcat 217 may 21 00:41 localhost_access_log.2021-05-21.log -rw-r-----. 1 tomcat tomcat 5539 may 21 00:41 manager.2021-05-21.log [tomcat@miguel logs]$ cat mis-logs/localhost_access_log.2021-05-21.log 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:22 +0200] \"GET /manager/text/stop?path=/app2 HTTP/1.1\" 200 70 70 0:0:0:0:0:0:0:1 - admin [21/May/2021:00:41:25 +0200] \"GET /manager/text/start?path=/app2 HTTP/1.1\" 200 73 73 Ahora hacemos un ejemplo que permita o no unas ips o puertos: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\"/> solo acceden los que tienen la ip de localhost 127.0.0.1. Si ponemos nuestra ip:8888 no nos deja porque no hemos entrado por la de localhost indicada en el valve.","title":"VALVES"},{"location":"tomcat/#jdbc-bbdd","text":"Instalamos la herramienta probe para ver de otra manera el contenido de tomcat como una consola. Instalamos mariadb como bbdd mysql. MariaDB [seguridad]> select * from usuarios; +----------+ | nombre | +----------+ | Miguel | | Cristina | | Isabel | +----------+ 3 rows in set (0.000 sec) MariaDB [seguridad]> create user 'desa'@'localhost' identified by 'desa'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> grant all on seguridad.* to 'desa'@'localhost'; Query OK, 0 rows affected (0.000 sec) MariaDB [seguridad]> commit; Instalamos el driver jdbc de mariadb en nuestro caso en: [isx46410800@miguel tomcat]$ sudo cp ~/Downloads/mariadb-java-client-2.6.1-sources.jar /home/tomcat/lib Configuramos el recurso de bbdd en tomcat que utilice ese driver de jdbc: <Context path=\"/jdbc\" > <Resource name=\"jdbc/cursoDB\" auth=\"Container\" type=\"javax.sql.DataSource\" username=\"desa\" password=\"desa\" driverClassName=\"org.mariadb.jdbc.Driver\" url=\"jdbc:mariadb://localhost:3306/seguridad\" initialSize=\"4\" maxActive=\"15\" maxIdle=\"3\"/> </Context>","title":"JDBC BBDD"},{"location":"tomcat/#realm","text":"Son otras opciones de seguridad para en contra de Tomcat, proteger recursos, impedir accesos indeseados... <!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --> <Realm className=\"org.apache.catalina.realm.LockOutRealm\"> <!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key \"UserDatabase\". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --> <Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/> </Realm> Sirven para proteger recursos webs y se basa en usuarios y roles asociados a las aplicaciones. Creamos un realm: <Context path=\"/ejeRealm\"> <Realm className=\"org.apache.catalina.realm.MemoryRealm\" pathname=\"conf/usuarios.xml\"/> </Context> en usuarios.xml: <!xml version=\"1.0\" enconding=\"UTF-8\"?> <tomcat-users> <user username=\"usu1\" password=\"usu1\" roles=\"usuario\"/> </tomcat-users> Y con la app que pasamos, indicamos que solo podr\u00e1 entrar segun el usuario o roles indicados en la app.","title":"REALM"},{"location":"tomcat/#apache-con-tomcat","text":"Descargamos el conector mod_jk Luego tenemos que instalar como root una serie de paquetes como el paquete de http de desarrollador, el framework runtime apr para trabajar como dev, el compilador de c... [root@miguel tomcat]# dnf install httpd-devel apr apr-devel apr-util apr-util-devel gcc make libtool Despues vamos como root al directorio para compilar un arhcivo y meterlo en el tomcat lo que necesitamos. 1- /home/isx46410800/Downloads/tomcat-connectors-1.2.48-src/native 2- Configure para configurar, preparar librerias, el codigo fuente 3- make para compilar 4- make-install para instalar 5- tendremos el fichero mod_jk.so para poder conectar el apache con el tomcat, y puedan entenderse entre ellos. [root@miguel native]# #./configure --with-apxs=/bin/apxs [root@miguel native]# whereis apxs apxs: /usr/bin/apxs /usr/share/man/man1/apxs.1.gz [root@miguel native]# make [root@miguel apache-2.0]# cd apache-2.0/ [root@miguel apache-2.0]# ll mod_jk.s* -rwxr-xr-x. 1 root root 1961464 may 22 15:49 mod_jk.so [root@miguel apache-2.0]# cp mod_jk.so /etc/httpd/modules/ Despues a\u00f1adimos un archivo a la configuracion de apache en /etc/httpd/conf/httpd.conf: IncludeOptional conf/mod_jk.conf [root@miguel conf]# cat mod_jk.conf # Cargamos el modulo LoadModule jk_module modules/mod_jk.so # Indicamos dinde esta el fichero workers.properties JkWorkersFile conf/workers.properties # Podemos generar ficheros de log JkShmFile logs/mod_jk.shm JkLogFile logs/mod_jk.log JkLogLevel info # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus <Location /jkmanager/> </Location> # Mapeamos las URL contra un conjunto de workers JkMount /* MisTomcat Creamos nuestros ficheros de workers.properties de lo que ser\u00e1 nuestros tomcats diferentes, clusters etc(conf/workers.properties). # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status #worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 #worker.worker2.type=ajp13 #worker.worker2.host=localhost #worker.worker2.port=9010 #worker.worker3.type=ajp13 #worker.worker3.host=localhost #worker.worker3.port=10011 # Workers en el Load Balancer worker.MisTomcat.balance_workers=worker1 #worker.MisTomcat.balance_workers=worker1,worker2,worker3 Por ahora comentamos los workers2 y 3 porque solo trabajamos con uno. La info de el puerto y tipo ajp13 la vemos en la info de conf/server.xml y descomentamos la linea de ajp, indicamos los datos y a\u00f1adimos un parametro para que no de fallo: <!-- Define an AJP 1.3 Connector on port 8009 --> <Connector protocol=\"AJP/1.3\" address=\"::1\" port=\"8009\" secretRequired=\"false\" redirectPort=\"8443\" /> Ahora vemos que al pasarle al localhost sin puerto algo, nos manda al tomcat:","title":"APACHE CON TOMCAT"},{"location":"tomcat/#clusters","text":"Instalaremos dos tomcats m\u00e1s y en cada uno de ellos modificamos el puertos y el ajp correspondiente: [tomcat@miguel ~]$ vim tomcat1/conf/server.xml [tomcat@miguel ~]$ vim tomcat2/conf/server.xml Eliminamos la variable del .bashrc que apuntaba cual era el tomcat, ya que ahora al tener 3 no podemos indicar que sea el primero solo. Tambi\u00e9n podemos abrir tres terminales y exportar la variable con el home correspondiente: [tomcat@miguel tomcat1]$ export CATALINA_HOME=/home/tomcat/tomcat1 Despues arrancamos cada maquina desde su bin para que pille esa variable: [tomcat@miguel bin]$ ./catalina.sh run Ahora modificamos el worker.properties de apache para que haya 3 workers con su ajp correspondiente, en modo balanceador de carga y para que mantenga la sesion con el el sticky sesion: # Lista de Workers worker.list=jkstatus, MisTomcat # Activar los workers #worker.jkstatus.type=status worker.MisTomcat.type=lb # A\u00f1adir instancias como workers, 3 en este caso worker.worker1.type=ajp13 worker.worker1.host=localhost worker.worker1.port=8009 worker.worker2.type=ajp13 worker.worker2.host=localhost worker.worker2.port=9009 worker.worker3.type=ajp13 worker.worker3.host=localhost worker.worker3.port=7009 # Workers en el Load Balancer #worker.MisTomcat.balance_workers=worker1 worker.MisTomcat.balance_workers=worker1,worker2,worker3 worker.MisTomcat.sticky_session=true Paramos y arrancamos de nuevo el apache. Para probar el entorno de cluster copiamos el war de cluster para saber en cual estoy en los 3 tomcats: [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat1/webapps/. [isx46410800@miguel tomcat]$ sudo cp cluster.war /home/tomcat/tomcat2/webapps/. Probamos: http://localhost/cluster/hello.jsp Probamos la parte del jkstatus como worker: # Con esto podemos monitorizar jkstatus JkMount /jkmanager/* jkstatus Del worker properties: # Activar los workers worker.jkstatus.type=status Comprobamos con http://localhost/jkmanager/","title":"CLUSTERS"},{"location":"tomcat/#persistencia-en-tomcat","text":"Sirve para cuando estamos en una session alojada por ejemplo en tomcat 1, todo se quede alojado ahi y no recargue en otro tomcat y perdamos todo lo de la sesion. En el server.xml de cada uno a\u00f1adimos el jvmroute. Lo que hace es identificar cada sesion con un ID de sesion y lo asigna al worker que ya indicamos el el worker.properties: <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker1\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker2\"> <Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"worker3\"> Descomentamos tambi\u00e9n la opcion de cluster : <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/> Si vamos al navegador y ponemos ajustes - web developer -storage inspector, veremos que nos sale siempre el mismo id worker de la sesion.","title":"PERSISTENCIA EN TOMCAT"},{"location":"tomcat/#virtualhosts","text":"Estos crea dentro de un mismo tomcat, diferentes dominios repartidos para que segun lo que sea, trabaja una cosa u otra. En server.xml ponemos: #Virtual host 1 <Host name=\"empresa1.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa1.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"example_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa1\" debug=\"0\" reloadable=\"true\"/> </Host> #Virtual host 2 <Host name=\"empresa2.com\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"> <Alias>www.empresa2.com</Alias> <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"mydomain_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t %r %s %b\" /> <Context path=\"\" docBase=\"/home/tomcat/tomcat/webapps/empresa2\" debug=\"0\" reloadable=\"true\"/> </Host> el alias es a lo que responde la petici\u00f3n, con este nombre referido. Para probar estos alias tenemos que ponerlos en el etc/hosts, dns o en el hosting. 192.168.1.113 www.empresa1.com 192.168.1.113 www.empresa2.com Copiamos la app de prueba de virtual host y comprobamos que por los dos alias te responde segun lo indicado en el server.xml Tenemos una herramienta que es el VHOST MANAGER. Para ello necesitamos crear un usuario con los permisos en el tomcat-users.xml: <user username=\"admin1\" password=\"jupiter\" roles=\"admin-gui,admin-script\"/> Rebotamos y entramos desde inicio - host manager o http://localhost:8888/host-manager/html Aqu\u00ed podemos eliminar, a\u00f1adirlo con opciones indicadas y para que se guarde los cambios a final de pagina hemos de a\u00f1adir la siguiente directiz en server.xml. <Listener className=\"org.apache.catalina.storeconfig.StoreConfigLifecycleListener\"/>","title":"VIRTUALHOSTS"},{"location":"virtuales/","text":"HYPER-V DOCUMENTACION HYPER-V CONFIGURAR YOUTUBE Para ver si tenemos habilitado la virtualizaci\u00f3n se puede hacer desde la BIOS y en windows en Administrador de tareas - CPU y abajo ver que ponga habilitada. A\u00f1adimos el Hyper-V en panel de control - programas - activar o desactivar caracteristicas y a\u00f1adimos todo lo de HyperV. Con powershell lo hacemos con Enable-WindowsOptionalFeature -Online - FeatureName:Microsift-Hyper-V -All . Acciones configuraci\u00f3n de Hyper-V nos sirve para indicar en que carpeta se guardar\u00e1n los discos duros y las maquinas virtuales. Crear MV: nuevo - maquina virtual y ponemos nombre, generacion, donde guardala, asignar memoria, la red, el disco duro y el SO. Tambi\u00e9n en creacion rapida - nombre - iso y red. Luego te saldr\u00e1n configuracion predeterminadas que se pueden cambiar en configuraci\u00f3n. Tipos de redes: Externa: las maquinas se conectan con las maquinas y con la externa, ya que coge el adaptador fisico propio. Crea un puente Interna: solo se conecta a las maquinas virtuales sin salida a internet. Privada: solamente a red interna. Se crean en administrador de commutadores virtuales. Tambien se pueden crear discos duros y luego se pueden asignar a maquinas en los pasos de creaci\u00f3n de maquinas virtuales en nuevo - discos duros. Conectar dispositivos USB de fuera a la maquina virtual: cuando le damos a conectar - le damos a mostrar - recursos locales - mas y selecionamos. Habilitar HyperV o Vmware segun lo que se vaya a usar: bcedit /set hypervisorlaunchtype off (habilita vmware) bcedit /set hypervisorlaunchtype auto (habilita vmware) Cuando da error de PING entre maquinas puede ser que haya que desactivar el firewall o en las reglas de firewall en la seguridad de windows, activar el protocolo ICMP entrantes. El error de PXE que no deja el modo seguro arrancar la maquina virtual se soluciona yendo a la condiguraci\u00f3n - seguridad y desactivar el arranque seguro. Hacer plantillas de maquinas virtuales: Hacer snapshoot o instantaneas y despues clonar desde una instantanea. Dentro de una maquina, en el estado que ya queremos que se haga luego la copia, vamos a Equipo - unidad C - system32 - sysprep y ejecutamos y elegimos iniciar y habilitamos opcion y apagado. Una vez apagado, vamos a la MV , boton derecho exportar y la guardamos donde queramos. Tambi\u00e9n podemos copiar el disco duro de la MV para tambi\u00e9n hacer una plantilla del disco duro y asi seleccionamos ese disco al crear la nueva MV. Luego importar maquina en acciones - selecionamos donde la otra, nueva identificador. Para poder tener HyperV dentro de una maquina virtual (virtualizacion anidada) se ha de indicar en la maquina fisica: Set-VMProcessor -VMName DC01 -ExposeVirtualizationExtensions $true Se puede migrar toda la maquina virtual de dentro de una maquina virtual a otro servidor de maquina virtual para meterle dentro esta maquina virtual (Shared Nothing Live Migration), se hace en caliente y en funcionamiento sin perder tiempo ni informaci\u00f3n. Vamos a la MV - boton derecho - mover - mover MV y seleccionar donde - y selecionamos el host a mover - mover todos los elementos y elegimos la carpeta de destino. VMWARE DOCUMENTACION VMWARE NUBE CONFIGURAR VIRTUALBOX DOCUMENTACI\u00d3N VIRTUALBOX CONFIGURAR KVM En linux podemos usar el virt-manager para crear una maquina virtual. VAGRANT DOCUMENTACION VAGRANT es otro entorno de maquina virtuales que nos permite desplegar de manera mas rapida diferentes maquinas virtuales y probar distintos entornos. Instalacion: lsmod | grep kvm sudo dnf install -y dnf-plugins-core sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo sudo dnf -y install vagrant vagrant init hashicorp/bionic64 vagrant up vagrant ssh vagrant destroy El vagrantfile es el fichero que describe como seran configuradas y provisionadas las maquinas virtuales. Este fichero est\u00e1 escrito en Ruby. Ejemplo: # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| # Every Vagrant development environment requires a box. # You can search for boxes at # https://app.vagrantup.com/boxes/search config.vm.box = \"ubuntu/focal64\" # Set the HOSTNAME of the guest VM config.vm.hostname = \"vagrant-host\" # Create a private network, which allows host-only access # to the machine using a specific IP config.vm.network \"private_network\", ip: \"192.168.56.100\" # Vagrant VirtualBox provider specific VM properties config.vm.provider \"virtualbox\" do |vb| # Set VM name to be displayed in the VirtualBox VM Manager window vb.name = \"vagrant-vm\" # Customize the amount of CPUs on the VM vb.cpus = 2 # Customize the amount of memory (2GB RAM) on the VM vb.memory = 2048 end # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # config.vm.synced_folder \"../data\", \"/vagrant_data\" # Vagrant shell provisioner to automatically # install packages 'vim', 'curl', 'podman' config.vm.provision \"shell\", inline: <<-SCRIPT sudo apt update sudo apt install -y vim curl . /etc/os-release echo \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /\" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list curl -L \"https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/Release.key\" | sudo apt-key add - sudo apt update sudo apt -y upgrade sudo apt install -y podman echo NGINX PODMAN CONTAINER RUNNING ON VIRTUALBOX VM > /vagrant-demo echo Provisioned at: >> /vagrant-demo date >> /vagrant-demo SCRIPT # Vagrant Podman provisioner automatically installs Podman # and then runs the 'nginx' image in a container # (We have pre-installed Podman because of provisioner bugs) config.vm.provision \"podman\" do |p| p.run \"nginx\" end end Estoy crea una maquina virtual en virtualbox con ubuntu y las caracteristicas que se especifican. comando vagrant up para subir el fichero Vagrantfile. vagrant status para ver el estado vagrant ssh para conectarte a la maquina virtual por consola directamente. cat /etc/os-release ver la versiones cat /vagrant-demo ver cosas de las caracteristicas del nombre de la VM. sudo podman container ls para ver el pod creado vagrant destroy para eliminar la maquina. DIGITAL OCEAN DIGITAL OCEAN Es una plataforma de la nube como aws y azure que te permite crear maquinas virtuales. Un droplet no es m\u00e1s que una m\u00e1quina virtual en la nube con todas las caracter\u00edsticas de un servidor, totalmente escalable de acuerdo a las necesidades del negocio, es decir si tienes un droplet con un disco duro de por ejemplo 10 gb. PODMAN Podman es como Docker y lo que se diferencia es que para interactuar docker primero pasa por el demonio de docker y luego al cli y podman es serveless. As a result of the CLI similarities, to preserve the developers experience when transitioning from Docker to Podman, simply set the following alias docker=podman. While not an exhaustive list, the basic Podman operations are enumerated below: List images available in the local cache: $ podman image ls Pulling an alpine image from the Docker Hub registry into the local cache: $ podman image pull docker.io/library/alpine Run a container from an image (if the image is not found in local cache it will be pulled from the registry). The run command is the equivalent of podman container create followed by podman container <id> start: $ podman container run -it alpine sh Run a container in the background (-d option) from an nginx image from the Docker Hub registry: $ podman container run -d docker.io/library/nginx List only running containers: $ podman container ls List all containers: $ podman container ls -a Inject a process inside a running container. This will start a bash shell in interactive (-i option) terminal (-t option) mode inside the container: $ podman container exec -it <container_id/name> bash Stop a running container: $ podman container stop <container id/name> Delete a stopped container: $ podman container rm <container id/name> Podman network operations allow us to list networks, create custom networks, inspect them, attach containers to them, and finally remove them when no longer needed. Podman supports the creation of CNI-compliant container networks. To create container networks, Podman supports drivers for various network types. Default is the bridge driver, usable in both rootless and rooted modes. However, in rooted mode, additional two drivers can be used, the macvlan and ipvlan drivers with options such as parent to designate a host device and the mode to be set on the interface. The macvlan and ipvlan drivers are only available in rooted mode because they require access to the host network interfaces, otherwise the rootless networking needs a separate network namespace: A mix of rootless ($) and rooted (#) network operations are provided below: $ podman network ls $ podman network create --subnet 192.168.20.0/24 --gateway 192.168.20.3 bridgenet $ podman network inspect bridgenet # podman network create -d macvlan -o parent=eth0 macvnet # podman network inspect macvnet PUPET PUPPET Es una herramienta de gestion de configuracion que usa el modelo cliente/servidor. El puppet agent tiene que estar instalado en cada maquina a la que se quiera acceder y el server solo en un LINUX. Se crea en un archivo manifiesto los recursos que se quieran crear: user { 'student': ensure => present, uid => '1001', gid => '1001', shell => '/bin/bash', home => '/home/student' } CHEF CHEF Es otra herramienta de gestion cliente/servidor donde el cliente tambien tiene que ser instalado en cada maquina. Los manifiestos son llamados cookbooks para instalar cosas: package \"apache2\" do action :install end service \"apache2\" do action [:enable, :start] end Son llamados recipes Knife es la herramienta de conexion entre las maquinas. PACKER PACKER es una herramienta de software libre desarrollada por Hashicorp que nos permite crear im\u00e1genes de sistema operativo de forma automatizada y utilizando archivos de configuraci\u00f3n para tal efecto y sirve para diferentes plataformas a la vez. Una vez creada la configuraci\u00f3n (que veremos en pr\u00f3ximas entradas) Packer se conectar\u00e1 a la infraestructura de destino, crear\u00e1 la m\u00e1quina virtual y aplicar\u00e1 todas las configuraciones que hayamos definido en nuestros archivos. Todo esto mientras nos tomamos un rico caf\u00e9. Una vez hemos creado la plantilla, Packer permite crear autom\u00e1ticamente tantas m\u00e1quinas virtuales como necesitemos. L\u00f3gicamente, cuantas m\u00e1s m\u00e1quinas virtuales creemos a partir de una plantilla, m\u00e1s rentabilizaremos el tiempo empleado en la elaboraci\u00f3n de la plantilla, pero, a\u00fan en el caso de que s\u00f3lo creemos una vez la m\u00e1quina virtual, la plantilla nos servir\u00e1 como documentaci\u00f3n y especificaci\u00f3n del proceso. Packer incluye la funcionalidad necesaria para trabajar con las grandes plataformas del mercado sin la necesidad de instalar ning\u00fan plugin adicional: Aws. Azure. DigitalOcean. Docker. Google Cloud Platform. Hyper-V. VMware. La generaci\u00f3n automatizada de plantillas con Packer puede aportarnos muchos beneficios a nuestro flujo de trabajo: Velocidad de aprovisionado: Con Packer nuestras plantillas pueden disponer siempre de las \u00faltimas versiones de sistema operativo y de aplicaciones, con lo que podemos desplegar una plantilla siempre lista para su uso en cuesti\u00f3n de segundos o pocos minutos. Consistencia: Utilizando Packer nos aseguramos de que todas las m\u00e1quinas virtuales que despleguemos partir\u00e1n de la misma configuraci\u00f3n. Consistencia multiplataforma: Siguiendo con el punto anterior, con Packer podemos partir de im\u00e1genes id\u00e9nticas si disponemos de entornos h\u00edbridos. En todo momento estaremos seguros que nuestra instancia de Compute Engine de GCP es identica a nuestra m\u00e1quina virtual de VMware vSphere. Versionado de configuraci\u00f3n: Tener nuestras configuraciones en archivos de texto nos permite versionarlas de forma muy sencilla, por ejemplo, con Git, con lo que siempre podremos echar la vista atr\u00e1s a la hora de hacer troubleshooting de nuestras m\u00e1quinas. Packer utiliza templates en formato JSON con los que definiremos las distintas configuraciones necesarias para el despliegue, configuraci\u00f3n y post procesado de nuestras plantillas. Instalacion Ejemplo de template: { \"variables\": { \"aws_access_key\": \"YOUR_AWS_ACCESS_KEY\", \"aws_secret_key\": \"YOUR_AWS_SECRET_KEY\" }, \"builders\": [{ \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"099720109477\"], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" }] } Ejemplo 2: { \"builders\": [{ \"type\": \"azure-arm\", \"client_id\": \"f5b6a5cf-fbdf-4a9f-b3b8-3c2cd00225a4\", \"client_secret\": \"0e760437-bf34-4aad-9f8d-870be799c55d\", \"tenant_id\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\", \"subscription_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\", \"managed_image_resource_group_name\": \"myResourceGroup\", \"managed_image_name\": \"myPackerImage\", \"os_type\": \"Linux\", \"image_publisher\": \"Canonical\", \"image_offer\": \"UbuntuServer\", \"image_sku\": \"16.04-LTS\", \"azure_tags\": { \"dept\": \"Engineering\", \"task\": \"Image deployment\" }, \"location\": \"East US\", \"vm_size\": \"Standard_DS2_v2\" }], \"provisioners\": [{ \"execute_command\": \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\", \"inline\": [ \"apt-get update\", \"apt-get upgrade -y\", \"apt-get -y install nginx\", \"/usr/sbin/waagent -force -deprovision+user && export HISTSIZE=0 && sync\" ], \"inline_shebang\": \"/bin/sh -x\", \"type\": \"shell\" }] } Lo construimos con packer build template.json","title":"M.Virtuales"},{"location":"virtuales/#hyper-v","text":"DOCUMENTACION HYPER-V CONFIGURAR YOUTUBE Para ver si tenemos habilitado la virtualizaci\u00f3n se puede hacer desde la BIOS y en windows en Administrador de tareas - CPU y abajo ver que ponga habilitada. A\u00f1adimos el Hyper-V en panel de control - programas - activar o desactivar caracteristicas y a\u00f1adimos todo lo de HyperV. Con powershell lo hacemos con Enable-WindowsOptionalFeature -Online - FeatureName:Microsift-Hyper-V -All . Acciones configuraci\u00f3n de Hyper-V nos sirve para indicar en que carpeta se guardar\u00e1n los discos duros y las maquinas virtuales. Crear MV: nuevo - maquina virtual y ponemos nombre, generacion, donde guardala, asignar memoria, la red, el disco duro y el SO. Tambi\u00e9n en creacion rapida - nombre - iso y red. Luego te saldr\u00e1n configuracion predeterminadas que se pueden cambiar en configuraci\u00f3n. Tipos de redes: Externa: las maquinas se conectan con las maquinas y con la externa, ya que coge el adaptador fisico propio. Crea un puente Interna: solo se conecta a las maquinas virtuales sin salida a internet. Privada: solamente a red interna. Se crean en administrador de commutadores virtuales. Tambien se pueden crear discos duros y luego se pueden asignar a maquinas en los pasos de creaci\u00f3n de maquinas virtuales en nuevo - discos duros. Conectar dispositivos USB de fuera a la maquina virtual: cuando le damos a conectar - le damos a mostrar - recursos locales - mas y selecionamos. Habilitar HyperV o Vmware segun lo que se vaya a usar: bcedit /set hypervisorlaunchtype off (habilita vmware) bcedit /set hypervisorlaunchtype auto (habilita vmware) Cuando da error de PING entre maquinas puede ser que haya que desactivar el firewall o en las reglas de firewall en la seguridad de windows, activar el protocolo ICMP entrantes. El error de PXE que no deja el modo seguro arrancar la maquina virtual se soluciona yendo a la condiguraci\u00f3n - seguridad y desactivar el arranque seguro. Hacer plantillas de maquinas virtuales: Hacer snapshoot o instantaneas y despues clonar desde una instantanea. Dentro de una maquina, en el estado que ya queremos que se haga luego la copia, vamos a Equipo - unidad C - system32 - sysprep y ejecutamos y elegimos iniciar y habilitamos opcion y apagado. Una vez apagado, vamos a la MV , boton derecho exportar y la guardamos donde queramos. Tambi\u00e9n podemos copiar el disco duro de la MV para tambi\u00e9n hacer una plantilla del disco duro y asi seleccionamos ese disco al crear la nueva MV. Luego importar maquina en acciones - selecionamos donde la otra, nueva identificador. Para poder tener HyperV dentro de una maquina virtual (virtualizacion anidada) se ha de indicar en la maquina fisica: Set-VMProcessor -VMName DC01 -ExposeVirtualizationExtensions $true Se puede migrar toda la maquina virtual de dentro de una maquina virtual a otro servidor de maquina virtual para meterle dentro esta maquina virtual (Shared Nothing Live Migration), se hace en caliente y en funcionamiento sin perder tiempo ni informaci\u00f3n. Vamos a la MV - boton derecho - mover - mover MV y seleccionar donde - y selecionamos el host a mover - mover todos los elementos y elegimos la carpeta de destino.","title":"HYPER-V"},{"location":"virtuales/#vmware","text":"DOCUMENTACION VMWARE NUBE CONFIGURAR","title":"VMWARE"},{"location":"virtuales/#virtualbox","text":"DOCUMENTACI\u00d3N VIRTUALBOX CONFIGURAR","title":"VIRTUALBOX"},{"location":"virtuales/#kvm","text":"En linux podemos usar el virt-manager para crear una maquina virtual.","title":"KVM"},{"location":"virtuales/#vagrant","text":"DOCUMENTACION VAGRANT es otro entorno de maquina virtuales que nos permite desplegar de manera mas rapida diferentes maquinas virtuales y probar distintos entornos. Instalacion: lsmod | grep kvm sudo dnf install -y dnf-plugins-core sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo sudo dnf -y install vagrant vagrant init hashicorp/bionic64 vagrant up vagrant ssh vagrant destroy El vagrantfile es el fichero que describe como seran configuradas y provisionadas las maquinas virtuales. Este fichero est\u00e1 escrito en Ruby. Ejemplo: # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| # Every Vagrant development environment requires a box. # You can search for boxes at # https://app.vagrantup.com/boxes/search config.vm.box = \"ubuntu/focal64\" # Set the HOSTNAME of the guest VM config.vm.hostname = \"vagrant-host\" # Create a private network, which allows host-only access # to the machine using a specific IP config.vm.network \"private_network\", ip: \"192.168.56.100\" # Vagrant VirtualBox provider specific VM properties config.vm.provider \"virtualbox\" do |vb| # Set VM name to be displayed in the VirtualBox VM Manager window vb.name = \"vagrant-vm\" # Customize the amount of CPUs on the VM vb.cpus = 2 # Customize the amount of memory (2GB RAM) on the VM vb.memory = 2048 end # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # config.vm.synced_folder \"../data\", \"/vagrant_data\" # Vagrant shell provisioner to automatically # install packages 'vim', 'curl', 'podman' config.vm.provision \"shell\", inline: <<-SCRIPT sudo apt update sudo apt install -y vim curl . /etc/os-release echo \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /\" | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list curl -L \"https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/Release.key\" | sudo apt-key add - sudo apt update sudo apt -y upgrade sudo apt install -y podman echo NGINX PODMAN CONTAINER RUNNING ON VIRTUALBOX VM > /vagrant-demo echo Provisioned at: >> /vagrant-demo date >> /vagrant-demo SCRIPT # Vagrant Podman provisioner automatically installs Podman # and then runs the 'nginx' image in a container # (We have pre-installed Podman because of provisioner bugs) config.vm.provision \"podman\" do |p| p.run \"nginx\" end end Estoy crea una maquina virtual en virtualbox con ubuntu y las caracteristicas que se especifican. comando vagrant up para subir el fichero Vagrantfile. vagrant status para ver el estado vagrant ssh para conectarte a la maquina virtual por consola directamente. cat /etc/os-release ver la versiones cat /vagrant-demo ver cosas de las caracteristicas del nombre de la VM. sudo podman container ls para ver el pod creado vagrant destroy para eliminar la maquina.","title":"VAGRANT"},{"location":"virtuales/#digital-ocean","text":"DIGITAL OCEAN Es una plataforma de la nube como aws y azure que te permite crear maquinas virtuales. Un droplet no es m\u00e1s que una m\u00e1quina virtual en la nube con todas las caracter\u00edsticas de un servidor, totalmente escalable de acuerdo a las necesidades del negocio, es decir si tienes un droplet con un disco duro de por ejemplo 10 gb.","title":"DIGITAL OCEAN"},{"location":"virtuales/#podman","text":"Podman es como Docker y lo que se diferencia es que para interactuar docker primero pasa por el demonio de docker y luego al cli y podman es serveless. As a result of the CLI similarities, to preserve the developers experience when transitioning from Docker to Podman, simply set the following alias docker=podman. While not an exhaustive list, the basic Podman operations are enumerated below: List images available in the local cache: $ podman image ls Pulling an alpine image from the Docker Hub registry into the local cache: $ podman image pull docker.io/library/alpine Run a container from an image (if the image is not found in local cache it will be pulled from the registry). The run command is the equivalent of podman container create followed by podman container <id> start: $ podman container run -it alpine sh Run a container in the background (-d option) from an nginx image from the Docker Hub registry: $ podman container run -d docker.io/library/nginx List only running containers: $ podman container ls List all containers: $ podman container ls -a Inject a process inside a running container. This will start a bash shell in interactive (-i option) terminal (-t option) mode inside the container: $ podman container exec -it <container_id/name> bash Stop a running container: $ podman container stop <container id/name> Delete a stopped container: $ podman container rm <container id/name> Podman network operations allow us to list networks, create custom networks, inspect them, attach containers to them, and finally remove them when no longer needed. Podman supports the creation of CNI-compliant container networks. To create container networks, Podman supports drivers for various network types. Default is the bridge driver, usable in both rootless and rooted modes. However, in rooted mode, additional two drivers can be used, the macvlan and ipvlan drivers with options such as parent to designate a host device and the mode to be set on the interface. The macvlan and ipvlan drivers are only available in rooted mode because they require access to the host network interfaces, otherwise the rootless networking needs a separate network namespace: A mix of rootless ($) and rooted (#) network operations are provided below: $ podman network ls $ podman network create --subnet 192.168.20.0/24 --gateway 192.168.20.3 bridgenet $ podman network inspect bridgenet # podman network create -d macvlan -o parent=eth0 macvnet # podman network inspect macvnet","title":"PODMAN"},{"location":"virtuales/#pupet","text":"PUPPET Es una herramienta de gestion de configuracion que usa el modelo cliente/servidor. El puppet agent tiene que estar instalado en cada maquina a la que se quiera acceder y el server solo en un LINUX. Se crea en un archivo manifiesto los recursos que se quieran crear: user { 'student': ensure => present, uid => '1001', gid => '1001', shell => '/bin/bash', home => '/home/student' }","title":"PUPET"},{"location":"virtuales/#chef","text":"CHEF Es otra herramienta de gestion cliente/servidor donde el cliente tambien tiene que ser instalado en cada maquina. Los manifiestos son llamados cookbooks para instalar cosas: package \"apache2\" do action :install end service \"apache2\" do action [:enable, :start] end Son llamados recipes Knife es la herramienta de conexion entre las maquinas.","title":"CHEF"},{"location":"virtuales/#packer","text":"PACKER es una herramienta de software libre desarrollada por Hashicorp que nos permite crear im\u00e1genes de sistema operativo de forma automatizada y utilizando archivos de configuraci\u00f3n para tal efecto y sirve para diferentes plataformas a la vez. Una vez creada la configuraci\u00f3n (que veremos en pr\u00f3ximas entradas) Packer se conectar\u00e1 a la infraestructura de destino, crear\u00e1 la m\u00e1quina virtual y aplicar\u00e1 todas las configuraciones que hayamos definido en nuestros archivos. Todo esto mientras nos tomamos un rico caf\u00e9. Una vez hemos creado la plantilla, Packer permite crear autom\u00e1ticamente tantas m\u00e1quinas virtuales como necesitemos. L\u00f3gicamente, cuantas m\u00e1s m\u00e1quinas virtuales creemos a partir de una plantilla, m\u00e1s rentabilizaremos el tiempo empleado en la elaboraci\u00f3n de la plantilla, pero, a\u00fan en el caso de que s\u00f3lo creemos una vez la m\u00e1quina virtual, la plantilla nos servir\u00e1 como documentaci\u00f3n y especificaci\u00f3n del proceso. Packer incluye la funcionalidad necesaria para trabajar con las grandes plataformas del mercado sin la necesidad de instalar ning\u00fan plugin adicional: Aws. Azure. DigitalOcean. Docker. Google Cloud Platform. Hyper-V. VMware. La generaci\u00f3n automatizada de plantillas con Packer puede aportarnos muchos beneficios a nuestro flujo de trabajo: Velocidad de aprovisionado: Con Packer nuestras plantillas pueden disponer siempre de las \u00faltimas versiones de sistema operativo y de aplicaciones, con lo que podemos desplegar una plantilla siempre lista para su uso en cuesti\u00f3n de segundos o pocos minutos. Consistencia: Utilizando Packer nos aseguramos de que todas las m\u00e1quinas virtuales que despleguemos partir\u00e1n de la misma configuraci\u00f3n. Consistencia multiplataforma: Siguiendo con el punto anterior, con Packer podemos partir de im\u00e1genes id\u00e9nticas si disponemos de entornos h\u00edbridos. En todo momento estaremos seguros que nuestra instancia de Compute Engine de GCP es identica a nuestra m\u00e1quina virtual de VMware vSphere. Versionado de configuraci\u00f3n: Tener nuestras configuraciones en archivos de texto nos permite versionarlas de forma muy sencilla, por ejemplo, con Git, con lo que siempre podremos echar la vista atr\u00e1s a la hora de hacer troubleshooting de nuestras m\u00e1quinas. Packer utiliza templates en formato JSON con los que definiremos las distintas configuraciones necesarias para el despliegue, configuraci\u00f3n y post procesado de nuestras plantillas. Instalacion Ejemplo de template: { \"variables\": { \"aws_access_key\": \"YOUR_AWS_ACCESS_KEY\", \"aws_secret_key\": \"YOUR_AWS_SECRET_KEY\" }, \"builders\": [{ \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"099720109477\"], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" }] } Ejemplo 2: { \"builders\": [{ \"type\": \"azure-arm\", \"client_id\": \"f5b6a5cf-fbdf-4a9f-b3b8-3c2cd00225a4\", \"client_secret\": \"0e760437-bf34-4aad-9f8d-870be799c55d\", \"tenant_id\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\", \"subscription_id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\", \"managed_image_resource_group_name\": \"myResourceGroup\", \"managed_image_name\": \"myPackerImage\", \"os_type\": \"Linux\", \"image_publisher\": \"Canonical\", \"image_offer\": \"UbuntuServer\", \"image_sku\": \"16.04-LTS\", \"azure_tags\": { \"dept\": \"Engineering\", \"task\": \"Image deployment\" }, \"location\": \"East US\", \"vm_size\": \"Standard_DS2_v2\" }], \"provisioners\": [{ \"execute_command\": \"chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh '{{ .Path }}'\", \"inline\": [ \"apt-get update\", \"apt-get upgrade -y\", \"apt-get -y install nginx\", \"/usr/sbin/waagent -force -deprovision+user && export HISTSIZE=0 && sync\" ], \"inline_shebang\": \"/bin/sh -x\", \"type\": \"shell\" }] } Lo construimos con packer build template.json","title":"PACKER"},{"location":"vscode/","text":"VISUAL STUDIO CODE Partimos del curso gratis con tips de VSCODE EDICIONES Y TIPS B\u00c1SICOS SELECCIONAR ALGO Se apreta SHIFT + FLECHAS QUITAR ESPACIOS Se apreta SHIFT + TAB QUITAR BARRA LATERAL Se apreta CONTROL + b ORDENAR LINEAS Se apreta ALT + FLECHA ARRIBA/ABAJO en cualquier parte de la frase y movemos para cambiar el orden: <ul> <li>L\u00ednea 1</li> <li>L\u00ednea 2</li> <li>L\u00ednea 3</li> <li>L\u00ednea 4</li> <li>L\u00ednea 5</li> <li>L\u00ednea 6</li> <li>L\u00ednea 7</li> </ul> Para lineas de m\u00e1s de una linea se SELECCIONA TODAS LAS LINEAS y se apreta ALT + FLECHA ARRIBA/ABAJO: <ul> <li> <span>l\u00ednea 1</span> <span>Nada importante 1</span> </li> <li> <span>l\u00ednea 2</span> <span>Nada importante 2</span> </li> <li> <li> <span>l\u00ednea 3</span> <span>Nada importante 3</span> </li> <span>l\u00ednea 4</span> <span>Nada importante 4</span> </li> </ul> COMENTAR C\u00d3DIGO Se apreta CONTROL + SHIFT + A o CONTROL + SHIFT + / Para trozos en medio de frases solo la opci\u00f3n de CONTROL + SHIFT + A. CREAR UNA RUTA DE ARCHIVO Apretamos CONTROL + CLICK y creamos file de la ruta clicada y se crea todos los direcotorios y ficheros: <script src=\"assets/js/app.js\"></script> Tambi\u00e9n sirve para ir a una funci\u00f3n concreta de un archivo enlazado. O vista previa sin salir del documento con SHIFT + F12. BORRAR LINEAS Borra una linea CONTROL + SHIFT + K. Borrar todo que se llame igual CONTROL + SHIFT + L y CONTROL + SHIFT + K. REHACER / DESHACER CONTROL + Z deshacer. CONTROL + SHIFT + Z rehacer. ZEN MODE Se apreta CONTROL + K y luego Z. NAVEGACI\u00d3N PESTA\u00d1AS Ctrl + W Cerrar tab Ctrl + K Ctrl + W Cerrar todas Ctrl + Shift + T Reabrir anterior Ctrl + TAB Cambiar de tab ABRIR TERMINAL Se apreta CONTROL + ` o icono abajo derecha de >. Aqui se puede usar como bash normal, abrir mas terminales, matar terminal, etc. LLAMAR A LA PALETA DE BUSQUEDA Se apreta CONTROL + SHIFT + P. Ejemplo de wrap with abreviation y despues como queremos encapsular unas palabras: code, ul>li... CONFIGURAR UN SHORTCUT. Manage -> keyboard shorcuts -> buscamos por ejemos wrap y ponemos la combinaci\u00f3n que queremos. Tambien CONTROL + K + CONTROL + S. MULTICURSORES Y EDICI\u00d3N R\u00c1PIDA CLONAR LINEAS Shortcut COPY LINE DOWN Y UP. CREAR MULTICURSOR ARRIBA/ABAJO Se apreta CONTROL + SHIFT + FLECHAS y escribes una sola vez que sirve para todo. MULTICURSOS CON COPY Partimos de: <span>amarillo</span> <span>rojo</span> <span>verde</span> <span>naranja</span> <span>morado</span> <span>negro</span> <span>blanco</span> Creamos un cursos despues de span, luego nos ponemos inico de la palabra del color y CONTROL + SHIFT + FELCHA ADELANTE, copiamos, vamos para atras y pegamos: <span class=\"amarillo\">amarillo</span> <span class=\"rojo\">rojo</span> <span class=\"verde\">verde</span> <span class=\"naranja\">naranja</span> <span class=\"morado\">morado</span> <span class=\"negro\">negro</span> <span class=\"blanco\">blanco</span> MULTICURSOR PARA FORMATO Se crea multicursor con CONTROL + SHIFT + FLECHAS y damos espacios para quedar alineado. De palabras seleccionamos con ALT copiamos, vamos para atras y pegar: <span>amarillo</span> <p>rojo</p> <div-personalizado>verde</div-personalizado> <bold>naranja</bold> <otro-div-complejo>naranja-azul</otro-div-complejo> <!-- Objetivo final --> <span class=\"amarillo\">amarillo</span> <p class=\"rojo\">rojo</p> <div-personalizado class=\"verde\">verde</div-personalizado> <bold class=\"naranja\">naranja</bold> <otro-div-complejo class=\"naranja-azul\">naranja-azul</otro-div-complejo> LOWECASE / UPPERCASE Creamos multicursor y CONTROL + SHIFT + U/L SELECCION VARIAS COSAS A LA VEZ CONTROL + D. DEFINICIONES Y SNIPPETS BUSCAR DEFINICIONES Ctrl + P => luego escribir la @: Ctrl + Shift = O BUSCAR LINEAS CONTROL + P + :n\u00balinea MARKDOWN PREVIEW CONTROL + SHIFT + V CONTROL + K V Ctrl + P : Markdown Open Preview Ctrl + P : Markdown Open Preview to the side REPLACE SYMBOL Para reemplazar la misma palabra en todos los documentos enlazados o llamados se apreta F2. CREAR SNIPPET Es un fragmento de c\u00f3digo ya creado y poder llamarlo. Manage -> usar snippet -> elegir lenguaje. \"Print to console\": { \"prefix\": \"log\", \"body\": [ \"console.log('${1:Hola mundo}');\", \"$2\" ], \"description\": \"Log output to console\" } Lo llamamos con el nombre del 'prefix'. EXTENSIONES PASTE JSON AS CODE TERMINAL TODO FREE BOOKMARKS MATERIAL ICON THEME MATERIAL THEME LIVE SERVER COLOR HIGHLIGHT BRAKET PAIR COLORIZED 2 GIT https://code.visualstudio.com/docs/editor/versioncontrol https://code.visualstudio.com/docs/editor/github DOCKER https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker https://code.visualstudio.com/docs/containers/overview","title":"VSCode"},{"location":"vscode/#visual-studio-code","text":"Partimos del curso gratis con tips de VSCODE","title":"VISUAL STUDIO CODE"},{"location":"vscode/#ediciones-y-tips-basicos","text":"","title":"EDICIONES Y TIPS B\u00c1SICOS"},{"location":"vscode/#seleccionar-algo","text":"Se apreta SHIFT + FLECHAS","title":"SELECCIONAR ALGO"},{"location":"vscode/#quitar-espacios","text":"Se apreta SHIFT + TAB","title":"QUITAR ESPACIOS"},{"location":"vscode/#quitar-barra-lateral","text":"Se apreta CONTROL + b","title":"QUITAR BARRA LATERAL"},{"location":"vscode/#ordenar-lineas","text":"Se apreta ALT + FLECHA ARRIBA/ABAJO en cualquier parte de la frase y movemos para cambiar el orden: <ul> <li>L\u00ednea 1</li> <li>L\u00ednea 2</li> <li>L\u00ednea 3</li> <li>L\u00ednea 4</li> <li>L\u00ednea 5</li> <li>L\u00ednea 6</li> <li>L\u00ednea 7</li> </ul> Para lineas de m\u00e1s de una linea se SELECCIONA TODAS LAS LINEAS y se apreta ALT + FLECHA ARRIBA/ABAJO: <ul> <li> <span>l\u00ednea 1</span> <span>Nada importante 1</span> </li> <li> <span>l\u00ednea 2</span> <span>Nada importante 2</span> </li> <li> <li> <span>l\u00ednea 3</span> <span>Nada importante 3</span> </li> <span>l\u00ednea 4</span> <span>Nada importante 4</span> </li> </ul>","title":"ORDENAR LINEAS"},{"location":"vscode/#comentar-codigo","text":"Se apreta CONTROL + SHIFT + A o CONTROL + SHIFT + / Para trozos en medio de frases solo la opci\u00f3n de CONTROL + SHIFT + A.","title":"COMENTAR C\u00d3DIGO"},{"location":"vscode/#crear-una-ruta-de-archivo","text":"Apretamos CONTROL + CLICK y creamos file de la ruta clicada y se crea todos los direcotorios y ficheros: <script src=\"assets/js/app.js\"></script> Tambi\u00e9n sirve para ir a una funci\u00f3n concreta de un archivo enlazado. O vista previa sin salir del documento con SHIFT + F12.","title":"CREAR UNA RUTA DE ARCHIVO"},{"location":"vscode/#borrar-lineas","text":"Borra una linea CONTROL + SHIFT + K. Borrar todo que se llame igual CONTROL + SHIFT + L y CONTROL + SHIFT + K.","title":"BORRAR LINEAS"},{"location":"vscode/#rehacer-deshacer","text":"CONTROL + Z deshacer. CONTROL + SHIFT + Z rehacer.","title":"REHACER / DESHACER"},{"location":"vscode/#zen-mode","text":"Se apreta CONTROL + K y luego Z.","title":"ZEN MODE"},{"location":"vscode/#navegacion-pestanas","text":"Ctrl + W Cerrar tab Ctrl + K Ctrl + W Cerrar todas Ctrl + Shift + T Reabrir anterior Ctrl + TAB Cambiar de tab","title":"NAVEGACI\u00d3N PESTA\u00d1AS"},{"location":"vscode/#abrir-terminal","text":"Se apreta CONTROL + ` o icono abajo derecha de >. Aqui se puede usar como bash normal, abrir mas terminales, matar terminal, etc.","title":"ABRIR TERMINAL"},{"location":"vscode/#llamar-a-la-paleta-de-busqueda","text":"Se apreta CONTROL + SHIFT + P. Ejemplo de wrap with abreviation y despues como queremos encapsular unas palabras: code, ul>li...","title":"LLAMAR A LA PALETA DE BUSQUEDA"},{"location":"vscode/#configurar-un-shortcut","text":"Manage -> keyboard shorcuts -> buscamos por ejemos wrap y ponemos la combinaci\u00f3n que queremos. Tambien CONTROL + K + CONTROL + S.","title":"CONFIGURAR UN SHORTCUT."},{"location":"vscode/#multicursores-y-edicion-rapida","text":"","title":"MULTICURSORES Y EDICI\u00d3N R\u00c1PIDA"},{"location":"vscode/#clonar-lineas","text":"Shortcut COPY LINE DOWN Y UP.","title":"CLONAR LINEAS"},{"location":"vscode/#crear-multicursor-arribaabajo","text":"Se apreta CONTROL + SHIFT + FLECHAS y escribes una sola vez que sirve para todo.","title":"CREAR MULTICURSOR ARRIBA/ABAJO"},{"location":"vscode/#multicursos-con-copy","text":"Partimos de: <span>amarillo</span> <span>rojo</span> <span>verde</span> <span>naranja</span> <span>morado</span> <span>negro</span> <span>blanco</span> Creamos un cursos despues de span, luego nos ponemos inico de la palabra del color y CONTROL + SHIFT + FELCHA ADELANTE, copiamos, vamos para atras y pegamos: <span class=\"amarillo\">amarillo</span> <span class=\"rojo\">rojo</span> <span class=\"verde\">verde</span> <span class=\"naranja\">naranja</span> <span class=\"morado\">morado</span> <span class=\"negro\">negro</span> <span class=\"blanco\">blanco</span>","title":"MULTICURSOS CON COPY"},{"location":"vscode/#multicursor-para-formato","text":"Se crea multicursor con CONTROL + SHIFT + FLECHAS y damos espacios para quedar alineado. De palabras seleccionamos con ALT copiamos, vamos para atras y pegar: <span>amarillo</span> <p>rojo</p> <div-personalizado>verde</div-personalizado> <bold>naranja</bold> <otro-div-complejo>naranja-azul</otro-div-complejo> <!-- Objetivo final --> <span class=\"amarillo\">amarillo</span> <p class=\"rojo\">rojo</p> <div-personalizado class=\"verde\">verde</div-personalizado> <bold class=\"naranja\">naranja</bold> <otro-div-complejo class=\"naranja-azul\">naranja-azul</otro-div-complejo>","title":"MULTICURSOR PARA FORMATO"},{"location":"vscode/#lowecase-uppercase","text":"Creamos multicursor y CONTROL + SHIFT + U/L","title":"LOWECASE / UPPERCASE"},{"location":"vscode/#seleccion-varias-cosas-a-la-vez","text":"CONTROL + D.","title":"SELECCION VARIAS COSAS A LA VEZ"},{"location":"vscode/#definiciones-y-snippets","text":"","title":"DEFINICIONES Y SNIPPETS"},{"location":"vscode/#buscar-definiciones","text":"Ctrl + P => luego escribir la @: Ctrl + Shift = O","title":"BUSCAR DEFINICIONES"},{"location":"vscode/#buscar-lineas","text":"CONTROL + P + :n\u00balinea","title":"BUSCAR LINEAS"},{"location":"vscode/#markdown-preview","text":"CONTROL + SHIFT + V CONTROL + K V Ctrl + P : Markdown Open Preview Ctrl + P : Markdown Open Preview to the side","title":"MARKDOWN PREVIEW"},{"location":"vscode/#replace-symbol","text":"Para reemplazar la misma palabra en todos los documentos enlazados o llamados se apreta F2.","title":"REPLACE SYMBOL"},{"location":"vscode/#crear-snippet","text":"Es un fragmento de c\u00f3digo ya creado y poder llamarlo. Manage -> usar snippet -> elegir lenguaje. \"Print to console\": { \"prefix\": \"log\", \"body\": [ \"console.log('${1:Hola mundo}');\", \"$2\" ], \"description\": \"Log output to console\" } Lo llamamos con el nombre del 'prefix'.","title":"CREAR SNIPPET"},{"location":"vscode/#extensiones","text":"PASTE JSON AS CODE TERMINAL TODO FREE BOOKMARKS MATERIAL ICON THEME MATERIAL THEME LIVE SERVER COLOR HIGHLIGHT BRAKET PAIR COLORIZED 2 GIT https://code.visualstudio.com/docs/editor/versioncontrol https://code.visualstudio.com/docs/editor/github DOCKER https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker https://code.visualstudio.com/docs/containers/overview","title":"EXTENSIONES"},{"location":"windows/","text":"ADMINISTRAR WINDOWS SERVER 2019 INSTALACI\u00d3N Para probar lo haremos con una maquina virtual. En este caso probaremos con HYPER-V , una herramienta de virtualizaci\u00f3n propia de windows que utiliza un hypervisor. Hyper-V es un programa de virtualizaci\u00f3n de Microsoft basado en un hipervisor para los sistemas de 64 bits\u200b con los procesadores basados en AMD-V o Tecnolog\u00eda de virtualizaci\u00f3n Intel. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Los windows 10 home no lo tienen, no obstante con este fichero lo podremos aplicar: Enable HYPER-V Al reiniciar si no funciona directamente tendremos que hacer las siguientes opciones: Habilitar virtualizaci\u00f3n en la BIOS En la powershell comando bcdedit /set hypervisorlaunchtype auto En la powershell comando dism /online /enable-feature /featurename:Microsoft-Hyper-V -All Iniciamos maquina virtual y configuramos: Idioma/Regi\u00f3n S.O. versi\u00f3n standard evaluation con experiencia de escritorio para que tenga ventanas y no solo consola Instalaci\u00f3n personalizada (si le damos a SHIFT+F10 nos abre una consola de cmd para poder hacer cosas tambi\u00e9n) Varios reinicios Contrase\u00f1a administrador El controlador de dominio es el centro neur\u00e1lgico de un dominio Windows, tal como un servidor Network Information Service (NIS) lo es del servicio de informaci\u00f3n de una red Unix. Los controladores de dominio tienen una serie de responsabilidades, y una de ellas es la autenticaci\u00f3n, que es el proceso de garantizar o denegar a un usuario el acceso a recursos compartidos o a otra m\u00e1quina de la red, normalmente a trav\u00e9s del uso de una contrase\u00f1a. Esto permite validar a los usuarios de una red para ser partes de la plataforma de clientes que recibir\u00e1n los servicios de informaci\u00f3n. Cada controlador de dominio usa un security account manager (SAM), o NTDS en Windows 2003 Server (que es la forma promovida de la SAM, al pasar como controlador de dominio), para mantener una lista de pares de nombre de usuario y contrase\u00f1a. El controlador de dominio entonces crea un repositorio centralizado de contrase\u00f1as, que est\u00e1n enlazados a los nombres de usuarios (una clave por usuario), lo cual es m\u00e1s eficiente que mantener en cada m\u00e1quina cliente centenares de claves para cada recurso de red disponible. En un dominio Windows, cuando un cliente no autorizado solicita un acceso a los recursos compartidos de un servidor, el servidor act\u00faa y pregunta al controlador de dominio si ese usuario est\u00e1 autentificado. Si lo est\u00e1, el servidor establecer\u00e1 una conexi\u00f3n de sesi\u00f3n con los derechos de acceso correspondientes para ese servicio y usuario. Si no lo est\u00e1, la conexi\u00f3n es denegada. Una vez que el controlador de dominio autentifica a un usuario, se devuelve al cliente una ficha especial (token) de autenticaci\u00f3n, de manera que el usuario no necesitar\u00e1 volver a iniciar sesi\u00f3n para acceder a otros recursos en dicho dominio, ya que el usuario se considera autentificado en el dominio. SERVER MANAGER El server manager(administrador del servidor) es el aplicativo para configurar todas las cosas del server. Antes miramos las tarjetas de red con el comando ncpa.cpl . Elegimos la tarjeta - propiedades - ipv4 - propiedades y indicamos la IP que queremos ponerle. Cambiar nombre en servidor local y lo ponemos como un control de dominio(domain controller): En servidor local activamos el escritorio remoto y desactivamos aqui solo(en produccion no) la seguridad de Iexplorer. En servidor local abajo en rendimiento, iniciamos contador y a la dercha configuramos las alertas para 7 dias. En servidor local , parte arriba derecha en administrar le damos a agregar roles y caracteristicas. Seleccionamos el servidor y despues en roles de servidor, seleccionamos servicios de dominio de Active Directory . Siguiente, siguiente e instalamos. En powershell ser\u00eda con el comando install-windowsfeature y para ver los que hay get-windowsfeature Despues de instalar le damos a promover servicio para configurar el AD: Si se trabaja con aparatos mas antiguos, el nivel funcional hay que bajarlos. Siguiente pantalla En Panel - derecha Herramientas - DNS: En DC01 boton derecho ejecutar nslookup(nos sale unkown) Solucinamos dandole a zona inversa crear nueva. Siguiente hasta encontrar el cajon de poner ip, ponemos 192.168.1 Ahora actualizamos el puntero PTR en la directa dando boton derecho a la de la ip y le damos a actualizar y nos sale En boton derecho DC01 propiedades - reenviadores le ponemos los DNS. En revisi\u00f3n hacemos la prueba ESTRUCTURANDO EL AD Representaremos este ejemplo: Vamos al Panel - derecha Herramientas - Usuarios y equipos de Active Directory. Entramos a la organizaci\u00f3n creada de miguel.local y dentro creamos la organizaci\u00f3n global Miguel con boton derecho - nuevo - organizaci\u00f3n. Hemos creado un global porque luego para crear directivas de grupo, afectar\u00e1 a todo lo de abajo y sino tendriamos problemas porque afectaria a cosas que estan en la misma altura que no queremos que tengan esto. Ahora entramos en esta y vamos creando dentro nuevos deparamentos: Si tenemos algun error, al tener la casilla marcada de no poder eliminar. Vamos al menu ver - caracteriticas avanzadas - buscamos el departamento - propiedades - recursos y eliminamos la casilla y luego ya podremos mover o eliminar ese departamento. Ahora entramos en cada departamento y vamos creando usuarios en cada uno con una misma contrase\u00f1a. Ahora dentro de cada departamento , creamos un grupo con el mismo nombre. Esto se hace para poder luego compartir recursos, ya que por unidades organizativas no se puede. El departamento global de Miguel tambien tiene que tener un grupo creado. Porque asi si creamos un recurso o algo global, si se lo damos a esto, se lo aplica a todos los de su union: Ahora en cada departamento, seleccionamos todos los usuarios que hayan, boton derecho asignar un nuevo grupo. Escribimos el nombre del grupo, le damos a comprobar nombres y aceptamos. Si entramos a las propiedades del grupo, veremos que est\u00e1n como miembros. Despues de cada grupo, lo asignamos al grupo general Miguel . GPO Cpanel - Herramientas - Administracion de directivas de grupo. Todas las directivas se guardan en objetos de directivas de grupo. Vemos la que hay y las configuraciones que tienen: Para editar una vamos a boton derecho y editar. Desde aqu\u00ed para modificar algo se busca siguiendo la misma ruta que en la configuraci\u00f3n. Para crear una directiva de grupo(GPO) vamos a objetos y boton derecho crear. Todas las GPO para vincularlas necesitan una unidad organizativa. Para ello, vamos a la unidad de Contabilidad, boton derecho y vincular a la creada. Tambien se puede crear directamente yendo a la unidad y boton derecho crear GPO y vincular: GPO de inicio: sirve para que esa directiva sea comun para todos. Vamos a GPO de inicio y crear. Unas de las m\u00e1s comunes es el fondo de escritorio o que no usen los USB. Para crear una, en contenido boton derecho nuevo. Ahora si creamos una directiva de grupo, le podemos indicar que sea como base la de inicio: Para editar una GPO de inicio, entramos vamos a configuracion boton derecho edicion. Tocamos lo que sea y en la proxima GPO que creemos veremos que tiene estas configuraciones, partir\u00e1 con la reciente modificaci\u00f3n. Siempre parten de como esten en ese momento, los cambios no afectan a todas las que esten asignadas. Se puede crear GPO para la raiz y hereda para todos. No obstante, si queremos que algun departamento no herede, vamos a la unidad organizativa, boton derecho eliminar herencia: No obstante, aunque se quita la herencia. Si las GPO de la raiz le doy boton derecho y exigido. La van a tener si o si: La parte ultima de resultados de directivas de grupo, se puede generar un informe de un usuario/grupo de todas las directivas que tiene. Se puede hacer por consola con la orden gpresult /h file.html RAS(Remote Access Server) Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos acceso remoto que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Una vez instalado vamos a herramientas - enrutamiento y acceso remoto Vamos a boton derecho en DC01 y configurar, despues a tradducion de NAT y elegimos cual es la interfaz a la que conectarse y aceptar. NAT permite que con una solo IP publica puedan varios dispositivos salir a internet por esta IP DHCP Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos DHCP que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Despues de con configuraci\u00f3n posterior, vamos a herramientas - dhcp. En ipv4 creamos ambito nuevo, ponemos la ip del servidor y le damos un rango a partir de la siguiente a esa ip. PRACTICA DC Y 3 SERVERS Nombre servidor: dc01 -Tarjetas de Red propia y privada(ip/dnd 192.168.16.100 Instalar active directory y configurar nuevo bosque Hacer dns inverso de la red 192.168.16 Instalar acceso remoto y enruntamiento, configurar enruntamiento, opci\u00f3n Nat con la Red wan instalar Dhcp y configurar. Creamos \u00e1mbito, rango de ips 1-50 ejemplo. Puerta enlace ser\u00e1 nuestra ip del server abrimos otra m\u00e1quina de Windows y probamos que se pueda conectar x Dhcp del server y pueda navegar. No conectar directo a la Red que sale. crear unidades/usuarios con centro de administraci\u00f3n de AD o en grupo de unidades organizativas o usuarios y equipos AD. Siempre creamos una ra\u00edz. Los usuarios gestionamos su horario de conexi\u00f3n. Podemos hacer uno de modo plantilla y el resto copiar a partir de ah\u00ed. Creamos un grupo tmb de cada cosa. Agregamos cada usuario a su grupo. Y grupo con subgrupos instalamos en servicio de archivos - scsi - desduplicacion de datos. En configuraci\u00f3n luego en vol\u00famenes seleccionamos que unidad queremos hacerlo y su programaci\u00f3n. Esto ahorra mucho espacio y junta fragmentos de mismo archivos. crear una GPO de pol\u00edtica en objetos de administraci\u00f3n de unidades organizativas y editamos. Habilitamos directiva de redes y el ping(firewall permitir excepciones compartir archivos e impresoras entrantes y ponemos la ip de la Red global) y la de (firewallbpermir excepciones icmp). Tmb inicio de sesi\u00f3n interactivo t\u00edtulo mensaje al iniciar y luego el de texto mensaje. Vinculamos esta pol\u00edtica a la unidad general creada para que solo afecte a ellos. instalamos en servicios archivos de almacenamiento - scsi - administrador de recursos del servidor de archivos. Dentro en caracter\u00edsticas a\u00f1adimos copias de seguridad de Windows server. creamos una copia de seguridad - programamos personalizada todo lo que queremos hacer de copia y donde guardarlo. en recursos compartidos - tarea nuevo recurso y seleccionar carpeta a compartir y seleccionar qui\u00e9n puede acceder y sus permisos. Podemos decir que en la carpeta compartida los users no puedan almacenar cierto tipo de archivos. Vamos a herramientas - administrador de recursos compartidos del servidor - filtrado - filtros. Seleccionamos carpeta y ponemos po ejemplo bloquear archivos d audio y v\u00eddeo. Tambi\u00e9n podemos crear cuota para ponerle l\u00edmites a las carpetas abrimos otro Windows y cambiamos nombre del equipo y lueg lo a\u00f1adimos al dominio creado. se puede hacer una prueba de reiniciar remotamente el equipo desde cmd server. de un volumen o disco podemos ir a propiedades y habilitar instant\u00e1neas para poder volver a Estados anteriores. recuperar servidor desde una copia de seguridad. Arrancamos con la iso, reparar equipo, solucionar problemas, recuperar de una imagen del sistema y elegimos la copia. vpn modo sencillo: enrutamienti y acceso remoto. Bot\u00f3n derecho y en mi dc01 y configurar. Seleccionar vpn y elegimos interfaz que da Internet wan. Autom\u00e1ticamente elegimos, no radius. Vamos a otro equipo,configuraci\u00f3n de vpn vpn manual. Lo mismo pero elegimos Nat la interfaz. Propiedades de dc01, enrutador de la y servidor remoto. En ipv4 creamos interfaz /prot\u00f3 igmp(wan proxy y lan enrutador). Interfaz /prot\u00f3 Nat (wan interfaz Internet y habilitar;puertos de puerta enlace y de seguridad 4,ip loop, lan interfaz privada). Puertos/clientes propiedades (enrutador enrutamienti y servidor ipv4. Puertos propiedades activamos las dos opciones. 3 servers; ras, Dhcp, AD. AD en dominio con puerta enlace ras y dns el mismo 192.168.16.200 // 254//200 Ras dis interfaces. 192.168.100. 254 se instala enrutamiento y acceso remoto. Dhcp instala Dhcp puerta enlace ras //251 //254 En el ad se a\u00f1ade d servidor el dhcp Cliente coge IP del dhcp k coge Internet por el ras NOTAS Nat Y Red interna. Dhcp el del otro adaptador Crear equipos en herramientas de directivas de grupo, y en los usuarios creados cuando inicien sesi\u00f3n, en propiedades del sistema, agregar al dominio creado. En Linux en admin - autenticaci\u00f3 - cuentas de usuarios de winbind. Controladores es la IP y dominio con nombre corto Agregar caracter\u00edsticas para recursos compartidos y creamos una carpeta que sea compartida por los usuarios que digamos Si conectamos otro pc cn sistema en la misma Red interna del Dhcp como adaptador, le da la IP el dhcp Comandos cmd.: D: dir hostname sconfig cd c:\\Windows\\ruta shutdown Comandos powershell: dsadd agregar objetos, cuentas... Crear vlans: En Dhcp creamos \u00e1mbitos con diferentes rangos de ups y despu\u00e9s activamos \u00e1mbito. En opciones de servidores se crean los registros que ser\u00e1n comunes para todos los ambitos: se a\u00f1aden registros ips d dns, correo, enrutador Se puede crear superambito para unir zonas de \u00e1mbito como por ejemplo pisos de un sitio. Las reservas sirven para reservar la misma IP al que se conecta En dns se crean las zonas inversas de cada Red a\u00f1adida ACTIVE DIRECTORY Por powershell vemos las caracteristicas e instalamos: Get-WindowsFeature Install-WindowsFeature ad-domain-services, dns, dhcpserver, dhcp, rsat-dhcp -IncludeAllSubFeature Get-Command -module dhcpserver Import-Module / Import-Module addsdeployment MICROSOFT INTUNE Microsoft Intune es un servicio basado en la nube que se centra en la administraci\u00f3n de dispositivos m\u00f3viles (MDM) y en la administraci\u00f3n de aplicaciones m\u00f3viles (MAM). Puede controlar c\u00f3mo se usan los dispositivos de la organizaci\u00f3n, incluidos los tel\u00e9fonos m\u00f3viles, las tabletas y los equipos port\u00e1tiles. Tambi\u00e9n puede configurar directivas espec\u00edficas para controlar las aplicaciones. Por ejemplo, puede evitar que se env\u00eden mensajes de correo electr\u00f3nico a personas ajenas a la organizaci\u00f3n. Intune tambi\u00e9n permite que las personas de la organizaci\u00f3n usen sus dispositivos personales para la escuela o el trabajo. En los dispositivos personales, Intune ayuda a que los datos de la organizaci\u00f3n permanezcan protegidos y puede aislar los datos de la organizaci\u00f3n de los datos personales: Elegir estar al 100 % en una nube con Intune, o aplicar una administraci\u00f3n conjunta con Configuration Manager e Intune. Establecer reglas y configurar opciones en dispositivos personales y de propiedad de la organizaci\u00f3n para que obtengan acceso a los datos y las redes. Implementar y autenticar aplicaciones en dispositivos, locales y m\u00f3viles. Proteger la informaci\u00f3n de la empresa controlando la forma en que los usuarios acceden a la informaci\u00f3n y la comparten. Asegurarse de que los dispositivos y las aplicaciones cumplan los requisitos de seguridad. TUTORIAL Microsoft Intune , que forma parte de Microsoft Endpoint Manager, proporciona la infraestructura en la nube, la administraci\u00f3n de dispositivos m\u00f3viles (MDM) basada en la nube, la administraci\u00f3n de aplicaciones m\u00f3viles (MAM) basada en la nube y la administraci\u00f3n de equipos basada en la nube para su organizaci\u00f3n. Intune ayuda a asegurarse de que los dispositivos, aplicaciones y datos empresariales cumplen los requisitos de seguridad de la empresa. Tiene el control para establecer qu\u00e9 requisitos se deben comprobar y qu\u00e9 sucede cuando no se cumplen. En el Centro de administraci\u00f3n de Microsoft Endpoint Manager puede encontrar el servicio Microsoft Intune, as\u00ed como otras opciones relacionadas con la administraci\u00f3n de dispositivos. Comprender las caracter\u00edsticas disponibles en Intune le ayudar\u00e1 a realizar diferentes tareas de administraci\u00f3n de dispositivos m\u00f3viles (MDM) y administraci\u00f3n de aplicaciones m\u00f3viles (MAM). Podemos acceder desde: Microsoft azure - intune En devicemanagement.microsoft.com En portal.manage.microsoft.com (en usuarios finales) MICROSOFT AZURE ACTIVE DIRECTORY CONNECT es la herramienta para vincular la sincronizacion de AD con Intune para que los usuarios puedan acceder a los recursos locales y de la nube. AUTOPILOT es un conjunto de tecnolog\u00edas que se utilizan para configurar y preconfigurar nuevos dispositivos, prepar\u00e1ndolos para un uso productivo. Windows Autopilot se puede usar para implementar Windows equipos o HoloLens 2 dispositivos. Para obtener m\u00e1s informaci\u00f3n acerca de c\u00f3mo implementar HoloLens 2 con Autopilot, vea Windows Autopilot for HoloLens 2. Tambi\u00e9n se puede usar Windows Autopilot para restablecer, reasignar y recuperar dispositivos. Esta soluci\u00f3n permite a un departamento de TI lograr lo anterior con poca o ninguna infraestructura que administrar, con un proceso f\u00e1cil y sencillo. Windows Autopilot simplifica el ciclo de vida Windows dispositivo, tanto para ti como para usuarios finales, desde la implementaci\u00f3n inicial hasta el final de la vida \u00fatil. Con servicios basados en la nube, Windows Autopilot: reduce el tiempo que IT dedica a implementar, administrar y retirar dispositivos. reduce la infraestructura necesaria para mantener los dispositivos. maximiza la facilidad de uso para todos los tipos de usuarios finales.","title":"Windows"},{"location":"windows/#administrar-windows-server-2019","text":"","title":"ADMINISTRAR WINDOWS SERVER 2019"},{"location":"windows/#instalacion","text":"Para probar lo haremos con una maquina virtual. En este caso probaremos con HYPER-V , una herramienta de virtualizaci\u00f3n propia de windows que utiliza un hypervisor. Hyper-V es un programa de virtualizaci\u00f3n de Microsoft basado en un hipervisor para los sistemas de 64 bits\u200b con los procesadores basados en AMD-V o Tecnolog\u00eda de virtualizaci\u00f3n Intel. Un hipervisor o monitor de m\u00e1quina virtual \u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora. Los windows 10 home no lo tienen, no obstante con este fichero lo podremos aplicar: Enable HYPER-V Al reiniciar si no funciona directamente tendremos que hacer las siguientes opciones: Habilitar virtualizaci\u00f3n en la BIOS En la powershell comando bcdedit /set hypervisorlaunchtype auto En la powershell comando dism /online /enable-feature /featurename:Microsoft-Hyper-V -All Iniciamos maquina virtual y configuramos: Idioma/Regi\u00f3n S.O. versi\u00f3n standard evaluation con experiencia de escritorio para que tenga ventanas y no solo consola Instalaci\u00f3n personalizada (si le damos a SHIFT+F10 nos abre una consola de cmd para poder hacer cosas tambi\u00e9n) Varios reinicios Contrase\u00f1a administrador El controlador de dominio es el centro neur\u00e1lgico de un dominio Windows, tal como un servidor Network Information Service (NIS) lo es del servicio de informaci\u00f3n de una red Unix. Los controladores de dominio tienen una serie de responsabilidades, y una de ellas es la autenticaci\u00f3n, que es el proceso de garantizar o denegar a un usuario el acceso a recursos compartidos o a otra m\u00e1quina de la red, normalmente a trav\u00e9s del uso de una contrase\u00f1a. Esto permite validar a los usuarios de una red para ser partes de la plataforma de clientes que recibir\u00e1n los servicios de informaci\u00f3n. Cada controlador de dominio usa un security account manager (SAM), o NTDS en Windows 2003 Server (que es la forma promovida de la SAM, al pasar como controlador de dominio), para mantener una lista de pares de nombre de usuario y contrase\u00f1a. El controlador de dominio entonces crea un repositorio centralizado de contrase\u00f1as, que est\u00e1n enlazados a los nombres de usuarios (una clave por usuario), lo cual es m\u00e1s eficiente que mantener en cada m\u00e1quina cliente centenares de claves para cada recurso de red disponible. En un dominio Windows, cuando un cliente no autorizado solicita un acceso a los recursos compartidos de un servidor, el servidor act\u00faa y pregunta al controlador de dominio si ese usuario est\u00e1 autentificado. Si lo est\u00e1, el servidor establecer\u00e1 una conexi\u00f3n de sesi\u00f3n con los derechos de acceso correspondientes para ese servicio y usuario. Si no lo est\u00e1, la conexi\u00f3n es denegada. Una vez que el controlador de dominio autentifica a un usuario, se devuelve al cliente una ficha especial (token) de autenticaci\u00f3n, de manera que el usuario no necesitar\u00e1 volver a iniciar sesi\u00f3n para acceder a otros recursos en dicho dominio, ya que el usuario se considera autentificado en el dominio.","title":"INSTALACI\u00d3N"},{"location":"windows/#server-manager","text":"El server manager(administrador del servidor) es el aplicativo para configurar todas las cosas del server. Antes miramos las tarjetas de red con el comando ncpa.cpl . Elegimos la tarjeta - propiedades - ipv4 - propiedades y indicamos la IP que queremos ponerle. Cambiar nombre en servidor local y lo ponemos como un control de dominio(domain controller): En servidor local activamos el escritorio remoto y desactivamos aqui solo(en produccion no) la seguridad de Iexplorer. En servidor local abajo en rendimiento, iniciamos contador y a la dercha configuramos las alertas para 7 dias. En servidor local , parte arriba derecha en administrar le damos a agregar roles y caracteristicas. Seleccionamos el servidor y despues en roles de servidor, seleccionamos servicios de dominio de Active Directory . Siguiente, siguiente e instalamos. En powershell ser\u00eda con el comando install-windowsfeature y para ver los que hay get-windowsfeature Despues de instalar le damos a promover servicio para configurar el AD: Si se trabaja con aparatos mas antiguos, el nivel funcional hay que bajarlos. Siguiente pantalla En Panel - derecha Herramientas - DNS: En DC01 boton derecho ejecutar nslookup(nos sale unkown) Solucinamos dandole a zona inversa crear nueva. Siguiente hasta encontrar el cajon de poner ip, ponemos 192.168.1 Ahora actualizamos el puntero PTR en la directa dando boton derecho a la de la ip y le damos a actualizar y nos sale En boton derecho DC01 propiedades - reenviadores le ponemos los DNS. En revisi\u00f3n hacemos la prueba","title":"SERVER MANAGER"},{"location":"windows/#estructurando-el-ad","text":"Representaremos este ejemplo: Vamos al Panel - derecha Herramientas - Usuarios y equipos de Active Directory. Entramos a la organizaci\u00f3n creada de miguel.local y dentro creamos la organizaci\u00f3n global Miguel con boton derecho - nuevo - organizaci\u00f3n. Hemos creado un global porque luego para crear directivas de grupo, afectar\u00e1 a todo lo de abajo y sino tendriamos problemas porque afectaria a cosas que estan en la misma altura que no queremos que tengan esto. Ahora entramos en esta y vamos creando dentro nuevos deparamentos: Si tenemos algun error, al tener la casilla marcada de no poder eliminar. Vamos al menu ver - caracteriticas avanzadas - buscamos el departamento - propiedades - recursos y eliminamos la casilla y luego ya podremos mover o eliminar ese departamento. Ahora entramos en cada departamento y vamos creando usuarios en cada uno con una misma contrase\u00f1a. Ahora dentro de cada departamento , creamos un grupo con el mismo nombre. Esto se hace para poder luego compartir recursos, ya que por unidades organizativas no se puede. El departamento global de Miguel tambien tiene que tener un grupo creado. Porque asi si creamos un recurso o algo global, si se lo damos a esto, se lo aplica a todos los de su union: Ahora en cada departamento, seleccionamos todos los usuarios que hayan, boton derecho asignar un nuevo grupo. Escribimos el nombre del grupo, le damos a comprobar nombres y aceptamos. Si entramos a las propiedades del grupo, veremos que est\u00e1n como miembros. Despues de cada grupo, lo asignamos al grupo general Miguel .","title":"ESTRUCTURANDO EL AD"},{"location":"windows/#gpo","text":"Cpanel - Herramientas - Administracion de directivas de grupo. Todas las directivas se guardan en objetos de directivas de grupo. Vemos la que hay y las configuraciones que tienen: Para editar una vamos a boton derecho y editar. Desde aqu\u00ed para modificar algo se busca siguiendo la misma ruta que en la configuraci\u00f3n. Para crear una directiva de grupo(GPO) vamos a objetos y boton derecho crear. Todas las GPO para vincularlas necesitan una unidad organizativa. Para ello, vamos a la unidad de Contabilidad, boton derecho y vincular a la creada. Tambien se puede crear directamente yendo a la unidad y boton derecho crear GPO y vincular: GPO de inicio: sirve para que esa directiva sea comun para todos. Vamos a GPO de inicio y crear. Unas de las m\u00e1s comunes es el fondo de escritorio o que no usen los USB. Para crear una, en contenido boton derecho nuevo. Ahora si creamos una directiva de grupo, le podemos indicar que sea como base la de inicio: Para editar una GPO de inicio, entramos vamos a configuracion boton derecho edicion. Tocamos lo que sea y en la proxima GPO que creemos veremos que tiene estas configuraciones, partir\u00e1 con la reciente modificaci\u00f3n. Siempre parten de como esten en ese momento, los cambios no afectan a todas las que esten asignadas. Se puede crear GPO para la raiz y hereda para todos. No obstante, si queremos que algun departamento no herede, vamos a la unidad organizativa, boton derecho eliminar herencia: No obstante, aunque se quita la herencia. Si las GPO de la raiz le doy boton derecho y exigido. La van a tener si o si: La parte ultima de resultados de directivas de grupo, se puede generar un informe de un usuario/grupo de todas las directivas que tiene. Se puede hacer por consola con la orden gpresult /h file.html","title":"GPO"},{"location":"windows/#rasremote-access-server","text":"Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos acceso remoto que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Una vez instalado vamos a herramientas - enrutamiento y acceso remoto Vamos a boton derecho en DC01 y configurar, despues a tradducion de NAT y elegimos cual es la interfaz a la que conectarse y aceptar. NAT permite que con una solo IP publica puedan varios dispositivos salir a internet por esta IP","title":"RAS(Remote Access Server)"},{"location":"windows/#dhcp","text":"Primero instalamos el servicio de enrutamiento. Cpanel - administrar - roles y caracteristicas. En este caso seleccionamos DHCP que es donde esta y en los servicios agregamos enrutamiento. Todo siguiente hasta instalar. Despues de con configuraci\u00f3n posterior, vamos a herramientas - dhcp. En ipv4 creamos ambito nuevo, ponemos la ip del servidor y le damos un rango a partir de la siguiente a esa ip.","title":"DHCP"},{"location":"windows/#practica-dc-y-3-servers","text":"Nombre servidor: dc01 -Tarjetas de Red propia y privada(ip/dnd 192.168.16.100 Instalar active directory y configurar nuevo bosque Hacer dns inverso de la red 192.168.16 Instalar acceso remoto y enruntamiento, configurar enruntamiento, opci\u00f3n Nat con la Red wan instalar Dhcp y configurar. Creamos \u00e1mbito, rango de ips 1-50 ejemplo. Puerta enlace ser\u00e1 nuestra ip del server abrimos otra m\u00e1quina de Windows y probamos que se pueda conectar x Dhcp del server y pueda navegar. No conectar directo a la Red que sale. crear unidades/usuarios con centro de administraci\u00f3n de AD o en grupo de unidades organizativas o usuarios y equipos AD. Siempre creamos una ra\u00edz. Los usuarios gestionamos su horario de conexi\u00f3n. Podemos hacer uno de modo plantilla y el resto copiar a partir de ah\u00ed. Creamos un grupo tmb de cada cosa. Agregamos cada usuario a su grupo. Y grupo con subgrupos instalamos en servicio de archivos - scsi - desduplicacion de datos. En configuraci\u00f3n luego en vol\u00famenes seleccionamos que unidad queremos hacerlo y su programaci\u00f3n. Esto ahorra mucho espacio y junta fragmentos de mismo archivos. crear una GPO de pol\u00edtica en objetos de administraci\u00f3n de unidades organizativas y editamos. Habilitamos directiva de redes y el ping(firewall permitir excepciones compartir archivos e impresoras entrantes y ponemos la ip de la Red global) y la de (firewallbpermir excepciones icmp). Tmb inicio de sesi\u00f3n interactivo t\u00edtulo mensaje al iniciar y luego el de texto mensaje. Vinculamos esta pol\u00edtica a la unidad general creada para que solo afecte a ellos. instalamos en servicios archivos de almacenamiento - scsi - administrador de recursos del servidor de archivos. Dentro en caracter\u00edsticas a\u00f1adimos copias de seguridad de Windows server. creamos una copia de seguridad - programamos personalizada todo lo que queremos hacer de copia y donde guardarlo. en recursos compartidos - tarea nuevo recurso y seleccionar carpeta a compartir y seleccionar qui\u00e9n puede acceder y sus permisos. Podemos decir que en la carpeta compartida los users no puedan almacenar cierto tipo de archivos. Vamos a herramientas - administrador de recursos compartidos del servidor - filtrado - filtros. Seleccionamos carpeta y ponemos po ejemplo bloquear archivos d audio y v\u00eddeo. Tambi\u00e9n podemos crear cuota para ponerle l\u00edmites a las carpetas abrimos otro Windows y cambiamos nombre del equipo y lueg lo a\u00f1adimos al dominio creado. se puede hacer una prueba de reiniciar remotamente el equipo desde cmd server. de un volumen o disco podemos ir a propiedades y habilitar instant\u00e1neas para poder volver a Estados anteriores. recuperar servidor desde una copia de seguridad. Arrancamos con la iso, reparar equipo, solucionar problemas, recuperar de una imagen del sistema y elegimos la copia. vpn modo sencillo: enrutamienti y acceso remoto. Bot\u00f3n derecho y en mi dc01 y configurar. Seleccionar vpn y elegimos interfaz que da Internet wan. Autom\u00e1ticamente elegimos, no radius. Vamos a otro equipo,configuraci\u00f3n de vpn vpn manual. Lo mismo pero elegimos Nat la interfaz. Propiedades de dc01, enrutador de la y servidor remoto. En ipv4 creamos interfaz /prot\u00f3 igmp(wan proxy y lan enrutador). Interfaz /prot\u00f3 Nat (wan interfaz Internet y habilitar;puertos de puerta enlace y de seguridad 4,ip loop, lan interfaz privada). Puertos/clientes propiedades (enrutador enrutamienti y servidor ipv4. Puertos propiedades activamos las dos opciones. 3 servers; ras, Dhcp, AD. AD en dominio con puerta enlace ras y dns el mismo 192.168.16.200 // 254//200 Ras dis interfaces. 192.168.100. 254 se instala enrutamiento y acceso remoto. Dhcp instala Dhcp puerta enlace ras //251 //254 En el ad se a\u00f1ade d servidor el dhcp Cliente coge IP del dhcp k coge Internet por el ras","title":"PRACTICA DC Y 3 SERVERS"},{"location":"windows/#notas","text":"Nat Y Red interna. Dhcp el del otro adaptador Crear equipos en herramientas de directivas de grupo, y en los usuarios creados cuando inicien sesi\u00f3n, en propiedades del sistema, agregar al dominio creado. En Linux en admin - autenticaci\u00f3 - cuentas de usuarios de winbind. Controladores es la IP y dominio con nombre corto Agregar caracter\u00edsticas para recursos compartidos y creamos una carpeta que sea compartida por los usuarios que digamos Si conectamos otro pc cn sistema en la misma Red interna del Dhcp como adaptador, le da la IP el dhcp Comandos cmd.: D: dir hostname sconfig cd c:\\Windows\\ruta shutdown Comandos powershell: dsadd agregar objetos, cuentas... Crear vlans: En Dhcp creamos \u00e1mbitos con diferentes rangos de ups y despu\u00e9s activamos \u00e1mbito. En opciones de servidores se crean los registros que ser\u00e1n comunes para todos los ambitos: se a\u00f1aden registros ips d dns, correo, enrutador Se puede crear superambito para unir zonas de \u00e1mbito como por ejemplo pisos de un sitio. Las reservas sirven para reservar la misma IP al que se conecta En dns se crean las zonas inversas de cada Red a\u00f1adida","title":"NOTAS"},{"location":"windows/#active-directory","text":"Por powershell vemos las caracteristicas e instalamos: Get-WindowsFeature Install-WindowsFeature ad-domain-services, dns, dhcpserver, dhcp, rsat-dhcp -IncludeAllSubFeature Get-Command -module dhcpserver Import-Module / Import-Module addsdeployment","title":"ACTIVE DIRECTORY"},{"location":"windows/#microsoft-intune","text":"Microsoft Intune es un servicio basado en la nube que se centra en la administraci\u00f3n de dispositivos m\u00f3viles (MDM) y en la administraci\u00f3n de aplicaciones m\u00f3viles (MAM). Puede controlar c\u00f3mo se usan los dispositivos de la organizaci\u00f3n, incluidos los tel\u00e9fonos m\u00f3viles, las tabletas y los equipos port\u00e1tiles. Tambi\u00e9n puede configurar directivas espec\u00edficas para controlar las aplicaciones. Por ejemplo, puede evitar que se env\u00eden mensajes de correo electr\u00f3nico a personas ajenas a la organizaci\u00f3n. Intune tambi\u00e9n permite que las personas de la organizaci\u00f3n usen sus dispositivos personales para la escuela o el trabajo. En los dispositivos personales, Intune ayuda a que los datos de la organizaci\u00f3n permanezcan protegidos y puede aislar los datos de la organizaci\u00f3n de los datos personales: Elegir estar al 100 % en una nube con Intune, o aplicar una administraci\u00f3n conjunta con Configuration Manager e Intune. Establecer reglas y configurar opciones en dispositivos personales y de propiedad de la organizaci\u00f3n para que obtengan acceso a los datos y las redes. Implementar y autenticar aplicaciones en dispositivos, locales y m\u00f3viles. Proteger la informaci\u00f3n de la empresa controlando la forma en que los usuarios acceden a la informaci\u00f3n y la comparten. Asegurarse de que los dispositivos y las aplicaciones cumplan los requisitos de seguridad. TUTORIAL Microsoft Intune , que forma parte de Microsoft Endpoint Manager, proporciona la infraestructura en la nube, la administraci\u00f3n de dispositivos m\u00f3viles (MDM) basada en la nube, la administraci\u00f3n de aplicaciones m\u00f3viles (MAM) basada en la nube y la administraci\u00f3n de equipos basada en la nube para su organizaci\u00f3n. Intune ayuda a asegurarse de que los dispositivos, aplicaciones y datos empresariales cumplen los requisitos de seguridad de la empresa. Tiene el control para establecer qu\u00e9 requisitos se deben comprobar y qu\u00e9 sucede cuando no se cumplen. En el Centro de administraci\u00f3n de Microsoft Endpoint Manager puede encontrar el servicio Microsoft Intune, as\u00ed como otras opciones relacionadas con la administraci\u00f3n de dispositivos. Comprender las caracter\u00edsticas disponibles en Intune le ayudar\u00e1 a realizar diferentes tareas de administraci\u00f3n de dispositivos m\u00f3viles (MDM) y administraci\u00f3n de aplicaciones m\u00f3viles (MAM). Podemos acceder desde: Microsoft azure - intune En devicemanagement.microsoft.com En portal.manage.microsoft.com (en usuarios finales) MICROSOFT AZURE ACTIVE DIRECTORY CONNECT es la herramienta para vincular la sincronizacion de AD con Intune para que los usuarios puedan acceder a los recursos locales y de la nube. AUTOPILOT es un conjunto de tecnolog\u00edas que se utilizan para configurar y preconfigurar nuevos dispositivos, prepar\u00e1ndolos para un uso productivo. Windows Autopilot se puede usar para implementar Windows equipos o HoloLens 2 dispositivos. Para obtener m\u00e1s informaci\u00f3n acerca de c\u00f3mo implementar HoloLens 2 con Autopilot, vea Windows Autopilot for HoloLens 2. Tambi\u00e9n se puede usar Windows Autopilot para restablecer, reasignar y recuperar dispositivos. Esta soluci\u00f3n permite a un departamento de TI lograr lo anterior con poca o ninguna infraestructura que administrar, con un proceso f\u00e1cil y sencillo. Windows Autopilot simplifica el ciclo de vida Windows dispositivo, tanto para ti como para usuarios finales, desde la implementaci\u00f3n inicial hasta el final de la vida \u00fatil. Con servicios basados en la nube, Windows Autopilot: reduce el tiempo que IT dedica a implementar, administrar y retirar dispositivos. reduce la infraestructura necesaria para mantener los dispositivos. maximiza la facilidad de uso para todos los tipos de usuarios finales.","title":"MICROSOFT INTUNE"}]}