<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Miguel Amor√≥s">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Kubernetes - Miguel's Notes</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Kubernetes";
    var mkdocs_page_input_path = "kubernetes.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Miguel's Notes</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">MkDocs</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../linux/">Linux</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../gitlab/">Gitlab</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bash_scripting/">BashScripting</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../docker/">Docker</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Kubernetes</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#arquitectura">ARQUITECTURA</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#instalacion-minikubekubectl">INSTALACI√ìN MINIKUBE/KUBECTL</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods-vs-contenedores">PODS VS CONTENEDORES</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods">PODS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod">CREAR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs-pods">LOGS PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#api-resources">API-RESOURCES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminar-pods">ELIMINAR PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#obtener-yaml-pod">OBTENER YAML POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ip-pod">IP POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#entrar-al-pod">ENTRAR AL POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod-yaml">CREAR POD YAML</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-container-por-pod">2+ CONTAINER POR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#labels">LABELS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas-pods">PROBLEMAS PODs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#replicasets">REPLICASETS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-replicaset">CREAR REPLICASET</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminarmodificar">ELIMINAR/MODIFICAR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs">LOGS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#owner-refernce">OWNER REFERNCE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#adopcion-de-pods-planos">ADOPCI√ìN DE PODS PLANOS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas">PROBLEMAS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#deployments">DEPLOYMENTS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-deployment">CREAR DEPLOYMENT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rolling-update">ROLLING UPDATE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#historial-de-deployments">HISTORIAL DE DEPLOYMENTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#roll-backs">ROLL BACKS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#servicios">SERVICIOS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-servicio">CREAR SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#info-servicio">INFO SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#endpoints">ENDPOINTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns">DNS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-cluster-ip">SERVICIO CLUSTER-IP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-node-port">SERVICIO NODE-PORT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-load-balancer">SERVICIO LOAD BALANCER</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#golang">GOLANG</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-api-rest-go">CREAR API REST GO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mensaje-1">MENSAJE 1</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#mensaje-2">MENSAJE 2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dockerfile-golang">DOCKERFILE GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deployment-golang">DEPLOYMENT GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#consumo-del-servicio">CONSUMO DEL SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fronted">FRONTED</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#manifiesto-fronted">MANIFIESTO FRONTED</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#namespaces">NAMESPACES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-namespace">CREAR NAMESPACE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#asignar-namespaces">ASIGNAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#borrar-namespaces">BORRAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-namespaces">DEPLOY NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns-namespaces">DNS NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#contextos-namespaces">CONTEXTOS NAMESPACES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#limitar-ramcpu">LIMITAR RAM/CPU</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#limitsrequest">LIMITS/REQUEST</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#ram">RAM</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpu">CPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#qosquality-of-service">QOS(Quality of Service)</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../jenkins/">Jenkins</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ansible/">Ansible</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../markdown/">Markdown</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bbdd/">Base de datos</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../files/">Archivos Destacados</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Miguel's Notes</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Kubernetes</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/isx46410800/miguelamoros.github.io/edit/master/docs/kubernetes.md"> Edit on isx46410800/python</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="kubernetes">KUBERNETES</h1>
<ul>
<li>
<p><code>K8S</code> Es una herramienta extensible y de c√≥digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci√≥n declarativa como la automatizaci√≥n. Tiene un ecosistema grande y de r√°pido crecimiento. Los servicios, el soporte y las herramientas est√°n ampliamente disponibles.  </p>
</li>
<li>
<p>Funciones:  </p>
<ul>
<li>Service discovery: mira cuantos nodos hay, los escanea para saber de ellos.  </li>
<li>Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma.  </li>
<li>Optimizaci√≥n de recursos en nodos: mira donde colocar el contenedor al host con menos carga.  </li>
<li>Self-healing: crea automaticamente un contenedor cuando uno muere.  </li>
<li>Configuraci√≥n de secretos</li>
<li>Escalamiento horizontal</li>
</ul>
</li>
</ul>
<h2 id="arquitectura">ARQUITECTURA</h2>
<p><img alt="" src="../images/kubernetes.png" />  </p>
<ul>
<li>
<p><strong>MASTER/NODE</strong>: Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m√°quinas, host, m√°quinas virutal.<br />
El master es como la aduana y los nodes son  los barcos que se llevan los contenedores de la duana.  </p>
</li>
<li>
<p><strong>API SERVER</strong>: Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav√©s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci√≥n. Ambos son en JSON, por lo que acaba procesando todo en c√≥digo JSON.  </p>
</li>
<li>
<p><strong>KUBE-SCHEDULE</strong>: es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y √©ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor.  </p>
</li>
<li>
<p><strong>KUBE-CONTROLLER</strong>: dentro tiene el <em>node controler</em> (se encarga de ver nodos, si se cae uno, levanta otra m√°quina), el <em>replication</em>(encargado de mantener todas las r√©plicas especificadas), el <em>end point controller</em>(se encarga de la red y pods) y tenemos el <em>service account y tokens controller</em>(para la autenticaci√≥n).  </p>
</li>
<li>
<p><strong>ETCD</strong>: es la base de datos de kubernetes donde est√°n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi√≥n nueva y queremos volver atr√°s, en el <em>etcd</em> est√° guardado el estado y configuraci√≥n anterior.  </p>
</li>
<li>
<p><strong>KUBELET</strong>: se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci√≥n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo.  </p>
</li>
<li>
<p><strong>KUBE-PROXY</strong>:  se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods.  </p>
</li>
<li>
<p><strong>CONTAINER-RUNTIME</strong>: el software de contenedores que tiene instalado el nodo: docker,etc.  </p>
</li>
</ul>
<h2 id="instalacion-minikubekubectl">INSTALACI√ìN MINIKUBE/KUBECTL</h2>
<ul>
<li>
<p><strong>MINIKUBE</strong>: crea o simula un cluster peque√±o que nos permite hacerlo en local.  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Documentaci√≥n Kubernetes</a>  </p>
</li>
<li>
<p>Ejecutamos esta orden y sino sale vac√≠o , vamos bien:<br />
<code>grep -E --color 'vmx|svm' /proc/cpuinfo</code>  </p>
</li>
<li>
<p>Instalamos <code>kubectl</code>, la intermediario para hablar con kubernetes:  </p>
<ul>
<li>
<p><code>curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"</code>  </p>
</li>
<li>
<p><code>chmod +x ./kubectl</code>  </p>
</li>
<li>
<p><code>sudo mv ./kubectl /usr/bin/kubectl</code>  </p>
</li>
<li>
<p><code>kubectl version --client</code>  </p>
</li>
</ul>
</li>
<li>
<p>Para usar minikube se necesita un <code>Hypervisor</code>(o monitor de m√°quina virtual (virtual machine monitor)1‚Äã es una plataforma que permite aplicar diversas t√©cnicas de control de virtualizaci√≥n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora):  </p>
<ul>
<li>KVM</li>
<li>VirtualBox</li>
<li>Docker</li>
</ul>
</li>
<li>
<p>Descargamos <code>minikube</code>:  </p>
<ul>
<li>
<p><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube</code>  </p>
</li>
<li>
<p><code>sudo mv minikube /usr/bin/</code>  </p>
</li>
<li>
<p><code>minikube status</code>  </p>
</li>
</ul>
<p><code>[isx46410800@miguel curso_kubernetes]$ minikube status
ü§∑  There is no local cluster named "minikube"
üëâ  To fix this, run: "minikube start"
[isx46410800@miguel curso_kubernetes]$ minikube start
üòÑ  minikube v1.13.1 on Fedora 27
‚ú®  Automatically selected the docker driver
üëç  Starting control plane node minikube in cluster minikube
üöú  Pulling base image ...
üíæ  Downloading Kubernetes v1.19.2 preload ...
    &gt; preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB
üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
üßØ  Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity)
üí°  Suggestion: 
    Try at least one of the following to free up space on the device:
    1. Run "docker system prune" to remove unused docker data
    2. Increase the amount of memory allocated to Docker for Desktop via
    Docker icon &gt; Preferences &gt; Resources &gt; Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the docker container runtime
üçø  Related issue: https://github.com/kubernetes/minikube/issues/9024
üê≥  Preparing Kubernetes v1.19.2 on Docker 19.03.8 ...
üîé  Verifying Kubernetes components...
üåü  Enabled addons: default-storageclass, storage-provisioner
üèÑ  Done! kubectl is now configured to use "minikube" by default</code>  </p>
</li>
<li>
<p>Comprobamos de nuevo que s√≠ funciona <code>minikube status</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>

<ul>
<li>
<p><strong>COMANDOS B√ÅSICOS MINIKUBE</strong>:  </p>
<ul>
<li><code>minikube status</code></li>
<li><code>minikube stop/start/delete</code></li>
</ul>
</li>
<li>
<p>Repositorio <a href="https://github.com/ricardoandre97/k8s-resources">curso Kubernetes</a>  </p>
</li>
</ul>
<h2 id="pods-vs-contenedores">PODS VS CONTENEDORES</h2>
<p><img alt="" src="../images/kubernetes2.png" />  </p>
<ul>
<li>
<p>Los <strong>contenedores</strong> se ejecutan de manera aislada en un namespace:  </p>
<ul>
<li>IPC (Inter Process Communication)</li>
<li>Cgroup</li>
<li>Network</li>
<li>Mount</li>
<li>PID</li>
<li>User</li>
<li>UTS (Unix Timesharing System)</li>
</ul>
</li>
<li>
<p>Los <strong>PODS</strong> sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como:  </p>
<ul>
<li>De red(verse en la misma red)</li>
<li>IPC(verse los procesos)</li>
<li>UTS</li>
</ul>
</li>
</ul>
<blockquote>
<p>Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.</p>
</blockquote>
<h2 id="pods">PODS</h2>
<h3 id="crear-pod">CREAR POD</h3>
<ul>
<li>
<p>Primero tenemos que tener encendido el simulador:<br />
<code>minikube start</code>  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/reference/kubectl/conventions/">Documentaci√≥n</a>:<br />
<code>versi√≥n v1.19 la √∫ltima</code>  </p>
</li>
<li>
<p>Creamos un pod de prueba <code>kubectl run nombrePod --image:xxx:tag</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl run pod-test --image=nginx:alpine
pod/pod-test created
</code></pre>

<ul>
<li>Vemos que lo hemos creado y est√° corriendo:  </li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
pod-test   1/1     Running   0          22s
</code></pre>

<blockquote>
<p>Normalmente hay un contenedor por pod, se suele asimilar a eso.  </p>
</blockquote>
<h3 id="logs-pods">LOGS PODS</h3>
<ul>
<li>
<p>Un pod es la unidad m√°s peque√±a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre.   </p>
</li>
<li>
<p>Creamos uno pod mal aposta para ver el error:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll
pod/pod-test2 created
[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME        READY   STATUS         RESTARTS   AGE
pod-test    1/1     Running        0          5m19s
pod-test2   0/1     ErrImagePull   0          14s
</code></pre>

<ul>
<li>Para ver los ¬¥logs¬¥ usamos <code>kubectl describe pod nombrePod</code>:<br />
<code>kubectl describe pod pod-test</code>  <blockquote>
<p>En el apartado <code>events</code> nos describe los logs paso a paso.  </p>
</blockquote>
</li>
</ul>
<h3 id="api-resources">API-RESOURCES</h3>
<ul>
<li>Para ver todos los recursos que hay y los shortnames de comandos se usa:<br />
<code>kubectl api-resources</code>  </li>
</ul>
<h3 id="eliminar-pods">ELIMINAR PODS</h3>
<ul>
<li>
<p>Para eliminar pods usamos <code>kubectl delete pod podName ...</code>:<br />
<code>kubectl delete pod pod-test2</code>  </p>
</li>
<li>
<p>Todos:<br />
<code>kubectl delete pod --all</code>  </p>
</li>
</ul>
<h3 id="obtener-yaml-pod">OBTENER YAML POD</h3>
<ul>
<li>
<p>Podemos obtener info solo del pod concreto:<br />
<code>kubectl get pod pod-test</code></p>
</li>
<li>
<p>Para m√°s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request:<br />
<code>kubectl get pod pod-test -o yaml</code>  </p>
</li>
<li>
<p>Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav√©s de un fichero ser√° mejor que no ir poniendo una orden 50 veces.  </p>
</li>
</ul>
<h3 id="ip-pod">IP POD</h3>
<ul>
<li>Para poder ver la IP del POD podemos usar cualquiera de estos comandos:<br />
<code>kubectl describe pod pod-test</code><br />
<code>kubectl get pod pod-test -o yaml</code>  </li>
</ul>
<blockquote>
<p>En este caso es 172.18.0.3  </p>
</blockquote>
<ul>
<li>
<p>Para verlo ingresamos directamente al navegador la ip.  </p>
</li>
<li>
<p>Si no funciona tenemos que mapear el puerto:<br />
<code>kubectl port-forward pod-test 7000:80</code>  </p>
</li>
</ul>
<p><img alt="" src="../images/kubernetes4.png" />  </p>
<ul>
<li>Comprobamos la respuesta:<br />
<code>curl 172.18.0.3:80</code>  </li>
</ul>
<h3 id="entrar-al-pod">ENTRAR AL POD</h3>
<ul>
<li>
<p>Para ingresar a la consola del POD:<br />
<code>kubectl exec -it pod-test -- sh</code>  </p>
<blockquote>
<p>Cuando solo hay un contenedor, no se especifica el nombre del pod.  </p>
</blockquote>
</li>
<li>
<p>Cuando hay m√°s contenedores <code>c, --container=''</code>:<br />
<code>kubectl exec -it pod-test -c containerName -- sh</code>  </p>
</li>
</ul>
<h3 id="crear-pod-yaml">CREAR POD YAML</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>
<p>Las api versions las podemos ver en:<br />
<code>kubectl api-versions</code>  </p>
</li>
<li>
<p>Los kind los podemos ver en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Para crearlo a trav√©s del fichero YAML:<br />
<code>kubectl apply -f pod.yaml</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl apply -f pod.yaml
pod/pod-test2 created
[isx46410800@miguel pods]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test    1/1     Running   0          6h54m
pod-test2   1/1     Running   0          7s
</code></pre>

<ul>
<li>
<p>Para borrarlo:<br />
<code>kubectl delete -f pod.yaml</code>  </p>
</li>
<li>
<p>Para crear dos o m√°s PODS, se pone <code>---</code> de separaci√≥n:  </p>
</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
spec:
  containers:
    - name: container2
      image: nginx:alpine
</code></pre>

<h3 id="2-container-por-pod">2+ CONTAINER POR POD</h3>
<ul>
<li>Para crear dos o  m√°s containers en un POD se a√±ade en la subsecci√≥n containers:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
    - name: container2
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
</code></pre>

<blockquote>
<p>Nos dar√° error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente.  </p>
</blockquote>
<ul>
<li>Vemos los <code>logs</code> en <code>kubectl logs podName -c container</code>:  </li>
</ul>
<pre><code>263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2
Traceback (most recent call last):
...
  File &quot;/usr/local/lib/python3.6/socketserver.py&quot;, line 470, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address in use
</code></pre>

<ul>
<li>Arreglamos el fallo del puerto y comprobamos cada container del POD:  </li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh
/ # cat index.html 
cont1
/ # exit
[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh
/ # cat index.html 
cont2
</code></pre>

<h3 id="labels">LABELS</h3>
<ul>
<li>Los labels son etiquetas que se ponen debajo de los <code>metadata</code>:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
  labels:
    app: front-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
---   
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>Los labels nos sirve para poder filtrar PODs con <code>kubectl get pods -l nombre=valor</code>:</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl get pods -l app=back-end
NAME        READY   STATUS    RESTARTS   AGE
pod-test3   1/1     Running   0          62s
[isx46410800@miguel pods]$ kubectl get pods -l env=dev
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   1/1     Running   0          78s
pod-test3   1/1     Running   0          78s
</code></pre>

<blockquote>
<p>Los LABELS m√°s usado es el de APP. Muy importantes para administrar replicas.  </p>
</blockquote>
<h3 id="problemas-pods">PROBLEMAS PODs</h3>
<ul>
<li>
<p>Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion <code>image</code> y al hacer el apply puede ser que te deje actualizar.  </p>
</li>
</ul>
<h2 id="replicasets">REPLICASETS</h2>
<ul>
<li>
<p>Es un objeto separado del POD a un nivel m√°s alto(el replicaset crea PODs y es su due√±o).  </p>
</li>
<li>
<p>Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar.  </p>
</li>
<li>
<p>En la metadata del POD mete el <code>OWNER REFERENCE</code> para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.  </p>
</li>
</ul>
<h3 id="crear-replicaset">CREAR REPLICASET</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los replicasets en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-test
  labels:
    app: rs-test
spec:
  # modify replicas according to your case
  replicas: 5
  selector:
    matchLabels:
      app: pod-label
  # pertenece a los PODs que vas a crear
  template:
    metadata:
      labels:
        app: pod-label
    spec:
      containers:
        - name: container1
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
        - name: container2
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8083']
</code></pre>

<ul>
<li>
<p>Lo creamos:<br />
<code>kubectl apply -f replica-set.yaml</code>  </p>
<blockquote>
<p>Lo que creamos son 5 PODs con label(pod-label, sino est√° lo crea) y dentro de cada POD creamos dos containers con label(pod-label)  </p>
</blockquote>
</li>
<li>
<p>Comprobamos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          105s
rs-test-9jpjg   2/2     Running   0          105s
rs-test-fbwjb   2/2     Running   0          105s
rs-test-hz2kx   2/2     Running   0          105s
rs-test-s6cxx   2/2     Running   0          105s
[isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          119s
rs-test-9jpjg   2/2     Running   0          119s
rs-test-fbwjb   2/2     Running   0          119s
rs-test-hz2kx   2/2     Running   0          119s
rs-test-s6cxx   2/2     Running   0          119s
</code></pre>

<ul>
<li>Ver los <code>replicasets</code> con <code>kubectl get rs</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get rs
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m53s
[isx46410800@miguel replicaset]$ kubectl get replicaset
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m56s
</code></pre>

<h3 id="eliminarmodificar">ELIMINAR/MODIFICAR</h3>
<ul>
<li>En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx
pod &quot;rs-test-s6cxx&quot; deleted
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          5m43s
rs-test-9jpjg   2/2     Running   0          5m43s
rs-test-b9lf4   2/2     Running   0          43s
rs-test-fbwjb   2/2     Running   0          5m43s
rs-test-hz2kx   2/2     Running   0          5m43s
</code></pre>

<ul>
<li>Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ vim replica-set.yaml 
[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml 
replicaset.apps/rs-test configured
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS        RESTARTS   AGE
rs-test-5tsfh   2/2     Running       0          8m29s
rs-test-9jpjg   2/2     Terminating   0          8m29s
rs-test-b9lf4   2/2     Terminating   0          3m29s
rs-test-fbwjb   2/2     Running       0          8m29s
rs-test-hz2kx   2/2     Terminating   0          8m29s
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          9m26s
rs-test-fbwjb   2/2     Running   0          9m26s
</code></pre>

<h3 id="logs">LOGS</h3>
<ul>
<li>
<p>Por describe:<br />
<code>kubectl get rs rs-test -o yaml</code>  </p>
</li>
<li>
<p>Por manifiesto YAML:<br />
<code>kubectl describe rs rs-test</code>  </p>
</li>
</ul>
<h3 id="owner-refernce">OWNER REFERNCE</h3>
<ul>
<li>Lo vemos en la metadata de un pod creado por ReplicaSet <code>kubectl get pod podName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get pod rs-test-5tsfh -o yaml
name: rs-test-5tsfh
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: rs-test
    uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<ul>
<li>Comprobamos que el <code>UID</code> anterior coincide con el replicaset creado <code>kubectl get rs rsName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get rs rs-test -o yaml
name: rs-test
  namespace: default
  resourceVersion: &quot;22732&quot;
  selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test
  uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<h3 id="adopcion-de-pods-planos">ADOPCI√ìN DE PODS PLANOS</h3>
<ul>
<li>Vamos a crear primero dos PODs manualmente:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine
pod/pod-test created
[isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine
pod/pod-test2 created
</code></pre>

<ul>
<li>Les creamos un LABEL a cada uno con <code>kubectl label pods podName label=valor</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label
pod/pod-test labeled
[isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label
pod/pod-test2 labeled
</code></pre>

<blockquote>
<p>Tendran el nuevo label pero no tendr√°n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET.  </p>
</blockquote>
<ul>
<li>Ahora mediante replicaset cremos 3 replicas con mismo label:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml
replicaset.apps/rs-test created
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
pod-test        1/1     Running   0          3m49s
pod-test2       1/1     Running   0          3m45s
rs-test-8mk72   2/2     Running   0          10s
</code></pre>

<blockquote>
<p>Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.  </p>
</blockquote>
<h3 id="problemas">PROBLEMAS</h3>
<ul>
<li>
<p>Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene.  </p>
</li>
<li>
<p>NO se auto-actualizan solos.  </p>
</li>
<li>
<p>Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.  </p>
</li>
</ul>
<h2 id="deployments">DEPLOYMENTS</h2>
<ul>
<li>
<p>Es un objeto de nivel mayor que los replicaset. Es el due√±o del replicaset que a su vez es de sus PODs.  </p>
</li>
<li>
<p>Al deployment se le da una imagen o una nueva versi√≥n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as√≠ succesivamente.  </p>
</li>
<li>
<p>Esto se logra porque los deployments tienen dos valores: Uno de m√°ximo extra y otra de un m√°ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m√°s y solo dejar 1 pod inutilizado.    </p>
</li>
<li>
<p>Los deployments pueden mantener un m√°ximo de 10 replicasets  </p>
</li>
</ul>
<h3 id="crear-deployment">CREAR DEPLOYMENT</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los deployments en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>Lo creamos con <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test created
</code></pre>

<ul>
<li>Vemos el deployment creado <code>kubectl get deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           8s
</code></pre>

<ul>
<li>Vemos los labels del deployment <code>kubectl get deployment --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment --show-labels
NAME              READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
deployment-test   3/3     3            3           21s   app=front
</code></pre>

<ul>
<li>Vemos el estado del deployment <code>kubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>Vemos que se ha creado un replicaset y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       4m28s
[isx46410800@miguel deployments]$ kubectl get replicaset --show-labels
NAME                         DESIRED   CURRENT   READY   AGE    LABELS
deployment-test-659b64d66c   3         3         3       5m8s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos que se ha creado 3 replicas del pod y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running   0          4m32s
deployment-test-659b64d66c-pzdct   1/1     Running   0          4m32s
deployment-test-659b64d66c-thknz   1/1     Running   0          4m32s
[isx46410800@miguel deployments]$ kubectl get pods --show-labels
NAME                               READY   STATUS    RESTARTS   AGE     LABELS
deployment-test-659b64d66c-n5qgr   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-pzdct   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-thknz   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos la jerarqu√≠a de lo creado para saber quien es el <code>owner reference</code> de cada cosa con <code>kubectl get rs/pod/deployment NAME -o yaml</code>:  </li>
<li>Deployment no tiene due√±o</li>
<li>Replicaset su due√±o es deployment</li>
<li>Pod su due√±o es replicaset</li>
</ul>
<h3 id="rolling-update">ROLLING UPDATE</h3>
<ul>
<li>Actualizamos por ejemplo la imagen de un container del POD en vez de <code>nginx:alpine</code> ponemos <code>nginx</code> y hacemos de nuevo el <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test configured
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running             0          13m
deployment-test-659b64d66c-pzdct   1/1     Running             0          13m
deployment-test-659b64d66c-thknz   1/1     Running             0          13m
deployment-test-69b674677d-2cq4l   0/1     ContainerCreating   0          5s
[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     1            3           14m
[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       14m
deployment-test-69b674677d   1         1         0       18s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   0/1     Terminating         0          14m
deployment-test-659b64d66c-pzdct   1/1     Running             0          14m
deployment-test-659b64d66c-thknz   1/1     Terminating         0          14m
deployment-test-69b674677d-2cq4l   1/1     Running             0          25s
deployment-test-69b674677d-dwdlr   0/1     ContainerCreating   0          1s
deployment-test-69b674677d-dwspw   1/1     Running             0          6s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-69b674677d-2cq4l   1/1     Running   0          43s
deployment-test-69b674677d-dwdlr   1/1     Running   0          19s
deployment-test-69b674677d-dwspw   1/1     Running   0          24s
</code></pre>

<ul>
<li>Vemos el estado en directo de lo que hace con <code>ubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>Tambi√©n podemos ver el resultado en <code>kubectl describe deployment deploymentName</code>:  </li>
</ul>
<pre><code>Events:
  Type    Reason             Age                    From                   Message
  ----    ------             ----                   ----                   -------
  Normal  ScalingReplicaSet  19m                    deployment-controller  Scaled up replica set deployment-test-659b64d66c to 3
  Normal  ScalingReplicaSet  5m18s                  deployment-controller  Scaled up replica set deployment-test-69b674677d to 1
  Normal  ScalingReplicaSet  4m59s                  deployment-controller  Scaled down replica set deploy
</code></pre>

<ul>
<li>Aqu√≠ vemos tambi√©n la estrateg√≠a de los valores que comentamos en la introducci√≥n:<br />
<code>RollingUpdateStrategy:  25% max unavailable, 25% max surge</code>  </li>
</ul>
<h3 id="historial-de-deployments">HISTORIAL DE DEPLOYMENTS</h3>
<ul>
<li>Podemos ver las actualizaciones o revisiones en el historial de deployments en <code>kubectl rollout history deployment deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
2         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre>

<ul>
<li>Podemos con esto volver a cualquier versi√≥n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a√±adiento en la parte de replicaset del manifiesto YAML <code>revisionHistoryLimit: 5</code>:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  revisionHistoryLimit: 5
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>
<p>Para poner un motivo en el <code>change-cause</code> cuando hacemos una versi√≥n de deployments indicamos dos maneras:  </p>
</li>
<li>
<p>Con la linea de desplegar <code>kubectl apply -f deployment.yaml --record</code>:<br />
<code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record
  deployment.apps/deployment-test configured
  [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true</code>  </p>
</li>
<li>
<p>Con una subsecci√≥n en el manifiesto deployment.yaml <code>annotations-&gt; kubernetes.io/change-cause: "message"</code>:<br />
<code>esto es del deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-test
    annotations:
      kubernetes.io/change-cause: "changes port to 110"
    labels:
      app: front</code><br />
<code>kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true
  4         changes port to 110</code>  </p>
</li>
<li>
<p>Para luego ver una revisi√≥n en concreta usamos <code>kubectl rollout history deployment deployment-test --revision=3</code>:  </p>
</li>
</ul>
<pre><code>deployment.apps/deployment-test with revision #3
Pod Template:
  Labels:   app=front
    pod-template-hash=fd8445c88
  Annotations:  kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true
  Containers:
   nginx:
    Image:  nginx:alpine
    Port:   90/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>

<h3 id="roll-backs">ROLL BACKS</h3>
<ul>
<li>Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci√≥n de la imagen:  </li>
</ul>
<pre><code>containers:
      - name: nginx
        image: nginx:fake
        ports:
        - containerPort: 110
</code></pre>

<ul>
<li>Vemos el nuevo historial y su fallo:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
3         kubectl apply --filename=deployment.yaml --record=true
4         changes port to 110
5         new version nginx
#
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS         RESTARTS   AGE
deployment-test-5c6896bcd5-h5qts   0/1     ErrImagePull   0          32s
deployment-test-74fb9c6d9f-7dwnr   1/1     Running        0          6m50s
deployment-test-74fb9c6d9f-f5qs8   1/1     Running        0          6m45s
deployment-test-74fb9c6d9f-lsmzj   1/1     Running        0          6m54s
</code></pre>

<ul>
<li>Volvemos haciendo un <code>rollback</code> a una versi√≥n anterior con <code>kubectl rollout undo deployment deployment-test --to-revision=4</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4
deployment.apps/deployment-test rolled back
#
[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test
Name:                   deployment-test
Namespace:              default
CreationTimestamp:      Sun, 11 Oct 2020 19:21:04 +0200
Labels:                 app=front
Annotations:            deployment.kubernetes.io/revision: 6
                        kubernetes.io/change-cause: changes port to 110
Selector:               app=front
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=front
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         110/TCP
    Host Port:    0/TCP
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test 
Normal  ScalingReplicaSet  117s (x12 over 15m)  deployment-controller  (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0
</code></pre>

<h2 id="servicios">SERVICIOS</h2>
<ul>
<li>
<p>Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y √©ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique.  </p>
</li>
<li>
<p>Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi√©n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo.  </p>
</li>
<li>
<p>Los <code>endpoints</code> se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as√≠ el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.  </p>
</li>
</ul>
<h3 id="crear-servicio">CREAR SERVICIO</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los servicios en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---       
# a√±adimos el servicio que observar√° los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<blockquote>
<p>El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.  </p>
</blockquote>
<h3 id="info-servicio">INFO SERVICIO</h3>
<ul>
<li>Vemos lo creado con <code>kubectl get services/svc</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    41h
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   63s
[isx46410800@miguel services]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           79s
</code></pre>

<ul>
<li>Vemos por el label que le indicamos en el YAML:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=front
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   3m35s
</code></pre>

<blockquote>
<p>El cluster-ip se lo da kubernetes si no se lo asignamos directamente  </p>
</blockquote>
<ul>
<li>Profundizamos el servicio con <code>kubectl describe svc my-service</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl describe svc my-service
Name:              my-service
Namespace:         default
Labels:            app=front
Annotations:       &lt;none&gt;
Selector:          app=front
Type:              ClusterIP
IP:                10.97.182.119
Port:              &lt;unset&gt;  8888/TCP
TargetPort:        80/TCP
Endpoints:         172.18.0.2:80,172.18.0.4:80,172.18.0.5:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.  </p>
</blockquote>
<h3 id="endpoints">ENDPOINTS</h3>
<ul>
<li>
<p>Lista de IPs de los pods que tienen el label de mi servicio creado.  </p>
</li>
<li>
<p>Vemos la lista de endpoints con <code>kubectl get endpoints</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.4:80,172.18.0.5:80   10m
</code></pre>

<ul>
<li>Comprobamos que son las mismas de los PODS:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-cgds6   1/1     Running   0          10m   172.18.0.4   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          10m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          10m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<ul>
<li>Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6
pod &quot;deployment-test-b7c99d94b-cgds6&quot; deleted
[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.5:80,172.18.0.6:80   13m
[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          14m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-kcdnx   1/1     Running   0          39s   172.18.0.6   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          14m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<h3 id="dns">DNS</h3>
<ul>
<li>
<p>Creamos un POD nuevo:<br />
<code>[isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx_alpine -- sh</code>  </p>
</li>
<li>
<p>Funciona que escucha al servicio:  </p>
</li>
</ul>
<pre><code># curl 10.97.182.119:8888
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<blockquote>
<p>Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web.  </p>
</blockquote>
<ul>
<li>Se crea como un tipo de DNS ya que por el nombre del servicio tambi√©n se comunica y obtiene respuesta:<br />
<code># curl my-service:8888</code>  </li>
</ul>
<h3 id="servicio-cluster-ip">SERVICIO CLUSTER-IP</h3>
<ul>
<li>
<p>IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior.  </p>
</li>
<li>
<p>Le podemos poner un tipo de servicio a los servicios que creamos:  </p>
</li>
</ul>
<pre><code># a√±adimos el servicio que observar√° los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  type: ClusterIP
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<h3 id="servicio-node-port">SERVICIO NODE-PORT</h3>
<ul>
<li>
<p>IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi√©n un ClusterIP.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test2
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---
# a√±adimos el servicio que observar√° los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service2
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=backend
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
my-service2   NodePort   10.101.52.193   &lt;none&gt;        8888:30072/TCP   21s
[isx46410800@miguel services]$ kubectl get pods -l app=backend
NAME                                READY   STATUS    RESTARTS   AGE
deployment-test2-77448c6d65-gj6l7   1/1     Running   0          36s
deployment-test2-77448c6d65-n8td7   1/1     Running   0          36s
deployment-test2-77448c6d65-sd6zq   1/1     Running   0          36s
</code></pre>

<ul>
<li>
<p>Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio:<br />
<code>http://192.168.1.104:30072</code>  </p>
</li>
<li>
<p>Si no hace en minikube podemos hacer lo siguiente y lo veremos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ minikube service my-service2
|-----------|-------------|-------------|-------------------------|
| NAMESPACE |    NAME     | TARGET PORT |           URL           |
|-----------|-------------|-------------|-------------------------|
| default   | my-service2 |        8888 | http://172.17.0.2:30072 |
|-----------|-------------|-------------|-------------------------|
</code></pre>

<blockquote>
<p>Esa url nos dar√° el servicio web a trav√©s del node port.  </p>
</blockquote>
<h3 id="servicio-load-balancer">SERVICIO LOAD BALANCER</h3>
<ul>
<li>
<p>Hace referencia a un servicio de balanzador de carga.  </p>
</li>
<li>
<p>Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.  </p>
</li>
</ul>
<h2 id="golang">GOLANG</h2>
<ul>
<li>Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici√≥n al front y este le devuelva el servicio que est√° en backend.  </li>
</ul>
<h3 id="crear-api-rest-go">CREAR API REST GO</h3>
<ul>
<li>
<p><a href="https://dev.to/moficodes/build-your-first-rest-api-with-go-2gcj">DOCUMENTACI√ìN</a>  </p>
</li>
<li>
<p>Creamos un fichero simple de API REST en Goland:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel src]$ cat main.go 
package main
import (
    &quot;log&quot;
    &quot;net/http&quot;
)
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write([]byte(`{&quot;message&quot;: &quot;hello world&quot;}`))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil))
}
</code></pre>

<blockquote>
<p>Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar√° a la petici√≥n el hello wolld como respuesta.  </p>
</blockquote>
<ul>
<li>Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo:<br />
<code>[isx46410800@miguel k8s-hands-on]$ docker pull golang</code>  </li>
</ul>
<p><code>[isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash</code>  </p>
<ul>
<li>Iniciamos el fichero y comprobamos el resultado:  </li>
</ul>
<pre><code>[isx46410800@miguel src]$ docker exec -it goland /bin/bash
root@miguel:/go# go run main.go 
</code></pre>

<p>![./images/kubernetes4.png]  </p>
<h3 id="cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</h3>
<h3 id="mensaje-1">MENSAJE 1</h3>
<ul>
<li>A√±adimos unas variables para cambiar el mensaje de respuesta a la petici√≥n de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;time&quot;
)
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    resp := fmt.Sprintf(&quot;La hora es %v y el hostname es %v&quot;, time.Now(), os.Getenv(&quot;HOSTNAME&quot;))
    w.Write([]byte(resp))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<p>![./images/kubernetes5.png]  </p>
<h4 id="mensaje-2">MENSAJE 2</h4>
<ul>
<li>A√±adimos unas variables para cambiar el mensaje de respuesta a la petici√≥n de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;os&quot;
    &quot;time&quot;
    &quot;encoding/json&quot;
)
type HandsOn struct {
   Time     time.Time   `json:&quot;time&quot;`
   Hostname string      `json:&quot;hostname&quot;`
}
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    if r.URL.Path != &quot;/&quot; {
        http.NotFound(w, r)
        return
    }
    resp := HandsOn{
        Time:       time.Now(),
        Hostname:   os.Getenv(&quot;HOSTNAME&quot;),
    }
    jsonResp, err := json.Marshal(&amp;resp)
    if err != nil {
        w.Write([]byte(&quot;Error&quot;))
        return
    }
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write(jsonResp)
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<blockquote>
<p>Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi√©n darle un mensaje de error si no acaba en /.  </p>
</blockquote>
<p>![./images/kubernetes6.png]  </p>
<p>![./images/kubernetes7.png]  </p>
<h3 id="dockerfile-golang">DOCKERFILE GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM golang:1.13 as builder
# DIRECTORIO A TRABAJAR
WORKDIR /app
# COPIAMOS FICHERO MAIN
COPY main.go .
RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go
# DESDE IMAGEN ALPINE
FROM alpine:latest
# mailcap adds mime detection and ca-certificates help with TLS (basic stuff)
WORKDIR /app
COPY --from=builder /app/app .
# PARA EJECUTARLO
ENTRYPOINT [&quot;./app&quot;]
</code></pre>

<ul>
<li>
<p>Construimos imagen:<br />
<code>[isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on .</code>  </p>
</li>
<li>
<p>Encendemos:<br />
<code>[isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on</code>  </p>
</li>
</ul>
<p>![./images/kubernetes8.png]  </p>
<blockquote>
<p>Ahora nuestra aplicaci√≥n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.  </p>
</blockquote>
<h3 id="deployment-golang">DEPLOYMENT GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on
        imagePullPolicy: IfNotPresent
---
# a√±adimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<blockquote>
<p>Ponemos lo de <code>imagePullPolicy: IfNotPresent</code> para que primero busque si la imagen est√° constuida localmente antes de mirar en los repos de internet de dockerhub.  </p>
</blockquote>
<ul>
<li>Comprobaciones:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-5d548949c7-dgw9l   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-fg8wr   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-q9s6g   1/1     Running   0          15m
[isx46410800@miguel backend]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-5d548949c7   3         3         3       15m
[isx46410800@miguel backend]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           15m
[isx46410800@miguel backend]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
backend-k8s-hands-on   ClusterIP   10.101.44.56   &lt;none&gt;        80/TCP    16m
kubernetes             ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   3d19h
</code></pre>

<ul>
<li>Visual cambiando a nodeport, nos contestar√° unos de los PODs la respuesta a la request del usuario:  </li>
</ul>
<p>![./images/kubernetes9.png]  </p>
<h3 id="consumo-del-servicio">CONSUMO DEL SERVICIO</h3>
<ul>
<li>Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar√°n las respuestas los PODs del servicio:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh
/ # apk add -U curl
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:57:49.446174694Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:58:10.218346403Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
{&quot;time&quot;:&quot;2020-10-13T19:58:25.365295183Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-66dgx&quot;}/ # 
</code></pre>

<h3 id="fronted">FRONTED</h3>
<ul>
<li>Creamos ahora un index.html de respuesta en un fronted/src/index.html:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine
vi /usr/share/nginx/html/index.html
&lt;div id=&quot;id01&quot;&gt;&lt;/div&gt;

&lt;script&gt;
var xmlhttp = new XMLHttpRequest();
var url = &quot;http://backend-k8s-hands-on&quot;;

xmlhttp.onreadystatechange = function() {
    if (this.readyState == 4 &amp;&amp; this.status == 200) {
        var resp = JSON.parse(this.responseText);
        document.getElementById(&quot;id01&quot;).innerHTML = &quot;&lt;h2&gt;La hora es &quot; + resp.time + &quot;y el hostname es&quot; + resp.hostname &quot;&lt;/h2&quot;&gt;;
    }
};
xmlhttp.open(&quot;GET&quot;, url, true);
xmlhttp.send();
&lt;/script&gt;
</code></pre>

<ul>
<li>Dockerfile:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM nginx:alpine
# COPIAMOS FICHERO MAIN
COPY ./src/index.html /usr/share/nginx/html/index.html
</code></pre>

<h3 id="manifiesto-fronted">MANIFIESTO FRONTED</h3>
<ul>
<li>Despliegue del fronted:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fronted
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: fronted
    spec:
      containers:
      - name: fronted
        image: isx46410800/k8s-hands-on:fronted
        imagePullPolicy: IfNotPresent
---
# a√±adimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
spec:
  type: NodePort
  selector:
    app: fronted
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 80
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort    10.111.54.241   &lt;none&gt;        80:30740/TCP   78m
fronted-k8s-hands-on   NodePort    10.105.156.14   &lt;none&gt;        80:30159/TCP   9m22s
kubernetes             ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        3d20h
[isx46410800@miguel k8s-hands-on]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-lzrr4   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-mdjh7   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-qxzdv   1/1     Running   0          51m
fronted-k8s-hands-on-78f59c5f77-dpvck   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-q7h9r   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-r7fnm   1/1     Running   0          9m27s
[isx46410800@miguel k8s-hands-on]$ kubectl cluster-info
Kubernetes master is running at https://172.17.0.2:8443
KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<h2 id="namespaces">NAMESPACES</h2>
<ul>
<li>
<p>Son como ambientes separados dentro del cluster de kubernetes.  </p>
</li>
<li>
<p>Cada uno de estos ambientes tienen su deployment, replicaset, pods...  </p>
</li>
<li>
<p>Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster.  </p>
</li>
<li>
<p>En cada namespace se puede limitar los pods, la memoria, usuarios...</p>
</li>
<li>
<p>Ordenes b√°sicas:<br />
<code>kubectl get namespaces</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   3d23h
kube-node-lease   Active   3d23h
kube-public       Active   3d23h
kube-system       Active   3d23h
</code></pre>

<ul>
<li>
<p>Especifica por namespace:<br />
<code>kubectl get pods --namespace default</code>  </p>
</li>
<li>
<p>El default van todos los recursos, lo creado donde no se asignan ningun namespace.  </p>
</li>
<li>Todos los usuarios pueden ver este namespace kube-public.  </li>
<li>
<p>El kube-system tiene todos los objetos del kubernetes.  </p>
</li>
<li>
<p>Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr√°n los asignados. Habr√° que poner -n/--namespace namespaceName</p>
</li>
</ul>
<h3 id="crear-namespace">CREAR NAMESPACE</h3>
<ul>
<li>Por comando <code>kubectl create namespace nameNamespace</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns
namespace/test-ns created
</code></pre>

<ul>
<li>Para verlo <code>kubectl get namespaces</code> y <code>kubectl describe namespaces test-ns</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
test-ns           Active   4s
[isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns
Name:         test-ns
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Active
No resource quota.
No LimitRange resource.
</code></pre>

<ul>
<li>Por manifiesto YAML:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
</code></pre>

<ul>
<li>Comprobamos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml 
namespace/development created
[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE     LABELS
default           Active   4d      &lt;none&gt;
development       Active   19s     name=development
kube-node-lease   Active   4d      &lt;none&gt;
kube-public       Active   4d      &lt;none&gt;
kube-system       Active   4d      &lt;none&gt;
test-ns           Active   6m33s   &lt;none&gt;
</code></pre>

<h3 id="asignar-namespaces">ASIGNAR NAMESPACES</h3>
<ul>
<li>Creamos un pod y lo asignamos:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns
pod/podtest2 created
[isx46410800@miguel namespaces]$ kubectl get pods -n test-ns
NAME       READY   STATUS    RESTARTS   AGE
podtest2   1/1     Running   0          22s
</code></pre>

<h3 id="borrar-namespaces">BORRAR NAMESPACES</h3>
<ul>
<li>
<p>Borramos POD asignado a namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns</code>  </p>
</li>
<li>
<p>Borrar manifiesto:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml</code>  </p>
</li>
<li>
<p>Borrar namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns</code>  </p>
</li>
</ul>
<h3 id="deploy-namespaces">DEPLOY NAMESPACES</h3>
<ul>
<li>Creamos dos namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
--- 
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
</code></pre>

<ul>
<li>Lo vemos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE   LABELS
default           Active   4d    &lt;none&gt;
dev               Active   6s    name=dev
kube-node-lease   Active   4d    &lt;none&gt;
kube-public       Active   4d    &lt;none&gt;
kube-system       Active   4d    &lt;none&gt;
prod              Active   6s    name=prod
</code></pre>

<ul>
<li>Creamos un deployment con los namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
--- 
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-dev
  namespace: dev
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---     
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-prod
  namespace: prod
  labels:
    app: back
# aqui viene el replicaset
spec:
  replicas: 5
  selector:
    matchLabels:
      app: back
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: back
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml 
namespace/dev unchanged
namespace/prod unchanged
deployment.apps/deploy-dev created
deployment.apps/deploy-prod created
[isx46410800@miguel namespaces]$ kubectl get deploy -n dev
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
deploy-dev   1/1     1            1           26s
[isx46410800@miguel namespaces]$ kubectl get deploy -n prod
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
deploy-prod   5/5     5            5           29s
[isx46410800@miguel namespaces]$ kubectl get rs -n dev
NAME                   DESIRED   CURRENT   READY   AGE
deploy-dev-b7c99d94b   1         1         1       36s
[isx46410800@miguel namespaces]$ kubectl get rs -n prod
NAME                     DESIRED   CURRENT   READY   AGE
deploy-prod-7bfb7875fd   5         5         5       38s
[isx46410800@miguel namespaces]$ kubectl get pods -n dev
NAME                         READY   STATUS    RESTARTS   AGE
deploy-dev-b7c99d94b-xc696   1/1     Running   0          50s
[isx46410800@miguel namespaces]$ kubectl get pods -n prod
NAME                           READY   STATUS    RESTARTS   AGE
deploy-prod-7bfb7875fd-49kzd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-9m7x8   1/1     Running   0          54s
deploy-prod-7bfb7875fd-nbhfd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-tl5gs   1/1     Running   0          54s
deploy-prod-7bfb7875fd-wxrwc   1/1     Running   0          54s
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
dev               Active   10m
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
prod              Active   10m
</code></pre>

<h3 id="dns-namespaces">DNS NAMESPACES</h3>
<ul>
<li>Creamos un namespace y un deploy asignados:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: ci
  labels:
    name: ci
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on:v2
        imagePullPolicy: IfNotPresent
---
# a√±adimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml 
namespace/ci created
deployment.apps/backend-k8s-hands-on created
service/backend-k8s-hands-on created
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
ci                Active   15s
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
[isx46410800@miguel namespaces]$ kubectl get deploy -n ci
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           32s
[isx46410800@miguel namespaces]$ kubectl get svc -n ci
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   38s
[isx46410800@miguel namespaces]$ kubectl get rs -n ci
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       46s
[isx46410800@miguel namespaces]$ kubectl get pods -n ci
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          49s
</code></pre>

<ul>
<li>
<p>Ahora creamos un POD con el namespace default:<br />
<code>[isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh</code>  </p>
</li>
<li>
<p>Por defecto, cuando los dns que se crean en un namespace siguen esta regla:<br />
<code>serviceName + namespaceName + service.cluster.local</code>  </p>
</li>
<li>
<p>As√≠ desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior:<br />
<code>/ # curl backend-k8s-hands-on.ci.svc.cluster.local</code><br />
<code>{"time":"2020-10-14T01:09:56.22990857Z","hostname":"backend-k8s-hands-on-7d5b6dc559-7xv59"}/</code>  </p>
</li>
<li>
<p>Si no dar√≠a error:  </p>
</li>
</ul>
<pre><code>/ # curl backend-k8s-hands-on
curl: (6) Could not resolve host: backend-k8s-hands-on
</code></pre>

<h3 id="contextos-namespaces">CONTEXTOS NAMESPACES</h3>
<ul>
<li>
<p>Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato.  </p>
</li>
<li>
<p>Para ver en que contexto estamos usamos:<br />
<code>kubectl config current-context</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config current-context
minikube
</code></pre>

<ul>
<li>Vemos el archivo de configuraci√≥n <code>./kube/config</code> que es de donde lee el current-context:  </li>
</ul>
<pre><code>[root@miguel ~]# cat /home/isx46410800/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Mejor con este comando <code>kubectl config view</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Crear un nuevo contexto <code>ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube
Context &quot;ci-context&quot; created.
</code></pre>

<ul>
<li>Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    namespace: ci
    user: minikube
  name: ci-context
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Para cambiar de contexto usamos <code>kubectl config use-context Namecontext</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config use-context ci-context
Switched to context &quot;ci-context&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          19m
[isx46410800@miguel namespaces]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           19m
[isx46410800@miguel namespaces]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       20m
[isx46410800@miguel namespaces]$ kubectl get services
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   20m
[isx46410800@miguel namespaces]$ kubectl config use-context minikube
Switched to context &quot;minikube&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
No resources found in default namespace.
[isx46410800@miguel namespaces]$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   4d
</code></pre>

<h2 id="limitar-ramcpu">LIMITAR RAM/CPU</h2>
<ul>
<li>
<p>La RAM se puede limitar en B, MB y G.  </p>
</li>
<li>
<p>La CPU: 1 cpu es 1000 milicores/milicpus.  </p>
</li>
</ul>
<h3 id="limitsrequest">LIMITS/REQUEST</h3>
<ul>
<li>
<p>Los <strong>LIMITS</strong> es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr√≠a tener m√°s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar√° o reiniciar√° el pod y lo pondr√° en otro sitio que le garantice esa cantidad de recursos indicada.  </p>
</li>
<li>
<p>Los <strong>REQUESTS</strong> es la cantidad de recursos que el pod siempre va a poder disponer. Estar√° garantizado la cantidad que se le indique.  </p>
</li>
</ul>
<h4 id="ram">RAM</h4>
<ul>
<li>Creamos un ejemplo de limite de RAM:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: &quot;200Mi&quot;
      requests:
        memory: &quot;100Mi&quot;
    command: [&quot;stress&quot;]
    # se indica que le va a dar 150Megas
    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]
</code></pre>

<blockquote>
<p>Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M.  </p>
</blockquote>
<ul>
<li>Comprobamos que lo ha creado <code>kubectl apply -f limit-request.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   1/1     Running   0          38s
</code></pre>

<ul>
<li>Si ponemos el ejemplo anterior con 250M vemos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml 
pod/memory-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
memory-demo   0/1     ContainerCreating   0          4s
[isx46410800@miguel limits-requests]$ kubectl get pods --watch
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   1          12s
memory-demo   0/1     OOMKilled          2          25s
memory-demo   0/1     CrashLoopBackOff   2          26s
^C[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   2          48s
</code></pre>

<ul>
<li>Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient memory.
</code></pre>

<h4 id="cpu">CPU</h4>
<ul>
<li>Ejemplo de limitar CPU:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
spec:
  containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;0.5&quot;
    args:
    - -cpus
    - &quot;2&quot;
    # se le pide 2 cpus y hay limite de 1
</code></pre>

<blockquote>
<p>Aunque se le pida 2, no se eliminar√° como la RAM sino que soolo tendr√° de m√°ximo el LIMIT indicado(1).  </p>
</blockquote>
<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml 
pod/cpu-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS              RESTARTS   AGE
cpu-demo   0/1     ContainerCreating   0          7s
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
cpu-demo   1/1     Running   0          11s
</code></pre>

<ul>
<li>Si vemos la capacidad total de mi cluster <code>kubectl describe node minikube</code>: </li>
</ul>
<pre><code>Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1150m (28%)  1 (25%)
  memory             70Mi (0%)    170Mi (2%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
</code></pre>

<blockquote>
<p>Siempre hay un poco m√°s para que no sobrepase el limite y me vaya todo lento.  </p>
</blockquote>
<ul>
<li>Nuestra cantidad de CPU <code>kubectl describe node minikube</code>:  </li>
</ul>
<pre><code>kubectl describe node minikube
Capacity:
  cpu:                4
</code></pre>

<ul>
<li>Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
cpu-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.
</code></pre>

<h3 id="qosquality-of-service">QOS(Quality of Service)</h3>
<ul>
<li>
<p>Es una propiedad que se le asigna a los pods.  </p>
</li>
<li>
<p>Podemos ver el estado de QoS con:<br />
<code>kubectl get pod podName -o yaml | grep -i qos</code>  </p>
</li>
<li>
<p>Hay diferentes tipos de clases de estado en el que entra el POD:  </p>
<ul>
<li><strong>BestEffort</strong>: No se definen los limites y request. Los asignar√° el schedule pero puede ser que este consuma y consuma recursos sin parar.</li>
<li><strong>Guaranteed</strong>: Tiene los mismos limites que de request</li>
<li><strong>Burstable</strong>: cuando pueda aumentar el request. El limite es mayor que el request.</li>
</ul>
</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../jenkins/" class="btn btn-neutral float-right" title="Jenkins">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../docker/" class="btn btn-neutral" title="Docker"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../docker/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../jenkins/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
