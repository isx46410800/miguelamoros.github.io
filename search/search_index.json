{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apuntes Miguel For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"MkDocs"},{"location":"#apuntes-miguel","text":"For full documentation visit mkdocs.org .","title":"Apuntes Miguel"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Deploy to GitHub Pages.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"ansible/","text":"ANSIBLE Apuntes Ansible Libros Ansible Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX. Instalaci\u00f3n yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada. Inventarios Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Comando b\u00e1sico ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta Ayuda Ansible ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user Playbook Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula) M\u00f3dulos Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes Variables Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" Condicionales Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\" Bucles Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario Roles Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks... Ansible Galaxy Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker'","title":"Ansible"},{"location":"ansible/#ansible","text":"Apuntes Ansible Libros Ansible Es un software de gesti\u00f3n de la configuraci\u00f3n autom\u00e1tica y remota. Nos permite centralizar la configuraci\u00f3n de numerosas servidores, dispositivos de red y Cloud Providers de una forma sencilla y automatizada. Podremos aprovisionar servidores en AWS, Azure o VMWARE y automatizar la configuraci\u00f3n de dichos servidores. Ventajas: No requiere agentes Multiplataforma, eficiente y seguro Aprovisiona infraestructuras Configura dispositivos de red Se necesita un Ansible Controller ejecutando en un SO Linux. Se puede administrar equipos Windows/Max pero el Ansible Controller debe ser LINUX.","title":"ANSIBLE"},{"location":"ansible/#instalacion","text":"yum install ansible RedHat dnf install ansible Fedora apt-get install ansible Ubuntu pip install ansible Python-Pip brew install ansible MAC ansible --version comprobamos la versi\u00f3n instalada.","title":"Instalaci\u00f3n"},{"location":"ansible/#inventarios","text":"Ansible trabaja ejecutando tareas contra diferentes equipos remotos, dispositivos de red o APIs. Nos permiten definir dichos equipos, agruparlos y especificar valores grupales o individuales de los mismos. Formato Ansible INI, YAML o JSON. /etc/ansible/hosts fichero por defecto donde se define o ruta concreta -i file . ansbible.cfg fichero de configuraci\u00f3n. EJEMPLO: [masters] # nombre general master ansible_host=IP/FQDN/service_docker ansible_user=remote_user ansible_private_key_file=xxx.pem # nombre - maqquina a conectar - usuario a conectar - private_key Comprobamos la conexi\u00f3n: ansible -i inventory -m ping all ansible -m ping -i hosts master -m de modulo -i fichero y maquina master | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"Inventarios"},{"location":"ansible/#comando-basico","text":"ansible -i <inventory_path> -m {modulo} -a \"{modulo opciones}\" <nodos: all/master> Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"echo 'hola'\" all Ejemplo: ansible -i hosts -m shell -a \"ls -l /etc\" all/masters Ejemplo: ansible -i hosts -b -m user -a \"name=andy state=present shell=/bin/bash\" all atacamos a todos los users(all) y le creamos un usuario andy. -b de superuser, con una shell concreta","title":"Comando b\u00e1sico"},{"location":"ansible/#ayuda-ansible","text":"ansible-doc -l Ejemplo de ayuda de un m\u00f3dulo concreto: ansible-doc (-s) user","title":"Ayuda Ansible"},{"location":"ansible/#playbook","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Ejemplo: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Ejemplo: - hosts: test1 tasks: - debug: var: MSG Ejemplo: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec Ejemplo completo de crear un user: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create user miguel user: name=andy state= present ORDEN: ansible-playbook -i hosts playbook.yml --syntax ansible-playbook -i hosts playbook.yml --check (solo simula)","title":"Playbook"},{"location":"ansible/#modulos","text":"Conocidos tambi\u00e9n task plugins o library plugins, son unidades discretas de c\u00f3digo que se pueden utilizar desde linea de comandos o playbook. Se suelen utilizar en el nodo de destino remoto y recopila los valores de retorno. Se pueden utilizar en ad-hoc commands, playbooks y roles. Ejemplo m\u00f3dulo apt: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: ansible state: present Ejemplo m\u00f3dulo authorized_keys: - hosts: master become: yes # ser superuser tasks: - name: create user andy user: name: andy state: present shell: /bin/bash - name: create ssh keys authorized_keys: user: andy key: \"{{ item }}\" state: present with_file: - ~/.ssh/id_rsa.pub no_log: yes","title":"M\u00f3dulos"},{"location":"ansible/#variables","text":"Ejemplo de variables para Ansible: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\"","title":"Variables"},{"location":"ansible/#condicionales","text":"Realizar tareas segun ciertas cosas o par\u00e1metros: Ejemplo condicional: - name: Demo Install Ansible hosts: all become: yes ## definimos las variables vars: package: ansible state: present tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ package }}\" state: \"{{ state }}\" ## indicando la condicion de solo en master when: \"'master' in inventory_hostname\"","title":"Condicionales"},{"location":"ansible/#bucles","text":"Ejemplo de bucle: - name: Demo Install Ansible hosts: all become: yes tasks: ## instalando ansible usando apt-get - name: install ansible using apt apt: name: \"{{ item }}\" state: present ## indicando bucle de paquetes a instalar loop: - ansible - apache2 - name: Demo Install Ansible hosts: all become: yes tasks: - name: create users user: name: \"{{ item }}\" state: present/absent ## indicando bucle de crear users with_items: - andy - miguel - mario","title":"Bucles"},{"location":"ansible/#roles","text":"Los roles son formas de cargar autom\u00e1ticamente una estructura de archivos/directorios, archivos de variables, tareas y controladores basados en una estructura de archivos conocida. Agrupar contenido por roles permite compartir los roles con otros usuarios y poder reutilizar c\u00f3digo. Los roles esperan que los archivos esten en ciertos directorios, deben incluir al menos uno de estos. Ejemplo de role: - name: Play to demo roles hosts: all become: yes ## roles block roles: ## the role we want to install - apache ## dentro de este directorio hay muchos files, playbooks, tasks...","title":"Roles"},{"location":"ansible/#ansible-galaxy","text":"Es un sitio gratuito para buscar, descargar, calificar y revisar toto tipo de roles de Ansible desarrollados por la comunidad y puede ser una excelente manera de impulsar nuestros proyectos de automatizaci\u00f3n. El cliente ansible-galaxy est\u00e1 incluido en Ansible. Ejemplo: ## ansible-galaxy ## install a role in 'roles' folder ansible-galaxy install \"ansible.docker\" -p roles/ ## create a role folders/files structure ansible-galaxy init \"my-role\" ## search for a role ansible-galaxy search 'docker'","title":"Ansible Galaxy"},{"location":"bash_scripting/","text":"Comandos para Bash Scripting #! /bin/bash validar argumentos # indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0 validar help #help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi comparadores -lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi conficional if edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi bucle for count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0 bucle while cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0 bucle esac for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0 leer por entrada standard count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0 separar opciones con shift opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0 mirar si es directorio, regular file... #files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0 Mayusculas , copiar , buscar , ordenar tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null) Funciones function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"BashScripting"},{"location":"bash_scripting/#comandos-para-bash-scripting","text":"#! /bin/bash","title":"Comandos para Bash Scripting"},{"location":"bash_scripting/#validar-argumentos","text":"# indica el numero de argumentos pasados echo $# # si no tiene 2 args, plegar if [ $# -ne 2 ] then echo \"Num de args incorrecte\" echo \"USAGE: prog.sh arg1 arg2\" exit 1 fi # mostrar los args echo \"Los argumentos son $1 y $2\" exit 0","title":"validar argumentos"},{"location":"bash_scripting/#validar-help","text":"#help if [ $1 == \"--help\" -o $1 == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi","title":"validar help"},{"location":"bash_scripting/#comparadores","text":"-lt / -gt / -eq / -ne / -ge / -le m\u00e1s grande, menor que... -d / -f / -h directorio, file, link -n mientras sea nulo $? estatus level de error /dev/stderr / >>/dev/null if [ $# -eq 3 ] then if ! [ \"$1\" == \"-b\" -a \"$2\" == \"-a\" ] then echo \"ERROR: format args incorrecte\" echo \"USAGE: prog arg -h / prog -a arg / prog -b -a arg\" exit $ERR_NARGS fi fi","title":"comparadores"},{"location":"bash_scripting/#conficional-if","text":"edad=$1 echo $edad if [ $edad -lt 18 ] then echo \"Es menor de edat\" elif [ $edad -eq 18 ] then echo \"18 recien cumplidos\" else echo \"Ya es mayor de edad\" fi","title":"conficional if"},{"location":"bash_scripting/#bucle-for","text":"count=0 for arg in $* do count=$((count+1)) echo \"$count: $arg\" done exit 0","title":"bucle for"},{"location":"bash_scripting/#bucle-while","text":"cont=0 MAX=$1 while [ $cont -le $MAX ] do echo $cont cont=$((cont+1)) done exit 0","title":"bucle while"},{"location":"bash_scripting/#bucle-esac","text":"for mes in $* do case $mes in '2') echo \"$mes tiene 28 dias\";; '1'|'3'|'5'|'7'|'8'|'10'|'12') echo \"$mes tiene 31 dias\";; '2'|'4'|'6'|'9'|'11') echo \"$mes tiene 30 dias\";; *) echo \"$mes no es un mes correcto\" esac done exit 0","title":"bucle esac"},{"location":"bash_scripting/#leer-por-entrada-standard","text":"count=0 while read -r line do count=$((count+1)) echo \"$count: $line\" done exit 0 MAX=$1 cont=1 while read -r line && [ $cont -le $MAX ] do echo \"$cont: $line\" cont=$((cont+1)) done exit 0","title":"leer por entrada standard"},{"location":"bash_scripting/#separar-opciones-con-shift","text":"opciones='' argumentos='' numeros='' files='' while [ -n \"$1\" ] do case $1 in '-a') files=$2 shift;; '-c'|'-b'|'-e') opciones=\"$opciones $1\";; '-d') numeros=$2 shift;; *) argumentos=\"$argumentos $1\";; esac shift done echo \"opciones: $opciones\" echo \"argumentos: $argumentos\" echo \"numeros: $numeros\" echo \"files: $files\" exit 0","title":"separar opciones con shift"},{"location":"bash_scripting/#mirar-si-es-directorio-regular-file","text":"#files y dir a copiar llistafiles=$(echo $* | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $* | cut -d' ' -f$#) #comprobar que sea un directorio if ! [ -d $dirdesti ] then echo \"error, no es un dir desti correcto\" exit 2 fi # para cada file validada lo copiamos al dir for file in $llistafiles do if ! [ -f $file ] then echo \"error, no es un file\" >> /dev/stderr exit 3 fi cp $file $dirdesti/. done exit 0 for dir in $* do # miramos si es un help if [ $dir == \"--help\" -o $dir == \"-h\" ] then echo \"mostramos ayuda\" exit 0 fi # verificar que es un dir if ! [ -d $dir ] then echo \"error arg no es un dir\" echo \"USAGE: prog.sh arg[dir]\" exit 2 fi llistadir=$(ls $dir) count=1 for file in $llistadir do #de cada cosa que imprime decir si es un regular file, dir, link o altre cosa if [ -d $dir/$file ] then echo \"es un directorio\" echo \"$count: $file\" elif [ -f $dir/$file ] then echo \"es un file\" echo \"$count: $file\" elif [ -d $dir/$file ] then echo \"es una altra cosa\" echo \"$count: $file\" fi count=$((count+1)) done done exit 0","title":"mirar si es directorio, regular file..."},{"location":"bash_scripting/#mayusculas-copiar-buscar-ordenar","text":"tr echo \"hola\" | tr -s '[a-z]' '[A-Z]' echo \"hola\" | tr -s '[:blank:]' ' ' #'\\t' tabulador cut + cut -d' ' -f1-5 corta por campos y delimitador + cut -c1-50 corta por caracteres llistafiles=$(echo $ | cut -d' ' -f1-$(($#-1))) dirdesti=$(echo $ | cut -d' ' -f$#) uid=$(echo $loginLine | cut -d: -f3) grep/egrep + egrep \"^[^ ]{10,}\" busca de lo que se pase o line algo que tenga mas de 10 chars + echo $arg | egrep \".{4,}\" busca de lo que se pase que tenga mas de 4 chars + egrep \"^$user:\" /etc/passwd buscar el campo user del /etc/passwd + egrep \"^[^:] :[^:] :[^:]*:$gid:\" /etc/passwd + sort -u 2> /dev/null) ordenar unico y quita los repetidos + listshells=$(cut -d: -f7 /etc/passwd | sort -u 2> /dev/null)","title":"Mayusculas , copiar , buscar , ordenar"},{"location":"bash_scripting/#funciones","text":"function nombreFuncion(){ return xxxxx } function showUser(){ ERR_NARGS=1 ERR_NOLOGIN=2 OK=0 #validar args if [ $# -ne 1 ] then echo \"ERR: num args incorrecte\" echo \"USAGE: $0 login\" return $ERR_NARGS fi #validar si existe el login login=$1 userLine=$(egrep \"^$login:\" /etc/passwd 2> /dev/null) if [ $? -ne 0 ] then echo \"ERR: el login $login no existe\" return $ERR_NOLOGIN fi #mostrar uid=$(echo $userLine | cut -d: -f3) gid=$(echo $userLine | cut -d: -f4) gecos=$(echo $userLine | cut -d: -f5) home=$(echo $userLine | cut -d: -f6) shell=$(echo $userLine | cut -d: -f7) echo \"uid: $uid\" echo \"gid: $gid\" echo \"gecos: $gecos\" echo \"home: $home\" echo \"shell: $shell\" return $OK }","title":"Funciones"},{"location":"bbdd/","text":"BASES DE DATOS POSTGRESQL Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); `` PHPPGADMIN phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin MARIADB Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql MONGODB Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"Base de datos"},{"location":"bbdd/#bases-de-datos","text":"","title":"BASES DE DATOS"},{"location":"bbdd/#postgresql","text":"Crear database y conectarnos: create database diccionari; \\c diccionari; Crear tablas: create table exemplar_soci ( id_exemplar_soci serial primary key, id_exemplar int not null, id_soci int not null, foreign key (id_exemplar) references exemplars(id_exemplar), foreign key (id_soci) references socis(id_soci) ); create table exemplars ( id_exemplar serial primary key, id_volum int not null, num_exemplars int not null, foreign key (id_volum) references grup_volums(id_volum) ); create table obres ( id_obra serial primary key, nom_obra varchar(200) not null, data_publicacio date not null, descripcio varchar(500) not null, id_autor int not null, foreign key (id_autor) references autors(id_autor) ); create table socis ( id_soci serial primary key, nom_soci varchar(100) not null, cognom_soci varchar(200) not null, naixement date not null, localitat varchar(100) not null, unique(nom_soci, cognom_soci) ); Consulta simple: select * from paraules where paraula='casa' Consultas con comparaciones: training=# select objetivo, ventas, ciudad, region from oficinas where region='Este' order by ciudad; training=# select nombre, contrato from repventas where ventas>300000; training=# select nombre from repventas where director=104; training=# select nombre, contrato from repventas where contrato < '1988-1- 1'; training=# select id_fab, id_producto, descripcion from productos where id_fab like '%i'; training=# select id_fab, descripcion, (existencias*precio) as valor_inventario from productos; training=# select num_pedido, importe from pedidos where importe between 20000 and 29999; training=# select num_pedido, importe from pedidos where importe>=20000 and importe<=29999; training=# select id_fab, descripcion from productos where (id_fab='imm' and existencias>=200) or (id_fab='bic' and existencias>=50); training=# select empresa from clientes where (empresa not like '%Corp.%' or empresa not like '%Inc.%') and limite_credito>30000; practica1=# select nomalumne from alumnes where nomalumne like 'Ann_'; JOIN: training=# select ciudad, nombre, titulo from oficinas join repventas on dir=num_empl; training=# select ciudad, nombre, titulo from oficinas, repventas where dir=num_empl; training=# select num_pedido, descripcion from pedidos, productos where fab=id_fab and producto=id_producto; training=# select num_pedido, nombre, empresa from pedidos, clientes, repventas where clie=num_clie and rep=num_empl and importe>25000; training=# select treballador.nombre, treballador.cuota, dir.nombre, dir.cuota from repventas treballador, repventas dir where treballador.director=dir.num_empl and treballador.cuota>dir.cuota; LEFT/RIGHT JOIN + GROUP BY: > EJERCICIO ENTENDER JOIN Y JOIN LEFT/RIGHT **CODI_VENDEDOR, NOM_VENDEDOR, CODI_CAP, NOM_CAP, OFICINA, CIUTAT, CODI_CAPOFICINA, NOM_CAPOFICINA** > CON JOIN LEFT MANDA LA COLUMNA QUE QUEREMOS Y SALEN TODOS LOS CAMPOS AUNQUE HAYA NULL se ha de coger la tabla principal, ver las relaciones manualmente como si tuvieramos una al lado de otra. coger el campo central y ver como se relaciona con la otra tabla. si es la misma tabla se ha de ir haciendo copias de tablas con cada tabla con un nombre predefinido. SOLO CON JOIN TE SALDR\u00cdA SIN LOS NULL training=# select id_fab, sum(existencias) from productos where precio>54 group by id_fab having sum(existencias)>300; training=# select id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where fecha_pedido>='01-01-89' and fecha_pedido<='12-31-89' group by id_fab, id_producto order by 4; training=# select id_fab, id_producto, descripcion, existencias, count(distinct clie) from productos join pedidos on id_fab=fab and id_producto=producto group by id_fab, id_producto order by 5 desc, 4 desc, 3 limit 5; training=# select oficina, ciudad, count(*) from oficinas right join repventas venedors on oficina=venedors.oficina_rep join pedidos on venedors.num_empl=rep group by oficina order by oficina; UNION: training=# select 'producto individual', id_fab, id_producto, descripcion, importe from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' union select 'total', id_fab, id_producto, descripcion, sum(importe) from productos join pedidos on id_fab=fab and id_producto=producto where descripcion ilike 'Bisagra%' or descripcion ilike 'Articulo%' group by id_fab, id_producto, descripcion order by 2,3,1; SUBQUERIES: training=# select id_fab, id_producto, descripcion, (select count(num_pedido) as num_comandes from pedidos where fab=id_fab and producto=id_producto), (select count(distinct clie) as clientes from pedidos where fab=id_fab and producto=id_producto), (select sum(importe) from pedidos where fab=id_fab and producto=id_producto) from productos where id_fab in (select fab from pedidos group by fab) or id_producto in (select producto from pedidos group by producto) or id_fab not in (select fab from pedidos group by fab) or id_producto not in (select producto from pedidos group by producto) order by 1,2; INSERT/UPDATE/DELETE: INSERT training=# insert into copia_repventas values (1012, 'Enric Jimenez', 99, 18, 'Dir Ventas', '2012-01-02', 101, 0, 0); training=# insert into copia_clientes (num_clie, empresa, rep_clie) values (3001, 'C2', 1013); training=# insert into copia_repventas values (1013, 'Pere Mendoza', null, null, null, '2011-08-15', null, null, 0); DELETE training=# delete from copia_pedidos where rep=102; training=# delete from copia_repventas where nombre='Enric Jimenez'; training=# delete from copia_pedidos where fecha_pedido<'1989-11-15'; training=# delete from copia_clientes where rep_clie=105 or rep_clie=109 or rep_clie=101; (rep_clie in (select num_empl from repventas where nombre like '%adams'); training=# delete from copia_repventas where contrato<'1988-07-01' and cuota is null; UPDATE training=# update copia_clientes set limite_credito=60000, rep_clie=109 where num_clie=2103; training=# update copia_clientes set limite_credito=60000, rep_clie=(select num_empl from repventas where nombre='Mary Jones') where empresa='Acme Mfg.'; training=# update copia_repventas set oficina_rep=11, cuota=cuota-cuota*0.10 where oficina_rep=12; training=# update copia_clientes set rep_clie=102 where rep_clie in (105,106,107); training=# update copia_repventas set cuota= cuota + cuota*0.05; training=# update copia_oficinas set objetivo=2*(select sum(ventas) from copia_repventas where oficina_rep=oficina) where objetivo<ventas; ALTER TABLE: alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl) on delete set null --si un rep desapareix, la oficina quedara temporalmente vacia on update cascade -- si un rep canvia de codi, canviara tambe el codi a la oficina ALTER TABLE copia_clientes ADD tel numeric(9,0) unique not null check (tel >=100000000 and tel<=999999999); ALTER TABLE copia_repventas ADD CHECK(edad>=18 and edad<=65); ALTER TABLE copia_repventas DROP titulo; alter table oficinas add constraint director_de_la_oficina foreign key (dir) references repventas(num_empl); alter table pedidos add constraint cliente_que_ha_hecho_pedido foreign (clie) references clientes(num_clie) add constraint rep_que_atendido_pedido foreign key (rep) references repventas(num_empl) add constraint producto_del_pedido foreign key (fab,producto) references productos(id_fab,id_producto); Crear funciones: CREATE FUNCTION suma (INTEGER,INTEGER) RETURNS INTEGER AS $$ BEGIN RETURN $1+$2; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION hola (TEXT) RETURNS TEXT AS $$ BEGIN RETURN 'HOLA '|| $1; END; $$ LANGUAGE 'plpgsql'; -------------------------------------------------------------------------------- CREATE FUNCTION INS_ALUM() RETURNS VOID AS $$ INSERT INTO prova(a,b) VALUES (100,200); $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE FUNCTION llista_Alum() RETURNS RECORD -- RECORD DEVUELVE UNA LISTA CON CAMPOS AS $$ -- SI QUEREMOS QUE DEVUELVA M\u00c1S \"RETURNS SETOF RECORD\" SELECT * FROM prova; $$ LANGUAGE sql; -------------------------------------------------------------------------------- CREATE OR REPLACE FUNCTION counter () RETURNS SETOF INTEGER AS $$ BEGIN FOR counter IN 1..10 LOOP RAISE NOTICE 'Counter: %', counter; END LOOP; END; $$ LANGUAGE 'plpgsql'; Crear triggers: CREATE OR REPLACE FUNCTION afegeix_audit() RETURNS TRIGGER AS $$ BEGIN IF (TG_OP = 'DELETE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'D', CURRENT_USER); RETURN OLD; ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'U', CURRENT_USER); RETURN NEW; ELSIF (TG_OP = 'INSERT') THEN INSERT INTO auditoria VALUES (DEFAULT, CURRENT_TIMESTAMP, TG_TABLE_NAME, 'I', CURRENT_USER); RETURN NEW; END IF; --Aqu\u00ed no hauria d'arribar-hi mai: RETURN NULL; END $$ LANGUAGE plpgsql; CREATE TRIGGER tauditresultats AFTER INSERT OR UPDATE OR DELETE ON resultats FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); CREATE TRIGGER tauditanalitiques AFTER INSERT OR UPDATE OR DELETE ON analitiques FOR EACH ROW EXECUTE PROCEDURE afegeix_audit(); Crear Squema y roles: --crear roles CREATE ROLE lc_consultar NOLOGIN; CREATE ROLE lc_inserir NOLOGIN; CREATE ROLE lc_admin NOLOGIN; --- --CREACION DE UN SCHEMA CREATE SCHEMA lcschema AUTHORIZATION postgres; --TODOS LOS SCHEMAS SE CREAN EN PUBLIC (PUBLIC-POSTGRES OWNER) REVOKE ALL PRIVILEGES ON SCHEMA lschema FROM public; -- todos los privilegios para los users: GRANT ALL PRIVILEGES ON SCHEMA lschema TO postgres, lc_admin; --privilegios para usar este schema para: GRANT USAGE ON SCHEMA lschema TO ROLE lc_consultar, lc_inserir, lc_admin; --s'hauria de crear un SCHEMA de lab_clinic i tamb\u00e9 fer aixo; GRANT USAGE ON SCHEMA lschema to lc_consultar, lc_inserir, lc_admin; GRANT CREATE ON SCHEMA lschema to lc_admin; --DAR LOS PRIVILEGIOS A CIERTOS USUARIOS ---para poder conectarse a la bbdd GRANT CONNECT ON DATABASE lab_clinic to lschema to lc_consultar, lc_inserir, lc_admin; ---para ver las tablas, y podra crear tablas y filas a su nombre GRANT SELECT ON ALL TABLES IN SCHEMA public TO lc_consultar; ---para poder ver y insertar (no borrar) GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public to lc_inserir; ---para poder insertar, tambien necesita permisos de inserir secuencias GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public to lc_inserir; ---para tener todos los privilegios en la bbdd GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to lc_admin; --creamos usuarios, para poder tener roles se necesita LOGIN con create role postgres=# create user miguel password 'miguel'; CREATE ROLE --create role miguel login password 'miguel'; postgres=# create user walid password 'walid'; CREATE ROLE --asignar los roles a los usuarios GRANT lc_consultar to walid; GRANT lc_inserir to miguel; GRANT lc_admin to miguel with admin option; --permiso de superusuario postgres=# alter role miguel with superuser; ALTER ROLE --todos los privilegios en esta base de datos postgres=# grant all privileges on database lab_clinic to walid; GRANT --entrar a la bbdd como miguel [isx46410800@i05 ipc2019]$ su postgres Password: bash-4.4$ psql lab_clinic miguel; Password for user miguel: psql (9.6.10) Ver tiempos de ejecucion: explain analyse select * from paraulesraw where paraula = 'comparar/00'; Crear indices: create index index_paraula on paraulesraw using btree(paraula); ``","title":"POSTGRESQL"},{"location":"bbdd/#phppgadmin","text":"phpPgAdmin \u00e9s una aplicaci\u00f3 web que ens permet gestionar una BD Postgres SQL amb un entorn gr\u00e0fic. Existeix un an\u00e0leg per a MariaDB (o MySQL) anomenat phpMySql. El primer pas \u00e9s instal\u00b7lar l\u2019aplicaci\u00f3 al mateix ordinador que fa de servidor postgres. sudo dnf install phpPgAdmin Editant l'arxiu /var/lib/pgsql/data/pg_hba.conf i activant els m\u00e8todes d'autenticaci\u00f3 md5 ). Editar l\u2019arxiu /etc/phpPgAdmin/config.inc.php i posar a false la l\u00ednia: $conf['extra_login_security'] = true; // use 'localhost' for TCP/IP connection on this computer\\\\ $conf['servers'][0]['host'] = 'localhost'; Entrar: http://localhost/phpPgAdmin","title":"PHPPGADMIN"},{"location":"bbdd/#mariadb","text":"Instalar: sudo dnf install mariadb mariadeb-server sudo systemctl start mariadb Crear database: create database instagram; Entrar a una bbdd: USE Nom_BD; Crear Usuario: create user 'miguel'@'localhost' identified by \"miguel14031993\"; Bloquear tablas: FLUSH TABLES WITH READ LOCK; UNLOCK TABLES; Agafem les dades del servidor mestre per replicar des d'aquest punt: show master status; Copiem la BD amb mydqldump: mysqldump -u root -p --routines --opt nomdb > db-dump.sql","title":"MARIADB"},{"location":"bbdd/#mongodb","text":"Entrar: systemctl start mongod mongo Entrar bbdd: use database Ver bases: show databases Importar tablas: mongoimport --db foursquare --collection restaurants --file '/run/media/isx46410800/TOSHIBA EXT/HISX2/M10/UF2/json_dades_exemple/json/foursquare/restaurants.json' Entrar a una bbdd,tabla y ver algo: Database: imdb > use imdb switched to db imdb > db imdb > db.createCollection(\u201cmovies\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201coscars\u201d) {\u201cok\u201d: 1} > db.createCollection(\u201cpeople\u201d) {\u201cok\u201d: 1} > show collections movies oscars people Consultas ejemplos: { \"_id\" : \"0000002\", \"name\" : \"Lauren Bacall\", \"dob\" : \"1924-9-16\", \"pob\" : \"New York, New York, USA\", \"hasActed\" : true } { \"_id\" : \"0000004\", \"name\" : \"John Belushi\", \"dob\" : \"1949-1-24\", \"pob\" : \"Chicago, Illinois, USA\", \"hasActed\" : true } db.people.find({hasActed: true, hasDirected: { $exists: false}}).pretty().count()->1909 { \"_id\" : 6, \"name\" : { \"first\" : \"Guido\", \"last\" : \"van Rossum\" }, \"birthYear\" : 1931, \"contribs\" : [ \"Python\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : 2001, \"by\" : \"Free Software Foundation\" }, { \"award\" : \"NLUUG Award\", \"year\" : 2003, \"by\" : \"NLUUG\" } ] } > db.bios.find({\"awards.year\" : 2001}).count() 3 Buscar las personas que haya obtenido un premio del tipo 'National Medal of' { \"_id\" : 7, \"name\" : { \"first\" : \"Dennis\", \"last\" : \"Ritchie\" }, \"birthYear\" : 1956, \"deathYear\" : 2011, \"contribs\" : [ \"UNIX\", \"C\" ], \"awards\" : [ { \"award\" : \"Turing Award\", \"year\" : 1983, \"by\" : \"ACM\" }, { \"award\" : \"National Medal of Technology\", \"year\" : 1998, \"by\" : \"United States\" }, { \"award\" : \"Japan Prize\", \"year\" : 2011, \"by\" : \"The Japan Prize Foundation\" } ] } > db.bios.find({ \"awards.award\" : /^National Medal of/}).pretty().count() 4 //6.- Buscar las personas de la colecci\u00f3n bios que destaquen en el terreno de Java, Ruby o Python (3) { \"_id\" : 9, \"name\" : { \"first\" : \"James\", \"last\" : \"Gosling\" }, \"birthYear\" : 1965, \"contribs\" : [ \"Java\" ], \"awards\" : [ { \"award\" : \"The Economist Innovation Award\", \"year\" : 2002, \"by\" : \"The Economist\" }, { \"award\" : \"Officer of the Order of Canada\", \"year\" : 2007, \"by\" : \"Canada\" } ] } > db.bios.find({ contribs : { $in: [ \"Java\", \"Ruby\", \"Python\" ] } }).pretty().count() 3 //9.- Buscar las personas de la colecci\u00f3n bios con 1 premio conseguido (1) { \"_id\" : 8, \"name\" : { \"first\" : \"Yukihiro\", \"aka\" : \"Matz\", \"last\" : \"Matsumoto\" }, \"birthYear\" : 1941, \"contribs\" : [ \"Ruby\" ], \"awards\" : [ { \"award\" : \"Award for the Advancement of Free Software\", \"year\" : \"2011\", \"by\" : \"Free Software Foundation\" } ] } #size sirve para que el tama\u00f1o del array de tal cosa sea la medida que indiquemos > db.bios.find({ awards : { $size: 1}}).pretty().count() 1 //10.- Buscar las personas de la colecci\u00f3n bios con 3 o m\u00e1s premios conseguidos (6) #que no tenga ni 2 ni 1 ni 0 ni no existe el campo > db.bios.find({ $nor: [ {awards: {$exists: false}}, {awards: {$size: 2}}, {awards: {$size: 1}}, {awards: {$size: 0}}]}).pretty().count() 6 #que tenga o 4 o 3 o 2 > db.bios.find({ $or: [ {awards: {$size: 4}}, {awards: {$size: 3}}, {awards: {$size: 2}}]}).pretty().count() 8 //1.- Buscar todos los libros con precio superior a 100 USD (7) #COMPRUEBO SI TODOS LOS PRECIOS SON EN USD > db.books.find().count() 333 > db.books.find({\"price.currency\": \"USD\"}).count() 333 > db.books.find({\"price.msrp\": {$gt: 100}}).pretty().count() 7 //3.- Buscar los libros que tengan el tag 'programming', 'agile' y \"java\" (5) #all para que salgan los 3 en los tags > db.books.find({tags: {$all : [\"programming\", \"agile\", \"java\"]}}).pretty().count() 5 //5.- Buscar los libros escritos por 3 autores (17) > db.books.find({author: {$size: 3}}).pretty().count() 17 Insertar varios: > db.stores.insertMany( [ { _id: 1, name: \"Java Hut\", description: \"Coffee and cakes\" }, { _id: 2, name: \"Burger Buns\", description: \"Gourmet hamburgers\" }, { _id: 3, name: \"Coffee Shop\", description: \"Just coffee\" }, { _id: 4, name: \"Clothes Clothes Clothes\", description: \"Discount clothing\" }, { _id: 5, name: \"Java Shopping\", description: \"Indonesian goods\" } ] ) Ver indices: > db.stores.getIndexes() Crear indices: db.stores.createIndex( { name: \"text\", description: \"text\" } ) Consulta una: > db.tweets.findOne() 2.1) Buscar quants twits tenim amb Obama President > db.tweets.find( { $text: { $search: \"Obama President\" } } ).count() 52 2.2) Buscar le textScore de cada twit amb Obama President > db.tweets.find({ $text: { $search: \"Obama President\" } }, { puntuacio: { $meta: \"textScore\" } }).sort( { puntuacio: { $meta: \"textScore\" } } ).pretty() - frase exacta :\"Yes we can\" > db.tweets.find( { $text: { $search: \"\\\"Yes we can\\\"\" } } ).count() 2 2.6) Busca les ciutats que estan a entre 20 i 50 km de Barcelona > db.cities.find({ \"loc\": { \"$near\": { \"$geometry\": { type: \"Point\" , coordinates: [2.15899, 41.38879]}, \"$maxDistance\": 50000, \"$minDistance\": 20000 } } }).count() 72 Update/delete/insert: Y los cambiamos por Miky > db.students.updateMany( { name: \"Mikel\" }, { $set: { name: \"Miky\" } }) Vemos cuantos usuarios tienen m\u00e1s de 50000 amigos: > db.tweets.find({ \"user.friends_count\" : { $gt: 50000}}).pretty().count() Incrementamos +1 a estos usuarios el contador de amigos: > db.tweets.updateMany({ \"user.friends_count\" : { $gt : 50000 }}, { $inc: { \"user.friends_count\" : +1}})","title":"MONGODB"},{"location":"docker/","text":"DOCKER INSTALACI\u00d3N Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world COMANDOS Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container Redes en Docker: docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed Volumes en Docker: docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash Docker Compose: docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap Docker SWARM: docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode ARQUITECTURA Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real. DOCKER IMAGES Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images DOCKERFILE El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito CMD VS ENTRYPOINT CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"] CENTOS-PHP-SSL Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi NGINX-PHP Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh MULTI-STAGE-BUILD Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo. PRUEBA REAL La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php DOCKER CONTAINERS Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen LISTAR/MAPEO PUERTOS docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080. INICIAR/DETENE/PAUSAR Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id VARIABLES DE ENTORNO En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins MYSQL Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333 MONGODB Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones APACHE/NGINX/TOMCAT Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine POSTGRES Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows) JENKINS Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins. LIMITAR RECURSOS Ayuda con: docker --help | grep \"xxxx\" MEMORIA Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb CPU Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2. COPIA DE ARCHIVOS De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/. CONTENEDOR A IMAGEN Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!! SOBREESCRIBIR CMD Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos DESTRUIR CONTAINER Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm... DOCUMENT ROOT El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi. DOCKER VOLUMES Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes VOLUMES HOST Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 VOLUMES ANONYMOYS Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root VOLUMES NAMED VOLUMES Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root PRUEBA REAL Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev DOCKER NETWORK Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping CREAR REDES Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB VER REDES Para ver las redes creadas: docker network inspect netA AGREGAR/CONECTAR REDES Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known ELIMINAR REDES Para eliminar redes: docker network remove netA netB ASIGNAR IPs Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos RED HOST Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel RED NONE Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos EXPONER IPs CONCRETAS Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2 DOCKER COMPOSE Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes INSTALACI\u00d3N Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose EJEMPLO Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down VARIABLES ENTORNO Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env VOL\u00daMENES Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\" REDES Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms BUILD DOCKERFILE Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache CMD CAMBIADO Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net: LIMITAR RECURSOS Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\" POL\u00cdTICA DE REINICIO Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca. NOMBRE PROYECTO Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d DIFERENTE DOCKER-COMPOSE Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d OTROS COMANDOS docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap PROYECTOS DOCKER-COMPOSE MYSQL-WORDPRESS Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: DRUPAL-POSTGRESQL Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados: PRESTASHOP-MYSQL Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: JOOMLA-MYSQL Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados: REACT-MONGODB-NODE.JS Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados: GUACAMOLE DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados: ZABBIX Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados: PHPMYADMIN-MYSL Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados: DOCKER SWARM Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio. COMANDOS B\u00c1SICOS docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2 INICIALIZAR Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377 DEPLOY SWARM Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago ESCALAR SERVICIOS Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp MODO GLOBAL Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] NODO DRAIN/PAUSE/ACTIVE DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName LABELS Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] DOCKER REGISTRY Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"Docker"},{"location":"docker/#docker","text":"","title":"DOCKER"},{"location":"docker/#instalacion","text":"Instalar Docker: $ sudo dnf remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine $ sudo dnf -y install dnf-plugins-core $ sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo $ sudo dnf install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo docker run hello-world","title":"INSTALACI\u00d3N"},{"location":"docker/#comandos","text":"Crear un container Docker: docker run --rm -it fedora:27//isx46410800/netcat:latest /bin/bash docker run --rm --name ldap -h ldap -d imagen Descagar una imagen: docker pull fedora:27/imagen Ver imagenes de mi sistema: docker images Iniciar un container: docker start container Entrar dentro de un container en otra terminal: docker exec -it nomcontainer /bin/bash Entrar dentro de un container en detached: docker attach container Procesos de docker: docker ps -a docker top container \u00daltimo container creado: docker ps -l Document Root: docker info | grep -i root Memoria y cpu limitada y variables de entorno: docker run -m \"MB\" --cpuset-cpus 0-1 -e \"NAME=miguel\" Iniciar un container: docker start/stop IDcontainer Cambiar nombre container: docker rename IDcontainer NuevoNombre Borrar varias cosas: docker rm $(docker ps -aq) Docker version: docker version Info de un docker: docker info Lista de containers: docker container ls -a Borrar una imagen: docker rmi imagen Borrar un container: docker rm container Cambiar etiqueta de un container: docker tag imagen nombreNuevo:tag Borrar imagenes none: docker images -f dangling=true | xargs docker rmi Crear y subir una imagen a DockerHub: docker login docker tag imagen nuevoimagen:tag docker push nuevoimagen:tag Copiar un fichero a fuera del docker o dentro: docker cp file container:/opt/docker/. docker cp container:/opt/docker/. file Docker con puerto mapeado para el exterior: docker run --rm --name ldap -h ldap -p 389:389 -p 80:80 -it isx/ldap /bin/bash -p puertoMiMaquina:puertoContenedor -x dirActivo dentro del container","title":"COMANDOS"},{"location":"docker/#redes-en-docker","text":"docker network create NameRed docker network rm NameRed docker network inspect NameRed/container docker network create --subnet 172.19.0.0/16 NameRed","title":"Redes en Docker:"},{"location":"docker/#volumes-en-docker","text":"docker volume create NOMBREVOLUMEN docker volume ls docker volume inspect NOMVOLUMEN ls /var/lib/docker/volumes --privileged -v volumen:contenido docker run --rm --name ldap -h ldap -v NOMVOLUMEN:/var/lib/sambaloQueGuarda --privileged -it isx/ldap /bin/bash","title":"Volumes en Docker:"},{"location":"docker/#docker-compose","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"Docker Compose:"},{"location":"docker/#docker-swarm","text":"docker swarm init #inicia el docker swarm docker node ls # lista de nodos del swarm docker swarm join-tocken manager/worker #une workers o manager docker stack deploy -c docker_compose.yml nombreAPP #hace deploy docker stack ps NombreAPP #procesos docker stack ls #listado docker stack services nombreAPP #servicios docker stack rm NombreAPP #parar docker service ls docker service ps nombreservicio docker service inspect nomservicio docker service scale nomservicio=3 docker swarm leave --force #se desune del swarm docker swarm init --advertise-addr IP docker node update --label-add tipus=valor nomNode docker node inspect nomNode docker node update --availability active/drain/pause nomNode","title":"Docker SWARM:"},{"location":"docker/#arquitectura","text":"Docker Host es el servidor f\u00edsico/real donde se encuentra instalado Docker. Docker servicio: Docker Client. Rest API: es el intermediario encargado de comunicar al Docker client con el Docker server. Docker Server. Arquitectura Imagen docker (Dockerfile): Capa 1 - From: Sistema operativo minimo a elegir. Capa 2 - Run: lo que se quiera instalar, ejemplo apache. Capa 3 - CMD: lo que se tiene que poner para que cuando se arranque la imagen empiece con ese comando. Normalmente la activaci\u00f3n de un servicio en detached. SON CAPAS DE SOLO LECTURA Y NO SE PUEDE MODIFICAR NI BORRAR FROM centos:7 RUN yum install -y httpd CMD[\"apachectl\",\"-DFOREGROUND\"] Contenedor es una capa addicional en tiempo real de ejecuci\u00f3n, el empaquetado de todo el dockerfile. CAPA DE ESCRITURA. Recuerda que la capa del contenedor es temporal y que al eliminar el contenedor, todo lo que haya dentro de ella desaparecer\u00e1. Se diferencia de una m\u00e1quina virtual es que un contenedor es como un proceso m\u00e1s del sistema mientras que una MV hay que bajarse una ISO, instalar y agregar RAM, CPU y HD de nuestra propia m\u00e1quina real.","title":"ARQUITECTURA"},{"location":"docker/#docker-images","text":"Poniendo docker + SistemaOperativo podemos adquirir im\u00e1genes oficiales de los propios creadores para poder descargar del repositorio de DockerHub para nuestros contenedores. Por defecto, sino podemos un tag a la distribuci\u00f3n, nos coger\u00e1 el tag latest sino tendremos que poner la versi\u00f3n concreta como docker pull mongo:3.6-jessie . Se actualiza el tag si te bajas una imagen pero est\u00e1 recientemente actualizada y la antigua se queda en none . Vemos las im\u00e1genes con: docker images","title":"DOCKER IMAGES"},{"location":"docker/#dockerfile","text":"El fichero para crear nuestra imagen Docker se llama Dockerfile . Para construir la imagen es docker build -t/--tag imagen:tag . \u00f3 -f /rutaDockerfile .: docker build -t isx46410800/centos:inicial . Si modificamos algo del Dockerfile, hay que volver hacer el comando anterior. docker build -t isx46410800/centos:detached images/centos/. Ver el historial de construcci\u00f3n de capas de mi imagen: docker history -H imagen:tag Borrar una imagen: docker rmi idImagen Borrar un contenedor: docker rm contenedorName Ver los contenedores: docker ps / docker ps -a COMANDOS DOCKERFILE: FROM: desde donde se baja la imagen de SO. RUN: para instalar paquetes. COPY: copia ficheros de fuera hacia el container, ponemos ruta absoluta o del directorio actual. ADD: lo mismo que copy pero se puede pasar URLs y copiar\u00eda la info de la url a donde indiquemos. ENV: crea variable de entorno. WORKDIR: directorio activo al entrar. LABEL: es una etiqueta que puede ir en cualquier sitio, son informativas, es metadata. USER: quien ejecuta la tarea, por defecto es root. EXPOSE: puertos por donde escucha y puedes indicar qu\u00e9 puertos va funcionar mi contenedor. VOLUME: indica donde metemos la data cuando el container se muere. CMD: comando por el cual se ejecuta el container, normalmente un servicio detached CMD [\"apachectl\", \"-DFOREGORUND\"] . Ejemplo Dockerfile: # De que sistema operativo partimos FROM centos:7 # Labels de metadata extra LABEL author=\"Miguel Amor\u00f3s\" LABEL description=\"Mi primer container con Dockerfile\" # Que paquetes a instalar RUN yum install -y httpd # Creamos variables de entorno ENV saludo \"Hola Miguel\" # Directorio activo WORKDIR /var/www/html # Copiamos un fichero de fuera COPY ./listaCompra.txt ~/listaCompra.txt # Prueba de la variable RUN echo \"$saludo\" > ~/saludo.txt # Usuario que ejecuta la tarea RUN echo \"$(whoami)\" > ~/user1.txt RUN useradd miguel RUN useradd miguelito RUN echo \"miguel\" | passwd --stdin miguel RUN echo \"miguelito\" | passwd --stdin miguelito RUN chown miguel /var/www/html USER miguel RUN echo \"$(whoami)\" > ~/user2.txt USER root # Volumen para meter la chicha de cuando se muere el container VOLUME /tmp/ # Como arrancar el container CMD [\"apachectl\", \"-DFOREGROUND\"] Podemos usar un fichero .dockerignore para ignorar ficheros que no queremos que copiemos en el container. Para ver cualquier CMD para dejar por ejemplo un servicio encendido en detached se usa el comando: docker history -h SO / en docker hub Buenas pr\u00e1cticas, cuantas menos lineas de codigo, menos capas se utilizan al construir la imagen: RUN \\ useradd miguel && \\ useradd miguelito","title":"DOCKERFILE"},{"location":"docker/#cmd-vs-entrypoint","text":"CMD : Este comando se encarga de pasar valores por defecto a un contenedor. Entre estos valores se pueden pasar ejecutables. Este comando tiene tres posibles formas de pasar los par\u00e1metros: CMD [\u201cparametro1\u201d, \u201cparametro2\u201d, \u2026.] CMD [\"apachectl\", \"-DFOREGORUND\"] ENTRYPOINT : Este comando se ejecuta cuando se quiere ejecutar un ejecutable en el contenedor en su arranque. Los ejemplos tipo de su uso, son cuando se quiere levantar un servidor web, una base de datos, etc \u2026. ENTRYPOINT comando parametro1 parametro2 ENTRYPOINT cal 2020 ENTRYPOINT cal # Y pasar por comando los par\u00e1metros Como se ha comentado anteriormente el comando CMD se puede utilizar para pasar par\u00e1metros al comando ENRYPOINT. Una posible forma de realizarlo es: ENTRYPOINT [\"cal\"] CMD [\"2020\"]","title":"CMD VS ENTRYPOINT"},{"location":"docker/#centos-php-ssl","text":"Crear unaas llaves para certificado SSL: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout dockerssl.key -out dockerssl.crt Ponemos de commom name localhost Dockerfile: # SO FROM centos:7 # paquetes de apache php y ssl RUN \\ yum -y install httpd php php-cli php-commom mod_ssl openssl # dir creado RUN mkdir /opt/docker # indice de comprobacion de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/hola.php # web de prueba COPY startbootstrap /var/www/html # conf del ssl en el fichero de apache de conf COPY ssl.conf /etc/httpd/conf.d/default.conf # copia de certificados y startup COPY dockerssl.crt /opt/docker/dockerssl.crt COPY dockerssl.key /opt/docker/dockerssl.key COPY startup.sh /opt/docker/startup.sh # permisos del startup RUN chmod +x /opt/docker/startup.sh # escuchar puerto 443 EXPOSE 443 # arranque CMD [\"/opt/docker/startup.sh\"] Podemos eliminar imagenes none con el comando: docker images -f dangling=true | xargs docker rmi","title":"CENTOS-PHP-SSL"},{"location":"docker/#nginx-php","text":"Dockerfile: # SO FROM centos:7 # copiar el repo de nginx COPY nginx.repo /etc/yum.repos.d/nginx.repo # instalar paquetes RUN \\ yum -y install nginx --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-jason \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # dir RUN mkdir /opt/docker # puertos escuchando EXPOSE 80 443 # volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # copiamos files de conf COPY index.php /var/www/html/index.php COPY nginx.conf /etc/nginx/conf.d/default.conf COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # arranque CMD /opt/docker/startup.sh","title":"NGINX-PHP"},{"location":"docker/#multi-stage-build","text":"Ejemplo de instalar varias capas de sistemas operativos: # SO FROM maven:3.5-alpine as builder # copiamos la carpeta dentro COPY app /app # entramos y empaquetamos RUN cd /app && mvn package # desde java FROM openjdk:8-alpine # copiamos desde maven y lanzamos la app COPY --from=builder /app/target/my-app-1.0-SNAPSHOT.jar /opt/app.jar # ejecutamos la app CMD java -jar /opt/app.jar [isx46410800@miguel multi]$ docker build -t isx46410800/java:app . [isx46410800@miguel multi]$ docker run -d isx46410800/java:app [isx46410800@miguel multi]$ docker logs trusting_galois Hello World! Otro ejemplo: FROM centos as test RUN fallocate -l 10M /opt/file1 RUN fallocate -l 20M /opt/file2 RUN fallocate -l 30M /opt/file3 FROM alpine COPY --from=test /opt/file2 /opt/myfile El centos con los 3 files serian 260M pero solo coge de alpine que son 4 y coge el file que le interesa. El total de la imagen es 24M y no la suma de todo.","title":"MULTI-STAGE-BUILD"},{"location":"docker/#prueba-real","text":"La idea de este articulo es que le des soluci\u00f3n al siguiente problema utilizando lo que has aprendido. En donde trabajas, solicitan una imagen Docker base para ser reutilizada. Tu tarea es crear un Dockerfile con las siguientes especificaciones y entregarlo a tu jefe: Sistema Operativo Base: CentOs o Debian (A tu elecci\u00f3n): Herramientas a instalar: Apache (\u00daltima versi\u00f3n) PHP 7.0 Debes usar buenas pr\u00e1cticas. Deber\u00e1s comprobar su funcionamiento creando un index.php con la funci\u00f3n de phpinfo. Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # Volumenes VOLUME /var/www/html /var/log/php-fpm /var/lib/php-fpm # copia del startup y permisos COPY startup.sh opt/docker/startup.sh RUN chmod +x opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"opt/docker/startup.sh\"] Startup.sh: #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Imagen: docker build -t isx46410800/apache:php . Contenedor: docker run --name apache_php -p 80:80 -d isx46410800/apache:php Funcionamiento: docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES abda827fb9f5 isx46410800/apache:php \"opt/docker/startup.\u2026\" 3 seconds ago Up 1 second 0.0.0.0:80->80/tcp apache_php Entramos a localhost:80 y nos saldr\u00e1 la web index.php","title":"PRUEBA REAL"},{"location":"docker/#docker-containers","text":"Son una instancia de ejecuci\u00f3n de una imagen Son temporales Capa de lectura y escritura Podemos crear varios partiendo de una misma imagen","title":"DOCKER CONTAINERS"},{"location":"docker/#listarmapeo-puertos","text":"docker ps / docker ps -a / docker ps -q(ids) PuertoLocal-PuertoContainer: docker run --name jenkins -p 8080:8080 -d jenkins 0.0.0.0:8080 todas las interfaces de nuestra m\u00e1quina est\u00e1n mapeadas al puerto 8080. Si mapeamos la misma imagen con otros puertos, tenemos varias imagenes en diferentes puertos. docker run --name jenkins -p :8080 -d jenkins Cualquier primer puerto libre que coja mi maquina se mapea al 8080.","title":"LISTAR/MAPEO PUERTOS"},{"location":"docker/#iniciardetenepausar","text":"Renombrar un contenedor: docker rename nombre_viejo nombre_nuevo Parar contenedor: docker stop nombre/id Iniciar contenedor: docker start nombre/id Reiniciar contenedor: docker restart nombre/id Entrar con una terminal al contenedor: docker exec -it nombre /bin/bash docker exec -u root/user -it nombre /bin/bash jenkins@bh45fdiu ---> user@id","title":"INICIAR/DETENE/PAUSAR"},{"location":"docker/#variables-de-entorno","text":"En Dockerfile: ENV variable valor En la linea de construir container: docker run --name jenkins -e \"varible=valor\" -p :8080 -d jenkins","title":"VARIABLES DE ENTORNO"},{"location":"docker/#mysql","text":"Se ha de instalar el mysql client en las versiones que descargamos de dockerhub, ya que nos falta eso para poder usarlo: yum install -y mysql / apt-get install mysql-client / dnf install mysql-community-server AYUDA MYSQL Creamos contenedor MYSQL siguiendo las instrucciones: docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 docker run --name mysql-db --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -d mysql:5.7 fc84bdb48a389c9e7183fd633c0edfb03a7867104e1e867ef321a223f044fe87 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc84bdb48a38 mysql:5.7 \"docker-entrypoint.s\u2026\" 3 seconds ago Up 2 seconds 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db Mensaje final de ready for connections por tal puerto. Para conectarnos tendr\u00edamos que haber mapeado el puerto, no obstante podemos conectarnos sabiendo la IP de nuestro container y a\u00f1adirsela al comando de mysql de conexion con docker inspect mysql-db : [isx46410800@miguel mysql]$ mysql -u root -h 172.17.0.3 -pjupiter Mapeando puerto(el de mysql del log) para tambi\u00e9n poder usarlo mi maquina local, con nuevas variables de entorno siguiendo la gu\u00eda, creando una db con usuario y passwd: docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 [isx46410800@miguel mysql]$ docker run --name mysql-db2 --rm -e \"MYSQL_ROOT_PASSWORD=jupiter\" -e \"MYSQL_DATABASE=docker-db\" -e \"MYSQL_USER=docker\" -e \"MYSQL_PASSWORD=docker\" -p 3333:3306 -d mysql:5.7 b24dff85293ef892f2f9033c231e7a594a1261e9b5924e2a955691cc403eee11 [isx46410800@miguel mysql]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b24dff85293e mysql:5.7 \"docker-entrypoint.s\u2026\" 4 seconds ago Up 2 seconds 33060/tcp, 0.0.0.0:3333->3306/tcp mysql-db2 b3254fe3706b mysql:5.7 \"docker-entrypoint.s\u2026\" 3 minutes ago Up 3 minutes 3306/tcp, 33060/tcp mysql-db Para que arranque con todo lo necesario el container: docker logs -f mysql-db2 Comprobamos por localhost: [isx46410800@miguel mysql]$ mysql -u root -h 127.0.0.1 -pjupiter --port=3333","title":"MYSQL"},{"location":"docker/#mongodb","text":"Descargamos imagen mongodb Encendemos dos containers: [isx46410800@miguel images]$ docker run --name mongodb -p 27017:27017 -d mongo [isx46410800@miguel images]$ docker run --name mongodb2 -p 27018:27017 -d mongo Para ver cuanta memoria usa, se utiliza la orden: docker stats mongodb Con algun software de bbdd podemos conectarnos a este container poniendo la IP y el puerto y ya entrar\u00edamos remotamente. robomongo es un cliente de mondodb para estas conexiones","title":"MONGODB"},{"location":"docker/#apachenginxtomcat","text":"Creamos nuestro container nginx oficial mapeado: [isx46410800@miguel images]$ docker run --name nginx -p 8888:80 -d nginx Creamos nuestro container apache(httpd) oficial mapeado: [isx46410800@miguel images]$ docker run --name apacheweb -p 9999:80 -d httpd Creamos nuestro container tomcat version alpine oficial mapeado: [isx46410800@miguel images]$ docker run --name tomcat -p 7070:8080 -d tomcat:9.0.8-jre8-alpine","title":"APACHE/NGINX/TOMCAT"},{"location":"docker/#postgres","text":"Descargamos imagen : docker pull postgres Creamos container postgres creando user, pass y db: docker run --name postgres -e \"POSTGRES_PASSWORD=jupiter\" -e \"POSTGRES_USER=docker\" -e \"POSTGRES_DB=docker-db\" -p 5432:5432 -d postgres Entramos y comprobamos: root@1ff7388f08b3:/# psql -d docker-db -U docker psql (13.0 (Debian 13.0-1.pgdg100+1)) Type \"help\" for help. docker-db=# docker-db=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+--------+----------+------------+------------+------------------- docker-db | docker | UTF8 | en_US.utf8 | en_US.utf8 | postgres | docker | UTF8 | en_US.utf8 | en_US.utf8 | template0 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker template1 | docker | UTF8 | en_US.utf8 | en_US.utf8 | =c/docker + | | | | | docker=CTc/docker (4 rows)","title":"POSTGRES"},{"location":"docker/#jenkins","text":"Descargamos imagen : docker pull jenkins Creamos container jenkins: docker run --name jenkins -p 9090:8080 -d jenkins Luego tendr\u00edamos que copiar la contrase\u00f1a del fichero de password y arrancar la instalaci\u00f3n de Jenkins.","title":"JENKINS"},{"location":"docker/#limitar-recursos","text":"Ayuda con: docker --help | grep \"xxxx\"","title":"LIMITAR RECURSOS"},{"location":"docker/#memoria","text":"Para gestionar le memoria que puede usar mi docker se usa -m \"500Mb\" : docker run --name web -m \"500Mb\" -d httpd Lo comprobamos con: docker stats web --> LIMIT 10/500mb","title":"MEMORIA"},{"location":"docker/#cpu","text":"Vemos cuantas CPUs tenemos con: grep \"model name\" /proc/cpuinfo | wc -l --> 4 Indicar cual es la CPU que tiene usar cpuset-cpus 0 /0-1/0-3 : docker run --name web -m \"500Mb\" cpuset-cpus 0-2 -d httpd Comparte 3 cpus, la 0 , 1 y 2.","title":"CPU"},{"location":"docker/#copia-de-archivos","text":"De mi directorio al contenedor: docker cp index.html apache:/var/www/html Del contenedor a mi directorio: docker cp apache:/var/www/html/index.html /var/www/html/.","title":"COPIA DE ARCHIVOS"},{"location":"docker/#contenedor-a-imagen","text":"Para guardar todo lo a\u00f1adido dentro de un contenedor y convertirlo en una imagen guardada y actualizada se hace: docker commit imagen imagen-nueva Nota, todo lo que est\u00e1 dentro de un volumen NO SE GUARDAR\u00c1!!","title":"CONTENEDOR A IMAGEN"},{"location":"docker/#sobreescribir-cmd","text":"Para que el ultimo comando del docker no sea en la gran mayoria el /bin/bash o el servicio en foreground podemos poner otras \u00f3rdenes y el CMD ser\u00e1 diferente: docker run -p 8080:8080 -d centos python -m SimpleHTTPServer 8080 docker ps docker logs centos","title":"SOBREESCRIBIR CMD"},{"location":"docker/#destruir-container","text":"Para destruir containers autom\u00e1ticamente se usa en la linea de docker: docker run --rm...","title":"DESTRUIR CONTAINER"},{"location":"docker/#document-root","text":"El directorio root de Docker est\u00e1 en: docker info | grep -i root --> /var/libdocker Lo podemos cambiar a\u00f1adiendo en el fichero /var/lib/systemd/system/docker.service : linea ExecStart: xxxxx --data-root /opt/docker Tendriamos ahora en /opt/docker el nuevo document root. Cargamos y reiniciamos: systemctl daemon-reload systemctl restart docker Podemos copiar todo el contenido de /var/lib/docker a la nueva carpeta y tendriamos todo ahi.","title":"DOCUMENT ROOT"},{"location":"docker/#docker-volumes","text":"Los vol\u00famenes permiten almacenar data persistente del contenedor: Host Anonymous Named Volumes","title":"DOCKER VOLUMES"},{"location":"docker/#volumes-host","text":"Son los que se han de crear una carpeta antes y mapear a la carpeta del contenedor el cual queremos guardar la xixa: mkdir mysql docker run --name mysql-db -v mysql:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7","title":"VOLUMES HOST"},{"location":"docker/#volumes-anonymoys","text":"Son los que no ponemos ning\u00fan volumen de host y se nos a\u00f1ade a cualquier directorio al azar: docker run --name mysql-db -v /var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /var/lib/docker/volumes // /user/home/docker/volumes ): docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES ANONYMOYS"},{"location":"docker/#volumes-named-volumes","text":"Son los que creamos directamente con las ordenes: docker volume create my-vol Lo vemos con: docker volume ls Y se guardan en: /var/lib/docker/volumes // /user/home/docker/volumes docker run --name mysql-db -v my-vol:/var/lib/sql -e \"MYSQL_ROOT_PASSWORD=jupiter\" -p 3306:3306 -d mysql:5-7 Lo podemos descubrir(Normalmente en /user/home/docker/volumes ): docker volume inspect volumenName docker inspect container | grep mount docker info | grep -i root","title":"VOLUMES NAMED VOLUMES"},{"location":"docker/#prueba-real_1","text":"Dockerfile: # SO FROM centos:7 # Instalar apache RUN yum install -y httpd # A\u00f1adir repo de php para centos7 e instalamos version 7.0 RUN yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm && \\ yum update -y && \\ yum install -y yum-utils && \\ yum install -y php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo # Test de pagina index de php RUN echo \"<?php phpinfo(); ?>\" > /var/www/html/index.php # copia del startup y permisos COPY startup.sh /opt/docker/startup.sh RUN chmod +x /opt/docker/startup.sh # Arrancamos el servicio apache en segundo plano CMD [\"/opt/docker/startup.sh\"] Startup.sh: [isx46410800@miguel prueba2]$ cat startup.sh #!/bin/bash # Iniciar contenedor echo \"iniciando container...\" # Encendiendo servicio apache apachectl -DFOREGROUND Creaci\u00f3n volumen: [isx46410800@miguel prueba2]$ mkdir data_apache Imagen: Sending build context to Docker daemon 4.096kB docker build -t apache_volume . Contenedor con -m 500mb limite, uso en la cpu 0, -e las variables de entorno -v del volumen y -p del puerto indicado: docker run --rm --name apache_volume -m 500Mb --cpuset-cpus 0 -v $PWD/data_apache:/var/www/html/ -e \"ENV=dev\" -e \"VIRTUALIZATION=docker\" -p 5555:80 -d apache_volume + Resultados: set VIRTUALIZATION=docker ENV=dev","title":"PRUEBA REAL"},{"location":"docker/#docker-network","text":"Tipos: Bridge Host None Overlay La red por defecto es docker0 que se obtiene de ip -a : 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 La red por defecto de docker es bridge : docker network inspect bridge Entre containers de misma red se pueden hacer ping","title":"DOCKER NETWORK"},{"location":"docker/#crear-redes","text":"Para crear redes: docker network create netA Para ver las redes: docker network ls | grep netA Opci\u00f3n -d para el driver de gesti\u00f3n de la red bridge: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 netB","title":"CREAR REDES"},{"location":"docker/#ver-redes","text":"Para ver las redes creadas: docker network inspect netA","title":"VER REDES"},{"location":"docker/#agregarconectar-redes","text":"Para agregar una red a un contenedor se una --net : [isx46410800@miguel images]$ docker run --rm --name test1 --net netA -d nginx [isx46410800@miguel images]$ docker run --rm --name test2 --net netB -d nginx [isx46410800@miguel images]$ docker run --rm --name test3 --net netB -dit centos Con contenedores de la misma red creadas con el network create, podemos hacer ping a la ip o al nombre del container, es como si tuviera un DNS resolver: test1-----> 172.18.0.2 -------> netA test2-----> 172.124.10.2 -----> netB test3-----> 172.124.10.3 -----> netB [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test2\" PING test2 (172.124.10.2) 56(84) bytes of data. 64 bytes from test2.netB (172.124.10.2): icmp_seq=1 ttl=64 time=0.148 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from test2.netB (172.124.10.2): icmp_seq=3 ttl=64 time=0.101 ms --- test2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 26ms rtt min/avg/max/mdev = 0.090/0.113/0.148/0.025 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.124.10.2\" PING 172.124.10.2 (172.124.10.2) 56(84) bytes of data. 64 bytes from 172.124.10.2: icmp_seq=1 ttl=64 time=0.060 ms 64 bytes from 172.124.10.2: icmp_seq=2 ttl=64 time=0.131 ms 64 bytes from 172.124.10.2: icmp_seq=3 ttl=64 time=0.085 ms --- 172.124.10.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 53ms rtt min/avg/max/mdev = 0.060/0.092/0.131/0.029 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 61ms Para conectar con diferentes redes se utiliza connect pero solo se conectan con el nombre del container y no por la ip: docker network connect netB test1 Quiere decir que conectamos a test1 a la red de netB. \"Networks\": { \"netA\": { \"IPAMConfig\": null, ... }, \"netB\": { \"IPAMConfig\": {}, .... Comprobamos: [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" PING test1 (172.124.10.4) 56(84) bytes of data. 64 bytes from test1.netB (172.124.10.4): icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=2 ttl=64 time=0.086 ms 64 bytes from test1.netB (172.124.10.4): icmp_seq=3 ttl=64 time=0.085 ms --- test1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 44ms rtt min/avg/max/mdev = 0.085/0.090/0.101/0.013 ms [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 172.18.0.2\" PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data. --- 172.18.0.2 ping statistics --- 3 packets transmitted, 0 received, 100% packet loss, time 80ms Para volver a desconectar se utiliza: docker network disconnect netB test1 [isx46410800@miguel images]$ docker exec test3 /bin/bash -c \"ping -c3 test1\" ping: test1: Name or service not known","title":"AGREGAR/CONECTAR REDES"},{"location":"docker/#eliminar-redes","text":"Para eliminar redes: docker network remove netA netB","title":"ELIMINAR REDES"},{"location":"docker/#asignar-ips","text":"Creamos una red: docker network create -d bridge --subnet 172.124.10.0/24 --gateway 172.124.10.1 mynet Asignamos una IP aleatoria que coger\u00e1 del rango que creamos: docker run --rm --name test3 --net mynet -dit centos Asignar una IP concreta con el --ip : docker run --rm --name test3 --net mynet --ip 172.124.10.50 -dit centos","title":"ASIGNAR IPs"},{"location":"docker/#red-host","text":"Esta red ya existe por defecto con docker igual que la de brigde. Para conectarnos a esta red, que ser\u00eda la misma que la IP real de mi m\u00e1quina, tendr\u00eda todo, como el hostname, ser\u00eda: docker run --rm --name test3 --net host -dit centos [root@miguel /]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether b4:b5:2f:cb:e2:65 brd ff:ff:ff:ff:ff:ff inet 192.168.1.104/24 brd 192.168.1.255 scope global dynamic enp4s0 valid_lft 66351sec preferred_lft 66351sec inet6 fe80::227a:4836:6df:23b/64 scope link valid_lft forever preferred_lft forever 3: wlp3s0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether f2:aa:5b:7e:c0:70 brd ff:ff:ff:ff:ff:ff 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:9f:2c:43:a0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:9fff:fe2c:43a0/64 scope link valid_lft forever preferred_lft forever [root@miguel /]# hostname miguel","title":"RED HOST"},{"location":"docker/#red-none","text":"Esta red ya existe por defecto con docker igual que la de brigde. Sirve para que los container que creemos no tengan ninguna IP, no tendr\u00eda apartado network: docker run --rm --name test3 --net none -dit centos","title":"RED NONE"},{"location":"docker/#exponer-ips-concretas","text":"Tomaremos como premisa que la IP de nuestro Docker Host es 192.168.100.2 Al exponer un puerto en un contenedor, por defecto, este utiliza todas las interfaces de nuestra m\u00e1quina. Ve\u00e1mos un ejemplo: docker run -d -p 8080:80 nginx 196a13fe6198e1a3e8d55aedda90882f6abd80f4cdf41b2f29219a9632e5e3a1 [docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 196a13fe6198 nginx \"nginx -g 'daemon of\u2026\" 5 seconds ago Up 2 seconds 0.0.0.0:8080->80/tcp frosty_jenning Si observamos la parte de ports, veremos un 0.0.0.0 . Esto significa que podremos acceder al servicio en el puerto 8080 utilizando localhost: 8080 , o 127.0.0.1:8080 , 192.168.100.2:8080 . Si quisi\u00e9ramos que sea accesible solamente v\u00eda localhost y no v\u00eda 192.168.100.2 , entonces har\u00edamos lo siguiente: docker run -d -p 127.0.0.1:8081:80 nginx 1d7e82ff15da55b8c774baae56827aef12d59ab848a5f5fb7f883d1f6d1ee6e1 docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1d7e82ff15da nginx \"nginx -g 'daemon of\u2026\" 3 seconds ago Up 1 second 127.0.0.1:8081->80/tcp musing_tesla Como observamos, ahora en vez de 0.0.0.0 vemos un 127.0.0.1 , lo que indica que nuestro servicio es accesible s\u00f3lo v\u00eda localhost y no usando 192.168.100.2","title":"EXPONER IPs CONCRETAS"},{"location":"docker/#docker-compose_1","text":"Herramienta de Docker de aplicaciones multicontenedor. El archivo es docker-compose.yml y contiene: Contenedores Im\u00e1genes Vol\u00famenes Redes","title":"DOCKER COMPOSE"},{"location":"docker/#instalacion_1","text":"Docker-compose del curso Instalaci\u00f3n: sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"INSTALACI\u00d3N"},{"location":"docker/#ejemplo","text":"Documentaci\u00f3n Siempre ha de ponerse si hay como secciones principales: Version Services Volumes Networks Ejemplo: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8080:80\" Para arrancarlo: docker-compose up -d Para apagarlo: docker-compose down","title":"EJEMPLO"},{"location":"docker/#variables-entorno","text":"Podemos poner las variables con la opci\u00f3n environment o a trav\u00e9s de un ficheros con todas las variables de entono con la opci\u00f3n env_file : version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" version: '3' services: db: container_name: mysql image: mysql:5.7 ports: - \"3306:3306\" env_file: variables.env","title":"VARIABLES ENTORNO"},{"location":"docker/#volumenes","text":"Para los vol\u00famenes, podemos crearlo a\u00f1andiendolo en su secci\u00f3n y luego para asignarlo a un contenedor, a\u00f1adimos la subsecci\u00f3n volumes: version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"my-vol:/usr/share/nginx/html\" volumes: my-vol: Creamos el volumen Named my-vol y lo a\u00f1adimos al contenedor de nginx. El volumen se crea en la ruta del Document Root--> docker info | grep -i root . [isx46410800@miguel nginx]$ docker-compose -f docker-compose_volumes.yml up -d Creating network \"nginx_default\" with the default driver Creating volume \"nginx_my-vol\" with default driver Creating nginx ... Creating nginx ... done se llama de prefijo nginx, porque siempre coge el nombre del directorio actual. Si vamos al volumen y cambiamos el contenido, al volver a formarse saldr\u00e1 lo que hayamos puesto. Para un volumen de host, hemos de poner la ruta absoluta de la carpeta que usaremos como volumen, en este caso creamos el volumen de html : version: '3' services: nginx: container_name: nginx image: nginx ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/nginx/html:/usr/share/nginx/html\"","title":"VOL\u00daMENES"},{"location":"docker/#redes","text":"Para crear redes, se ha de crear la seccion de networks y de cada contenedor si son diferentes, indicar la subsecci\u00f3n network indicando la red: version: '3' services: web: container_name: apache image: httpd ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net web2: container_name: apache2 image: httpd ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/apache/html:/var/www/html\" networks: - my-net networks: my-net: Creamos la red my-net y al estar en una red creada tiene DNS y podemos contactar por nombre de container, por nombre de servicio o por IP. root@3893b20251af:/usr/local/apache2# ping web PING web (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.056 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.041 ms ^C --- web ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 16ms rtt min/avg/max/mdev = 0.041/0.048/0.056/0.010 ms root@3893b20251af:/usr/local/apache2# ping apache PING apache (172.21.0.2) 56(84) bytes of data. 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=1 ttl=64 time=0.046 ms 64 bytes from 3893b20251af (172.21.0.2): icmp_seq=2 ttl=64 time=0.056 ms ^C --- apache ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 65ms rtt min/avg/max/mdev = 0.046/0.051/0.056/0.005 ms root@3893b20251af:/usr/local/apache2# ping 172.21.0.2 PING 172.21.0.2 (172.21.0.2) 56(84) bytes of data. 64 bytes from 172.21.0.2: icmp_seq=1 ttl=64 time=0.080 ms ^C --- 172.21.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms","title":"REDES"},{"location":"docker/#build-dockerfile","text":"Para poder poner en el docker-compose nuestra imagen personalizada de un Dockerfile : Podemos o solo construir la imagen indicando donde est\u00e1 seg\u00fan si se llama Dockerfile o con otro nombre y en qu\u00e9 carpeta. Si se llama Dockerfile y ruta del directorio ('.' si est\u00e1 aqu\u00ed), ponemos la opci\u00f3n build . Le ponemos tambi\u00e9n nombre de la imagen a construir: version: '3' services: web: container_name: apache image: isx46410800/httpd-build build: . ports: - \"8081:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net networks: my-net: Si se llama diferente a Dockerfile, ponemos context para ver en que directorio est\u00e1 y dockerfile y el nombre del archivo: web2: container_name: apache2 image: isx46410800/httpd-build2 build: context: . (directorio donde est\u00e1 el dockerfile) dockerfile: Dockerfile2 ports: - \"8082:80\" volumes: - \"/home/isx46410800/Documents/curso_docker/docker-compose/build/html:/var/www/html\" networks: - my-net La construimos con docker-compose build : [isx46410800@miguel build]$ docker-compose build Building web Step 1/1 : FROM httpd ---> 417af7dc28bc Successfully built 417af7dc28bc Successfully tagged isx46410800/httpd-build:latest O construir image y hacer container de golpe con docker-compose up -d : [isx46410800@miguel build]$ docker-compose up -d apache is up-to-date Recreating apache2 ... Recreating apache2 ... done [isx46410800@miguel build]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 240530fbf981 isx46410800/httpd-build2 \"httpd-foreground\" 4 seconds ago Up 2 seconds 0.0.0.0:8082->80/tcp apache2 e8722f8e391d isx46410800/httpd-build \"httpd-foreground\" 29 seconds ago Up 27 seconds 0.0.0.0:8081->80/tcp apache","title":"BUILD DOCKERFILE"},{"location":"docker/#cmd-cambiado","text":"Para cambiar el CMD de por defecto cuando se crea un contenedor podemos cambiarlo a\u00f1adiendo la subsecci\u00f3n command : version: '3' services: web: container_name: centos image: centos command: python -m SimpleHTTPServer 8080 ports: - \"8080:8080\" networks: - my-net networks: my-net:","title":"CMD CAMBIADO"},{"location":"docker/#limitar-recursos_1","text":"Solo se puede en versi\u00f3n 2 con opciones como mem_limit o cpuset : version: '2' services: web: container_name: nginx image: nginx:alpine mem_limit: 20Mb cpuset: \"0\"","title":"LIMITAR RECURSOS"},{"location":"docker/#politica-de-reinicio","text":"Existe la subsecci\u00f3n restart que indica cuando se reinicia un contenedor. Por defecto es restart: no , no se reinicie nunca pero est\u00e1n estas opciones: restart: no restart: always : siempre se reinicie cuando muera. restart: unless-stopped : siempre se reinicia a no ser que lo pare manualmente. restart: on failure : a no ser que tenga fallos distinto a 0, no se reinicia nunca.","title":"POL\u00cdTICA DE REINICIO"},{"location":"docker/#nombre-proyecto","text":"Cuando haces un docker-compose up -d coge el nombre de proyecto, redes, etc por el nombre del directorio actual, para cambiarlo: docker-compose -p proyecto_web up -d","title":"NOMBRE PROYECTO"},{"location":"docker/#diferente-docker-compose","text":"Cuando haces un docker-compose up -d coge el nombre de docker-compose.yml, para cambiarlo por un diferente: docker-compose -f nombre_docker_compose.yml up -d","title":"DIFERENTE DOCKER-COMPOSE"},{"location":"docker/#otros-comandos","text":"docker-compose up #enciende todos los dockers del file compose.yml docker-compose -f fileCompose.yml up (-d) #elegimos que fichero encendemos del compose docker-compose down #apaga todo docker-compose ps docker-compose images docker-compose top nom_servei docker-compose port ldap 389 #servicio y puerto elegido docker-compose push/pull #subir o bajar images docker-compose logs ldap #logs del servicio elegido docker-compose pause/unpause ldap #pausar el servicio docker-compose start/stop ldap #iniciar servicio docker-compose scale ldap=2 #dos container ldap","title":"OTROS COMANDOS"},{"location":"docker/#proyectos-docker-compose","text":"","title":"PROYECTOS DOCKER-COMPOSE"},{"location":"docker/#mysql-wordpress","text":"Podemos crear una base de datos mysql y un wordpress via web en el que la bbdd se comunique con el wordpress con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=wordpress\" - \"MYSQL_USER=wordpress\" - \"MYSQL_PASSWORD=wordpress\" ports: - \"3306:3306\" networks: - my-net wordpress: container_name: wordpress image: wordpress volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"WORDPRESS_DB_HOST=bbdd:3306\" - \"WORDPRESS_DB_USER=wordpress\" - \"WORDPRESS_DB_PASSWORD=wordpress\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"MYSQL-WORDPRESS"},{"location":"docker/#drupal-postgresql","text":"Podemos crear una base de datos postgres y un drupal via web en el que la bbdd se comunique con el drupal con la subsecci\u00f3n depends_on . Al entrar en drupal nos pedir\u00e1 la contrase\u00f1a que le ponemos de variable y por defecto el user es postgres : docker-compose.yml : version: '3' services: postgresql: container_name: postgres image: postgres:11 volumes: - \"$PWD/postgresql:/var/lib/postgresql/data\" environment: - \"POSTGRESQL_PASSWORD=jupiter\" networks: - my-net drupal: container_name: drupal image: drupal:8-apache volumes: - \"drupal:/var/www/html\" ports: - \"81:80\" networks: - my-net volumes: drupal: networks: my-net: Resultados:","title":"DRUPAL-POSTGRESQL"},{"location":"docker/#prestashop-mysql","text":"Podemos crear una base de datos mysql y un prestashop via web en el que la bbdd se comunique con el prestashop con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=prestashop\" - \"MYSQL_USER=prestashop\" - \"MYSQL_PASSWORD=prestashop\" ports: - \"3306:3306\" networks: - my-net prestashop: container_name: prestashop image: prestashop/prestashop volumes: - \"$PWD/html:/var/www/html\" depends_on: - bbdd environment: - \"DB_SERVER=bbdd:3306\" - \"DB_USER=presta\" - \"DB_PASSWD=presta\" - \"DB_NAME=presta\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"PRESTASHOP-MYSQL"},{"location":"docker/#joomla-mysql","text":"Podemos crear una base de datos mysql y un joomla via web en el que la bbdd se comunique con el joomla con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: bbdd: container_name: bd-mysql image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=joomla\" - \"MYSQL_USER=joomla\" - \"MYSQL_PASSWORD=joomla\" ports: - \"3306:3306\" networks: - my-net joomla: container_name: joomla image: joomla volumes: - \"$PWD/html:/var/www/html\" environment: - \"JOOMLA_DB_HOST=bbdd\" - \"JOOMLA_DB_USER=joomla\" - \"JOOMLA_DB_PASSWORD=joomla\" - \"JOOMLA_DB_NAME=joomla\" ports: - \"80:80\" networks: - my-net networks: my-net: Resultados:","title":"JOOMLA-MYSQL"},{"location":"docker/#react-mongodb-nodejs","text":"Podemos crear una base de datos mongo y un react ecommerce hecha en node.js via web en el que la bbdd se comunique con el react con la subsecci\u00f3n depends_on : docker-compose.yml : version: '3' services: mongo: container_name: mongo image: mongo ports: - \"27017:27017\" volumes: - \"$PWD/data:/data/db\" networks: - my-net react: container_name: react-nodejs image: reactioncommerce/reaction depends_on: - mongo environment: - \"ROOT_URL=http://localhost\" - \"MONGO_URL=mongodb://mongo:27017/reaction\" ports: - \"3000:3000\" networks: - my-net networks: my-net: Resultados:","title":"REACT-MONGODB-NODE.JS"},{"location":"docker/#guacamole","text":"DOCUMENTACI\u00d3N Para sacar el fichero necesario de bbdd: $ docker run --rm guacamole/guacamole /opt/guacamole/bin/initdb.sh --postgres > initdb.sql Sirve para que desde el navegador te puedes conectar a escritorios remotos por ssh: docker-compose.yml : version: '3' services: db: container_name: guacamole-db networks: - net image: mysql:5.7 volumes: - $PWD/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql - $PWD/data:/var/lib/mysql env_file: .env daemon: container_name: guacamole-daemon networks: - net image: guacamole/guacd depends_on: - db web: container_name: guacamole-web networks: - net image: guacamole/guacamole env_file: .env depends_on: - daemon proxy: container_name: guacamole-proxy networks: - net image: nginx ports: - \"80:80\" volumes: - $PWD/nginx.conf:/etc/nginx/nginx.conf depends_on: - web networks: net: Resultados:","title":"GUACAMOLE"},{"location":"docker/#zabbix","text":"Sirve para monitorizar servidores: Dockerfile de Zabbix: FROM centos:7 ENV ZABBIX_REPO http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm RUN \\ yum -y install $ZABBIX_REPO && \\ yum -y install \\ zabbix-get \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-agent EXPOSE 80 443 COPY ./bin/start.sh /start.sh COPY ./conf/zabbix-http.conf /etc/httpd/conf.d/zabbix.conf COPY ./conf/zabbix-server.conf /etc/zabbix/zabbix_server.conf COPY ./conf/zabbix-conf.conf /etc/zabbix/web/zabbix.conf.php VOLUME /usr/share/zabbix /var/log/httpd RUN chmod +x /start.sh CMD /start.sh docker-compose.yml : version: '3' services: zabbix: container_name: zabbix-web image: zabbix build: . volumes: - \"$PWD/html:/usr/share/zabbix\" ports: - \"80:80\" networks: - net db: container_name: zabbix-db image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_USER: zabbix MYSQL_PASSWORD: zabbix MYSQL_DATABASE: zabbix volumes: - \"$PWD/data:/var/lib/mysql\" - \"$PWD/conf/create.sql:/docker-entrypoint-initdb.d/zabbix.sql\" ports: - \"3306:3306\" networks: - net networks: net: Resultados:","title":"ZABBIX"},{"location":"docker/#phpmyadmin-mysl","text":"Crear un docker-compose v3 con dos servicios: db admin. En el servicio DB, debe ir una db con mysql:5.7 y las credenciales de tu preferencia. En el admin, debes usar la imagen oficial de phpmyadmin, y por medio de redes, comunicarla con mysql. Debes exponer el puerto de tu preferencia y para validar que funcione, debes loguearte en el UI de phpmyadmin v\u00eda navegador, usando las credenciales del root de mysql. Docker-compose.yml: version: '3' services: db: container_name: mysql-db image: mysql:5.7 volumes: - \"$PWD/data:/var/lib/mysql\" environment: - \"MYSQL_ROOT_PASSWORD=jupiter\" - \"MYSQL_DATABASE=phpmyadmin\" - \"MYSQL_USER=miguel\" - \"MYSQL_PASSWORD=jupiter\" ports: - \"3306:3306\" networks: - my-net admin: container_name: phpmyadmin image: phpmyadmin/phpmyadmin depends_on: - db environment: - \"PMA_HOST=db\" - \"PMA_PASSWORD=jupiter\" - \"PMA_USER=miguel\" ports: - \"9090:80\" networks: - my-net networks: my-net: Resultados:","title":"PHPMYADMIN-MYSL"},{"location":"docker/#docker-swarm_1","text":"Orquestador de servicios en diferentes m\u00e1quinas obteniendo as\u00ed clusters en m\u00e1quinas. Tiene que haber m\u00ednimo un MANAGER , el resto son workers . Los nodos son los diferentes hosts que forman el swarm. Los stacks son el conjunts de APPs. La RED MESH es la red que hace que todos los nodes respondan a todos los servicios aunque no lo tengan en el suyo. Puerto 2377. TCP port 2377 for cluster management communications TCP and UDP port 7946 for communication among nodes UDP port 4789 for overlay network traffic El routing Mesh hace el load balance en puertos 80 y 9000. Las \u00f3rdenes docker stack / services solo se pueden hacer desde el manager. Los deploys se pueden hacer: Modo global: un servicio se despliega a todos aleatoriamente. Modo individual: para cada nodo, se despliega el servicio. Modo replicas: varias veces el mismo servicio.","title":"DOCKER SWARM"},{"location":"docker/#comandos-basicos","text":"docker swarm init docker swarm init --advertise-addr IP docker swarm join-token manager/worker docker swarm leave --force docker node ls docker node update --availability active/drain/pause nodeName docker node update --label-add tipo=valor nodeName docker node inspect nodeName docker stack deploy -c docker-compose.yml nombreApp docker stack ps nombreApp docker stack ls docker stack rm nombreApp docker stack services nombreApp docker service ls docker service ps nombreServicio docker service inspect nombreServicio docker service scale nombreServicio=2","title":"COMANDOS B\u00c1SICOS"},{"location":"docker/#inicializar","text":"Al que queremos como manager, le indicamos la siguiente orden con la IP p\u00fablica, este caso en una AWS: docker swarm init --advertise-addr 35.177.139.97 Nos dar\u00e1 un token que para cualquier nodo worker que queramos agregar al cluster,tendremos que poner eso. En nuestro caso en una m\u00e1quina AWS y otro el de mi casa: docker swarm join --token SWMTKN-1-2et2rzxn0kyfzsh8dmop8n2grqri001owhomhk7ggfr3tbls4b-587tzjo1dxtmpbpmrqldtddu1 35.177.139.97:2377","title":"INICIALIZAR"},{"location":"docker/#deploy-swarm","text":"Creamos un docker-compose.yml: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager] Desplegamos con la orden: docker stack deploy -c docker-compose.yml AppMiguel [fedora@ip-172-31-18-60 swarm]$ sudo docker stack deploy -c docker-compose.yml AppMiguel Creating network AppMiguel_default Creating service AppMiguel_hello Creating service AppMiguel_visualizer Comprobaciones de que estan los dos servicios k19:hello(6) y visualizer (1): [fedora@ip-172-31-18-60 swarm]$ docker stack ls NAME SERVICES ORCHESTRATOR AppMiguel 2 Swarm [fedora@ip-172-31-18-60 swarm]$ docker stack ps AppMiguel ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bdbfuun9q7my AppMiguel_visualizer.1 dockersamples/visualizer:stable ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago w9f3dkx7rqic AppMiguel_hello.1 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago og22dynjynb1 AppMiguel_hello.2 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago 9qk5v9nixvc5 AppMiguel_hello.3 isx46410800/k19:hello miguel Running Running about a minute ago c0hgdykvxub7 AppMiguel_hello.4 isx46410800/k19:hello ip-172-31-19-185.eu-west-2.compute.internal Running Running about a minute ago rx4khrovr84t AppMiguel_hello.5 isx46410800/k19:hello ip-172-31-18-60.eu-west-2.compute.internal Running Running about a minute ago fyxes66lquup AppMiguel_hello.6 isx46410800/k19:hello miguel Running Running about a minute ago","title":"DEPLOY SWARM"},{"location":"docker/#escalar-servicios","text":"Como vemos los dos servicios que tenemos se llaman: [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 6/6 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp Escalamos con docker service scale AppMiguel_hello=3 : [fedora@ip-172-31-18-60 swarm]$ docker service ls ID NAME MODE REPLICAS IMAGE PORTS p46df6579rup AppMiguel_hello replicated 3/3 isx46410800/k19:hello *:80->80/tcp 9n3iyb7ofvfx AppMiguel_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080->8080/tcp","title":"ESCALAR SERVICIOS"},{"location":"docker/#modo-global","text":"Para que haya un servicio en cada hosts: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: mode: global ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"MODO GLOBAL"},{"location":"docker/#nodo-drainpauseactive","text":"DRAIN: hace que el nodo, todos sus servicios se los pasa a otro. PAUSE: pausa el nodo, siguen sus servicios pero no acepta m\u00e1s. ACTIVE: volvemos activar el nodo. Orden: docker node update --availability active/drain/pause nodeName","title":"NODO DRAIN/PAUSE/ACTIVE"},{"location":"docker/#labels","text":"Podemos poner etiquetas a los nodos y hacer deploy segun etiquetas. Orden: docker node update --label-add tipo=valor nodeName [fedora@ip-172-31-18-60 swarm]$ docker node update --label-add sexo=hombre miguel miguel Y hacemos deploy segun etiquetas: version: \"3\" services: hello: image: isx46410800/k19:hello deploy: replicas: 6 placement: constraints: [node.labels.sexo == hombre] ports: - \"80:80\" visualizer: image: dockersamples/visualizer:stable ports: - \"8080:8080\" volumes: - \"/var/run/docker.sock:/var/run/docker.sock\" deploy: placement: constraints: [node.role == manager]","title":"LABELS"},{"location":"docker/#docker-registry","text":"Ser\u00eda la misma funci\u00f3n que crear una cuenta en Dockerhub y despu\u00e9s hacer: docker login docker tag nombre isx4610800/nombre:tag docker commit isx4610800/nombre:tag docker push isx4610800/nombre:tag Documentaci\u00f3n Docker Registry Lo creamos: docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 registry:2 Tenemos que crear un diretorio data donde estemos y podemos ponerle cualquier puerto. [isx46410800@miguel registry]$ ls data [isx46410800@miguel registry]$ docker run --name registry -v $PWD/data:/var/lib/registry -p 5000:5000 -d registry:2 a52169f2861d43450071e5bedeb01380fc2a26fe9030975b127b4a2452e5f62e [isx46410800@miguel registry]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a52169f2861d registry:2 \"/entrypoint.sh /etc\u2026\" 3 seconds ago Up 1 second 0.0.0.0:5000->5000/tcp registry Subimos una imagen: [isx46410800@miguel registry]$ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc Status: Image is up to date for hello-world:latest [isx46410800@miguel registry]$ docker tag hello-world:latest localhost:5000/hello:registry [isx46410800@miguel registry]$ docker push localhost:5000/hello:registry The push refers to repository [localhost:5000/hello] 9c27e219663c: Pushed registry: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525 [isx46410800@miguel registry]$ ls data/docker/registry/v2/repositories/hello/ _layers _manifests _uploads Bajar la imagen del registry: docker pull localhost:5000/hello:registry Subir/Bajar una imagen desde nuestra IP o hac\u00eda nuestra IP: [isx46410800@miguel registry]$ sudo vi /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H unix:// --insecure-registry 192.168.1.144:5000 systemctl daemon-reload [isx46410800@miguel registry]$ docker push 192.168.1.144:5000/hello:registry Ya podremos hacer pull/push a esta IP o por ejemplo a una IP de AWS donde tuvieramos el registry.","title":"DOCKER REGISTRY"},{"location":"files/","text":"Archivos importantes de Linux Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos Destacados"},{"location":"files/#archivos-importantes-de-linux","text":"Usuarios del sistema (user.pass,uid,gid,gecos,home,shell) /etc/passwd Grupos del sistema (gnamegroup,pass,gid,list users) /etc/group Contrase\u00f1as de los usuarios (user,pass,last passwd change,min dias para cambiar pass,max dias con misma pass,warning dias aviso pass,inactivity,expire account) /etc/shadow Dominio DNS /etc/resolv.conf Lo que monta el sistema al encenderse(filesystem,mountpoint,type,options,dump,check): /etc/fstab Hosts del sistema (IP->nombre) /etc/hosts # #192.168.1.41 miguel Cambiar host de nombre: /etc/hostname miguel File para hacer crons: /etc/crontab Grub del sistema: /boot/grub2/grub.cfg Negar Ips ssh: /etc/hosts.deny Ficheros SSH /etc/ssh/sshd_config /etc/ssh/ssh_config Ficheros xinetd: /etc/xinetd.d Cosas exportables: /etc/exports Samba: /etc/samba/smb.conf Named: /etc/named APACHE HTTP: /etc/httpd/conf/httpd.conf /var/www/html/index.html Mail: /var/spool/mail /etc/mail/sendmailcf-mc Volumenes en Docker: /var/lib/docker/volumes Usuarios con los que hemos hecho contacto SSH: /.ssh/known_hosts /.ssh/authorized_keys","title":"Archivos importantes de Linux"},{"location":"gitlab/","text":"Comandos para GIT git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file Obtener claves para GIT ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com Github pages Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io GitKraken Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken Git Tags/Releases Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0 Gitflow Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/ Features A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch Hotfix Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch Releases Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master. Emulador CMDER Descargar","title":"Gitlab"},{"location":"gitlab/#comandos-para-git","text":"git add . git status git commit -m \"...\" git init . git config --global user.email git config --global user.name \"isx46410800\" git config --global user.email \"miguel14amoros@gmail.com\" git config --global --list git branch -va git checkout branch git checkout -b branch git checkout -d/-D branch git checkout -- file git checkout head~3 git merge branch git show hash git log branch /hash commit git diff branch...branch git remote remove/add origin master/branch git push/pull/fetch -u origin master/branch git reset ~2 git reset HEAD~1 git div #gestionar conflictos de archivos git tag / git tag -l / git tag -l \"v1.0.0\"` git tag -a v1.4 -m \"my version 1.4\" git tag v1.4-lw git show v1.4 git push origin v1.5 / git push origin --tags git tag -d v1.4 git push origin --delete git checkout -b version2 v2.0.0 git blame file","title":"Comandos para GIT"},{"location":"gitlab/#obtener-claves-para-git","text":"ssh-keygen Copiamos la publica en repo git Comprobamos con ssh -T xxx@gitlab.com","title":"Obtener claves para GIT"},{"location":"gitlab/#github-pages","text":"Tutorial GithHub pages Creamos repositorio con extensi\u00f3n github.io->https://github.com/isx46410800/miguelamoros.github.io Clonamos, metemos la chicha de MKdocs. Hacemos un mkdocs build y un mkdocs gh-deploy y nos dar\u00e1 un link de nuestra web est\u00e1tica generada por mkdocs en Github. https://isx46410800.github.io/miguelamoros.github.io","title":"Github pages"},{"location":"gitlab/#gitkraken","text":"Aplicaci\u00f3n de interfaz gr\u00e1fica para gestionar Git. Descargar GitKraken","title":"GitKraken"},{"location":"gitlab/#git-tagsreleases","text":"Sirve para poner hasta donde es de mi c\u00f3digo las diferentes versiones. Crear tag version: Ejemplo v.1.0.0 : Primer n\u00famero es major number, cambio de n\u00famero es cambio grande de versi\u00f3n. Segundo n\u00famero es minor number, cambio no tan trascendente, un cambio de alguna funci\u00f3n, interfaz.. Tercer n\u00famero es un patx, correci\u00f3n de bugs. Listar tags: git tag / git tag -l / git tag -l \"v1.0.0\" Crear tags: git tag -a v1.4 -m \"my version 1.4\" git show v1.4 git tag v1.4-lw # etiqueta en .git ligera Subir tag porque el git push no sube los tags: git push origin v1.5 git push origin --tags #varios a la vez Borrar tags: git tag -d v1.4 git push origin --delete <tagname> Cambio de ramas a esa versi\u00f3n: git checkout -b version2 v2.0.0","title":"Git Tags/Releases"},{"location":"gitlab/#gitflow","text":"Flujo de trabajo en las que se puede a\u00f1adir nuevas caracter\u00edsticas, funciones, releases,etc... Deben existir las dos ramas master y develop. Creamos el GitFlow: git flow init Esto crear\u00e1 tres ramas auxiliares por defecto: feature/ release/ hotfix/","title":"Gitflow"},{"location":"gitlab/#features","text":"A\u00f1adir una nueva caracter\u00edstica o funci\u00f3n, lo crea como si fuera una nueva branch, feature/nameFeature: # Crear caracter\u00edstica git flow feature start create-contat-form # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Create contact-form.php\" # Finalizar caracter\u00edstica git flow feature finish create-contat-form #Creaci\u00f3n de una rama de funci\u00f3n -Sin las extensiones de git-flow: git checkout develop git checkout -b feature_branch -Cuando se utiliza la extensi\u00f3n de git-flow: git flow feature start feature_branch #Finalizar feature -Sin las extensiones de git-flow: git checkout develop git merge feature_branch -Con las extensiones de git-flow: git flow feature finish feature_branch","title":"Features"},{"location":"gitlab/#hotfix","text":"Las ramas de mantenimiento o \"correcci\u00f3n\" (hotfix) se utilizan para reparar r\u00e1pidamente las publicaciones de producci\u00f3n. Las ramas de correcci\u00f3n son muy similares a las ramas de publicaci\u00f3n y a las de funci\u00f3n, salvo porque se basan en la maestra en vez de la de desarrollo. Es la \u00fanica rama que deber\u00eda bifurcarse directamente a partir de la maestra. Una vez que la soluci\u00f3n est\u00e9 completa, deber\u00eda fusionarse en la maestra y la de desarrollo (o la rama de publicaci\u00f3n actual), y la maestra deber\u00eda etiquetarse con un n\u00famero de versi\u00f3n actualizado. Tener una l\u00ednea de desarrollo espec\u00edfica para la soluci\u00f3n de errores permite que tu equipo aborde las incidencias sin interrumpir el resto del flujo de trabajo ni esperar al siguiente ciclo de publicaci\u00f3n. Puedes considerar las ramas de mantenimiento como ramas de publicaci\u00f3n ad hoc que trabajan directamente con la maestra. Una rama de correcci\u00f3n puede crearse utilizando los siguientes m\u00e9todos: # iniciar -Sin las extensiones de git-flow: git checkout master git checkout -b hotfix_branch -Cuando se utilizan las extensiones de git-flow: git flow hotfix start hotfix_branch # finalizar - sin git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch -con $ git flow hotfix finish hotfix_branch","title":"Hotfix"},{"location":"gitlab/#releases","text":"Mandar una nueva versi\u00f3n a producci\u00f3n: # Crear liberaci\u00f3n git flow release start 1.0.0 # Confirmar los cambios que se hayan realizado git status git add -A git commit -m \"Add release notes\" # Finalizar liberaci\u00f3n git flow release finish 1.0.0 # Subir cambios de la rama develop git checkout develop git push # Subir cambios de la rama master git checkout master git push # iniciar release -Sin las extensiones de git-flow: git checkout develop git checkout -b release/0.1.0 -Cuando se utilizan las extensiones de git-flow: git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' # finalizar -Sin las extensiones de git-flow: git checkout master git merge release/0.1.0 -con la extensi\u00f3n de git-flow: git flow release finish '0.1.0' Conclusi\u00f3n: En cada m\u00e1quina y directorio donde tengamos el repositorio la primera vez se debe inicializar el flujo de trabajo con git flow init. # Una vez finalizado un release o un hotfix se deben confirmar los cambios con un git push sobre develop y master # Se recomienda subir las etiquetas al repositorio con git push \u2013tags para tener un control de versiones sobre la rama de master.","title":"Releases"},{"location":"gitlab/#emulador-cmder","text":"Descargar","title":"Emulador CMDER"},{"location":"jenkins/","text":"Jenkins Instalaci\u00f3n FEDORA Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins UBUNTU/DEBIAN Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins DOCKER En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net: NOTAS A TENER EN CUENTA Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini... PROYECTO CON PARAMETROS Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script. SSH Creacion SSH container Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins Credenciales Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully. Ejercicio mandar un job a maquina remota En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota. BASE DE DATOS JENKINS Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p Creacion bbdd simple MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27); Creaci\u00f3n Buckets en amazon Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key Dump de la bbdd [root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql DUMP AUTOMATIZADO Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea). ANSIBLE Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Playbooks Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG TAGS Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job. DB MYSQL Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada. NGINX SERVER Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro. SECURITY JENKINS Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles: MANAGE USERS Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas: MANAGE ROLES Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron. TRIPS AND TICKS Variables de entorno Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A Cambio URL Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/ CRON Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue. GATILLAR JOBS Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22 MAIL Configurar envio de notificaciones Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent Gmail como server de correo Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo: Email de error Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email. MAVEN Instalacion Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app Configuracion de un job Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar Registrar los resultados A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results: Archivar los jar A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla: GIT SERVER Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido. CAMBIO URL MAVEN/GIT/JENKINS Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job. JOB DSL Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl SEED JOB Ejemplo estructura: job('job_dsl_example') { } DESCRIPCION Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada PAR\u00c1METROS Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas. SCM La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso. TRIGGERS Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron. STEPS Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world MAILER Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones. JOB DE ANSIBLE EN DSL En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/ JOB DE MAVEN EN DSL Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } DSL en GIT Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } } PIPELINES Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline JENKINSFILE Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente. MULTIPLE-STEPS pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } } POST-ACTIONS pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } } RETRY pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } } TIMEOUT pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } } VARIABLES ENV pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } } CREDENCIALES pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } } CI/CD BUILD Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } } TEST Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } PUSH A MAQUINA REMOTA AWS Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa CERTIFICADO SSL REGISTRY CON AUTENTICACION Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } } DEPLOY En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } } CI/CD Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"Jenkins"},{"location":"jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"jenkins/#instalacion","text":"","title":"Instalaci\u00f3n"},{"location":"jenkins/#fedora","text":"Actualizar repositorios: sudo dnf update -y Instalar Java: sudo dnf install -y java Agregar repositorios de Jenkins: sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key sudo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key Instalar Jenkins: sudo dnf install -y jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl status jenkins","title":"FEDORA"},{"location":"jenkins/#ubuntudebian","text":"Actualizar repositorios: sudo apt update Instalar Java: sudo apt install openjdk-8-jdk Agregar repositorios de Jenkins: wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' Instalar Jenkins: sudo apt update sudo apt install jenkins Encender el servicio Jenkins: sudo systemctl start jenkins sudo systemctl start jenkins","title":"UBUNTU/DEBIAN"},{"location":"jenkins/#docker","text":"En un fichero docker-compose.yml: docker-compose up -d version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" #cambiamos el 9090 de local nuestro volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net networks: net:","title":"DOCKER"},{"location":"jenkins/#notas-a-tener-en-cuenta","text":"Al instalar se crea el usuario y grupo Jenkins Jenkins trabaja en el directorio /var/lib/jenkins Los archivos de log est\u00e1n en /var/log/jenkins/jenkins.log Los par\u00e1metros de configuraci\u00f3n se encuentran en /etc/sysconfig/jenkins Por defecto, el puerto donde trabaja Jenkins es el 8080 La contrase\u00f1a de administrar Jenkins se encuentra en /var/jenkins_home/secrets/ini...","title":"NOTAS A TENER EN CUENTA"},{"location":"jenkins/#proyecto-con-parametros","text":"Aqu\u00ed podemos definir en la opci\u00f3n de this project is parameterized->string parameter se puede definir variable con valor para utilizarlas en la construcci\u00f3n del job con un build de execute shell . Tambi\u00e9n con choice parameter podemos hacer una variable con diferentes opciones a elegir: Tambi\u00e9n con boolean parameter podemos hacer una variable con true/false a elegir: Le pasamos los argumentos por las variables definidas en los par\u00e1metros y en el script.","title":"PROYECTO CON PARAMETROS"},{"location":"jenkins/#ssh","text":"","title":"SSH"},{"location":"jenkins/#creacion-ssh-container","text":"Vamos a crear un container con ssh server para poder conectarnos alli y hacer cosas con Jenkins. Creamos un Dockerfile con Centos e instalamos el ssh, creamos su directorio ssh y creamos unas llaves con ssh-keygen -f nombre-key para pasarle la publica al ssh y asi conectarnos directamente sin password. Modificamos el docker-compose.yml a\u00f1adiendo el servicio de ssh para ello creamos un nuevo servicio con una image: build: context: ssh y luego haremos un docker-compose build y nos generar\u00e1 una imagen a trav\u00e9s del dockerfile de dentro de donde pongamos la ubicacion en context : # Instalamos un container con SO centos FROM centos:7 # Instalamos el ssh server para poder conectarnos por ssh alli RUN yum -y install openssh-server # Creamos un usuario con pass por stdin y creamos su dir ssh y con permisos RUN useradd remote_user && \\ echo \"1234\" | passwd remote_user --stdin && \\ mkdir /home/remote_user/.ssh && \\ chmod 700 /home/remote_user/.ssh # Copiamos nuestra clave publica ssh y la copiamos en el authorized(se crea) para conectarnos sin passwd COPY remotessh-key.pub /home/remote_user/.ssh/authorized_keys # Cambiamos propetario y grupo a todo lo que haya abajo del home remoteuser y damos permisos RUN chown remote_user:remote_user -R /home/remote_user && \\ chmod 600 /home/remote_user/.ssh/authorized_keys # Para que no de errores por primera vez en un container RUN /usr/sbin/sshd-keygen > /dev/null 2>&1 # activamos servicio ssh detached CMD /usr/sbin/sshd -D version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net networks: net: Hacemos el Docker-compose [isx46410800@miguel jenkins]$ docker-compose up -d jenkins is up-to-date Creating remote_host ... Creating remote_host ... done [isx46410800@miguel jenkins]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 172b5c2a9f99 remote_host \"/bin/sh -c '/usr/sb\u2026\" 6 seconds ago Up 3 seconds remote_host 4f52a64e7618 jenkins/jenkins \"/sbin/tini -- /usr/\u2026\" 5 hours ago Up 3 hours 50000/tcp, 0.0.0.0:9090->8080/tcp jenkins","title":"Creacion SSH container"},{"location":"jenkins/#credenciales","text":"Para comprobar que el SSH y JENKINS se ven y comunican, hacemos primero un ping desde Jenkins con ping remote_host y despues nos conectamos por ssh con el usuario creado alli ssh remote_user@remote_host o copiando la llave publica a Jenkins y desde ahi ssh -i remotessh-key remote_user@remote_host Configurar en Credentials -System - Global - credentials para poner las credenciales de SSH con el usuario creado en dockerfile y la llave privada creada. Luego vamos a Manage - system configuration - ssh remote host y ponemos el nombre del servicio ssh del docker, puerto y las credenciales creadas antes. Damos a check conection y comprobamos que sale successfully.","title":"Credenciales"},{"location":"jenkins/#ejercicio-mandar-un-job-a-maquina-remota","text":"En este ejercicio lo que hacemos es que desde jenkins, mandamos un job creando un build - execute shell via ssh remote con las credenciales creadas de ssh a una m\u00e1quina remota. El resultado lo veremos dentro de esta maquina remota.","title":"Ejercicio mandar un job a maquina remota"},{"location":"jenkins/#base-de-datos-jenkins","text":"Modificamos el docker-compose y creamos un servicio que ser\u00e1 una bbdd de mysql creando un volumen para que la xixa se guarde ah\u00ed al salir. Indicamos un nuevo campo de environment para poner el campo de la pass de root MYSQL_ROOT_PASSWORD=1234 . version: '3' services: jenkins: container_name: jenkins image: jenkins/jenkins ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net remote_host: container_name: remote_host image: remote_host build: context: ssh networks: - net db_host: container_name: db image: mysql:5.7 environment: - \"MYSQL_ROOT_PASSWORD=1234\" volumes: - $PWD/db_data:/var/lib/mysql networks: - net networks: net: Nos conectamos al container nuevo y para entrar a la bbdd se pone el comando mysql -u root -p . A\u00f1adimos lo siguiente en el Dockerfile para poder utilizar mysql por ssh y aws: # Instalamos mysql para poder conectarnos a la bbdd con mysql como comando RUN yum -y install mysql # Instalamos aws cli para amazon que est\u00e1 en un paquete de epel-pip RUN yum -y install epel-release && yum -y install python-pip && pip install --upgrade pip && yum -y install awscli Una vez cambiado hacemos un docker-compose build para que vuelva construir todo con los cambios nuevos y despues enchegar de nuevo con las nuevas construcciones docker-compose up -d . Hacemos un ping desde ssh container a db container para comprobar conexion: [root@e1825be6ec48 /]# ping db_host PING db_host (172.21.0.4) 56(84) bytes of data. 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=1 ttl=64 time=0.162 ms 64 bytes from db.jenkins_net (172.21.0.4): icmp_seq=2 ttl=64 time=0.083 ms Despu\u00e9s de esto nos conectamos a la bbdd del container desde ssh con opcion -h de host: [root@e1825be6ec48 /]# mysql -u root -h db_host -p","title":"BASE DE DATOS JENKINS"},{"location":"jenkins/#creacion-bbdd-simple","text":"MySQL [(none)]> show databases MySQL [(none)]> create database testdb; MySQL [(none)]> use testdb; MySQL [testdb]> create table info (name varchar(20), surname varchar(20), age int(2)); MySQL [testdb]> show tables; MySQL [testdb]> desc info; MySQL [testdb]> insert into info values('Miguel', 'Amoros', 27);","title":"Creacion bbdd simple"},{"location":"jenkins/#creacion-buckets-en-amazon","text":"Amazon Simple Storage Service (Amazon S3) es almacenamiento para Internet. Puede usar Amazon S3 para almacenar y recuperar cualquier cantidad de datos en cualquier momento y desde cualquier parte de la Web. Puede realizar estas tareas usando la Consola de administraci\u00f3n de AWS, que es una sencilla e intuitiva interfaz web. Amazon S3 almacena datos a modo de objetos dentro de buckets. Un objeto es un archivo y cualquier metadato opcional que describe el archivo. Para almacenar un archivo en Amazon S3, lo carga a un bucket. Al cargar un archivo como objeto, puede configurar permisos en el objeto y en cualquier metadato. Los buckets son contenedores de objetos. Puede tener uno o m\u00e1s buckets. Puede controlar el acceso de cada bucket, decidiendo qui\u00e9n puede crear, eliminar y enumerar objetos en \u00e9l. Tambi\u00e9n puede elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido y ver los registros de acceso para el bucket y sus objetos. AWS - BUCKETS - CREATE BUCKET # jenkins-udemy-miguel Creamos un usuario de autenticaci\u00f3n para subir cosas al bucket: AWS-IAM-USERS-CREATE USER Opcion attach - full access - crear - download .csv key","title":"Creaci\u00f3n Buckets en amazon"},{"location":"jenkins/#dump-de-la-bbdd","text":"[root@e1825be6ec48 /]# mysqldump -u root -h db_host -p1234 testdb > /tmp/dbdump.sql -- MySQL dump 10.14 Distrib 5.5.65-MariaDB, for Linux (x86_64) -- -- Host: db_host Database: testdb -- ------------------------------------------------------ -- Server version 5.7.31 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `info` -- DROP TABLE IF EXISTS `info`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `info` ( `name` varchar(20) DEFAULT NULL, `surname` varchar(20) DEFAULT NULL, `age` int(2) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `info` -- LOCK TABLES `info` WRITE; /*!40000 ALTER TABLE `info` DISABLE KEYS */; INSERT INTO `info` VALUES ('Miguel','Amoros',27); /*!40000 ALTER TABLE `info` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2020-09-25 18:19:57 Configuramos a trav\u00e9s del aws cli de amazon para poder subir el DUMP al bucket de s3 de amazon. A trav\u00e9s de las credenciales obtenidas en bucket configuramos las variables de entorno. Las configuramos en el container de bbdd: ayuda Ahora con las credenciales podremos copiar el dump al bucket de amazon: ayuda [root@e1825be6ec48 /]# aws s3 cp /tmp/dbdump.sql s3://jenkins-udemy-miguel upload: tmp/dbdump.sql to s3://jenkins-udemy-miguel/dbdump.sql","title":"Dump de la bbdd"},{"location":"jenkins/#dump-automatizado","text":"Creamos un script dentro del container de nuestra bbdd para poder hacer desde jenkins una conexion a la bbdd remota y subir a amazon el dump al bucket de almacenaje. #!/bin/bash # definimos unas variables DB_HOST=$1 DB_PASSWORD=$2 DB_NAME=$3 DATE=%(date +$H-%M-%S) AWS_SECRET=$4 BUCKET_NAME=$5 # hacemos el dump de a bbdd diciendo el nombre host servicio, pass y name de la bbdd, exportamos las variables aws para subir al bucket mysqldump -u root -h $DB_HOST -p$DB_PASSWORD $DB_NAME > /tmp/db-$DATE.sql && \\ export AWS_ACCESS_KEY_ID=AKIA5RIFOUI3AQMRXFFQ && \\ export AWS_SECRET_ACCESS_KEY=$AWS_SECRET && \\ aws s3 cp /tmp/db-$DATE.sql s3://$BUCKET_NAME Configuramos ahora las credenciales de la bbdd en jenkins con una variable de db_name y el passwd de nuestra bbdd que era 1234: Configuramos ahora las credenciales del s3 bucket en jenkins poniendo la passwd secret key: Ahora configuramos en Jenkins las variables parametrizadas del script de bbdd: Despu\u00e9s en la opci\u00f3n de entorno de ejecuci\u00f3n selecionamos la opci\u00f3n de usar secret text y ponemos las credenciales creadas anteriormente y la variable del script creado en la bbdd. Build por ssh: Automatizamos: A\u00f1adimos en el docker-compose estas lineas para que el script creado en tmp de la bbdd no se borre cuando se elimine, por lo tanto la chicha del script de fuera lo mandamos alli copiado: volumes: - $PWD/dumpremotessh-aws.sh:/tmp/dumpremote.sh Ahora si creamos en mysql otra db y en amazon otro bucket, cambiamos los parametros del job y nos crea lo mismo sin cambiar el script. Podemos tambien hacerlo manualmente y en vez de llamar al script, lo copiamos dentro y hace lo mismo (opci\u00f3n m\u00e1s fea).","title":"DUMP AUTOMATIZADO"},{"location":"jenkins/#ansible","text":"Automatizaci\u00f3n de tareas hecho en python. Creamos un nuevo dockerfile: # sistema basado en jenkins FROM jenkins/jenkins # instalamos pip como root USER root RUN curl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\" && python get-pip.py RUN pip install -U ansible USER jenkins Modificamos el docker-compose: jenkins: container_name: jenkins image: jenkins-ansible build: context: jenkins-ansible ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home networks: - net Hacemos docker-compose build y up -d Creamos un fichero hosts con lenguaje ansible para crear nuestro primer fichero de inventario. # ARCHIVO DE INVENTARIO ANSIBLE # todas las variables se definen asi [all:vars] # todas las maquinas se conectaran por ssh ansible_connection = ssh [test] # aque maquina me voy a conectar con el nombre test1 y con que usuario y donde esta la llave privada para conectarme test1 ansible_host=remote_host ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Despues lo copiamos dentro de [isx46410800@miguel jenkins]$ cp hosts jenkins_home/ansible/ para que est\u00e9 dentro del container jenkins-ansible ya que aqui est\u00e1 el volumen de la xixa del container que se guarda. Comprobamos conexion de nuestro inventario ansible-jenkins con la m\u00e1quina ssh remote_host: jenkins@7cafd0984215:~/ansible$ ansible -m ping -i hosts test1 -m de modulo -i fichero y maquina test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" }","title":"ANSIBLE"},{"location":"jenkins/#playbooks","text":"Los Playbooks describen configuraciones, despliegue, y orquestaci\u00f3n en Ansible. \u200b El formato del Playbook es YAML. \u200b Cada Playbook asocia un grupo de hosts a un conjunto de roles. Cada rol est\u00e1 representado por llamadas a lo que Ansible define como Tareas. Creamos primer fichero playbook: cat play.yml - hosts: test1 tasks: - shell: echo \"Hola Mundo desde Ansible y Jenkins\" > /tmp/hola-ansible.txt Para comprobar el funcionamiento: jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts play.yml lo que hace es desde jenkins conectar el playbook a la maquina creada en ansible test1(que es remote_host de ssh container) por ssh. Instalamos el modulo ansible en jenkins y creamos un job con build de ansible playbook. Ponemos la ruta del playbook y la ruta del file hosts para la conexion. asi nos ahorramos poner toda la ruta de arriba, lo hacemos automatizado. Modificamos el fichero play.yml para pasar el texto por parametro: A\u00f1adimos los parametros y la variable extra para que en el script coja la variavle MSG con el parametro texto de arriba( seria como a\u00f1adir la opcion -e \"MSG=hola\" en hardcode): - hosts: test1 tasks: - debug: var: MSG","title":"Playbooks"},{"location":"jenkins/#tags","text":"Ponemos tags en nuestro script: - hosts: test1 tasks: - debug: var: MSG - debug: msg: \"Yo no me voy a ejecutar :(\" tags: no-exec - debug: msg: \"Yo s\u00ed me voy a ejecutar :)\" tags: si-exec solo se ejecutan las tareas que ponen en RUN de tags en jenkins, el resto no: PLUGIN: ANSICOLOR para que salga en colo en jenkins el resultado del job activando la opci\u00f3n color en configuracion del job.","title":"TAGS"},{"location":"jenkins/#db-mysql","text":"Creamos en el container db una bbdd de people con registros en la tabla registro. De un file con 50 nombres, hacemos un script para meterlos todos en la bbdd: #!/bin/bash #iniciamos contador count=0 #mientras sea menos de 50 personas del archivo, coger los campos while [ $count -lt 50 ] do count=$((count+1)) nombre=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f3) apellido=$(nl people.txt | grep -w $count | tr -s '[:blank:]' ',' | cut -d',' -f4) edat=$(shuf -i 20-25 -n1) mysql -u root -p1234 people -e \"insert into registro values($id, '$nombre', '$apellido', $edat)\" echo \"$count, $nombre, $apellido importado\" sleep 5 done copiamos el script en el container db y lo ejecutamos para que se llene la bbdd creada.","title":"DB MYSQL"},{"location":"jenkins/#nginx-server","text":"Creamos un container con nginx server y php a partir del container con ssh: # a partir de la imagen de ssh generada ya FROM remote_host # a\u00f1adimos el repo del web server nginx para centos COPY ./conf/nginx.repo /etc/yum.repos.d/nginx.repo # instalamos los paquetes necesarios y de php RUN \\ yum -y install nginx-1.12.2 openssl --enablerepo=nginx && \\ yum -y install https://repo.ius.io/ius-release-el7.rpm \\ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \\ yum -y install \\ php71u-fpm \\ php71u-cli \\ php71u-mysqlnd \\ php71u-soap \\ php71u-xml \\ php71u-zip \\ php71u-json \\ php71u-mcrypt \\ php71u-mbstring \\ php71u-zip \\ php71u-gd \\ --enablerepo=ius-archive && yum clean all # abrimos los puertos por donde escuchar EXPOSE 80 443 # nos quedamos con los volumenes VOLUME /var/www/html /var/log/nginx /var/log/php-fpm /var/lib/php-fpm # comando para dar permisos al usuario creado d ssh RUN setfacl -R -m u:remote_user:rwx /var/www/html # copiamos el fichero de configuracion COPY ./conf/nginx.conf /etc/nginx/conf.d/default.conf # copiamos el fichero de empezar COPY ./bin/start.sh /start.sh # damos permisos de ejecucucion RUN chmod +x /start.sh # arranca el container con el script CMD /start.sh Modificamos el docker-compose para a\u00f1adir el nuevo container nginx-php con ssh: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"80:80\" networks: - net creamos un servicio web con el nombre container y la imagen ansible-web que se crea a trav\u00e9s del dockerfile con la ruta en context. Hacemos un docker-compose build y up. NOTA: desactivo con systemctl stop httpd porque escucha por el puerto 80 del web que queremos crear. Entramos al container web y a\u00f1adimos el indice de index.php: [root@7d0d237e1686 /]# cat /var/www/html/index.php <?php phpinfo(); ?> Hacemos esto solo de prueba para nuestro navegador Creamos una tabla que muestra la informaci\u00f3n via web: [isx46410800@miguel jenkins-ansible]$ docker cp table.j2 web:/var/www/html/index.php Para integrar el webserver en nuestro inventario de Ansible modificamos el fichero host de /jenkins_home/ansible/hosts y a\u00f1adimos el nuevo alias y el nombre servicio: web1 ansible_host=web ansible_user=remote_user ansible_private_key_file=/var/jenkins_home/ansible/remotessh-key Comprobamos yendo al container jenkins que es donde est\u00e1 instalado Ansible y lo comprobamos como la otra vez: jenkins@7cafd0984215:~/ansible$ pwd /var/jenkins_home/ansible **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping web1 web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } **jenkins@7cafd0984215:~/ansible$ ansible -i hosts -m ping all test1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } web1 | SUCCESS => { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } Ahora vamos hacer lo anterior pero de manera automatizada en Jenkins. Para ello creamos un playbook nuevo: - hosts: web1 tasks: - name: Transfiere el template hacia web1 template: src: table.j2 dest: /var/www/html/index.php Cambiamos unos datos del fichero table.j2 donde contenia los datos a mostrar en el index.php para poder pasar las cosas por parametros en Jenkins: $sql = \"SELECT id, nombre, apellido, edat FROM registro where edat <= 25 and edat >=20\"; ----> CAMBIOS $sql = \"SELECT id, nombre, apellido, edat FROM registro {% IF EDAD is defined %} where edat = {{ EDAD }} {% endif %}\";---- queremos decir que si el parametro que pasamos EDAD est\u00e1 defenido haga la consulta donde la edad sea igual al parametro. Damos permisos para solucionar un fallo de poner escribir dentro del container web en la carpeta de html y despues dentro del container jenkins, probamos siempre lo del playbook: [root@7d0d237e1686 /]# chown remote_user:remote_user -R /var/www/html/ jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml jenkins@7cafd0984215:~/ansible$ ansible-playbook -i hosts people.yml -e \"EDAD=22\" con y sin parametros, y el cambio lo vemos en el index.php del container web. Vamos a jenkins y automatizamos la tarea, poniendo una variable de opcion, el path del playbook y del fichero de hosts para conectar con la maquina y despues una extra variable que sera la variable que pasamos como parametro.","title":"NGINX SERVER"},{"location":"jenkins/#security-jenkins","text":"Por defecto no est\u00e1 activado, pero si queremos que cualquier persona se pueda loguear al jenkins via navegador vamos a Manage Jenkins- Conf global Security y clicamos en la opcion de desactivar seguridad. Se puede activar la opci\u00f3n Allow users to sign up para permitir a usuarios crearse una cuenta para entrar a Jenkins igual que la otra opci\u00f3n de que tengan permisos especiales los usuarios registrados. Activamos lo de registrarse, nos ddesconectamos y creamos dos cuentas: Instalamos un potente plugin de seguidad que sirve para gestionar los roles y dar permisos a los usuarios: Role-based Authorization Strategy Entramos de nuevo a la conf de seguridad con el uuario admin y le damos a este role de usuarios. Veremos que nos aparece una nueva pesta\u00f1a de menu para que pueda gestionar los roles:","title":"SECURITY JENKINS"},{"location":"jenkins/#manage-users","text":"Vamos a manage jenkins-manage users aqui podremos crear/borrar/modificar usuarios sin tener que hacerlos creando cuentas:","title":"MANAGE USERS"},{"location":"jenkins/#manage-roles","text":"Vamos a manage jenkins-manage and assign roles y manage roles para gestionar los roles de un usuario: Creamos un nuevo role en role to add como por ejemplo que solo sea de lectura el role del usuario, solo podr\u00e1 ver jobs sin ejecutar ni nada mas: Ahora asignamos este role creado de solo-lectura a uno de los uusuarios. Vamos a manage jenkins-manage and assign roles y assign role. Veremos al loguearlos despues que solo puede ver, solo lectura. Si modificamos el manage role y le ponemos que pueda read los jobs, al loguearse veremos que pueda ver los jobs almenos. Ahora creamos un role de poder ejecutar y ver los jobs y se lo asignamos: Ahora lo que queremos hacer es que un usuario en vez de ver todos los jobs, solo veas los que le digamos y pueda hacer build solo a esos. Para ello le quitamos el read the jobs y creamos un item role y le a\u00f1adimos un patron para ver solo jobs con ese patron.","title":"MANAGE ROLES"},{"location":"jenkins/#trips-and-ticks","text":"","title":"TRIPS AND TICKS"},{"location":"jenkins/#variables-de-entorno","text":"Lista de variables de entorno propias de Jenkins: echo \"BUILD_NUMBER: $BUILD_NUMBER\" echo \"BUILD_ID: $BUILD_ID\" echo \"BUILD_URL: $BUILD_URL\" echo \"JOB_NAME: $JOB_NAME\" echo \"JAVA_HOME: $JAVA_HOME\" echo \"JENKINS_URL: $JENKINS_URL\" lista variables Resultado de un simple job: Console Output Started by user admin Running as SYSTEM Building in workspace /var/jenkins_home/workspace/7-ENV [7-ENV] $ /bin/sh -xe /tmp/jenkins7847738549255029537.sh + echo BUILD_NUMBER: 1 BUILD_NUMBER: 1 + echo BUILD_ID: 1 BUILD_ID: 1 + echo BUILD_URL: http://localhost:9090/job/7-ENV/1/ BUILD_URL: http://localhost:9090/job/7-ENV/1/ + echo JOB_NAME: 7-ENV JOB_NAME: 7-ENV + echo JAVA_HOME: /usr/local/openjdk-8 JAVA_HOME: /usr/local/openjdk-8 + echo JENKINS_URL: http://localhost:9090/ JENKINS_URL: http://localhost:9090/ Finished: SUCCESS Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de variables de entorno: echo \"PLATAFORMA: $PLATAFORMA\" echo \"PAIS: $PAIS\" + echo PLATAFORMA: UDEMY PLATAFORMA: UDEMY + echo PAIS: ESPA\u00d1A PAIS: ESPA\u00d1A","title":"Variables de entorno"},{"location":"jenkins/#cambio-url","text":"Podemos crear propias en manage jenkins- conf sistem y clicamos en la opcion de Jenkins Location: Cambiamos la url por la de dns (/etc/hosts): 192.168.1.44 host2 127.0.0.1 loopback.jenkins http://loopback.jenkins:9090/","title":"Cambio URL"},{"location":"jenkins/#cron","text":"Podemos ver una chuleta de crontab A la hora de construir un job hay que dar en la opci\u00f3n de Build triggers - execute periodically 5 * * * * cada 5 minutos Podemos poner una H en un * y quiere decir que coger\u00e1 cuando pueda de ese momento para que haya menos carga de jobs por si hay otras tareas tambi\u00e9n y no se sobrecargue.","title":"CRON"},{"location":"jenkins/#gatillar-jobs","text":"Quiere decir que lancemos un job sin necesidad sin entrar a jenkins y construir el job, sino desde un script desde la terminal. Vamos a Manage and Assign Roles - Manage Roles y creamos uno que se llame trigger-jobs. Creamos un usuario jenkins y le asignamos este rol. Va relacionado con la opci\u00f3n Crumb Issuer de seguridad global,ya viene por defecto. Instalamos un plugin para evitar error: Buscando en Internet he visto que el error se produce porque a partir de cierta versi\u00f3n de Jenkins (2.176.x) es necesario que ambas peticiones (para obtener el crumb y para lanzar el job) est\u00e9n dentro de la misma \"sesi\u00f3n web\" (ver https://jenkins.io/doc/upgrade-guide/2.176/#upgrading-to-jenkins-lts-2-176-3). Siguiendo la recomendaci\u00f3n en esa misma p\u00e1gina, instal\u00e9 el plugin \"Strict Crumb Issuer\" y lo configur\u00e9 para que no fuera necesario estar en la misma sesi\u00f3n web: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') # autenticamos el crumb a traves de variable pasada de crumb curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/7-ENV/build?delay=0sec Ahora con parametros: [isx46410800@miguel jenkins]$ cat crumb.sh # generamos el crum, el usuario que queremos, -s de silencioso el output y la url de jenkins crumb=$(curl -u \"jenkins:1234\" -s 'http://127.0.0.1:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') #con parametros curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://127.0.0.1:9090/job/6-db-playbook-ansible-nginx-php/buildWithParameters?EDAD=23 [isx46410800@miguel jenkins]$ bash crumb.sh nos sale el index-php solo con los de 22","title":"GATILLAR JOBS"},{"location":"jenkins/#mail","text":"","title":"MAIL"},{"location":"jenkins/#configurar-envio-de-notificaciones","text":"Plugin a instalar Email Extension Plugin Vamos a manage jenkins-conf sistem - E-mail Notification Vamos a Amazon - SES - Stmp settings y copiamos la direccion del mail email-smtp.eu-west-2.amazonaws.com Despues le damos a crear credenciales stmp de amazon y ponemos un usuario jenkins-user : [isx46410800@miguel jenkins]$ cat credentials.csv IAM User Name,Smtp Username,Smtp Password \"jenkins-user\",AKIA5RIFOUI3LWLFOOG7,BFW538mmwDzTr4eaMMAzSVlQA57NeH1/Hqvnn3ABJsZ6 Creamos un email de admin en amazon: Probamos el email: Test e-mail recipient \ufffc Test configuration: miguel14amoros@gmail.com Email was successfully sent","title":"Configurar envio de notificaciones"},{"location":"jenkins/#gmail-como-server-de-correo","text":"Ponemos nuestro gmail como direccion de correo y luego rellenamos la parte de correo:","title":"Gmail como server de correo"},{"location":"jenkins/#email-de-error","text":"Cogemos un build e indicamos en la opcion post-build nuestro correo para si falla, enviarnos email. Escribimos algo mal y recibimos el email. Lo ponemos correcto y recibimos email de que todo va bien. Si sigue yendo bien, no recibimos email.","title":"Email de error"},{"location":"jenkins/#maven","text":"","title":"MAVEN"},{"location":"jenkins/#instalacion_1","text":"Instalamos el plugin Maven Integration Ejemplo de git maven: maven sample app","title":"Instalacion"},{"location":"jenkins/#configuracion-de-un-job","text":"Configuracion del job: Los workspace son las mesas de trabajo donde se deja lo clonado de git y ahi tenemos toda la xixa para trabajar en jenkins. Configuracion e instalamos maven: A\u00f1adimos el paso de construir tarea de maven: lo que hace todo el proceso es descargar el codigo fuente de git, instalar la version de maven indicada y despues ejecuta el comando de -B -DskipTests clean package de maven que jenkins coja el codigo fuente y lo construya(package) un .jar de la app y se ejecuta en un workspaces donde jenkins crea un pom.xml que necesita maven. Despues a\u00f1adimos que despues de todo esto haga un test: A\u00f1adimos otra opci\u00f3n de desplegar el jar: + java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar","title":"Configuracion de un job"},{"location":"jenkins/#registrar-los-resultados","text":"A\u00f1adimos acci\u00f3n para ejecutar despues(post build) con la opcion de publicar los resultados de tests Junit(Publish JUnit test result report)--> target/surefire-reports Vemos que nos sale una grafica y una nueva pesta\u00f1a de test results:","title":"Registrar los resultados"},{"location":"jenkins/#archivar-los-jar","text":"A\u00f1adimos otra acci\u00f3n post build de archivar los *.jar y vemos que nos aparece una nueva pesta\u00f1a para descargar el archivo jar: Podemos a\u00f1adir la alerta de email si falla:","title":"Archivar los jar"},{"location":"jenkins/#git-server","text":"Creamos en el docker-compose un git-server siguiendo estas instruciones Cambiamos el puerto local del servicio web para que no se colpasen: web: container_name: web image: ansible-web build: context: jenkins-ansible/web ports: - \"8888:80\" networks: - net git: container_name: git-server hostname: gitlab.example.com ports: - \"443:443\" - \"80:80\" volumes: - \"/srv/gitlab/config:/etc/gitlab\" - \"/srv/gitlab/logs:/var/log/gitlab\" - \"/srv/gitlab/data:/var/opt/gitlab\" image: gitlab/gitlab-ce networks: - net Ponemos la url en /etc/hosts para asignar la ip al servicio mejor: 127.0.0.1 gitlab.example.com Entramos, nos registramos con root y 12345678 y creamos un grupo llamado jenkinsci . Despues creamos un proyecto, lo llamamos maven . Despues vamos a usuarios y creamos un usuario nuevo miguel con acceso regular. Luego editamos el usuario y le ponemos una contrase\u00f1a 12345678 . Luego vamos al proyecto creado de jenkinsci/maven y vamos a manage settings y a\u00f1adimos como usuario developer al user creado. NOTA: lo pondremos en modo mantainer, un nivel superior, para poder hacer el primer push al crear la rama master con git pusg -u origin master. Despues clonamos el repo de maven con el simple app maven y clonamos el nuevo repo vacio y copiamos los archivos de uno a otro, hacemos un push y ya tenemos todo el contenido.","title":"GIT SERVER"},{"location":"jenkins/#cambio-url-mavengitjenkins","text":"Vemos la url de mi repo git en: [isx46410800@miguel maven]$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [remote \"origin\"] url = http://gitlab.example.com/jenkinsci/maven.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \"master\"] remote = origin merge = refs/heads/master En jenkins vamos a credenciales y le damos al de la llave naranja y creamos las credenciales del git de dentro del docker: Una vez hecho esto, vamos a configurar el job que teniamos de maven y cambiamos el SCM por la url de nuestro git creado. Deberiamos poner la url de nuestro git http://gitlab.example.com/jenkinsci/maven.git pero como nuestro servicio especificado en docker-compose lo tenemos como git, ponemos http://git/jenkinsci/maven.git . Ponemos las credenciales de nuestro git y construimos el build viendo que lo descarga de nuestro gir y funciona. Vamos al container de git-server dentro donde se esconde el contenido del repo maven: root@gitlab:/var/opt/gitlab/git-data/repositories/@hashed/6b/86/6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b.git Creamos dentro el directorio mkdir custom_hooks y el file post-receive #!/bin/bash # Get branch name from ref head if ! [ -t 0 ]; then read -a ref fi IFS='/' read -ra REF <<< \"${ref[2]}\" branch=\"${REF[2]}\" # preguntamos por el nombre del branch(master) # si es master hacemos el gatillar con crumb if [ $branch == \"master\" ]; then crumb=$(curl -u \"jenkins:1234\" -s 'http://jenkins.local:9090/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\":\",//crumb)') curl -u \"jenkins:1234\" -H \"$crumb\" -X POST http://jenkins.local:9090/job/8-MavenJob/build?delay=0sec if [ $? -eq 0 ] ; then echo \"*** Ok\" else echo \"*** Error\" fi fi Con esto lo que queremos hacer es que cuando hagamos un push al repo git, como hay cambios, se haga automatico un job en el job de maven. Despues le damos chmod +x post-receive y chown git:git custom_hooks Hacemos un push y se deber\u00eda hacer automatico el build de maven job.","title":"CAMBIO URL MAVEN/GIT/JENKINS"},{"location":"jenkins/#job-dsl","text":"Instalamos el plugin Job DSL nos permite crear jobs codigo SEED JOB es el job padre que har\u00e1 ejecutar a los jobs hijos. Construimos un job y vamos a la opci\u00f3n build - process job DSLs Documentaci\u00f3n de job dsl","title":"JOB DSL"},{"location":"jenkins/#seed-job","text":"Ejemplo estructura: job('job_dsl_example') { }","title":"SEED JOB"},{"location":"jenkins/#descripcion","text":"Indicamos la descripcion del job hijo: job('job_dsl_example') { description('This is my awesome Job') } Con la descripcion te crea un job hijo que te dice la descripcion indicada","title":"DESCRIPCION"},{"location":"jenkins/#parametros","text":"Para poner parametros en el job: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } } Te crea el job fijo con una descripcion y tres variables parametrizadas.","title":"PAR\u00c1METROS"},{"location":"jenkins/#scm","text":"La administracion del codigo fuente: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } } Con SCM definimos la url y la branch del codigo fuente git en este caso.","title":"SCM"},{"location":"jenkins/#triggers","text":"Cron de tareas: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } } Definimos un trigger en este caso con un cron.","title":"TRIGGERS"},{"location":"jenkins/#steps","text":"Son los pasos que va hacer nuestro job, lo que se va ir ejecutando. job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") } } Paso de hacer un hello world","title":"STEPS"},{"location":"jenkins/#mailer","text":"Sirve para indicar el aviso de notificaciones por correo: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } Indicamos el aviso de notificaciones.","title":"MAILER"},{"location":"jenkins/#job-de-ansible-en-dsl","text":"En este ejemplo vamos a hacer el job n\u00famero de 6 de ansible con gnix php jenkins en JOBDSL: esto es lo que teniamos en el job6 de ansible. EJEMPLO JOBDSL, LO M\u00c1S UTILIZADO: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/etc/ansible/plays/i2b-cl/some_playbook.yml') { inventoryPath('/etc/ansible/plays/i2b-cl/hosts') tags('cool') forks(1) colorizedOutput(true) additionalParameters('--vault-password-file $HOME/pass-vault/i2b-cl.txt') extraVars { extraVar(\"whoami\", '${param1}', false) extraVar(\"my_pass\", 'some_pass', true) } } } publishers { mailer('me@example.com', true, true) } } AYUDA ANSIBLE DSL Creamos nuestro archivo jobdsl de ansible.js: job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } Nos da un error que ya nos daba en su momento y lo que tenemos que hacer es entrar al contenedor web y cambiar los permisos: chown remote_user:remote_user -R /var/www/html/","title":"JOB DE ANSIBLE EN DSL"},{"location":"jenkins/#job-de-maven-en-dsl","text":"Seguimos el job8 de maven pero ahora en DSL: job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"JOB DE MAVEN EN DSL"},{"location":"jenkins/#dsl-en-git","text":"Vamos a nuestro git-server http://gitlab.example.com:443 Creamos un nuevo proyecto dsl y lo clonamos y creamos un fichero copiando todo lo hecho en jobdsl padre: job('job_dsl_example') { description('This is my awesome Job') parameters { stringParam('Planet', defaultValue = 'world', description = 'This is the world') booleanParam('FLAG', true) choiceParam('OPTION', ['option 1 (default)', 'option 2', 'option 3']) } scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master') } triggers { cron('H 5 * * 7') } steps { shell(\"echo 'Hello World'\") shell(\"echo 'Hello World2'\") } publishers { mailer('me@example.com', true, true) } } job('ansible-dsl') { description('Este es un job de ansible con dsl') parameters { choiceParam('EDAD', ['20', '21', '22', '23', '24', '25']) } steps { wrappers { colorizeOutput(colorMap = 'xterm') } ansiblePlaybook('/var/jenkins_home/ansible/people.yml') { inventoryPath('/var/jenkins_home/ansible/hosts') colorizedOutput(true) extraVars { extraVar(\"EDAD\", '${EDAD}', false) } } } } job('maven_dsl') { description('Maven dsl project') scm { git('https://github.com/jenkins-docs/simple-java-maven-app', 'master', {node -> node / 'extensions' << '' }) } steps { maven { mavenInstallation('jenkins-maven') goals('-B -DskipTests clean package') } maven { mavenInstallation('jenkins-maven') goals('test') } shell(''' echo \"**************************\" echo \"Desplegando el jar\" echo \"**************************\" java -jar /var/jenkins_home/workspace/8-MavenJob/target/my-app-1.0-SNAPSHOT.jar ''') } publishers { archiveArtifacts('target/*.jar') archiveJunit('target/surefire-reports/*.xml') mailer('miguel14amoros@gmail.com', true, true) } }","title":"DSL en GIT"},{"location":"jenkins/#pipelines","text":"Flujo de trabajo por el que tiene que pasar nuestro c\u00f3digo para llegar a producci\u00f3n. Jenkins es, fundamentalmente, un motor de automatizaci\u00f3n que soporta un n\u00famero de patrones de automatizaci\u00f3n. Pipeline a\u00f1ade un poderoso conjunto de herramientas de automatizaci\u00f3n a Jenkins, soportando casos de uso que van desde la simple integraci\u00f3n continua hasta las tuber\u00edas completas de CD. Al modelar una serie de tareas relacionadas, los usuarios pueden aprovechar las muchas caracter\u00edsticas de Pipeline: C\u00f3digo: Pipeline se implementa en c\u00f3digo y normalmente se comprueba en el control de la fuente, dando a los equipos la capacidad de editar, revisar e iterar en su tuber\u00eda de entrega. Duradero: Los oleoductos pueden sobrevivir tanto a los reinicios planificados como a los no planificados del maestro Jenkins. Pausable: Los oleoductos pueden opcionalmente detenerse y esperar la entrada o aprobaci\u00f3n humana antes de continuar el recorrido del oleoducto. Vers\u00e1til: Los oleoductos soportan complejos requisitos de CD del mundo real, incluyendo la capacidad de bifurcarse/unirse, hacer bucles y realizar trabajos en paralelo. Extensible: El plugin Pipeline soporta extensiones personalizadas para su nota al pie de p\u00e1gina DSL:dsl:[] y m\u00faltiples opciones para la integraci\u00f3n con otros plugins. Mientras que Jenkins siempre ha permitido formas rudimentarias de encadenar Trabajos de Estilo Libre para realizar tareas secuenciales, [4] Pipeline hace de este concepto un ciudadano de primera clase en Jenkins. Construido sobre el valor central de Jenkins de la extensibilidad, Pipeline es tambi\u00e9n extensible tanto por los usuarios con las Bibliotecas Compartidas de Pipeline como por los desarrolladores de plugins. [5] El siguiente diagrama de flujo es un ejemplo de un escenario de CD f\u00e1cilmente modelado en la tuber\u00eda de Jenkins: Plugin Pipeline","title":"PIPELINES"},{"location":"jenkins/#jenkinsfile","text":"Estructura: pipeline { agent any stages { stage('Build') { steps { echo 'Building..' } } stage('Test') { steps { echo 'Testing..' } } stage('Deploy') { steps { echo 'Deploying....' } } } } AGENT: es quien ejecuta el pipeline. ANY quiere decir que cualquiera que est\u00e9 libre lo ejecute, sino, hay que especificar el agente.","title":"JENKINSFILE"},{"location":"jenkins/#multiple-steps","text":"pipeline { agent any stages { stage('Build') { steps { sh 'echo \"Este es mi primer pipeline\"' sh ''' echo \"Por cierto, puedo ejecutar m\u00e1s acciones aqu\u00ed\" ls -lah ''' } } } }","title":"MULTIPLE-STEPS"},{"location":"jenkins/#post-actions","text":"pipeline { agent any stages { stage('Test') { steps { sh 'echo \"Fail!\"; exit 1' } } } post { always { echo 'Siempre me voy a ejecutar :D' } success { echo 'Solo me ejecutar\u00e9 si el build no falla' } failure { echo 'Solo me ejecutar\u00e9 si el build falla' } unstable { echo 'Solo me ejecutar\u00e9 si me marco como inestable' } changed { echo 'El pipeline estaba fallando pero ahora est\u00e1 correcto o visceversa' } } }","title":"POST-ACTIONS"},{"location":"jenkins/#retry","text":"pipeline { agent any stages { stage('Timeout') { steps { retry(3) { sh 'No voy a funcionar :c' } } } } }","title":"RETRY"},{"location":"jenkins/#timeout","text":"pipeline { agent any stages { stage('Deploy') { steps { retry(3) { sh 'echo hola' } timeout(time: 3, unit: 'SECONDS') { sh 'sleep 5' } } } } } ######### pipeline { agent any stages { stage('Deploy') { steps { timeout(time: 2, unit: 'SECONDS') { retry(5) { sh 'sleep 3' } } } } } }","title":"TIMEOUT"},{"location":"jenkins/#variables-env","text":"pipeline { agent any environment { NOMBRE = 'ricardo' APELLIDO = 'gonzalez' } stages { stage('Build') { steps { sh 'echo $NOMBRE $APELLIDO' } } } }","title":"VARIABLES ENV"},{"location":"jenkins/#credenciales_1","text":"pipeline { agent any environment { secretito = credentials('TEST') } stages { stage('Example stage 1') { steps { sh 'echo $secretito' } } } }","title":"CREDENCIALES"},{"location":"jenkins/#cicd","text":"","title":"CI/CD"},{"location":"jenkins/#build","text":"Instalamos Docker dentro de un container Jenkins con el dockerfile de la carpeta pipelines y modificamos el Jenkins del docker-compose para poner el de la imagen creada por el dockerfile: version: '3' services: jenkins: container_name: jenkins image: jenkins/docker build: context: pipelines ports: - \"9090:8080\" volumes: - $PWD/jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock networks: - net Cambiamos permisos para tener docker dentro con usuario jenkins: [isx46410800@miguel jenkins]$ docker exec -it -u root jenkins /bin/bash chown jenkins /var/run/docker.sock Copiamos la carpeta de maven dentro de la carpeta pipelines: [isx46410800@miguel jenkins]$ cp -r maven/ pipelines/java-app Iniciamos un container: docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package lo que hacemos es crear un contenedor con los volumes donde va el contenido de maven, volcamos el contenido de javaapp a app, -w para indicar el directorio activo, la version de maven, el comando hacer para generar un jar y --rm para que se elimine. Tendremos el jar construido en nuestro java-app/target/*.jar Creamos script automatizado: #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine \"$@\" Ejecutamos: ./jenkins/build/mvn.sh mvn -B -DskipTests clean package Creamos un dockerfile con solo java y el jar creado en /jenkins/build/. Lo ejecutamos: [isx46410800@miguel build]$ docker build -f Dockerfile-java -t test . Comprobamos lo creado: [isx46410800@miguel build]$ docker run --rm -it test sh / # ls /app app.jar / # Creamos un docker-compose para automatizar esta creacion de la imagen: version: '3' services: app: image: \"app:$BUILD_TAG\" build: context: . dockerfile: Dockerfile-java Comprobamos: [isx46410800@miguel build]$ export BUILD_TAG=12 [isx46410800@miguel build]$ docker-compose -f docker-compose-build.yml build Crear un script para automatizar la creaci\u00f3n del docker-compose de la imagen: #!/bin/bash # Copia el jar cp -f java-app/target/*.jar jenkins/build/ echo \"######################\" echo \"*** Building image ***\" echo \"######################\" cd jenkins/build/ && docker-compose -f docker-compose-build.yml build --no-cache Lo comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/build/build.sh ###################### *** Building image *** ###################### Building app Step 1/4 : FROM openjdk:8-jre-alpine ---> f7a292bbb70c Step 2/4 : RUN mkdir /app ---> Running in 3997da6947f6 Removing intermediate container 3997da6947f6 ---> f5f751fbe6ab Step 3/4 : COPY *.jar /app/app.jar ---> 9dc51ae21e48 Step 4/4 : CMD java -jar /app/app.jar ---> Running in dd03ae766c0e Removing intermediate container dd03ae766c0e ---> 48409229a4e8 Successfully built 48409229a4e8 Successfully tagged app:13 Lo agregamos al Jenkinsfile: pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh 'echo test' } } stage('Push') { steps { sh 'echo push' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"BUILD"},{"location":"jenkins/#test","text":"Para hacer el test de maven de la aplicaci\u00f3n se utiliza el mvn test : [isx46410800@miguel build]$ docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn test Vemos los test en java-app/target/surefire-reports: [isx46410800@miguel pipelines]$ ll java-app/target/surefire-reports/ total 12 -rw-r--r--. 1 root root 270 Sep 30 02:45 com.mycompany.app.AppTest.txt -rw-r--r--. 1 root root 4764 Sep 30 02:45 TEST-com.mycompany.app.AppTest.xml Ahora queremos automatizar los tests con un script: [isx46410800@miguel pipelines]$ mkdir jenkins/test [isx46410800@miguel pipelines]$ vi jenkins/test/test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" docker run --rm -v /root/.m2:/root/.m2 -v /home/ricardo/jenkins/jenkins_home/workspace/pipeline-docker-maven/java-app:/app -w /app maven:3-alpine \"$@\" [isx46410800@miguel pipelines]$ chmod +x jenkins/test/test.sh Comprobamos: [isx46410800@miguel pipelines]$ bash jenkins/test/test.sh mvn test ################ *** Testing *** ################ Agregamos el test al Jenkinsfile: stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } }","title":"TEST"},{"location":"jenkins/#push-a-maquina-remota-aws","text":"Nos creamos una maquina virtual o maquina en amazon: [isx46410800@miguel .ssh]$ ssh -i mykeypair.pem fedora@18.133.221.84 Tenemos que tener unas llaves ssh creadas en la maquina remota para poder conectarnos sin contrase\u00f1a: [fedora@ip-172-31-28-138 ~]$ ssh-keygen -f ssh-aws-jenkins Creamos un DOCKER REGISTRY : [fedora@ip-172-31-28-138 .ssh]$ docker run -d -p 5000:5000 --name registry registry:2 Ayuda Vemos que est\u00e1: [fedora@ip-172-31-28-138 .ssh]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ebffab5d6d6 registry:2 \"/entrypoint.sh /etc\u2026\" 50 seconds ago Up 49 seconds 0.0.0.0:5000->5000/tcp registry En contenido est\u00e1 en /var/lib/registry Creamos un directorio para meter las cosas en este volumen de registros: [fedora@ip-172-31-28-138 ~]$ mkdir tmp_registry [fedora@ip-172-31-28-138 ~]$ docker run -d -p 5000:5000 --name registry -v $PWD/tmp_registry:/var/lib/registry registry:2 Estamos en el AWS en nuestra maquina remota, por lo tanto estamos en local, localhost y queremos ver como bajamos un container y lo subimos a nuestro docker de registros creado anteriormente: [fedora@ip-172-31-28-138 ~]$ docker pull hello-world [fedora@ip-172-31-28-138 ~]$ docker tag hello-world localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ docker push localhost:5000/hello-world [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/ total 4 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 docker [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/hello-world/ total 12 drwxr-xr-x. 3 root root 4096 Oct 1 18:38 _layers drwxr-xr-x. 4 root root 4096 Oct 1 18:38 _manifests drwxr-xr-x. 2 root root 4096 Oct 1 18:38 _uploads Como pusimos que el contenido que vaya al contenedor de registros se guarde en nuestra carpeta creada de tmp_registry, vemos ahi la xixa nueva. Ahora queremos que desde la maquina de casa se pueda subir cosas a este contenedor de registros de AWS: [isx46410800@miguel pipelines]$ sudo vim /lib/systemd/system/docker.service # A\u00f1adimos lo siguiente en la linea de EXECSTART de SERVICE(ip/puerto de aws) --insecure-registry 18.133.221.84:5000 # a\u00f1adimos el puerto 5000 en el security group de la maquina para poder verse amazon y mi maquina por ese puerto # comprobamos la conexion desde mi maquina a AWS con telnet [isx46410800@miguel pipelines]$ telnet 18.133.221.84 5000 [isx46410800@miguel pipelines]$ sudo systemctl daemon-reload [isx46410800@miguel pipelines]$ sudo systemctl restart docker Probamos ahora subirlo desde casa al docker registry de AWS: [isx46410800@miguel pipelines]$ docker pull hello-world [isx46410800@miguel pipelines]$ docker tag hello-world:latest 18.133.221.84:5000/hello-world-casa [isx46410800@miguel pipelines]$ docker push 18.133.221.84:5000/hello-world-casa [fedora@ip-172-31-28-138 ~]$ ll tmp_registry/docker/registry/v2/repositories/ total 8 drwxr-xr-x. 5 root root 4096 Oct 1 18:38 hello-world drwxr-xr-x. 5 root root 4096 Oct 1 18:56 hello-world-casa","title":"PUSH A MAQUINA REMOTA AWS"},{"location":"jenkins/#certificado-ssl-registry-con-autenticacion","text":"Creamos unos directorios tmp-jenkins/certs Creamos el fichero nginx.conf : server { listen 80; # reemplaza segun tus registros DNS server_name ec2-18-133-221-84.eu-west-2.compute.amazonaws.com; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /mnt; } } Arrancamos el contenedor: [fedora@ip-172-31-28-138 certs]$ docker run --rm -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf -v $PWD/letsencrypt:/etc/letsencrypt -p 80:80 -it nginx:alpine sh Instalamos certbot dentro del container que sirve para crear certificados SSL gratuidos durante 3 meses: / # nginx / # apk add --update certbot # certbot certonly --email miguel14amoros@gmail.com --agree-tos --non-interactive --webroot -w \"/mnt\" - d 18.133.221.84 PUSH de imagen con scrip a nuestro registry de amazon o dockerhub. Creamos un directorio en pipelines/jenkins/push: #!/bin/bash echo \"########################\" echo \"*** Preparing to push ***\" echo \"########################\" REGISTRY=\"isx46410800\" // \"18.133.211.84:5000\" IMAGE=\"app\" echo \"*** Logging in ***\" docker login echo \"*** Tagging image ***\" docker tag $IMAGE:$BUILD_TAG $REGISTRY/$IMAGE:$BUILD_TAG echo \"*** Pushing image ***\" docker push $REGISTRY/$IMAGE:$BUILD_TAG Tenemos ya bajada una imagen llamada APP y un export BUILD_TAG=13 Probamos primero y lo agregamos al Jenkinsfile: [isx46410800@miguel pipelines]$ bash jenkins/push/push.sh pipeline { agent any stages { stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } } stage('Push') { steps { sh './jenkins/push/push.sh' } } stage('Deploy') { steps { sh 'echo deploy' } } } }","title":"CERTIFICADO SSL REGISTRY CON AUTENTICACION"},{"location":"jenkins/#deploy","text":"En deploy/deploy.sh #!/bin/bash # Transferimos variables echo app > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Lo copiamos a nuestra AWS: scp -i mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth Creamos en AWS un docker-compose: version: '3' services: app: image: \"$REGISTRY/$IMAGE:$TAG\" container_name: app Exportamos las variables: [fedora@ip-172-31-28-138 jenkins]$ export REGISTRY=\"isx46410800\" [fedora@ip-172-31-28-138 jenkins]$ export IMAGE=$(sed -n '1p' /tmp/.auth) [fedora@ip-172-31-28-138 jenkins]$ export TAG=$(sed -n '2p' /tmp/.auth) Comprobamos que descarga la imagen: [fedora@ip-172-31-28-138 jenkins]$ docker-compose up -d Creamos otro fichero publish para pasar las cosas a la remota: [isx46410800@miguel jenkins]$ cat deploy/publish.sh #!/bin/bash export REGISTRY=\"isx46410800\" export IMAGE=$(sed -n '1p' /tmp/.auth) export TAG=$(sed -n '2p' /tmp/.auth) docker login cd ~/jenkins && docker-compose up -d A\u00f1adimos en deploy/deploy.sh: # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh [isx46410800@miguel pipelines]$ bash jenkins/deploy/deploy.sh En AWS ejecutamos el /tmp/publish.sh y se arranca el docker-compose creado en ~/jenkins. Ahora hacemos que se ejecute directamente todo esto desde el deploy.sh en la maquina remota: #!/bin/bash # Transferimos variables echo \"app\" > /tmp/.auth echo $BUILD_TAG >> /tmp/.auth # Copiamos el fichero a AWS scp -i ~/.ssh/mykeypair.pem /tmp/.auth fedora@18.133.221.84:/tmp/.auth scp -i ~/.ssh/mykeypair.pem ./jenkins/deploy/publish.sh fedora@18.133.221.84:/tmp/publish.sh ssh -i ~/.ssh/mykeypair.pem fedora@18.133.221.84 /tmp/publish.sh A\u00f1adimos al Jenkinsfile la parte del deploy: stage('Deploy') { steps { sh './jenkins/deploy/deploy.sh' } }","title":"DEPLOY"},{"location":"jenkins/#cicd_1","text":"Creamos un proyecto de pipeline-maven en nuestro git-server y seguimos los pasos que nos indica el repositorio vacio para poder meter todo el contenido de pipelines en nuestro git. [isx46410800@miguel pipelines]$ git init Initialized empty Git repository in /home/isx46410800/Documents/jenkins/pipelines/.git/ [isx46410800@miguel pipelines]$ git remote add origin http://gitlab.example.com/jenkinsci/pipeline-maven.git [isx46410800@miguel pipelines]$ rm -rf java-app/.git/ [isx46410800@miguel pipelines]$ git add Jenkinsfile java-app/ jenkins/ [isx46410800@miguel pipelines]$ git commit -m \"contenido jenkins ci/cd pipeline\"; git push -u origin master Cambiamos la ruta del deploy.sh por /opt y lo copiamos al container de jenkins para que use la llave ssh: [isx46410800@miguel pipelines]$ docker cp jenkins/deploy/deploy.sh jenkins:/opt/. jenkins@ee5ab67daa7d:/$ chmod +x /opt/deploy.sh Creamos un proyecto de tipo pipeline pipeline-docker-maven Configuramos el pipeline con SCM de git: Modificamos de los ficheros test.sh y deploy.sh la ruta absoluta: test.sh #!/bin/bash echo \"################\" echo \"*** Testing ***\" echo \"################\" PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" +++++++++++++++++++++ mvn.sh #!/bin/bash echo \"*************\" echo \"Construyendo jar de mi app java\" echo \"*************\" # Con esto construiriamos el container pero no deja la orden directa: #docker run --rm -v /root/.m2:/root/.m2 -v $PWD/java-app:/app -w /app maven:3-alpine mvn -B -Dskiptests clean package # Para luego pasarle como argumento la orden PROJECT=\"/home/isx46410800/Documents/jenkins/jenkins_home/workspace/pipeline-docker-maven\" docker run --rm -v /root/.m2:/root/.m2 -v $PROJECT/java-app:/app -w /app maven:3-alpine \"$@\" Despues entramos al container jenkins para hacer la conexion ssh manual para que no nos pida lo de autenticar conexion en los cripts: ssh -i /opt/mykeypair.pem fedora@18.133.221.84 Hemos copiado mi llave ssh de amazon a opt dentro de jenkins y la ruta de la llave del deploy.sh tambien. [isx46410800@miguel .ssh]$ docker cp mykeypair.pem jenkins:/opt/. A\u00f1adimos unos post-actions al Jenkisfile para nos de siempre un test de resultados y tambien por si va bien el build de maven guarde el jar: stage('Build') { steps { sh ''' ./jenkins/build/mvn.sh mvn -B -DskipTests clean package ./jenkins/build/build.sh ''' } post { success { archiveArtifacts artifacts 'java-app/target/*.jar', fingerprint: true } } } stage('Test') { steps { sh './jenkins/test/test.sh mvn test' } post { always { junit 'java-app/target/surefire-reports/*.xml' } } } Resultados finales: bajamos el codigo fuente de la app maven, la compilamos, subimos la imagen a dockerhub y mandamos los archivos a AWS para hacer el deploy alli.","title":"CI/CD"},{"location":"kubernetes/","text":"KUBERNETES K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal ARQUITECTURA MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc. INSTALACI\u00d3N MINIKUBE/KUBECTL MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes PODS VS CONTENEDORES Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores. PODS CREAR POD Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso. LOGS PODS Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso. API-RESOURCES Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources ELIMINAR PODS Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all OBTENER YAML POD Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces. IP POD Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80 ENTRAR AL POD Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh CREAR POD YAML Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine 2+ CONTAINER POR POD Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2 LABELS Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas. PROBLEMAS PODs Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar. REPLICASETS Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet. CREAR REPLICASET Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s ELIMINAR/MODIFICAR En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s LOGS Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test OWNER REFERNCE Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 ADOPCI\u00d3N DE PODS PLANOS Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena. PROBLEMAS Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones. DEPLOYMENTS Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets CREAR DEPLOYMENT Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset ROLLING UPDATE Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge HISTORIAL DE DEPLOYMENTS Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> ROLL BACKS Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0 SERVICIOS Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio. CREAR SERVICIO Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80. INFO SERVICIO Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint. ENDPOINTS Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none> DNS Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888 SERVICIO CLUSTER-IP IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) SERVICIO NODE-PORT IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port. SERVICIO LOAD BALANCER Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP. GOLANG Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend. CREAR API REST GO DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png] CAMBIOS MENSAJE RESPUESTA MENSAJE 1 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png] MENSAJE 2 A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png] DOCKERFILE GOLANG Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos. DEPLOYMENT GOLANG Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png] CONSUMO DEL SERVICIO Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ # FRONTED Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html MANIFIESTO FRONTED Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy NAMESPACES Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName CREAR NAMESPACE Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none> ASIGNAR NAMESPACES Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s BORRAR NAMESPACES Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns DEPLOY NAMESPACES Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m DNS NAMESPACES Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on CONTEXTOS NAMESPACES Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d LIMITAR RAM/CPU La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus. LIMITS/REQUEST Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique. RAM Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory. CPU Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu. QOS(Quality of Service) Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request. LIMITRANGE Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods VALORES POR DEFECTO Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi - VALORES POD Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M. LIMITES Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram RESOURCE QUOTA Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod. CREAR RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource. DEPLOY RQ Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA. LIMITAR N\u00ba PODS EN NS Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota. PROBES Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP TIPOS PROBES Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse. CREAR LIVENESS PROBE Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness LIVENESS TCP Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName LIVENESS HTTP Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName READINESS PROBE Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod. VARIABLES Y CONFIGMAP CREAR VARIABLES Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1 VARIABLES REFERENCIADAS Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc. CONFIGMAP Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> MONTANDO VOLUMEN CONFIGMAP Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo. VOLUMEN-ENV CONFIGMAP Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user SECRETS Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen. CREAR Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode MANIFIESTOS Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test ENVSUBTS Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado. VOLUME SECRETS Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ # ENV SECRETS Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456 VOLUMES Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). EMPTYDIR Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / # HOSTPATH-PV En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none> PVC El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC. PVC-PV Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector. PVC-PODS De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql> CLOUD VOLUMES Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC. RECLAIM POLICY Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data). USERS/GROUPS RBAC RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster. ROLES vs CLUSTERROLES En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo. ROLEBINDING vs CLUSTERROLEBINDING Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1. CREAR USERS & GROUPS Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\" HABILITAR RBAC Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC SIMPLIFICAMOS CONTEXTO Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel CREAR ROLES Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user. ENLAZAR ROLE & USER Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\" CONFIG MAPS Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again. CREAR CLUSTEROLE Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m CREAR USER ADMIN Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m ROLES A GRUPOS Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m SERVICES ACCOUNT Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr SECRET SA Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc CREAR SA Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s RELACION POD-SA Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ REQUESTS A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones. REQUEST JWT Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante. SA DEPLOYMENT Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa ROLE SA Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1, INGRESS Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada. INGRESS CONTROLLER Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud. CREAR INGRESS CONTROLLER Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s IP INGRESS CONTROLLER Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602 APP INGRESS-CONTROLLER Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv EXPONER EL PUERTO AL EXTERIOR Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 2 APPS EN IC Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: AWS KUBERNETES Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: 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 Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0 CREAR CLUSTER AWS EKSCTL","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"K8S Es una herramienta extensible y de c\u00f3digo abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuraci\u00f3n declarativa como la automatizaci\u00f3n. Tiene un ecosistema grande y de r\u00e1pido crecimiento. Los servicios, el soporte y las herramientas est\u00e1n ampliamente disponibles. Funciones: Service discovery: mira cuantos nodos hay, los escanea para saber de ellos. Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma. Optimizaci\u00f3n de recursos en nodos: mira donde colocar el contenedor al host con menos carga. Self-healing: crea automaticamente un contenedor cuando uno muere. Configuraci\u00f3n de secretos Escalamiento horizontal","title":"KUBERNETES"},{"location":"kubernetes/#arquitectura","text":"MASTER/NODE : Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las m\u00e1quinas, host, m\u00e1quinas virutal. El master es como la aduana y los nodes son los barcos que se llevan los contenedores de la duana. API SERVER : Aplication Program Interface, significa que yo me puedo comunicar con un servicio a trav\u00e9s de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programaci\u00f3n. Ambos son en JSON, por lo que acaba procesando todo en c\u00f3digo JSON. KUBE-SCHEDULE : es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y \u00e9ste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor. KUBE-CONTROLLER : dentro tiene el node controler (se encarga de ver nodos, si se cae uno, levanta otra m\u00e1quina), el replication (encargado de mantener todas las r\u00e9plicas especificadas), el end point controller (se encarga de la red y pods) y tenemos el service account y tokens controller (para la autenticaci\u00f3n). ETCD : es la base de datos de kubernetes donde est\u00e1n todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versi\u00f3n nueva y queremos volver atr\u00e1s, en el etcd est\u00e1 guardado el estado y configuraci\u00f3n anterior. KUBELET : se encuentra en cada nodo y tienen dos funciones, en enviar y recibir informaci\u00f3n al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo. KUBE-PROXY : se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods. CONTAINER-RUNTIME : el software de contenedores que tiene instalado el nodo: docker,etc.","title":"ARQUITECTURA"},{"location":"kubernetes/#instalacion-minikubekubectl","text":"MINIKUBE : crea o simula un cluster peque\u00f1o que nos permite hacerlo en local. Documentaci\u00f3n Kubernetes Ejecutamos esta orden y sino sale vac\u00edo , vamos bien: grep -E --color 'vmx|svm' /proc/cpuinfo Instalamos kubectl , la intermediario para hablar con kubernetes: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x ./kubectl sudo mv ./kubectl /usr/bin/kubectl kubectl version --client Para usar minikube se necesita un Hypervisor (o monitor de m\u00e1quina virtual (virtual machine monitor)1\u200b es una plataforma que permite aplicar diversas t\u00e9cnicas de control de virtualizaci\u00f3n para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora): KVM VirtualBox Docker Descargamos minikube : curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube sudo mv minikube /usr/bin/ minikube status [isx46410800@miguel curso_kubernetes]$ minikube status \ud83e\udd37 There is no local cluster named \"minikube\" \ud83d\udc49 To fix this, run: \"minikube start\" [isx46410800@miguel curso_kubernetes]$ minikube start \ud83d\ude04 minikube v1.13.1 on Fedora 27 \u2728 Automatically selected the docker driver \ud83d\udc4d Starting control plane node minikube in cluster minikube \ud83d\ude9c Pulling base image ... \ud83d\udcbe Downloading Kubernetes v1.19.2 preload ... > preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB \ud83d\udd25 Creating docker container (CPUs=2, Memory=2200MB) ... \ud83e\uddef Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity) \ud83d\udca1 Suggestion: Try at least one of the following to free up space on the device: 1. Run \"docker system prune\" to remove unused docker data 2. Increase the amount of memory allocated to Docker for Desktop via Docker icon > Preferences > Resources > Disk Image Size 3. Run \"minikube ssh -- docker system prune\" if using the docker container runtime \ud83c\udf7f Related issue: https://github.com/kubernetes/minikube/issues/9024 \ud83d\udc33 Preparing Kubernetes v1.19.2 on Docker 19.03.8 ... \ud83d\udd0e Verifying Kubernetes components... \ud83c\udf1f Enabled addons: default-storageclass, storage-provisioner \ud83c\udfc4 Done! kubectl is now configured to use \"minikube\" by default Comprobamos de nuevo que s\u00ed funciona minikube status : [isx46410800@miguel curso_kubernetes]$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured COMANDOS B\u00c1SICOS MINIKUBE : minikube status minikube stop/start/delete Repositorio curso Kubernetes","title":"INSTALACI\u00d3N MINIKUBE/KUBECTL"},{"location":"kubernetes/#pods-vs-contenedores","text":"Los contenedores se ejecutan de manera aislada en un namespace: IPC (Inter Process Communication) Cgroup Network Mount PID User UTS (Unix Timesharing System) Los PODS sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como: De red(verse en la misma red) IPC(verse los procesos) UTS Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.","title":"PODS VS CONTENEDORES"},{"location":"kubernetes/#pods","text":"","title":"PODS"},{"location":"kubernetes/#crear-pod","text":"Primero tenemos que tener encendido el simulador: minikube start Documentaci\u00f3n : versi\u00f3n v1.19 la \u00faltima Creamos un pod de prueba kubectl run nombrePod --image:xxx:tag : [isx46410800@miguel curso_kubernetes]$ pod/pod-test created Vemos que lo hemos creado y est\u00e1 corriendo: [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 22s Normalmente hay un contenedor por pod, se suele asimilar a eso.","title":"CREAR POD"},{"location":"kubernetes/#logs-pods","text":"Un pod es la unidad m\u00e1s peque\u00f1a para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre. Creamos uno pod mal aposta para ver el error: [isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll pod/pod-test2 created [isx46410800@miguel curso_kubernetes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 5m19s pod-test2 0/1 ErrImagePull 0 14s Para ver los \u00b4logs\u00b4 usamos kubectl describe pod nombrePod : kubectl describe pod pod-test En el apartado events nos describe los logs paso a paso.","title":"LOGS PODS"},{"location":"kubernetes/#api-resources","text":"Para ver todos los recursos que hay y los shortnames de comandos se usa: kubectl api-resources","title":"API-RESOURCES"},{"location":"kubernetes/#eliminar-pods","text":"Para eliminar pods usamos kubectl delete pod podName ... : kubectl delete pod pod-test2 Todos: kubectl delete pod --all","title":"ELIMINAR PODS"},{"location":"kubernetes/#obtener-yaml-pod","text":"Podemos obtener info solo del pod concreto: kubectl get pod pod-test Para m\u00e1s info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request: kubectl get pod pod-test -o yaml Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a trav\u00e9s de un fichero ser\u00e1 mejor que no ir poniendo una orden 50 veces.","title":"OBTENER YAML POD"},{"location":"kubernetes/#ip-pod","text":"Para poder ver la IP del POD podemos usar cualquiera de estos comandos: kubectl describe pod pod-test kubectl get pod pod-test -o yaml En este caso es 172.18.0.3 Para verlo ingresamos directamente al navegador la ip. Si no funciona tenemos que mapear el puerto: kubectl port-forward pod-test 7000:80 Comprobamos la respuesta: curl 172.18.0.3:80","title":"IP POD"},{"location":"kubernetes/#entrar-al-pod","text":"Para ingresar a la consola del POD: kubectl exec -it pod-test -- sh Cuando solo hay un contenedor, no se especifica el nombre del pod. Cuando hay m\u00e1s contenedores c, --container='' : kubectl exec -it pod-test -c containerName -- sh","title":"ENTRAR AL POD"},{"location":"kubernetes/#crear-pod-yaml","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine Las api versions las podemos ver en: kubectl api-versions Los kind los podemos ver en: kubectl api-resources Para crearlo a trav\u00e9s del fichero YAML: kubectl apply -f pod.yaml [isx46410800@miguel pods]$ kubectl apply -f pod.yaml pod/pod-test2 created [isx46410800@miguel pods]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 6h54m pod-test2 1/1 Running 0 7s Para borrarlo: kubectl delete -f pod.yaml Para crear dos o m\u00e1s PODS, se pone --- de separaci\u00f3n: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 spec: containers: - name: container2 image: nginx:alpine","title":"CREAR POD YAML"},{"location":"kubernetes/#2-container-por-pod","text":"Para crear dos o m\u00e1s containers en un POD se a\u00f1ade en la subsecci\u00f3n containers: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8082'] Nos dar\u00e1 error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente. Vemos los logs en kubectl logs podName -c container : 263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2 Traceback (most recent call last): ... File \"/usr/local/lib/python3.6/socketserver.py\", line 470, in server_bind self.socket.bind(self.server_address) OSError: [Errno 98] Address in use Arreglamos el fallo del puerto y comprobamos cada container del POD: [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh / # cat index.html cont1 / # exit [isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh / # cat index.html cont2","title":"2+ CONTAINER POR POD"},{"location":"kubernetes/#labels","text":"Los labels son etiquetas que se ponen debajo de los metadata : apiVersion: v1 kind: Pod metadata: name: pod-test2 labels: app: front-end env: dev spec: containers: - name: container1 image: nginx:alpine --- apiVersion: v1 kind: Pod metadata: name: pod-test3 labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Los labels nos sirve para poder filtrar PODs con kubectl get pods -l nombre=valor : [isx46410800@miguel pods]$ kubectl get pods -l app=back-end NAME READY STATUS RESTARTS AGE pod-test3 1/1 Running 0 62s [isx46410800@miguel pods]$ kubectl get pods -l env=dev NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 78s pod-test3 1/1 Running 0 78s Los LABELS m\u00e1s usado es el de APP. Muy importantes para administrar replicas.","title":"LABELS"},{"location":"kubernetes/#problemas-pods","text":"Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga. Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga. Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion image y al hacer el apply puede ser que te deje actualizar.","title":"PROBLEMAS PODs"},{"location":"kubernetes/#replicasets","text":"Es un objeto separado del POD a un nivel m\u00e1s alto(el replicaset crea PODs y es su due\u00f1o). Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar. En la metadata del POD mete el OWNER REFERENCE para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.","title":"REPLICASETS"},{"location":"kubernetes/#crear-replicaset","text":"Vemos a donde pertenece la api-version y el kind de los replicasets en: kubectl api-resources Ejemplo: apiVersion: apps/v1 kind: ReplicaSet metadata: name: rs-test labels: app: rs-test spec: # modify replicas according to your case replicas: 5 selector: matchLabels: app: pod-label # pertenece a los PODs que vas a crear template: metadata: labels: app: pod-label spec: containers: - name: container1 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont1\" > index.html && python -m http.server 8082'] - name: container2 image: python:3.6-alpine command: ['sh', '-c', 'echo \"cont2\" > index.html && python -m http.server 8083'] Lo creamos: kubectl apply -f replica-set.yaml Lo que creamos son 5 PODs con label(pod-label, sino est\u00e1 lo crea) y dentro de cada POD creamos dos containers con label(pod-label) Comprobamos: [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 105s rs-test-9jpjg 2/2 Running 0 105s rs-test-fbwjb 2/2 Running 0 105s rs-test-hz2kx 2/2 Running 0 105s rs-test-s6cxx 2/2 Running 0 105s [isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 119s rs-test-9jpjg 2/2 Running 0 119s rs-test-fbwjb 2/2 Running 0 119s rs-test-hz2kx 2/2 Running 0 119s rs-test-s6cxx 2/2 Running 0 119s Ver los replicasets con kubectl get rs : [isx46410800@miguel replicaset]$ kubectl get rs NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m53s [isx46410800@miguel replicaset]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE rs-test 5 5 5 3m56s","title":"CREAR REPLICASET"},{"location":"kubernetes/#eliminarmodificar","text":"En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados: [isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx pod \"rs-test-s6cxx\" deleted [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 5m43s rs-test-9jpjg 2/2 Running 0 5m43s rs-test-b9lf4 2/2 Running 0 43s rs-test-fbwjb 2/2 Running 0 5m43s rs-test-hz2kx 2/2 Running 0 5m43s Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos: [isx46410800@miguel replicaset]$ vim replica-set.yaml [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test configured [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 8m29s rs-test-9jpjg 2/2 Terminating 0 8m29s rs-test-b9lf4 2/2 Terminating 0 3m29s rs-test-fbwjb 2/2 Running 0 8m29s rs-test-hz2kx 2/2 Terminating 0 8m29s [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE rs-test-5tsfh 2/2 Running 0 9m26s rs-test-fbwjb 2/2 Running 0 9m26s","title":"ELIMINAR/MODIFICAR"},{"location":"kubernetes/#logs","text":"Por describe: kubectl get rs rs-test -o yaml Por manifiesto YAML: kubectl describe rs rs-test","title":"LOGS"},{"location":"kubernetes/#owner-refernce","text":"Lo vemos en la metadata de un pod creado por ReplicaSet kubectl get pod podName -o yaml : kubectl get pod rs-test-5tsfh -o yaml name: rs-test-5tsfh namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0 Comprobamos que el UID anterior coincide con el replicaset creado kubectl get rs rsName -o yaml : kubectl get rs rs-test -o yaml name: rs-test namespace: default resourceVersion: \"22732\" selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0","title":"OWNER REFERNCE"},{"location":"kubernetes/#adopcion-de-pods-planos","text":"Vamos a crear primero dos PODs manualmente: [isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine pod/pod-test created [isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine pod/pod-test2 created Les creamos un LABEL a cada uno con kubectl label pods podName label=valor : [isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label pod/pod-test labeled [isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label pod/pod-test2 labeled Tendran el nuevo label pero no tendr\u00e1n ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET. Ahora mediante replicaset cremos 3 replicas con mismo label: [isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml replicaset.apps/rs-test created [isx46410800@miguel replicaset]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test 1/1 Running 0 3m49s pod-test2 1/1 Running 0 3m45s rs-test-8mk72 2/2 Running 0 10s Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.","title":"ADOPCI\u00d3N DE PODS PLANOS"},{"location":"kubernetes/#problemas","text":"Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene. NO se auto-actualizan solos. Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.","title":"PROBLEMAS"},{"location":"kubernetes/#deployments","text":"Es un objeto de nivel mayor que los replicaset. Es el due\u00f1o del replicaset que a su vez es de sus PODs. Al deployment se le da una imagen o una nueva versi\u00f3n: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y as\u00ed succesivamente. Esto se logra porque los deployments tienen dos valores: Uno de m\u00e1ximo extra y otra de un m\u00e1ximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod m\u00e1s y solo dejar 1 pod inutilizado. Los deployments pueden mantener un m\u00e1ximo de 10 replicasets","title":"DEPLOYMENTS"},{"location":"kubernetes/#crear-deployment","text":"Vemos a donde pertenece la api-version y el kind de los deployments en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Lo creamos con kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test created Vemos el deployment creado kubectl get deployment : [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 8s Vemos los labels del deployment kubectl get deployment --show-labels : [isx46410800@miguel deployments]$ kubectl get deployment --show-labels NAME READY UP-TO-DATE AVAILABLE AGE LABELS deployment-test 3/3 3 3 21s app=front Vemos el estado del deployment kubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out Vemos que se ha creado un replicaset y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 4m28s [isx46410800@miguel deployments]$ kubectl get replicaset --show-labels NAME DESIRED CURRENT READY AGE LABELS deployment-test-659b64d66c 3 3 3 5m8s app=front,pod-template-hash=659b64d66c Vemos que se ha creado 3 replicas del pod y tiene los mismo labels: [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 4m32s deployment-test-659b64d66c-pzdct 1/1 Running 0 4m32s deployment-test-659b64d66c-thknz 1/1 Running 0 4m32s [isx46410800@miguel deployments]$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS deployment-test-659b64d66c-n5qgr 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-pzdct 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c deployment-test-659b64d66c-thknz 1/1 Running 0 5m14s app=front,pod-template-hash=659b64d66c Vemos la jerarqu\u00eda de lo creado para saber quien es el owner reference de cada cosa con kubectl get rs/pod/deployment NAME -o yaml : Deployment no tiene due\u00f1o Replicaset su due\u00f1o es deployment Pod su due\u00f1o es replicaset","title":"CREAR DEPLOYMENT"},{"location":"kubernetes/#rolling-update","text":"Actualizamos por ejemplo la imagen de un container del POD en vez de nginx:alpine ponemos nginx y hacemos de nuevo el kubectl apply -f deployment.yaml : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 1/1 Running 0 13m deployment-test-659b64d66c-pzdct 1/1 Running 0 13m deployment-test-659b64d66c-thknz 1/1 Running 0 13m deployment-test-69b674677d-2cq4l 0/1 ContainerCreating 0 5s [isx46410800@miguel deployments]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 1 3 14m [isx46410800@miguel deployments]$ kubectl get replicaset NAME DESIRED CURRENT READY AGE deployment-test-659b64d66c 3 3 3 14m deployment-test-69b674677d 1 1 0 18s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-659b64d66c-n5qgr 0/1 Terminating 0 14m deployment-test-659b64d66c-pzdct 1/1 Running 0 14m deployment-test-659b64d66c-thknz 1/1 Terminating 0 14m deployment-test-69b674677d-2cq4l 1/1 Running 0 25s deployment-test-69b674677d-dwdlr 0/1 ContainerCreating 0 1s deployment-test-69b674677d-dwspw 1/1 Running 0 6s [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-69b674677d-2cq4l 1/1 Running 0 43s deployment-test-69b674677d-dwdlr 1/1 Running 0 19s deployment-test-69b674677d-dwspw 1/1 Running 0 24s Vemos el estado en directo de lo que hace con ubectl rollout status deployment deploymentName : [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... Waiting for deployment \"deployment-test\" rollout to finish: 1 old replicas are pending termination... deployment \"deployment-test\" successfully rolled out Tambi\u00e9n podemos ver el resultado en kubectl describe deployment deploymentName : Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 19m deployment-controller Scaled up replica set deployment-test-659b64d66c to 3 Normal ScalingReplicaSet 5m18s deployment-controller Scaled up replica set deployment-test-69b674677d to 1 Normal ScalingReplicaSet 4m59s deployment-controller Scaled down replica set deploy Aqu\u00ed vemos tambi\u00e9n la estrateg\u00eda de los valores que comentamos en la introducci\u00f3n: RollingUpdateStrategy: 25% max unavailable, 25% max surge","title":"ROLLING UPDATE"},{"location":"kubernetes/#historial-de-deployments","text":"Podemos ver las actualizaciones o revisiones en el historial de deployments en kubectl rollout history deployment deployment : [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 2 <none> 3 <none> 4 <none> Podemos con esto volver a cualquier versi\u00f3n anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo a\u00f1adiento en la parte de replicaset del manifiesto YAML revisionHistoryLimit: 5 : # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: revisionHistoryLimit: 5 replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 Para poner un motivo en el change-cause cuando hacemos una versi\u00f3n de deployments indicamos dos maneras: Con la linea de desplegar kubectl apply -f deployment.yaml --record : [isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record deployment.apps/deployment-test configured [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true Con una subsecci\u00f3n en el manifiesto deployment.yaml annotations-> kubernetes.io/change-cause: \"message\" : esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"changes port to 110\" labels: app: front kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 Para luego ver una revisi\u00f3n en concreta usamos kubectl rollout history deployment deployment-test --revision=3 : deployment.apps/deployment-test with revision #3 Pod Template: Labels: app=front pod-template-hash=fd8445c88 Annotations: kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nginx: Image: nginx:alpine Port: 90/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none>","title":"HISTORIAL DE DEPLOYMENTS"},{"location":"kubernetes/#roll-backs","text":"Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualizaci\u00f3n de la imagen: containers: - name: nginx image: nginx:fake ports: - containerPort: 110 Vemos el nuevo historial y su fallo: [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test deployment.apps/deployment-test REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 kubectl apply --filename=deployment.yaml --record=true 4 changes port to 110 5 new version nginx # [isx46410800@miguel deployments]$ kubectl get pods NAME READY STATUS RESTARTS AGE deployment-test-5c6896bcd5-h5qts 0/1 ErrImagePull 0 32s deployment-test-74fb9c6d9f-7dwnr 1/1 Running 0 6m50s deployment-test-74fb9c6d9f-f5qs8 1/1 Running 0 6m45s deployment-test-74fb9c6d9f-lsmzj 1/1 Running 0 6m54s Volvemos haciendo un rollback a una versi\u00f3n anterior con kubectl rollout undo deployment deployment-test --to-revision=4 : [isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4 deployment.apps/deployment-test rolled back # [isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test deployment \"deployment-test\" successfully rolled out # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Name: deployment-test Namespace: default CreationTimestamp: Sun, 11 Oct 2020 19:21:04 +0200 Labels: app=front Annotations: deployment.kubernetes.io/revision: 6 kubernetes.io/change-cause: changes port to 110 Selector: app=front Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=front Containers: nginx: Image: nginx:alpine Port: 110/TCP Host Port: 0/TCP # [isx46410800@miguel deployments]$ kubectl describe deployment deployment-test Normal ScalingReplicaSet 117s (x12 over 15m) deployment-controller (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0","title":"ROLL BACKS"},{"location":"kubernetes/#servicios","text":"Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y \u00e9ste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique. Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio tambi\u00e9n tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo. Los endpoints se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y as\u00ed el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.","title":"SERVICIOS"},{"location":"kubernetes/#crear-servicio","text":"Vemos a donde pertenece la api-version y el kind de los servicios en: kubectl api-resources Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.","title":"CREAR SERVICIO"},{"location":"kubernetes/#info-servicio","text":"Vemos lo creado con kubectl get services/svc : [isx46410800@miguel services]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 41h my-service ClusterIP 10.97.182.119 <none> 8888/TCP 63s [isx46410800@miguel services]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 3/3 3 3 79s Vemos por el label que le indicamos en el YAML: [isx46410800@miguel services]$ kubectl get services -l app=front NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service ClusterIP 10.97.182.119 <none> 8888/TCP 3m35s El cluster-ip se lo da kubernetes si no se lo asignamos directamente Profundizamos el servicio con kubectl describe svc my-service : [isx46410800@miguel services]$ kubectl describe svc my-service Name: my-service Namespace: default Labels: app=front Annotations: <none> Selector: app=front Type: ClusterIP IP: 10.97.182.119 Port: <unset> 8888/TCP TargetPort: 80/TCP Endpoints: 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 Session Affinity: None Events: <none> Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.","title":"INFO SERVICIO"},{"location":"kubernetes/#endpoints","text":"Lista de IPs de los pods que tienen el label de mi servicio creado. Vemos la lista de endpoints con kubectl get endpoints : [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.4:80,172.18.0.5:80 10m Comprobamos que son las mismas de los PODS: [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-cgds6 1/1 Running 0 10m 172.18.0.4 minikube <none> <none> deployment-test-b7c99d94b-fmpdc 1/1 Running 0 10m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 10m 172.18.0.5 minikube <none> <none> Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint: [isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6 pod \"deployment-test-b7c99d94b-cgds6\" deleted [isx46410800@miguel services]$ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 172.17.0.2:8443 41h my-service 172.18.0.2:80,172.18.0.5:80,172.18.0.6:80 13m [isx46410800@miguel services]$ kubectl get pods -l app=front -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES deployment-test-b7c99d94b-fmpdc 1/1 Running 0 14m 172.18.0.2 minikube <none> <none> deployment-test-b7c99d94b-kcdnx 1/1 Running 0 39s 172.18.0.6 minikube <none> <none> deployment-test-b7c99d94b-t8bdz 1/1 Running 0 14m 172.18.0.5 minikube <none> <none>","title":"ENDPOINTS"},{"location":"kubernetes/#dns","text":"Creamos un POD nuevo: [isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh Funciona que escucha al servicio: # curl 10.97.182.119:8888 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web. Se crea como un tipo de DNS ya que por el nombre del servicio tambi\u00e9n se comunica y obtiene respuesta: # curl my-service:8888","title":"DNS"},{"location":"kubernetes/#servicio-cluster-ip","text":"IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior. Le podemos poner un tipo de servicio a los servicios que creamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)","title":"SERVICIO CLUSTER-IP"},{"location":"kubernetes/#servicio-node-port","text":"IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea tambi\u00e9n un ClusterIP. Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service2 labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Resultados: [isx46410800@miguel services]$ kubectl get services -l app=backend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service2 NodePort 10.101.52.193 <none> 8888:30072/TCP 21s [isx46410800@miguel services]$ kubectl get pods -l app=backend NAME READY STATUS RESTARTS AGE deployment-test2-77448c6d65-gj6l7 1/1 Running 0 36s deployment-test2-77448c6d65-n8td7 1/1 Running 0 36s deployment-test2-77448c6d65-sd6zq 1/1 Running 0 36s Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio: http://192.168.1.104:30072 Si no hace en minikube podemos hacer lo siguiente y lo veremos: [isx46410800@miguel services]$ minikube service my-service2 |-----------|-------------|-------------|-------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-------------|-------------|-------------------------| | default | my-service2 | 8888 | http://172.17.0.2:30072 | |-----------|-------------|-------------|-------------------------| Esa url nos dar\u00e1 el servicio web a trav\u00e9s del node port.","title":"SERVICIO NODE-PORT"},{"location":"kubernetes/#servicio-load-balancer","text":"Hace referencia a un servicio de balanzador de carga. Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.","title":"SERVICIO LOAD BALANCER"},{"location":"kubernetes/#golang","text":"Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petici\u00f3n al front y este le devuelva el servicio que est\u00e1 en backend.","title":"GOLANG"},{"location":"kubernetes/#crear-api-rest-go","text":"DOCUMENTACI\u00d3N Creamos un fichero simple de API REST en Goland: [isx46410800@miguel src]$ cat main.go package main import ( \"log\" \"net/http\" ) func ServeHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"message\": \"hello world\"}`)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil)) } Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestar\u00e1 a la petici\u00f3n el hello wolld como respuesta. Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo: [isx46410800@miguel k8s-hands-on]$ docker pull golang [isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash Iniciamos el fichero y comprobamos el resultado: [isx46410800@miguel src]$ docker exec -it goland /bin/bash root@miguel:/go# go run main.go ![./images/kubernetes4.png]","title":"CREAR API REST GO"},{"location":"kubernetes/#cambios-mensaje-respuesta","text":"","title":"CAMBIOS MENSAJE RESPUESTA"},{"location":"kubernetes/#mensaje-1","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"fmt\" \"os\" \"time\" ) func ServerHTTP(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) resp := fmt.Sprintf(\"La hora es %v y el hostname es %v\", time.Now(), os.Getenv(\"HOSTNAME\")) w.Write([]byte(resp)) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } ![./images/kubernetes5.png]","title":"MENSAJE 1"},{"location":"kubernetes/#mensaje-2","text":"A\u00f1adimos unas variables para cambiar el mensaje de respuesta a la petici\u00f3n de request: package main import ( \"net/http\" \"os\" \"time\" \"encoding/json\" ) type HandsOn struct { Time time.Time `json:\"time\"` Hostname string `json:\"hostname\"` } func ServerHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \"/\" { http.NotFound(w, r) return } resp := HandsOn{ Time: time.Now(), Hostname: os.Getenv(\"HOSTNAME\"), } jsonResp, err := json.Marshal(&resp) if err != nil { w.Write([]byte(\"Error\")) return } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusOK) w.Write(jsonResp) } func main() { http.HandleFunc(\"/\", ServerHTTP) http.ListenAndServe(\":9090\", nil) } Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y tambi\u00e9n darle un mensaje de error si no acaba en /. ![./images/kubernetes6.png] ![./images/kubernetes7.png]","title":"MENSAJE 2"},{"location":"kubernetes/#dockerfile-golang","text":"Ejemplo: # IMAGEN DE GOLAND FROM golang:1.13 as builder # DIRECTORIO A TRABAJAR WORKDIR /app # COPIAMOS FICHERO MAIN COPY main.go . RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go # DESDE IMAGEN ALPINE FROM alpine:latest # mailcap adds mime detection and ca-certificates help with TLS (basic stuff) WORKDIR /app COPY --from=builder /app/app . # PARA EJECUTARLO ENTRYPOINT [\"./app\"] Construimos imagen: [isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on . Encendemos: [isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on ![./images/kubernetes8.png] Ahora nuestra aplicaci\u00f3n de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.","title":"DOCKERFILE GOLANG"},{"location":"kubernetes/#deployment-golang","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on labels: app: backend spec: selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Ponemos lo de imagePullPolicy: IfNotPresent para que primero busque si la imagen est\u00e1 constuida localmente antes de mirar en los repos de internet de dockerhub. Comprobaciones: [isx46410800@miguel backend]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-5d548949c7-dgw9l 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-fg8wr 1/1 Running 0 15m backend-k8s-hands-on-5d548949c7-q9s6g 1/1 Running 0 15m [isx46410800@miguel backend]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-5d548949c7 3 3 3 15m [isx46410800@miguel backend]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 15m [isx46410800@miguel backend]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on ClusterIP 10.101.44.56 <none> 80/TCP 16m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d19h Visual cambiando a nodeport, nos contestar\u00e1 unos de los PODs la respuesta a la request del usuario: ![./images/kubernetes9.png]","title":"DEPLOYMENT GOLANG"},{"location":"kubernetes/#consumo-del-servicio","text":"Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos dar\u00e1n las respuestas los PODs del servicio: [isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh / # apk add -U curl / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:57:49.446174694Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # / # curl 10.111.54.241:80 {\"time\":\"2020-10-13T19:58:10.218346403Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-9qp82\"}/ # {\"time\":\"2020-10-13T19:58:25.365295183Z\",\"hostname\":\"backend-k8s-hands-on-5d548949c7-66dgx\"}/ #","title":"CONSUMO DEL SERVICIO"},{"location":"kubernetes/#fronted","text":"Creamos ahora un index.html de respuesta en un fronted/src/index.html: [isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine vi /usr/share/nginx/html/index.html <div id=\"id01\"></div> <script> var xmlhttp = new XMLHttpRequest(); var url = \"http://backend-k8s-hands-on\"; xmlhttp.onreadystatechange = function() { if (this.readyState == 4 && this.status == 200) { var resp = JSON.parse(this.responseText); document.getElementById(\"id01\").innerHTML = \"<h2>La hora es \" + resp.time + \"y el hostname es\" + resp.hostname \"</h2\">; } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); </script> Dockerfile: # IMAGEN DE GOLAND FROM nginx:alpine # COPIAMOS FICHERO MAIN COPY ./src/index.html /usr/share/nginx/html/index.html","title":"FRONTED"},{"location":"kubernetes/#manifiesto-fronted","text":"Despliegue del fronted: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: fronted-k8s-hands-on labels: app: fronted # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: fronted # aqui viene el pod template: metadata: labels: app: fronted spec: containers: - name: fronted image: isx46410800/k8s-hands-on:fronted imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: fronted-k8s-hands-on labels: app: fronted spec: type: NodePort selector: app: fronted ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 80 Resultados: [isx46410800@miguel k8s-hands-on]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.111.54.241 <none> 80:30740/TCP 78m fronted-k8s-hands-on NodePort 10.105.156.14 <none> 80:30159/TCP 9m22s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 3d20h [isx46410800@miguel k8s-hands-on]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-lzrr4 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-mdjh7 1/1 Running 0 51m backend-k8s-hands-on-7d5b6dc559-qxzdv 1/1 Running 0 51m fronted-k8s-hands-on-78f59c5f77-dpvck 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-q7h9r 1/1 Running 0 9m27s fronted-k8s-hands-on-78f59c5f77-r7fnm 1/1 Running 0 9m27s [isx46410800@miguel k8s-hands-on]$ kubectl cluster-info Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy","title":"MANIFIESTO FRONTED"},{"location":"kubernetes/#namespaces","text":"Son como ambientes separados dentro del cluster de kubernetes. Cada uno de estos ambientes tienen su deployment, replicaset, pods... Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster. En cada namespace se puede limitar los pods, la memoria, usuarios... Ordenes b\u00e1sicas: kubectl get namespaces [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 3d23h kube-node-lease Active 3d23h kube-public Active 3d23h kube-system Active 3d23h Especifica por namespace: kubectl get pods --namespace default El default van todos los recursos, lo creado donde no se asignan ningun namespace. Todos los usuarios pueden ver este namespace kube-public. El kube-system tiene todos los objetos del kubernetes. Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldr\u00e1n los asignados. Habr\u00e1 que poner -n/--namespace namespaceName","title":"NAMESPACES"},{"location":"kubernetes/#crear-namespace","text":"Por comando kubectl create namespace nameNamespace : [isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns namespace/test-ns created Para verlo kubectl get namespaces y kubectl describe namespaces test-ns : [isx46410800@miguel k8s-hands-on]$ kubectl get namespaces NAME STATUS AGE default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d test-ns Active 4s [isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns Name: test-ns Labels: <none> Annotations: <none> Status: Active No resource quota. No LimitRange resource. Por manifiesto YAML: apiVersion: v1 kind: Namespace metadata: name: development labels: name: development Comprobamos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml namespace/development created [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> development Active 19s name=development kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> test-ns Active 6m33s <none>","title":"CREAR NAMESPACE"},{"location":"kubernetes/#asignar-namespaces","text":"Creamos un pod y lo asignamos: [isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns pod/podtest2 created [isx46410800@miguel namespaces]$ kubectl get pods -n test-ns NAME READY STATUS RESTARTS AGE podtest2 1/1 Running 0 22s","title":"ASIGNAR NAMESPACES"},{"location":"kubernetes/#borrar-namespaces","text":"Borramos POD asignado a namespace: [isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns Borrar manifiesto: [isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml Borrar namespace: [isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns","title":"BORRAR NAMESPACES"},{"location":"kubernetes/#deploy-namespaces","text":"Creamos dos namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod Lo vemos kubectl get namespaces --show-labels : [isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 4d <none> dev Active 6s name=dev kube-node-lease Active 4d <none> kube-public Active 4d <none> kube-system Active 4d <none> prod Active 6s name=prod Creamos un deployment con los namespaces: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- apiVersion: v1 kind: Namespace metadata: name: prod labels: name: prod --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-dev namespace: dev labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deploy-prod namespace: prod labels: app: back # aqui viene el replicaset spec: replicas: 5 selector: matchLabels: app: back # aqui viene el pod template: metadata: labels: app: back spec: containers: - name: nginx image: nginx:alpine Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml namespace/dev unchanged namespace/prod unchanged deployment.apps/deploy-dev created deployment.apps/deploy-prod created [isx46410800@miguel namespaces]$ kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE deploy-dev 1/1 1 1 26s [isx46410800@miguel namespaces]$ kubectl get deploy -n prod NAME READY UP-TO-DATE AVAILABLE AGE deploy-prod 5/5 5 5 29s [isx46410800@miguel namespaces]$ kubectl get rs -n dev NAME DESIRED CURRENT READY AGE deploy-dev-b7c99d94b 1 1 1 36s [isx46410800@miguel namespaces]$ kubectl get rs -n prod NAME DESIRED CURRENT READY AGE deploy-prod-7bfb7875fd 5 5 5 38s [isx46410800@miguel namespaces]$ kubectl get pods -n dev NAME READY STATUS RESTARTS AGE deploy-dev-b7c99d94b-xc696 1/1 Running 0 50s [isx46410800@miguel namespaces]$ kubectl get pods -n prod NAME READY STATUS RESTARTS AGE deploy-prod-7bfb7875fd-49kzd 1/1 Running 0 54s deploy-prod-7bfb7875fd-9m7x8 1/1 Running 0 54s deploy-prod-7bfb7875fd-nbhfd 1/1 Running 0 54s deploy-prod-7bfb7875fd-tl5gs 1/1 Running 0 54s deploy-prod-7bfb7875fd-wxrwc 1/1 Running 0 54s [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE default Active 4d dev Active 10m kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d prod Active 10m","title":"DEPLOY NAMESPACES"},{"location":"kubernetes/#dns-namespaces","text":"Creamos un namespace y un deploy asignados: apiVersion: v1 kind: Namespace metadata: name: ci labels: name: ci --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: backend image: isx46410800/k8s-hands-on:v2 imagePullPolicy: IfNotPresent --- # a\u00f1adimos el servicio apiVersion: v1 kind: Service metadata: name: backend-k8s-hands-on namespace: ci labels: app: backend spec: type: NodePort selector: app: backend ports: - protocol: TCP port: 80 # servicio por donde escucha targetPort: 9090 Resultados: [isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml namespace/ci created deployment.apps/backend-k8s-hands-on created service/backend-k8s-hands-on created [isx46410800@miguel namespaces]$ kubectl get namespaces NAME STATUS AGE ci Active 15s default Active 4d kube-node-lease Active 4d kube-public Active 4d kube-system Active 4d [isx46410800@miguel namespaces]$ kubectl get deploy -n ci NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 32s [isx46410800@miguel namespaces]$ kubectl get svc -n ci NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 38s [isx46410800@miguel namespaces]$ kubectl get rs -n ci NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 46s [isx46410800@miguel namespaces]$ kubectl get pods -n ci NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 49s backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 49s Ahora creamos un POD con el namespace default: [isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh Por defecto, cuando los dns que se crean en un namespace siguen esta regla: serviceName + namespaceName + service.cluster.local As\u00ed desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior: / # curl backend-k8s-hands-on.ci.svc.cluster.local {\"time\":\"2020-10-14T01:09:56.22990857Z\",\"hostname\":\"backend-k8s-hands-on-7d5b6dc559-7xv59\"}/ Si no dar\u00eda error: / # curl backend-k8s-hands-on curl: (6) Could not resolve host: backend-k8s-hands-on","title":"DNS NAMESPACES"},{"location":"kubernetes/#contextos-namespaces","text":"Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato. Para ver en que contexto estamos usamos: kubectl config current-context [isx46410800@miguel namespaces]$ kubectl config current-context minikube Vemos el archivo de configuraci\u00f3n ./kube/config que es de donde lee el current-context: [root@miguel ~]# cat /home/isx46410800/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Mejor con este comando kubectl config view : [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Crear un nuevo contexto ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName : [isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube Context \"ci-context\" created. Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci: [isx46410800@miguel namespaces]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube namespace: ci user: minikube name: ci-context - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt client-key: /home/isx46410800/.minikube/profiles/minikube/client.key Para cambiar de contexto usamos kubectl config use-context Namecontext : [isx46410800@miguel namespaces]$ kubectl config use-context ci-context Switched to context \"ci-context\". [isx46410800@miguel namespaces]$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-k8s-hands-on-7d5b6dc559-7xv59 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-b4sqq 1/1 Running 0 19m backend-k8s-hands-on-7d5b6dc559-bdktk 1/1 Running 0 19m [isx46410800@miguel namespaces]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE backend-k8s-hands-on 3/3 3 3 19m [isx46410800@miguel namespaces]$ kubectl get rs NAME DESIRED CURRENT READY AGE backend-k8s-hands-on-7d5b6dc559 3 3 3 20m [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend-k8s-hands-on NodePort 10.105.98.188 <none> 80:31030/TCP 20m [isx46410800@miguel namespaces]$ kubectl config use-context minikube Switched to context \"minikube\". [isx46410800@miguel namespaces]$ kubectl get pods No resources found in default namespace. [isx46410800@miguel namespaces]$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 4d","title":"CONTEXTOS NAMESPACES"},{"location":"kubernetes/#limitar-ramcpu","text":"La RAM se puede limitar en B, MB y G. La CPU: 1 cpu es 1000 milicores/milicpus.","title":"LIMITAR RAM/CPU"},{"location":"kubernetes/#limitsrequest","text":"Los LIMITS es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podr\u00eda tener m\u00e1s si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminar\u00e1 o reiniciar\u00e1 el pod y lo pondr\u00e1 en otro sitio que le garantice esa cantidad de recursos indicada. Los REQUESTS es la cantidad de recursos que el pod siempre va a poder disponer. Estar\u00e1 garantizado la cantidad que se le indique.","title":"LIMITS/REQUEST"},{"location":"kubernetes/#ram","text":"Creamos un ejemplo de limite de RAM: apiVersion: v1 kind: Pod metadata: name: memory-demo spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \"200Mi\" requests: memory: \"100Mi\" command: [\"stress\"] # se indica que le va a dar 150Megas args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"] Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M. Comprobamos que lo ha creado kubectl apply -f limit-request.yaml : [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 38s Si ponemos el ejemplo anterior con 250M vemos los errores: [isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml pod/memory-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 ContainerCreating 0 4s [isx46410800@miguel limits-requests]$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 1 12s memory-demo 0/1 OOMKilled 2 25s memory-demo 0/1 CrashLoopBackOff 2 26s ^C[isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 CrashLoopBackOff 2 48s Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE memory-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient memory.","title":"RAM"},{"location":"kubernetes/#cpu","text":"Ejemplo de limitar CPU: apiVersion: v1 kind: Pod metadata: name: cpu-demo spec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: \"1\" requests: cpu: \"0.5\" args: - -cpus - \"2\" # se le pide 2 cpus y hay limite de 1 Aunque se le pida 2, no se eliminar\u00e1 como la RAM sino que soolo tendr\u00e1 de m\u00e1ximo el LIMIT indicado(1). Resultados: [isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml pod/cpu-demo created [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 ContainerCreating 0 7s [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 1/1 Running 0 11s Si vemos la capacidad total de mi cluster kubectl describe node minikube : Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1150m (28%) 1 (25%) memory 70Mi (0%) 170Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Siempre hay un poco m\u00e1s para que no sobrepase el limite y me vaya todo lento. Nuestra cantidad de CPU kubectl describe node minikube : kubectl describe node minikube Capacity: cpu: 4 Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM: [isx46410800@miguel limits-requests]$ kubectl get pods NAME READY STATUS RESTARTS AGE cpu-demo 0/1 Pending 0 5s [isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 25s (x2 over 25s) default-scheduler 0/1 nodes are available: 1 Insufficient cpu.","title":"CPU"},{"location":"kubernetes/#qosquality-of-service","text":"Es una propiedad que se le asigna a los pods. Podemos ver el estado de QoS con: kubectl get pod podName -o yaml | grep -i qos Hay diferentes tipos de clases de estado en el que entra el POD: BestEffort : No se definen los limites y request. Los asignar\u00e1 el schedule pero puede ser que este consuma y consuma recursos sin parar. Guaranteed : Tiene los mismos limites que de request Burstable : cuando pueda aumentar el request. El limite es mayor que el request.","title":"QOS(Quality of Service)"},{"location":"kubernetes/#limitrange","text":"Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces. Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods","title":"LIMITRANGE"},{"location":"kubernetes/#valores-por-defecto","text":"Ejemplo: apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default. Comprobamos con kubectl get limitrange -n namespaceName : [isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml namespace/dev created limitrange/mem-limit-range created # [isx46410800@miguel limitRange]$ kubectl get limitrange -n dev NAME CREATED AT mem-limit-range 2020-10-14T18:10:15Z Comprobamos con kubectl describe limitrange LRName -n NSName : [isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev Name: mem-limit-range Namespace: dev Type Resource Min Max Default Request Default Limit Max Limit/Request Ratio ---- -------- --- --- --------------- ------------- ----------------------- Container cpu - - 500m 1 - Container memory - - 256Mi 512Mi -","title":"VALORES POR DEFECTO"},{"location":"kubernetes/#valores-pod","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3 Started: Wed, 14 Oct 2020 20:21:43 +0200 Ready: True Restart Count: 0 Limits: cpu: 1 memory: 512Mi Requests: [isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3 Limits: cpu: 1 memory: 512Mi Requests: cpu: 500m memory: 256Mi Environment: <none> Vemos que se han asignado la cpu de 0.5 y Ram 256M.","title":"VALORES POD"},{"location":"kubernetes/#limites","text":"Ejemplo: # namespace apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # limit range para el namespace dev apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range namespace: dev spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container --- # pod apiVersion: v1 kind: Pod metadata: name: pod-test3 namespace: dev labels: app: back-end env: dev spec: containers: - name: container1 image: nginx:alpine resources: limits: memory: 500M cpu: 0.5 requests: memory: 400M cpu: 0.3 Si se superan los limites en los PODs te dar\u00e1 error, ya que sobrepasa los limites de memoria y ram","title":"LIMITES"},{"location":"kubernetes/#resource-quota","text":"Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro. Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus. El limitrange opera por objeto, por pod.","title":"RESOURCE QUOTA"},{"location":"kubernetes/#crear-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi Comprobamos con kubectl describe resourcequota -n nsName : [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml namespace/uat created resourcequota/mem-cpu-demo created [isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo Name: mem-cpu-demo Namespace: uat Resource Used Hard -------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi Resultados: [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi No LimitRange resource.","title":"CREAR RQ"},{"location":"kubernetes/#deploy-rq","text":"Ejemplo: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: uat labels: name: uat --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo namespace: uat spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test namespace: uat labels: app: front # aqui viene el replicaset spec: replicas: 2 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine resources: requests: memory: 500M cpu: 0.5 limits: memory: 500M cpu: 0.5 Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl get pods -n uat NAME READY STATUS RESTARTS AGE deployment-test-5f869977fb-84nqs 1/1 Running 0 2m40s deployment-test-5f869977fb-vg5cj 1/1 Running 0 2m45s [isx46410800@miguel resource-quota]$ kubectl get rs -n uat NAME DESIRED CURRENT READY AGE deployment-test-5f869977fb 2 2 2 2m54s deployment-test-df54c6d6d 0 0 0 5m41s [isx46410800@miguel resource-quota]$ kubectl get deploy -n uat NAME READY UP-TO-DATE AVAILABLE AGE deployment-test 2/2 2 2 5m47s [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat NAME AGE REQUEST LIMIT mem-cpu-demo 5m57s requests.cpu: 1/1, requests.memory: 1G/1Gi limits.cpu: 1/2, limits.memory: 1G/2Gi Con lo creado ahora podemos ver que hemos llegado a los limites kubectl describe ns nsName : [isx46410800@miguel resource-quota]$ kubectl describe ns uat Name: uat Labels: name=uat Annotations: <none> Status: Active Resource Quotas Name: mem-cpu-demo Resource Used Hard -------- --- --- limits.cpu 1 2 limits.memory 1G 2Gi requests.cpu 1 1 requests.memory 1G 1Gi No LimitRange resource. Si ahora modificamos el fichero y creamos 3 replicas, superar\u00e1 el limite indicado. Por lo que solo crear\u00e1 dos y no tres, ya que el 3 superar\u00e1 los limites asignados en el RESOURCE QUOTA.","title":"DEPLOY RQ"},{"location":"kubernetes/#limitar-no-pods-en-ns","text":"Vemos un ejemplo de como limitar el n\u00famero de pods que se pueden crear en un namespace a trav\u00e9s del ResourceQuota: --- # creamos namespaces apiVersion: v1 kind: Namespace metadata: name: qa labels: name: qa --- # creamos resoucequota apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo namespace: qa spec: hard: pods: \"3\" --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-qa namespace: qa labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine Comprobamos lo creado: [isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml namespace/qa created resourcequota/pod-demo created deployment.apps/deployment-qa created # [isx46410800@miguel resource-quota]$ kubectl get pods -n qa NAME READY STATUS RESTARTS AGE deployment-qa-b7c99d94b-h5bxr 1/1 Running 0 10s deployment-qa-b7c99d94b-tttpn 1/1 Running 0 10s deployment-qa-b7c99d94b-xdl45 1/1 Running 0 10s [isx46410800@miguel resource-quota]$ kubectl get rs -n qa NAME DESIRED CURRENT READY AGE deployment-qa-b7c99d94b 3 3 3 14s # [isx46410800@miguel resource-quota]$ kubectl get ns -n qa NAME STATUS AGE ci Active 18h default Active 4d19h kube-node-lease Active 4d19h kube-public Active 4d19h kube-system Active 4d19h qa Active 18s # [isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa NAME AGE REQUEST LIMIT pod-demo 99s pods: 3/3 M\u00e1s info kubectl describe resourcequota pod-demo -n qa : [isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa Name: pod-demo Namespace: qa Resource Used Hard -------- ---- ---- pods 3 3 Si ponemos 4 replicas, solo se habr\u00e1n creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota.","title":"LIMITAR N\u00ba PODS EN NS"},{"location":"kubernetes/#probes","text":"Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container. Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta. Puede ser este PROBE por: Comando TCP HTTP","title":"PROBES"},{"location":"kubernetes/#tipos-probes","text":"Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que est\u00e9 funcionando la aplicaci\u00f3n del contenedor. Readiness: nos ayuda a garantizar el servicio del pod est\u00e1 listo para el request. Startup: es una prueba que se sube para ver que est\u00e9 todo configurado y este listo la aplicaci\u00f3n para ejecutarse.","title":"TIPOS PROBES"},{"location":"kubernetes/#crear-liveness-probe","text":"Ejemplo: # probe liveness apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo. Vemos resultados de lo que pasa en kubectl describe pod podName Pruebas: [isx46410800@miguel probes]$ kubectl apply -f liveness.yaml pod/liveness-exec created [isx46410800@miguel probes]$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 0 9s # [isx46410800@miguel probes]$ kubectl describe pod liveness-exec Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 95s default-scheduler Successfully assigned default/liveness-exec to minikube Normal Pulled 90s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 3.165552593s Warning Unhealthy 46s (x3 over 56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 46s kubelet Container liveness failed liveness probe, will be restarted Normal Pulling 15s (x2 over 93s) kubelet Pulling image \"k8s.gcr.io/busybox\" Normal Pulled 15s kubelet Successfully pulled image \"k8s.gcr.io/busybox\" in 751.39074ms Normal Created 14s (x2 over 89s) kubelet Created container liveness Normal Started 14s (x2 over 88s) kubelet Started container liveness","title":"CREAR LIVENESS PROBE"},{"location":"kubernetes/#liveness-tcp","text":"Una probe con liveness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS TCP"},{"location":"kubernetes/#liveness-http","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 Vemos resultados de lo que pasa en kubectl describe pod podName","title":"LIVENESS HTTP"},{"location":"kubernetes/#readiness-probe","text":"Una probe con readiness TCP: apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren m\u00e1s peticiones de request y por lo tanto no se le de m\u00e1s carga a este contenedor/pod.","title":"READINESS PROBE"},{"location":"kubernetes/#variables-y-configmap","text":"","title":"VARIABLES Y CONFIGMAP"},{"location":"kubernetes/#crear-variables","text":"Ejemplo: apiVersion: v1 kind: Pod metadata: name: envar-demo spec: containers: - name: envar-demo-container image: nginx:alpine env: - name: VAR1 value: \"valor de prueba 1\" - name: VAR2 value: \"valor de prubea 2\" - name: VAR3 value: \"valor de prubea 3\" Prueba: [isx46410800@miguel env_variables]$ kubectl apply -f env.yaml pod/envar-demo created # [isx46410800@miguel env_variables]$ kubectl get pods NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 12s # [isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh / # env KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=envar-demo SHLVL=1 HOME=/root VAR1=valor de prueba 1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin VAR2=valor de prubea 2 VAR3=valor de prubea 3 PWD=/ # / # echo $VAR1 valor de prueba 1","title":"CREAR VARIABLES"},{"location":"kubernetes/#variables-referenciadas","text":"Se crearian a partir de conseguir la info del pod a partir del [isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml : apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: - name: test-container image: ngix:alpine env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc.","title":"VARIABLES REFERENCIADAS"},{"location":"kubernetes/#configmap","text":"Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creaci\u00f3n. Se forma con la estructura clave: valor . Desde el POD se indica que llave quiere consumir del configmap. Se puede crear mediante un file.conf o en un objeto configmap. Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero. Lo creamos con kubectl create configmap nginx-config --from-file=examples/nginx.conf y lo vemos con kubectl get cm : [isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf configmap/nginx-config created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 14s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config Name: nginx-config Namespace: default Labels: <none> Annotations: <none> Data ==== nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none> Vemos que se ha creado en formato llave(nginx.conf) y valor la configuraci\u00f3n. Ejemplo con todos los archivos del subdirectorio y vemos que se crean m\u00e1s llaves-valor: [isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples configmap/nginx-config2 created # [isx46410800@miguel configmap]$ kubectl get cm NAME DATA AGE nginx-config 1 4m27s nginx-config2 2 4s # [isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2 Name: nginx-config2 Namespace: default Labels: <none> Annotations: <none> Data ==== index.html: ---- hola nginx nginx.conf: ---- server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Events: <none>","title":"CONFIGMAP"},{"location":"kubernetes/#montando-volumen-configmap","text":"Ejemplo: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: test: hola nginx: | server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-volume mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas volumes: - name: nginx-volume configMap: name: nginx-config items: - key: nginx path: default.conf En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo.","title":"MONTANDO VOLUMEN CONFIGMAP"},{"location":"kubernetes/#volumen-env-configmap","text":"Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script: apiVersion: v1 kind: ConfigMap metadata: name: nginx-config labels: app: front data: nginx: | server { listen 9090; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } --- apiVersion: v1 kind: ConfigMap metadata: name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user script: | echo DB host es $DB_HOST y DB user es $DB_USER > /usr/share/nginx/html/test.html --- apiVersion: apps/v1 kind: Deployment metadata: name: deployment-test labels: app: front spec: replicas: 1 selector: matchLabels: app: front template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine env: - name: DB_HOST valueFrom: configMapKeyRef: name: vars key: db_host - name: DB_USER valueFrom: configMapKeyRef: name: vars key: db_user volumeMounts: - name: nginx-vol mountPath: /etc/nginx/conf.d - name: script-vol mountPath: /opt volumes: - name: nginx-vol configMap: name: nginx-config items: - key: nginx path: default.conf - name: script-vol configMap: name: vars items: - key: script path: script.sh Comprobamos: [isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh / # ls /opt script.sh PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 NJS_VERSION=0.4.4 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ DB_HOST=dev.host.local DB_USER=dev_user / # echo $DB_HOST dev.host.local / # apk add python / # sh /opt/script.sh / # cat /usr/share/nginx/html/test.html DB host es dev.host.local y DB user es dev_user","title":"VOLUMEN-ENV CONFIGMAP"},{"location":"kubernetes/#secrets","text":"Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no deber\u00eda de verse. Funciona al estilo configmap. Lo podemos montar como una variable de entorno o como un volumen.","title":"SECRETS"},{"location":"kubernetes/#crear","text":"Ejemplo de como crearlo: kubectl create secret generic mysecret --from-file=secret-files/text.txt kubectl get secrets [isx46410800@miguel secrets]$ cat secret-files/text.txt secret1=hola # [isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt secret/mysecret created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d mysecret Opaque 1 7s # [isx46410800@miguel secrets]$ kubectl describe secrets mysecret Name: mysecret Namespace: default Labels: <none> Annotations: <none> Type: Opaque Data ==== text.txt: 26 bytes # secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml apiVersion: v1 data: text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M= kind: Secret metadata: creationTimestamp: \"2020-10-17T00:55:07Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:text.txt: {} f:type: {} manager: kubectl-create operation: Update time: \"2020-10-17T00:55:07Z\" name: mysecret namespace: default resourceVersion: \"72991\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2 type: Opaque Vemos que el contenido de los secretos no se ven, est\u00e1n cifrados en BASE64, que se puede descrifrar poniendo | base65 -decode","title":"CREAR"},{"location":"kubernetes/#manifiestos","text":"Creando SECRETS con manifiesto: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Para descrifrarlo hay que pasarlo de base64. Con Datastring para que lo codifique en base64: apiVersion: v1 kind: Secret metadata: name: mysecret type: opaque stringData: username: usertest password: test","title":"MANIFIESTOS"},{"location":"kubernetes/#envsubts","text":"Herramienta para poder reemplazar contenido de variables por el contenido: apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: $VAR1 password: $VAR2 [isx46410800@miguel secrets]$ export VAR1=miguel [isx46410800@miguel secrets]$ export VAR2=amoros [isx46410800@miguel secrets]$ envsubst < secret-secure.yaml > tmp.yaml [isx46410800@miguel secrets]$ cat tmp.yaml apiVersion: v1 kind: Secret metadata: name: mysecret2 type: opaque data: username: miguel password: amoros [isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml Luego podemos decode con base64 y obtenemos el resultado.","title":"ENVSUBTS"},{"location":"kubernetes/#volume-secrets","text":"Un ejemplo de crear un secreto y montarlo como volumen: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo. Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml secret/secret1 created pod/mypod created # [isx46410800@miguel secrets]$ kubectl get secrets NAME TYPE DATA AGE default-token-xbv2l kubernetes.io/service-account-token 3 7d secret1 opaque 2 6s # [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # cat /opt/password 123456/ # / # cat /opt/username admin/ #","title":"VOLUME SECRETS"},{"location":"kubernetes/#env-secrets","text":"Un ejemplo de crear un secreto y montarlo como varibale de entorno: # creamos el secreto apiVersion: v1 kind: Secret metadata: name: secret1 type: opaque stringData: username: admin password: \"123456\" --- # montamos el secreto apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: nginx:alpine env: - name: USERTEST valueFrom: secretKeyRef: name: secret1 key: username - name: PASSWORDTEST valueFrom: secretKeyRef: name: secret1 key: password volumeMounts: - name: test #donde montamos el secreto mountPath: \"/opt\" readOnly: true volumes: - name: test secret: secretName: secret1 Comprobamos: [isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml secret/secret1 created pod/mypod created [isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh / # ls /opt/ password username / # echo $USERTEST $PASSWORDTEST admin 123456","title":"ENV SECRETS"},{"location":"kubernetes/#volumes","text":"Sirven para persistir data de los container y no se pierdan cuando se borran. Tipos de volumenes: EMPTYDIR : es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola. HOSTPATH : nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo. CLOUDVOLS : en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. As\u00ed si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube. PV y PVC : es la reclamaci\u00f3n de un PV. El PV contiene un mount y un volume de origen. A trav\u00e9s del PVC accedemos al PV, reclamando los recursos que necesita, y \u00e9ste accede al cloud. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"VOLUMES"},{"location":"kubernetes/#emptydir","text":"Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde. Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que emptydir te crea un directorio a la altura del pod con la xixa del contenedor. Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa. Ejemplo: apiVersion: v1 kind: Pod metadata: name: pod-test2 spec: containers: - name: cont-emptydir image: nginx:alpine volumeMounts: - name: vol-emptydir mountPath: var/log/nginx volumes: - name: vol-emptydir emptyDir: {} Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml pod/pod-test2 created # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 0 5s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh # / # touch /var/log/nginx/empytdir.txt # / # ps aix PID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 29 nginx 0:00 nginx: worker process 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 root 0:00 sh 39 root 0:00 ps aix / # pkill nginx / # command terminated with exit code 137 # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 1/1 Running 1 47s # [isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh / # ls /var/log/nginx/ access.log empytdir.txt error.log / #","title":"EMPTYDIR"},{"location":"kubernetes/#hostpath-pv","text":"En el hostpath la carpeta con el contenido se guarda en altura de nodo. El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta. Ejemplo: apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv Comprobar con kubectl get pv y kubectl describe pv pvName : [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 18s type=local # [isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations: <none> Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Available Claim: Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /test HostPathType: Events: <none>","title":"HOSTPATH-PV"},{"location":"kubernetes/#pvc","text":"El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear. Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 5s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 5m14s El estado ahora del PV es bound que significa que se ha unido a un PVC.","title":"PVC"},{"location":"kubernetes/#pvc-pv","text":"Para unir un PVC a un PV concreto, se hace con selectors. Ejemplo: # PV apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/test\" # donde esta el storage real d mi pv --- # PV con selector para un PVC concreto apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume2 labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml persistentvolume/task-pv-volume created persistentvolume/task-pv-volume2 created persistentvolumeclaim/task-pv-claim created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume2 10Gi RWO manual 3s # [isx46410800@miguel volumes]$ kubectl get pv --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS task-pv-volume 10Gi RWO Retain Available manual 19s type=local task-pv-volume2 10Gi RWO Retain Bound default/task-pv-claim manual 19s mysql=ready Vemos que se ha unido el PV2 con el PVC como indicamos en los selector.","title":"PVC-PV"},{"location":"kubernetes/#pvc-pods","text":"De esta manera sin indicar en el POD los volumenes, no persiste la informaci\u00f3n. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendr\u00e1 esa base de datos: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" Ahora lo creamos con volumenes para que persista la data: # PV apiVersion: v1 kind: PersistentVolume metadata: name: test-pv labels: mysql: ready spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/mysql\" # donde esta el storage real d mi pv --- # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi selector: matchLabels: mysql: ready --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: mysql annotations: kubernetes.io/change-cause: \"new version nginx\" labels: app: mysql # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: mysql # aqui viene el pod template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: \"12345678\" volumeMounts: # montamos dentro del contenedor, lo que queremos guardar - mountPath: \"/var/lib/mysql\" name: vol-mysql volumes: - name: vol-mysql persistentVolumeClaim: claimName: test-pvc Comprobamos: [isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml persistentvolume/test-pv created persistentvolumeclaim/test-pvc created deployment.apps/mysql created # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound test-pv 10Gi RWO manual 7s # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE test-pv 10Gi RWO Retain Bound default/test-pvc manual 9s # [isx46410800@miguel volumes]$ kubectl get rs NAME DESIRED CURRENT READY AGE mysql-555cf6cd95 1 1 1 16s [isx46410800@miguel volumes]$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE mysql 1/1 1 1 19s # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 22s # [isx46410800@miguel volumes]$ kubectl describe pv test-pv Name: test-pv Labels: mysql=ready Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: manual Status: Bound Claim: default/test-pvc Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: <none> Message: Source: Type: HostPath (bare host directory volume) Path: /mysql HostPathType: Events: <none> Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro est\u00e1 la bbdd creada de antes: [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-nj8xd 1/1 Running 0 56m # [isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd pod \"mysql-555cf6cd95-nj8xd\" deleted # [isx46410800@miguel volumes]$ kubectl get pods NAME READY STATUS RESTARTS AGE mysql-555cf6cd95-6ns2n 1/1 Running 0 12s # [isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh # mysql -u root -p12345678 mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test | +--------------------+ 5 rows in set (0.00 sec) mysql>","title":"PVC-PODS"},{"location":"kubernetes/#cloud-volumes","text":"Son los storages que estan en la nube. Son de provisionamiento din\u00e1mico, no hace falta crear manualmente el PV para unirlo al PVC. Para verlos se usa kubectl get sc|storageclass , por defecto en minikube es el standard : [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h Creamos un PVC con cloud: # PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-pvc spec: #storageClassName: standard(por defecto) accessModes: - ReadWriteOnce resources: requests: storage: 20Gi Comprobamos: [isx46410800@miguel volumes]$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) k8s.io/minikube-hostpath Delete Immediate false 7d18h # [isx46410800@miguel volumes]$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sc-pvc Bound pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO standard 11s test-pvc Bound test-pv 10Gi RWO manual 67m # [isx46410800@miguel volumes]$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8 20Gi RWO Delete Bound default/sc-pvc standard 14s test-pv 10Gi RWO Retain Bound default/test-pvc manual 67m Crea dinamicamente un PV al PVC.","title":"CLOUD VOLUMES"},{"location":"kubernetes/#reclaim-policy","text":"Por defecto, si creamos un PVC manualmente es retain y si lo creamos dinamicamente es delete . Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro. Para cambiar el estado del reclaim policy se usa kubectl edit pv pvName y lo cambiamos a recycle. El kubectl edit cualquiercosa se pueda usar para editar la gran mayoria de cosas. RECLAIM : un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).","title":"RECLAIM POLICY"},{"location":"kubernetes/#usersgroups-rbac","text":"RBAC(Role Base Access Control) control basado en roles. Nos permite dar/crear ciertos permisos para usuarios mediante roles. En un role definimos reglas que se enlazar\u00e1n a usuarios para lo que puedan hacer en el cluster.","title":"USERS/GROUPS RBAC"},{"location":"kubernetes/#roles-vs-clusterroles","text":"En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace. El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podr\u00e1 conectar a todo.","title":"ROLES vs CLUSTERROLES"},{"location":"kubernetes/#rolebinding-vs-clusterrolebinding","text":"Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazar\u00e1n este role con el sujeto que lo utilizar\u00e1.","title":"ROLEBINDING vs CLUSTERROLEBINDING"},{"location":"kubernetes/#crear-users-groups","text":"Se basa en la autenticaci\u00f3n de certificados para la C.A(Certification Authority) de kubernetes. Se necesita: Creamos el certificado Creamos el file de petici\u00f3n de firma CSR. El CommonName y Organization ser\u00e1n el user y el group. La firma Kubectl PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP + Creamos las keys: openssl genrsa -out miguel.key 2048 Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O: openssl req -new -key miguel.key -out miguel.csr -subj \"/CN=miguel/O=dev\" Vemos nuestro CA con kubectl config view para poder firmar nuestro certificado: [isx46410800@miguel rbac]$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /home/isx46410800/.minikube/ca.crt server: https://172.17.0.2:8443 Lo firmamos: sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500 [sudo] password for isx46410800: Signature ok subject=CN = miguel, O = dev Getting CA Private Key Comprobamos el certificado: openssl x509 -in miguel.crt -noout -text [isx46410800@miguel rbac]$ openssl x509 -in miguel.crt -noout -text Certificate: Data: Version: 1 (0x0) Serial Number: a5:c7:06:8f:8f:4c:ec:4e Signature Algorithm: sha256WithRSAEncryption Issuer: CN = minikubeCA Validity Not Before: Oct 19 17:28:14 2020 GMT Not After : Mar 3 17:28:14 2022 GMT Subject: CN = miguel, O = dev PASOS: CREAMOS UN CONTAINER DE PRUEBA + Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a trav\u00e9s de mis credenciales y mis llaves/certificados: kubectl config view | grep server docker run --rm -ti -v $PWD:/test -w /test -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh Configuramos el kubectl con el usuario CN indicado(miguel): kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel Comprobamos lo creado con kubectl config view : /test # kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority: /ca.crt server: https://172.17.0.2:8443 name: minikube contexts: - context: cluster: minikube user: miguel name: miguel current-context: miguel kind: Config preferences: {} users: - name: miguel user: client-certificate: /test/miguel.crt client-key: /test/miguel.key # /test # kubectl config current-context miguel Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos: /test # kubectl get pods Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\"","title":"CREAR USERS &amp; GROUPS"},{"location":"kubernetes/#habilitar-rbac","text":"Vemos si est\u00e1: [isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho \"--authorization-mode=Node,RBAC\", Sino, lo habitamos as\u00ed: minikube start --vm-driver=none --extra-config=apiserver.authorization-mode=RBAC","title":"HABILITAR RBAC"},{"location":"kubernetes/#simplificamos-contexto","text":"Ahora lo hacemos en real y as\u00ed simplificamos trabajo y ordenes en nuestro contexto creado: kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key kubectl config set-context miguel --cluster=minikube --user=miguel kubectl config use-context miguel","title":"SIMPLIFICAMOS CONTEXTO"},{"location":"kubernetes/#crear-roles","text":"Ejemplo: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones Comprobamos kubectl get roles : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader created # [isx46410800@miguel rbac]$ kubectl get roles -n default NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default Name: pod-reader Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list] No podemos hacer con el usuario miguel kubectl get pods porque todavia no est\u00e1 enlazado el role con el user.","title":"CREAR ROLES"},{"location":"kubernetes/#enlazar-role-user","text":"Para ver el tipo de api groups recordamos que es mirando kubectl api-resources Verbs o acciones que se pueden hacer: GET LIST WATCH DELETE UPDATE PATCH Hacemos el RoleBinding de enlazar el role con el user creado: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos lo creado con kubectl get rolebinding : [isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml role.rbac.authorization.k8s.io/pod-reader unchanged rolebinding.rbac.authorization.k8s.io/read-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-pods Role/pod-reader 21s # [isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods Name: read-pods Labels: <none> Annotations: <none> Role: Kind: Role Name: pod-reader Subjects: Kind Name Namespace ---- ---- --------- User miguel Comprobamos ahora con el usuario miguel s\u00ed puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso s\u00ed, unicamente en el namespace por default que fue el que indicamos: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci Error from server (Forbidden): pods is forbidden: User \"miguel\" cannot list resource \"pods\" in API group \"\" in the namespace \"ci\" # [isx46410800@miguel rbac]$ kubectl get rs Error from server (Forbidden): replicasets.apps is forbidden: User \"miguel\" cannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" Ahora creamos otro role con que tambi\u00e9n podamos ver deploys. Para ver el tipo de api groups recordamos que es mirando kubectl api-resources : # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-deploy-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones - apiGroups: [\"apps\"] # \"\" indicates the core API group resources: [\"deployments\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-deploy-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml role.rbac.authorization.k8s.io/pod-deploy-reader created rolebinding.rbac.authorization.k8s.io/read-deploy-pods created # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get rolebinding NAME ROLE AGE read-deploy-pods Role/pod-deploy-reader 14s read-pods Role/pod-reader 10m # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get deploy No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get svc Error from server (Forbidden): services is forbidden: User \"miguel\" cannot list resource \"services\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml Error from server (Forbidden): error when creating \"../pods/pod-2containers.yaml\": pods is forbidden: User \"miguel\" cannot create resource \"pods\" in API group \"\" in the namespace \"default\"","title":"ENLAZAR ROLE &amp; USER"},{"location":"kubernetes/#config-maps","text":"Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos: # CREAR NAMESPACE apiVersion: v1 kind: Namespace metadata: name: dev labels: name: dev --- # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: cm-role #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"configmaps\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cm-role namespace: dev subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io --- # CREAR CONFIGMAP apiVersion: v1 kind: ConfigMap metadata: namespace: dev name: vars labels: app: front data: db_host: dev.host.local db_user: dev_user Comprobamos resultados: [isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml namespace/dev created role.rbac.authorization.k8s.io/cm-role created rolebinding.rbac.authorization.k8s.io/cm-role created configmap/vars created # [isx46410800@miguel rbac]$ kubectl get roles -n dev NAME CREATED AT cm-role 2020-10-19T18:35:07Z # [isx46410800@miguel rbac]$ kubectl get rolebinding -n dev NAME ROLE AGE cm-role Role/cm-role 27s # [isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- configmaps [] [] [get watch list] # [isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev Name: cm-role Labels: <none> Annotations: <none> Role: Kind: Role Name: cm-role Subjects: Kind Name Namespace ---- ---- --------- User miguel # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 43s Como usuario miguel: [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get cm Error from server (Forbidden): configmaps is forbidden: User \"miguel\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 2m50s # [isx46410800@miguel rbac]$ kubectl edit cm vars Error from server (Forbidden): configmaps \"vars\" is forbidden: User \"miguel\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"default\" # [isx46410800@miguel rbac]$ kubectl edit cm vars -n dev error: configmaps \"vars\" could not be patched: configmaps \"vars\" is forbidden: User \"miguel\" cannot patch resource \"configmaps\" in API group \"\" in the namespace \"dev\" You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again.","title":"CONFIG MAPS"},{"location":"kubernetes/#crear-clusterole","text":"Creamos un clusterRole teniendo en cuenta que aqu\u00ed no se ponen namespaces: # CREAR ROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-pod-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-pod-reader subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos con el usuario miguel: [isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml role.rbac.authorization.k8s.io/cluster-pod-reader created rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n ci No resources found in ci namespace. # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-lgrd4 1/1 Running 0 49m etcd-minikube 1/1 Running 0 49m kube-apiserver-minikube 1/1 Running 0 49m kube-controller-manager-minikube 1/1 Running 0 49m kube-proxy-22t6g 1/1 Running 0 49m kube-scheduler-minikube 1/1 Running 0 49m storage-provisioner 1/1 Running 0 50m","title":"CREAR CLUSTEROLE"},{"location":"kubernetes/#crear-user-admin","text":"Miramos los clusteroles que hay con kubectl get clusterroles y vemos el de cluster-admin : [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z system:aggregate-to-admin 2020-10-19T18:00:44Z system:aggregate-to-edit 2020-10-19T18:00:44Z system:aggregate-to-view 2020-10-19T18:00:44Z Creamos un cluster-admin enlazando solo al usuario miguel al grupo: # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-miguel subjects: # You can specify more than one \"subject\" - kind: User name: miguel # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo: [isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get pods No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm No resources found in default namespace. # [isx46410800@miguel rbac]$ kubectl get cm -n dev NAME DATA AGE vars 2 24m # [isx46410800@miguel rbac]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 59m","title":"CREAR USER ADMIN"},{"location":"kubernetes/#roles-a-grupos","text":"Veremos como crear un grupo y como asignar roles a grupos. Creamos un nuevo usuario como miguel pero ahora como juan: [isx46410800@miguel rbac]$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ci-context minikube minikube ci juan minikube juan miguel minikube miguel * minikube minikube minikube Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios: # CREAR CLUSTERROLE apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: svc-clusterrole #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"services\"] #objetos verbs: [\"*\"] # acciones --- # CLUSTERBINDING-ENLAZAR ROLE-USER apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-svc subjects: # You can specify more than one \"subject\" - kind: Group name: dev # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: ClusterRole #this must be Role or ClusterRole name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos como juan y miguel podemos ver todo de services: [isx46410800@miguel rbac]$ kubectl get clusterroles NAME CREATED AT admin 2020-10-19T18:00:44Z cluster-admin 2020-10-19T18:00:44Z cluster-pod-reader 2020-10-19T18:50:22Z edit 2020-10-19T18:00:44Z kubeadm:get-nodes 2020-10-19T18:00:48Z svc-clusterrole 2020-10-19T19:09:44Z # [isx46410800@miguel rbac]$ kubectl config use-context juan Switched to context \"juan\". # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl config use-context miguel Switched to context \"miguel\". # [isx46410800@miguel rbac]$ kubectl get svc -n dev No resources found in dev namespace. # [isx46410800@miguel rbac]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 70m","title":"ROLES A GRUPOS"},{"location":"kubernetes/#services-account","text":"Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account. El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petici\u00f3n. Todos los namespaces tienen un service account por defecto. Lo podemos ver con kubectl get serviceaccount : [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h [isx46410800@miguel services_account]$ kubectl get serviceaccount -n default NAME SECRETS AGE default 1 23h Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace: [isx46410800@miguel services_account]$ kubectl describe sa default Name: default Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: default-token-6ccpr Tokens: default-token-6ccpr Events: <none> [isx46410800@miguel services_account]$ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-10-19T18:00:54Z\" name: default namespace: default resourceVersion: \"346\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b secrets: - name: default-token-6ccpr","title":"SERVICES ACCOUNT"},{"location":"kubernetes/#secret-sa","text":"Vemos que el token de un SA es un secreto y lo podemos investigar kubectl get secret TOKEN : [isx46410800@miguel services_account]$ kubectl get secret NAME TYPE DATA AGE default-token-6ccpr kubernetes.io/service-account-token 3 23h [isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml ... El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc","title":"SECRET SA"},{"location":"kubernetes/#crear-sa","text":"Ejemplo de crear un service account: apiVersion: v1 kind: ServiceAccount metadata: name: my-sa Comprobamos: [isx46410800@miguel services_account]$ kubectl apply -f sa.yaml serviceaccount/my-sa created [isx46410800@miguel services_account]$ kubectl get serviceaccount NAME SECRETS AGE default 1 23h my-sa 1 6s [isx46410800@miguel services_account]$ kubectl describe sa my-sa Name: my-sa Namespace: default Labels: <none> Annotations: <none> Image pull secrets: <none> Mountable secrets: my-sa-token-5lv4s Tokens: my-sa-token-5lv4s Events: <none> [isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml apiVersion: v1 kind: ServiceAccount secrets: - name: my-sa-token-5lv4s","title":"CREAR SA"},{"location":"kubernetes/#relacion-pod-sa","text":"Cuando creamos un pod sin especificar un SA, se asigna al por defecto: [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml pod/pod-test2 created [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE pod-test2 2/2 Running 0 29s [isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-6ccpr secret: defaultMode: 420 secretName: default-token-6ccpr Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/","title":"RELACION POD-SA"},{"location":"kubernetes/#requests","text":"A trav\u00e9s del servicio de kubernetes podemos llamar a objetos a trav\u00e9s de la api de kubernetes sin pasar por el comando kubectl: [isx46410800@miguel services_account]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 23h Podemos hacer request a la api con esta DOCS como por ejemplo querer listar los pods del namespace por defecto: /api/v1/namespaces/{namespace}/pods/{name} [isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml pod/pod-test2 created pod/pod-test3 created [isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl /api/v1/namespaces/default/pods curl: (3) URL using bad/illegal format or missing URL / # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:anonymous\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 }/ # nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticaci\u00f3n para poder hacer estas acciones.","title":"REQUESTS"},{"location":"kubernetes/#request-jwt","text":"Peticiones Jason Web Token autenticadas con el token/secret del service account. Dentro del pod podemos encontrar la info del SA y su token en: /var/run/secrets/kubernetes.io/serviceaccount/ Guardamos el token del POD en una variable: # TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) DOCS : / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1 --insecure Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante.","title":"REQUEST JWT"},{"location":"kubernetes/#sa-deployment","text":"Ejemplo de crear un deploy asignando un service account creado: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 15s [isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: my-sa-token-5lv4s readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: minikube preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: my-sa serviceAccountName: my-sa","title":"SA DEPLOYMENT"},{"location":"kubernetes/#role-sa","text":"Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados: # CREAMOS SERVICE ACCOUNT apiVersion: v1 kind: ServiceAccount metadata: name: my-sa --- # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: front # aqui viene el replicaset spec: replicas: 1 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: serviceAccountName: my-sa containers: - name: nginx image: nginx:alpine --- # CREAR ROLE SA apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: sa-reader #nombre role rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] #objetos verbs: [\"get\", \"watch\", \"list\"] # acciones --- # ROLEBINDING-ENLAZAR ROLE-SA apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: sa-pods namespace: default subjects: # You can specify more than one \"subject\" - kind: ServiceAccount name: my-sa # \"name\" is case sensitive apiGroup: roleRef: # \"roleRef\" specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io Comprobamos: [isx46410800@miguel services_account]$ kubectl get pods NAME READY STATUS RESTARTS AGE test-7bb9d96578-v6x5m 1/1 Running 0 12m [isx46410800@miguel services_account]$ kubectl get roles NAME CREATED AT pod-deploy-reader 2020-10-19T18:20:23Z pod-reader 2020-10-19T18:01:37Z sa-reader 2020-10-20T18:05:58Z [isx46410800@miguel services_account]$ kubectl get rolebinding NAME ROLE AGE cluster-pod-reader ClusterRole/cluster-pod-reader 23h read-deploy-pods Role/pod-deploy-reader 23h read-pods Role/pod-reader 23h sa-pods Role/sa-reader 3m39s [isx46410800@miguel services_account]$ kubectl get sa NAME SECRETS AGE default 1 24h my-sa 1 44m Comprobamos que ahora entramos al POD y podemos comunicarnos a trav\u00e9s de la api con JWT para listar los pods del namespace: / # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/api/v1/namespaces/default/pods --insecure \"hostIP\": \"172.17.0.2\", \"podIP\": \"172.18.0.3\", \"podIPs\": [ { \"ip\": \"172.18.0.3\" } ], \"startTime\": \"2020-10-20T17:56:32Z\", Si a\u00f1adimos el permiso de ver tambien deployments despues hariamos: # curl -H \"Authorization: Bearer ${TOKEN}\" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure \"restartPolicy\": \"Always\", \"terminationGracePeriodSeconds\": 30, \"dnsPolicy\": \"ClusterFirst\", \"serviceAccountName\": \"my-sa\", \"serviceAccount\": \"my-sa\", \"securityContext\": { \"status\": { \"observedGeneration\": 1, \"replicas\": 1, \"updatedReplicas\": 1, \"readyReplicas\": 1, \"availableReplicas\": 1,","title":"ROLE SA"},{"location":"kubernetes/#ingress","text":"Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios. Crea unas reglas en esta entrada redireccionando cada petici\u00f3n por el servicio que le toca. Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios. Tambi\u00e9n se puede crear reglas de DNS, IPs, servicios...que se definen en un \u00fanico punto de entrada.","title":"INGRESS"},{"location":"kubernetes/#ingress-controller","text":"Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller. Normalmente est\u00e1 en un deployment que apunta a este ingress para leer las reglas. Puede ser de dos tipos: nginx o cloud. Nginx define un node port para las peticiones del usuario y despu\u00e9s leer las reglas del ingress. Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud.","title":"INGRESS CONTROLLER"},{"location":"kubernetes/#crear-ingress-controller","text":"Documentacion Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando: [isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE nginx-ingress-controller-54b86f8f7b-s7vzl 1/1 Running 0 81s Creamos el servicio de ingress-controller nginx de tipo node-port: apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx Comprobamos que funciona: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 29s","title":"CREAR INGRESS CONTROLLER"},{"location":"kubernetes/#ip-ingress-controller","text":"Ip del cluster: Kubernetes master is running at https://172.17.0.2:8443 KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Ip del servicio node-port del IController Nginx: [isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.109.126.218 <none> 80:30540/TCP,443:32602/TCP 4m43s Obtenemos la url con la ip para conectarnos: [isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx http://172.17.0.2:30540 http://172.17.0.2:32602","title":"IP INGRESS CONTROLLER"},{"location":"kubernetes/#app-ingress-controller","text":"Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy labels: app: front # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: front # aqui viene el pod template: metadata: labels: app: front spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo VERSION 1.0 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: app-v1-svc labels: app: front spec: type: ClusterIP selector: app: front ports: - protocol: TCP port: 8080 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos: # a\u00f1adimos el servicio que observar\u00e1 los FRONT apiVersion: v1 kind: Service metadata: name: my-service labels: app: front spec: selector: app: front ports: - protocol: TCP port: 8888 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecuci\u00f3n del index.html: [isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh / # apk add curl fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz OK: 25 MiB in 42 packages / # curl app-v1-svc:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv / # curl 10.96.97.25:8080 VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv","title":"APP INGRESS-CONTROLLER"},{"location":"kubernetes/#exponer-el-puerto-al-exterior","text":"Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 Comprobamos que ahora con la url(ip/appv1) vemos tambi\u00e9n la respuesta al servicio: Podemos a\u00f1adirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts 172.17.0.2 app1.mydomain.com : apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080","title":"EXPONER EL PUERTO AL EXTERIOR"},{"location":"kubernetes/#2-apps-en-ic","text":"Ejemplo: # esto es del deployment apiVersion: apps/v1 kind: Deployment metadata: name: ingress-deploy2 labels: app: backend # aqui viene el replicaset spec: replicas: 3 selector: matchLabels: app: backend # aqui viene el pod template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:alpine command: [\"sh\",\"-c\", \"echo Soy app2 desde $HOSTNAME > /usr/share/nginx/html/index.html && nginx -g 'daemon off;'\"] --- # a\u00f1adimos el servicio que observar\u00e1 los backend apiVersion: v1 kind: Service metadata: name: app2-v1-svc labels: app: backend spec: type: ClusterIP selector: app: backend ports: - protocol: TCP port: 9090 # servicio por donde escucha targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80) Comprobamos que funcionan: [isx46410800@miguel ingress]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-v1-svc ClusterIP 10.96.97.25 <none> 8080/TCP 34m app2-v1-svc ClusterIP 10.106.106.71 <none> 9090/TCP 10s kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 30h [isx46410800@miguel ingress]$ kubectl get pods NAME READY STATUS RESTARTS AGE ingress-deploy-7cd6549d66-26cwb 1/1 Running 0 34m ingress-deploy-7cd6549d66-9b9d4 1/1 Running 0 34m ingress-deploy-7cd6549d66-ncjpv 1/1 Running 0 34m ingress-deploy2-69fcf646dd-m8zn4 1/1 Running 0 13s ingress-deploy2-69fcf646dd-nnn89 1/1 Running 0 13s ingress-deploy2-69fcf646dd-xq977 1/1 Running 0 13s Agregamos nueva regla para la app2: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /appv1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 - host: app2.mydomain.com http: paths: - path: /appv2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos: Ahora cambiando varios paths: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: app1.mydomain.com http: paths: - path: /myservice1 backend: serviceName: app-v1-svc # nombre del servicio de la app servicePort: 8080 paths: - path: /myservice2 backend: serviceName: app2-v1-svc # nombre del servicio de la app servicePort: 9090 Comprobamos:","title":"2 APPS EN IC"},{"location":"kubernetes/#aws-kubernetes","text":"Tenemos que crear cuenta en AWS. Instalar pip3 de python. Tenemos que instalar la herramienta AWS CLI: pip3 install -U awscli Comprobamos la version: [isx46410800@miguel ingress]$ aws --version aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0 Creamos un usuario administrador en IAM de AWS. COnfiguramos en nuestra m\u00e1quina real el AWS con el usuario creado: [isx46410800@miguel ingress]$ aws configure AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM AWS Secret Access Key [None]: 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 Default region name [None]: eu-west-2 Default output format [None]: Nos crea un home de AWS en nuestro home: [isx46410800@miguel .aws]$ pwd /home/isx46410800/.aws Testeamos con una petici\u00f3n para saber quien es el que hace el request: [isx46410800@miguel .aws]$ aws sts get-caller-identity { \"UserId\": \"AIDA5RIFOUI3IP6OESXCW\", \"Account\": \"930408735286\", \"Arn\": \"arn:aws:iam::930408735286:user/miguel\" } Instalamos la herramienta eksctl que es para gestionar los cluster de kubernetes en AWS: [isx46410800@miguel .aws]$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp [isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin [isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl [isx46410800@miguel .aws]$ eksctl version 0.30.0","title":"AWS KUBERNETES"},{"location":"kubernetes/#crear-cluster-aws-eksctl","text":"","title":"CREAR CLUSTER AWS EKSCTL"},{"location":"linux/","text":"Comandos LINUX Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd","title":"Linux"},{"location":"linux/#comandos-linux","text":"Hacer un listado: ls -la Manual de un comando(1-ordenes, 5-ficheros, 8-admin): man comando Ayuda de un comando: comando --help Crear/ver particiones: fdisk fdisck /dev/sda0 Editor: vim file.txt Ver un archivo: cat file.txt Montar algo: mount mount -t type device dir #mount -t ext4 /dev/sda5 /mnt mount /dir Montar todo lo que tenemos para montar: mount -a Ver tipo de cosas montadas o si est\u00e1 montado algo: mount -t ext4 Cambiar directorio: cd dir cd .. cd dir/file.txt cd /var/tmp Ver path de donde estoy: pwd Crear directorio: mkdir dir mkdir -p /dir1/dir2/dir2/ Borrar directorio(vac\u00edo): rmdir dir Borrar dir/ficheros: rm -rf dir/file Buscar una cadena, palabra..: grep [opciones] [el qu\u00e9] [donde] #grep -i web install.txt Fecha/hora: date Calendario: cal cal 3 2020 Informaci\u00f3n de nuestro usuario: who Indica el usuario: whoami Informaci\u00f3n de la sesi\u00f3n: w Cual es el S.O.: uname -a Tiempo de la sesi\u00f3n: uptime Cual es nuestro host: hostname Info de los usuarios del sistema: finger Numero identificaci\u00f3n del usuario en el sistema: id Ejecutable y man de un comando: whereis comando Lo que hace el ejecutable de un comando: which comando Buscar un fichero o algo de esa palabra en el sistema: locate palabra Primeras o ultimas 10 lineas de un fichero o busqueda: head -n10 /etc/passwd tail -n10 /etc/group Ver procesos en tiempo real: top htop Tipo de fichero: file Contar lineas de un archivo: nl file.txt Dar un numero aleatorio de un rango de numeros: shuf -i 10-20 -n 1 Texto que imprime o carga el kernel: dmesg Procesos: ps ps -u isx46410800 ps -ax ps -p n\u00baproces #indica cual es el proceso pidof nameproceso #pids de este proceso kill proceso kill -n\u00ba proceso killall proceso kill -l #9 mata #15 termina #19 para jobs kill %job ordre & #hacerlo en backgroung fg %job #hacerlo en foreground nohup orden & #desliga un proceso de la terminal disown %job # lo mismo Contar palabras, lineas... wc wc -l wc -c Ver estructura de \u00e1rbol de directorios: tree Copiar ficheros: cp [cosas..] [a donde] cp -r [dir/(cosas)] [a donde] Cambiar nombre de fichero o directorio: mv nombre nuevonombre Meter cosas en ficheros: echo \"hola\" > file.txt cat > file.txt ls -la > file.txt Pathname Expansion: * puede ser nada o muchas cosas ls *.txt ? cada ? es un char ls ???.* [25] coge 2 o 5 ls fit[25].txt [1-4] coge un char del 1 al 4 ls fit[-4].txt [7am7-8][0-9] coge un char del primero y otro del segundo ls fit[7am7-8][0-9].txt # fita8.txt [^abc] que no sea ni a ni b ni c ls fit[^abc].txt Info t\u00e9cnica de un file: stat file.txt Ver inodos: ls -i Crear hard link(no entre dirs ni entre file system diferentes, tama\u00f1o mismo): ln [de que cosa] [hacia donde cosa nueva] # ln file.txt /tmp/filenou.txt (mismo inodo apuntan, misma xixa) Crear simbolic link(equivale a un acceso directo, tama\u00f1o es el nombre): ln -s [de que cosa] [simbolic link creas nuevo] # ln -s file.txt file2.txt Renombre de muchos archivos: rename [donde dice tal cosa] [poner tal cosa] [a estos ficheros] # rename foo foo0 foo* Comparar ficheros: cmp/diff/diff3 file1 file2 Separar ficheros: split -n3/-b10k file prefijo Fichero ejecutable: chmod +x file Comprimir/descomprimir ficheros: gzip file -> file.gz gunzip file.gz bzip2 file -> file.bzp2 bunzip2 file.bzp2 Apagar o cerrar sesi\u00f3n: exit/poweroff/reboot/logout Instalar un paquete: dnf install paquete -y Buscar un paquete: dnf search paquete Donde esta el paquete: dnf provides paquete Lista contenido de un paquete: rpm -ql paquete | grep bin #busca los ejecutables Lista de paquetes instalados: rpm -qa Acciones con paquetes: dnf upgrade/update/reinstall/info paquete Repositorios: dnf repolist --all Permisos(r-leer,mirar,copiar/w-leer,modificar/x-ejecutable): chmod 640 file/dir chmod +rx file/dir Cambiar el propietario de un file/dir(root): chown user.group file/dir Cambiar el grupo de un file/dir: chgrp grupo file Agregar usuario: useradd usuario useradd usuario -g gprincipal -G gsecundario Contrase\u00f1a usuario: passwd usuario Crear grupo: groupadd grupo Borrar usuario y todo suyo: userdel -r usuario Redireccionamientos: 0 - stdin 1 - stdout 2 - stderr 2> salida de errores < entrada > salida 2>&1 donde esta la salida de errores rederiger a la stdout Traducir: tr -s '[a-z]' '[A-Z]' tr -s '[:blanck:]' ' ' Ver espacio ocupado en disco: du / du -sh /tmp Ver variables predefinidas del sistema: set Crear/eliminar variables, mayus SISTEMA, minus USUARIO: nom=valor nom=\"el valor\" usuario=$(id) unset nom Crear subbash: bash Arbol de procesos: pstree Exportar variable a otros niveles ENVIROMENT: export variable Crear alias: alias listar='ls -la' alias quiensoy='id;whoami' unalias listar Command substitution: $(orden) # file $(ls) Brace expansion: mkdir dir{1..20} echo hisx{1,2}-{01-20} Aritmetic expansion: echo $((2*8)) Cron o tareas programadas: at 9:12 --> >cal, date.. #crear tarea programada atq # lista de tareas atrm # borra tareas cron (file /etc/crontab) (min-horas-dia-mes-diasemana(0-7)-ordre) 15 14 1 * * script.sh crontab -l #lista crontab -e #crea o edita crontab -r #borra Ordenar: sort sort -r sort -t: -k3 /etc/passwd #campo 3 sort -t: -k3rg,3 /etc/passwd #descente y de numeric sort -u #unico Lista hardware: lshw Hora del hardware clock: hwclock Ordens grub: grub2-install /dev/sda grub2-mkconfig -o /boot/grub2/grub.cfg Ordenes en debian: apt-get install paquete dpkg -i paquete Compresi\u00f3n de archivos: tar -cvf nombreTar archivosAcomprimir -c crea -v verbose -x descomprimir -f nombre archivo -p permisos para dir tar -zcvf nombreGZIP ficheros tar -jcvf nombreBZIP2 ficheros tar -Jcvf nombreXZ ficheros Backup: tar --listed-incremental fichero.snar -czpf fichero-incremental.tar.gz directorio/. SSH: systemctl start sshd ssh hostname/ip #conectarte ssh user@server -P puerto ssh -p 22 i03/0.0.0.0 #conectarte ssh-keygen #crea llaves ssh -P puerto [fichero] [ip:a donde/.] #copiar fichero scp user@server:file user@server:/dirdestino scp origen destino ssh-copy-id user@ip #copia mi publica a ese ip con ese usuario FTP: ftp ip/host get file put filecopy filedesti wget schema://host-uri-ip/ruta-files #wget ftp://user10@localhost/file.txt Routing: ifconfig ip a ip r nslookup host/web netstat -putano ping -c3 web/ip nmap ip/localhost/host #ver puertos abiertos telnet gost/ip puerto #GET / HTTP/1.0 Netcat conectar: nc -l puerto #conectarte ponte el tuyo a escuchar nc hostname puerto #conectarse al puerto tuyo Activar servicios: systemctl start/stop/enable/disable servicio Cargar de nuevo los demonios: systemctl daemon-reload Culpa de lo que tarda cada cosa al encenderse: systemd-analyse blame Ver errores del sistema: journalctl / journalctl -u servicio Cargar las cosas montables: exportfs -rv SAMBA: meter lo compartido en smb.conf //server/recurso smbtree -L #lista smbtree -D #ver el dominio smbtree -S #ver el servicio smbclient //j17/manuals (-U marta) smbget smb://localhost/manuals/man1/ls mount -t -v cifs //localhost/manuals /mnt -o guest smbpasswd -a miguel #a\u00f1ade user samba pdbeddit -L #lista de cuentas samba MAIL: mail -v -s asunto aquien sendmail -bv user mailq Conectarte a AMAZON AWS: ssh -i ~/ssh/key.pem fedora@IPamazon Servicios mas comunes: /sbin/httpd /sbin/sshd","title":"Comandos LINUX"},{"location":"markdown/","text":"Comandos lenguaje MARKDOWN T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea","title":"Markdown"},{"location":"markdown/#comandos-lenguaje-markdown","text":"T\u00edtulos: #,##... Underlines: ------/==== Negrita: **/__ Cursiva: */_ Tachado ~~ Lista normal: */- Lista numerada: 1. / 2. ... L\u00edneas para encabezado despu\u00e9s de t\u00edtulo: ===/--- Notas: > C\u00f3digo de bloque: `` Bloque de texto: ```tipo_lenguaje L\u00edneas de separaci\u00f3n: ***/---/___ Link: titulo[]()web Link autom\u00e1tico: <> Imagen: titulo![]()ruta imagen Tablas: | letra | letra / --- linea","title":"Comandos lenguaje MARKDOWN"},{"location":"python/","text":"Programaci\u00f3n PYTHON Shebang # !/usr/bin/python3 # -*-coding: utf-8-*- Server Python python -m SimpleHTTPDServer 80 Mostrar algo print(\"Uso windows\") Mostrar algo con formato print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}') Importar librerias import platform from xxxx import xx Variables x=100 y=True z=\"Miguel\" Condicional x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\") Bucle for (iterar elementos) food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i) Bucle while food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1 Funciones def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2 Test main if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x) Objetos class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2() Mayusculas, minusculas, letra capital mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize() Ver tipo de dato print(type(w)) #float Listas x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print() Tuplas t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e) Diccionarios dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}') Rangos r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e) List Comprension # de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2) Len len(*args/lista) Objetos # definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\")) Ficheros Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo M\u00f3dulos import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day)","title":"Python"},{"location":"python/#programacion-python","text":"","title":"Programaci\u00f3n PYTHON"},{"location":"python/#shebang","text":"# !/usr/bin/python3 # -*-coding: utf-8-*-","title":"Shebang"},{"location":"python/#server-python","text":"python -m SimpleHTTPDServer 80","title":"Server Python"},{"location":"python/#mostrar-algo","text":"print(\"Uso windows\")","title":"Mostrar algo"},{"location":"python/#mostrar-algo-con-formato","text":"print(\"Hello World, my name is {}\" .format(name)) print(f'My Python version is {version}')","title":"Mostrar algo con formato"},{"location":"python/#importar-librerias","text":"import platform from xxxx import xx","title":"Importar librerias"},{"location":"python/#variables","text":"x=100 y=True z=\"Miguel\"","title":"Variables"},{"location":"python/#condicional","text":"x = 25 y = 15 if x > y: print(\"x is greater than y: where x is {} and y {}\" .format(x,y)) elif x == y: print(\"x and y are the same: where x is {} and y {}\" .format(x,y)) else: print(\"x is lesser than y: where x is {} and y {}\" .format(x,y)) tengoHambre = True y = \"Necesito comer\" if tengoHambre else \"solo necesito beber\" a = True b = False # comparamos a y b if a and b: print('Expresiones son TRUE') else: print(\"Expresiones son False\")","title":"Condicional"},{"location":"python/#bucle-for-iterar-elementos","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] for i in food: print(i)","title":"Bucle for (iterar elementos)"},{"location":"python/#bucle-while","text":"food = [\"breakfast\", \"lunch\", \"snack\", \"dinner\"] while m < 4: print(food[m]) m +=1","title":"Bucle while"},{"location":"python/#funciones","text":"def message(): print(\"Mi version de python es la {}\" .format(platform.python_version())) message() def operation(n=25): print(n) return n*2","title":"Funciones"},{"location":"python/#test-main","text":"if __name__ == \"__main__\": runMe() name(\"Miguel\") name(2) print(x)","title":"Test main"},{"location":"python/#objetos","text":"class Time: h = \"horas\" m = \"minutos\" s = \"segundos\" def hours(self): print(self.h) def minuts(self): print(self.m) def seconds(self): print(self.s) def main2(): how_time = Time() how_time.hours() how_time.minuts() how_time.seconds() main2()","title":"Objetos"},{"location":"python/#mayusculas-minusculas-letra-capital","text":"mayus = \"hello world\".upper() minus = \"hello world\".lower() capi = \"hello world\".capitalize()","title":"Mayusculas, minusculas, letra capital"},{"location":"python/#ver-tipo-de-dato","text":"print(type(w)) #float","title":"Ver tipo de dato"},{"location":"python/#listas","text":"x = [1,2,3,4] print(x[2]) x[2] = 10 for i in x: print(i) # LISTA ACCIONES -- TUPLAS SON INMUTABLES Y NO SE PUEDE def main(): lista = ['perro', 'gato', 'cerdo', 'caballo'] lista2 = ['perro', 'gato', 'cerdo', 'caballo'] print(lista[1]) # gato print(lista[1:3]) # gato, cerdo print(lista[0:5:2]) # perro, cerdo print(lista.index('gato')) # 1, busca la posicion de esa palabra lista.append('koala') # a\u00f1ade koala lista.insert(0, 'vaca') # a\u00f1ade vaca en posicion 0 lista.remove(\"vaca\") # borra de la lista vaca lista.pop() # borra el ultimo elemento de la lista lista.pop(1) # borra esa posicion de la lista del lista[1] # borra de la lista ese elemento del lista[0:1] # borra ese slicing print(len(lista)) # cuenta en numero de elementos de la lista lista.extend(lista2) # junta dos listas print_lista(lista) # funcion de iterar la lista # funcion para iterar la lista pasada por argumento def print_lista(lista): for i in lista: print(i, end=' ', flush=True) print()","title":"Listas"},{"location":"python/#tuplas","text":"t = (1,2,3,4,5) # cosas con tuplas ## t[2] = 10 NO SE PUEDE ASIGNAR PARA CAMBIAR print(t[2]) for e in t: print(e)","title":"Tuplas"},{"location":"python/#diccionarios","text":"dic = { 'x' : 5, 'y' : 'miguel', 'z' : False } # cosas con diccionarios print(dic['y']) dic['y'] = 'miguelito' for id, valor in dic.items(): print(f\"id: {id} valor: {valor}\") for e in dic.values(): print(e) for e in dic: print(f'el id es {e}') print(f'el valor es {dic[e]}') gente = {'1': \"miguel\", '2': \"cristina\", '3': \"isabel\"} gente['4'] = 'maria' for k in gente: print(k) for k,v in gente.items(): print(f'key: {k} valor: {v}') for k in gente.keys(): print(f'key: {k}') for v in gente.values(): print(f'valor: {v}')","title":"Diccionarios"},{"location":"python/#rangos","text":"r = range(5) # no se puede asignar sino es con una lista ra = list(range(5)) ra[2] = 20 rang = range(5,10,2) # del 5 al 10 de dos en dos # cosas con rangos for e in ra: print(e) for e in rang: print(e)","title":"Rangos"},{"location":"python/#list-comprension","text":"# de una lista lista = range(11) tupla = ((0,1),(1,2),(2,3)) # creas una lista,tupla iterando lista y operaciones lista2 = [ x * 2 for x in lista] tupla2 = [ (y*2, x*2) for x,y in tupla] # resultados print(lista2) print(tupla2)","title":"List Comprension"},{"location":"python/#len","text":"len(*args/lista)","title":"Len"},{"location":"python/#objetos_1","text":"# definimos una clase class mobile: #definimos unas variables con contenido old_phone = \"keypad\" new_phone = \"touch screen\" # definimos funciones que printes esas variables def old_mobile(self): print(self.old_phone) def new_mobile(self): print(self.new_phone) # creamos funcion,variable con objeto y sus dos partes de funciones def main(): x = mobile() x.old_mobile() x.new_mobile() class Animal: def __init__(self, type, name, sound): self._type = type self._name = name self._sound = sound def type(self): return self._type def name(self): return self._name def sound(self): return self._sound def print_animal(x): if not isinstance(x, Animal): raise TypeError(\"error, requiere un animal\") print(f'El {x.type()} se llama {x.name()} y dice {x.sound()}') # le pasamos a la funcion de hacer algo, los argumentos al objeto def main(): print_animal(Animal(\"Kitten\", \"Fluffly\", \"Meow\")) print_animal(Animal(\"Duck\", \"Donald\", \"Quak\"))","title":"Objetos"},{"location":"python/#ficheros","text":"Leer def main(): file = open('lines.txt', 'r') # file = open('lines.txt', 'r') # read only # file = open('lines.txt', 'w') # write only (empties files) # file = open('lines.txt', 'a') # a\u00f1adir data in files # file = open('lines.txt', 'r+') # optional + read or write for line in file: print(line.rstrip()) #rstrip elimina espacios o lo que se ponga en () Escribir def main(): fileInput = open('lines.txt', 'rt') # r read t text fileOutput = open('linesOutput.txt', 'wt') # w write t text for line in fileInput: print(line.rstrip(), file=fileOutput) # cada linea sin blancos la envia al nuevo file print('.', end='', flush=True) # aqui solo printa esto por cada linea leida fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo Copiar def main(): fileInput = open('cat.jpg', 'rb') # r read b binario fileOutput = open('cat_copy.jpg', 'wb') # w write b binario # mientras todo se pueda while True: # leemos datos y lo metemos en un buffer buffer = fileInput.read(102400) # mientras haya buffer por leer if buffer: # copiamos del buffer en el file nuevo fileOutput.write(buffer) print('.', end='', flush=True) # aqui solo printa esto por cada linea leida else: break fileOutput.close() # cierra el doc nuevo print('\\nDone.') # printa que se ha realizado todo","title":"Ficheros"},{"location":"python/#modulos","text":"import os, datetime, sys def main(): # modulo de system v = sys.version_info print('Mi version es {}.{}.{}' .format(*v)) # modulo de operating system x = os.name w = os.getcwdb() print(v) print(w) # modulo de datetime date = datetime.datetime.now() # fecha y hora de ahora print(date) print(date.year) print(date.month) print(date.day)","title":"M\u00f3dulos"}]}