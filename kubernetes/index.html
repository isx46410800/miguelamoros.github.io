<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Miguel Amorós">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Kubernetes - Miguel's Notes</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Kubernetes";
    var mkdocs_page_input_path = "kubernetes.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Miguel's Notes</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">MkDocs</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../linux/">Linux</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../vscode/">VSCode</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../gitlab/">Gitlab</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../python/">Python</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bash_scripting/">BashScripting</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../docker/">Docker</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Kubernetes</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#arquitectura">ARQUITECTURA</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#instalacion-minikubekubectl">INSTALACIÓN MINIKUBE/KUBECTL</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods-vs-contenedores">PODS VS CONTENEDORES</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pods">PODS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod">CREAR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs-pods">LOGS PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#api-resources">API-RESOURCES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminar-pods">ELIMINAR PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#obtener-yaml-pod">OBTENER YAML POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ip-pod">IP POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#entrar-al-pod">ENTRAR AL POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-pod-yaml">CREAR POD YAML</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-container-por-pod">2+ CONTAINER POR POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#labels">LABELS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas-pods">PROBLEMAS PODs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#replicasets">REPLICASETS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-replicaset">CREAR REPLICASET</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminarmodificar">ELIMINAR/MODIFICAR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#logs">LOGS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#owner-refernce">OWNER REFERNCE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#adopcion-de-pods-planos">ADOPCIÓN DE PODS PLANOS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#problemas">PROBLEMAS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#deployments">DEPLOYMENTS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-deployment">CREAR DEPLOYMENT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rolling-update">ROLLING UPDATE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#historial-de-deployments">HISTORIAL DE DEPLOYMENTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#roll-backs">ROLL BACKS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#servicios">SERVICIOS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-servicio">CREAR SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#info-servicio">INFO SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#endpoints">ENDPOINTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns">DNS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-cluster-ip">SERVICIO CLUSTER-IP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-node-port">SERVICIO NODE-PORT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#servicio-load-balancer">SERVICIO LOAD BALANCER</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#golang">GOLANG</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-api-rest-go">CREAR API REST GO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mensaje-1">MENSAJE 1</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#mensaje-2">MENSAJE 2</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dockerfile-golang">DOCKERFILE GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deployment-golang">DEPLOYMENT GOLANG</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#consumo-del-servicio">CONSUMO DEL SERVICIO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fronted">FRONTED</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#manifiesto-fronted">MANIFIESTO FRONTED</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#namespaces">NAMESPACES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-namespace">CREAR NAMESPACE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#asignar-namespaces">ASIGNAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#borrar-namespaces">BORRAR NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-namespaces">DEPLOY NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dns-namespaces">DNS NAMESPACES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#contextos-namespaces">CONTEXTOS NAMESPACES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#limitar-ramcpu">LIMITAR RAM/CPU</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#limitsrequest">LIMITS/REQUEST</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#ram">RAM</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#cpu">CPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#qosquality-of-service">QOS(Quality of Service)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#limitrange">LIMITRANGE</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#valores-por-defecto">VALORES POR DEFECTO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#valores-pod">VALORES POD</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limites">LIMITES</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#resource-quota">RESOURCE QUOTA</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-rq">CREAR RQ</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-rq">DEPLOY RQ</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limitar-no-pods-en-ns">LIMITAR Nº PODS EN NS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#probes">PROBES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#tipos-probes">TIPOS PROBES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-liveness-probe">CREAR LIVENESS PROBE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#liveness-tcp">LIVENESS TCP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#liveness-http">LIVENESS HTTP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#readiness-probe">READINESS PROBE</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#variables-y-configmap">VARIABLES Y CONFIGMAP</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-variables">CREAR VARIABLES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#variables-referenciadas">VARIABLES REFERENCIADAS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configmap">CONFIGMAP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#montando-volumen-configmap">MONTANDO VOLUMEN CONFIGMAP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#volumen-env-configmap">VOLUMEN-ENV CONFIGMAP</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#secrets">SECRETS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear">CREAR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#manifiestos">MANIFIESTOS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#envsubts">ENVSUBTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#volume-secrets">VOLUME SECRETS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#env-secrets">ENV SECRETS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#volumes">VOLUMES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#emptydir">EMPTYDIR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hostpath-pv">HOSTPATH-PV</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pvc">PVC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pvc-pv">PVC-PV</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pvc-pods">PVC-PODS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cloud-volumes">CLOUD VOLUMES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#reclaim-policy">RECLAIM POLICY</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#usersgroups-rbac">USERS/GROUPS RBAC</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#roles-vs-clusterroles">ROLES vs CLUSTERROLES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rolebinding-vs-clusterrolebinding">ROLEBINDING vs CLUSTERROLEBINDING</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-users-groups">CREAR USERS &amp; GROUPS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#habilitar-rbac">HABILITAR RBAC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#simplificamos-contexto">SIMPLIFICAMOS CONTEXTO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-roles">CREAR ROLES</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#enlazar-role-user">ENLAZAR ROLE &amp; USER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#config-maps">CONFIG MAPS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-clusterole">CREAR CLUSTEROLE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-user-admin">CREAR USER ADMIN</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#roles-a-grupos">ROLES A GRUPOS</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#services-account">SERVICES ACCOUNT</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#secret-sa">SECRET SA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-sa">CREAR SA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#relacion-pod-sa">RELACION POD-SA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#requests">REQUESTS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#request-jwt">REQUEST JWT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#sa-deployment">SA DEPLOYMENT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#role-sa">ROLE SA</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#ingress">INGRESS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#ingress-controller">INGRESS CONTROLLER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-ingress-controller">CREAR INGRESS CONTROLLER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ip-ingress-controller">IP INGRESS CONTROLLER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#app-ingress-controller">APP INGRESS-CONTROLLER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#exponer-el-puerto-al-exterior">EXPONER EL PUERTO AL EXTERIOR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-apps-en-ic">2 APPS EN IC</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#aws-kubernetes">AWS KUBERNETES</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#crear-cluster-aws-eksctl">CREAR CLUSTER AWS EKSCTL</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ingress-aws-eks">INGRESS AWS EKS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-ingress-controller-aws">DEPLOY INGRESS CONTROLLER AWS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#deploy-app">DEPLOY APP</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#exponer-la-app-externamente">EXPONER LA APP EXTERNAMENTE</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#modificando-reglas-ingress">MODIFICANDO REGLAS INGRESS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#borrar-todo">BORRAR TODO</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#aws-hpa-install">AWS HPA INSTALL</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#crear-un-hpa">CREAR UN HPA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#autoescalar-hpa">AUTOESCALAR HPA</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cluster-autoscaler">CLUSTER AUTOSCALER</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#eliminamos-todo-de-la-nube">ELIMINAMOS TODO DE LA NUBE</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../openshift/">Openshift</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../jenkins/">Jenkins</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ansible/">Ansible</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../tomcat/">Tomcat</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../azure/">Azure</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../office/">Office 365</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../markdown/">Markdown</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../bbdd/">Base de datos</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../sap/">SAP</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ldap/">LDAP</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../pam/">PAM</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../samba/">SAMBA</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../kerberos/">KERBEROS</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../lvm/">LVM/RAID</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../backup/">Backup</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../routing/">Routing</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../files/">Archivos Destacados</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../windows/">Windows</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../taller/">Taller</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Miguel's Notes</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Kubernetes</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/isx46410800/miguelamoros.github.io/edit/master/docs/kubernetes.md"> Edit on isx46410800/python</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="kubernetes">KUBERNETES</h1>
<ul>
<li>
<p><code>K8S</code> Es una herramienta extensible y de código abierto para gestionar cargas de trabajo y servicios en contenedores, que facilita tanto la configuración declarativa como la automatización. Tiene un ecosistema grande y de rápido crecimiento. Los servicios, el soporte y las herramientas están ampliamente disponibles.  </p>
</li>
<li>
<p>Funciones:  </p>
<ul>
<li>Service discovery: mira cuantos nodos hay, los escanea para saber de ellos.  </li>
<li>Rollouts/Rollbacks: para desplegar versiones nuevas o volver a la misma.  </li>
<li>Optimización de recursos en nodos: mira donde colocar el contenedor al host con menos carga.  </li>
<li>Self-healing: crea automaticamente un contenedor cuando uno muere.  </li>
<li>Configuración de secretos</li>
<li>Escalamiento horizontal</li>
</ul>
</li>
</ul>
<h2 id="arquitectura">ARQUITECTURA</h2>
<p><img alt="" src="../images/kubernetes.png" />  </p>
<ul>
<li>
<p><strong>MASTER/NODE</strong>: Kubernetes se divide en master, es el cerebro, es la parte que se encarga de todo el procesamiento, es donde estan todas las herramientas, es el centro de operaciones. Los nodos son las máquinas, host, máquinas virutal.<br />
El master es como la aduana y los nodes son  los barcos que se llevan los contenedores de la duana.  </p>
</li>
<li>
<p><strong>API SERVER</strong>: Aplication Program Interface, significa que yo me puedo comunicar con un servicio a través de la API. Puedo hacerlo con la herramienta kubectl o directamente por fichero de programación. Ambos son en JSON, por lo que acaba procesando todo en código JSON.  </p>
</li>
<li>
<p><strong>KUBE-SCHEDULE</strong>: es el que se encarga de colocar las cosas donde deben ir. Cuando comunico algo a la API, este le pasa las especificaciones al Schedule y éste busca a ver que nodo va mejor para poner todo, si hay muchos, mirar los 15 primeros aprox y lo pone donde mejor vea. Si no encuentra sitio, se espera hasta que quede uno libre correctamente para poder meter el contenedor.  </p>
</li>
<li>
<p><strong>KUBE-CONTROLLER</strong>: dentro tiene el <em>node controler</em> (se encarga de ver nodos, si se cae uno, levanta otra máquina), el <em>replication</em>(encargado de mantener todas las réplicas especificadas), el <em>end point controller</em>(se encarga de la red y pods) y tenemos el <em>service account y tokens controller</em>(para la autenticación).  </p>
</li>
<li>
<p><strong>ETCD</strong>: es la base de datos de kubernetes donde están todas las configuraciones, cambios, estados nuevos, anteriores, etc. Si ponemos algo en una versión nueva y queremos volver atrás, en el <em>etcd</em> está guardado el estado y configuración anterior.  </p>
</li>
<li>
<p><strong>KUBELET</strong>: se encuentra en cada nodo y tienen dos funciones, en enviar y recibir información al master y por otro lado, habla con el run controller(normalmente docker),que tiene que estar instalado en cada nodo, para decirle las especificaciones que debe desplegar/montar en el POD del nodo.  </p>
</li>
<li>
<p><strong>KUBE-PROXY</strong>:  se encuentra en cada nodo y se encarga de todo lo relacionado con la red del nodo y que se puedan comunicar entre contenedores/pods.  </p>
</li>
<li>
<p><strong>CONTAINER-RUNTIME</strong>: el software de contenedores que tiene instalado el nodo: docker,etc.  </p>
</li>
</ul>
<h2 id="instalacion-minikubekubectl">INSTALACIÓN MINIKUBE/KUBECTL</h2>
<ul>
<li>
<p><strong>MINIKUBE</strong>: crea o simula un cluster pequeño que nos permite hacerlo en local.  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Documentación Kubernetes</a>  </p>
</li>
<li>
<p>Ejecutamos esta orden y sino sale vacío , vamos bien:<br />
<code>grep -E --color 'vmx|svm' /proc/cpuinfo</code>  </p>
</li>
<li>
<p>Instalamos <code>kubectl</code>, la intermediario para hablar con kubernetes:  </p>
<ul>
<li>
<p><code>curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"</code>  </p>
</li>
<li>
<p><code>chmod +x ./kubectl</code>  </p>
</li>
<li>
<p><code>sudo mv ./kubectl /usr/bin/kubectl</code>  </p>
</li>
<li>
<p><code>kubectl version --client</code>  </p>
</li>
</ul>
</li>
<li>
<p>Para usar minikube se necesita un <code>Hypervisor</code>(o monitor de máquina virtual (virtual machine monitor)1​ es una plataforma que permite aplicar diversas técnicas de control de virtualización para utilizar, al mismo tiempo, diferentes sistemas operativos en una misma computadora):  </p>
<ul>
<li>KVM</li>
<li>VirtualBox</li>
<li>Docker</li>
</ul>
</li>
<li>
<p>Descargamos <code>minikube</code>:  </p>
<ul>
<li>
<p><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube</code>  </p>
</li>
<li>
<p><code>sudo mv minikube /usr/bin/</code>  </p>
</li>
<li>
<p><code>minikube status</code>  </p>
</li>
</ul>
<p><code>[isx46410800@miguel curso_kubernetes]$ minikube status
🤷  There is no local cluster named "minikube"
👉  To fix this, run: "minikube start"
[isx46410800@miguel curso_kubernetes]$ minikube start
😄  minikube v1.13.1 on Fedora 27
✨  Automatically selected the docker driver
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
💾  Downloading Kubernetes v1.19.2 preload ...
    &gt; preloaded-images-k8s-v6-v1.19.2-docker-overlay2-amd64.tar.lz4: 486.36 MiB
🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
🧯  Docker is nearly out of disk space, which may cause deployments to fail! (93% of capacity)
💡  Suggestion: 
    Try at least one of the following to free up space on the device:
    1. Run "docker system prune" to remove unused docker data
    2. Increase the amount of memory allocated to Docker for Desktop via
    Docker icon &gt; Preferences &gt; Resources &gt; Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the docker container runtime
🍿  Related issue: https://github.com/kubernetes/minikube/issues/9024
🐳  Preparing Kubernetes v1.19.2 on Docker 19.03.8 ...
🔎  Verifying Kubernetes components...
🌟  Enabled addons: default-storageclass, storage-provisioner
🏄  Done! kubectl is now configured to use "minikube" by default</code>  </p>
</li>
<li>
<p>Comprobamos de nuevo que sí funciona <code>minikube status</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>

<ul>
<li>
<p><strong>COMANDOS BÁSICOS MINIKUBE</strong>:  </p>
<ul>
<li><code>minikube status</code></li>
<li><code>minikube stop/start/delete</code></li>
</ul>
</li>
<li>
<p>Repositorio <a href="https://github.com/ricardoandre97/k8s-resources">curso Kubernetes</a>  </p>
</li>
</ul>
<h2 id="pods-vs-contenedores">PODS VS CONTENEDORES</h2>
<p><img alt="" src="../images/kubernetes2.png" />  </p>
<ul>
<li>
<p>Los <strong>contenedores</strong> se ejecutan de manera aislada en un namespace:  </p>
<ul>
<li>IPC (Inter Process Communication)</li>
<li>Cgroup</li>
<li>Network</li>
<li>Mount</li>
<li>PID</li>
<li>User</li>
<li>UTS (Unix Timesharing System)</li>
</ul>
</li>
<li>
<p>Los <strong>PODS</strong> sirven para compartir namespaces entre contenedores. Con docker permite que varios contenedores se puedan comunicar entre ellos por procesos, redes, files,etc. Kubernetes levanta un servicio y hace que el resto de contenedores compartan ese ID por ejemplo de red y se puedan comunicar y compartir namespaces como:  </p>
<ul>
<li>De red(verse en la misma red)</li>
<li>IPC(verse los procesos)</li>
<li>UTS</li>
</ul>
</li>
</ul>
<blockquote>
<p>Cuando hablamos de PODs entonces nos referimos a que solo tiene una unica IP para todo lo que haya dentro comunicado. Solo es una capa que agrupa estos contenedores.</p>
</blockquote>
<h2 id="pods">PODS</h2>
<h3 id="crear-pod">CREAR POD</h3>
<ul>
<li>
<p>Primero tenemos que tener encendido el simulador:<br />
<code>minikube start</code>  </p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/reference/kubectl/conventions/">Documentación</a>:<br />
<code>versión v1.19 la última</code>  </p>
</li>
<li>
<p>Creamos un pod de prueba <code>kubectl run nombrePod --image:xxx:tag</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ 

pod/pod-test created
</code></pre>

<ul>
<li>Vemos que lo hemos creado y está corriendo:  </li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
pod-test   1/1     Running   0          22s
</code></pre>

<blockquote>
<p>Normalmente hay un contenedor por pod, se suele asimilar a eso.  </p>
</blockquote>
<h3 id="logs-pods">LOGS PODS</h3>
<ul>
<li>
<p>Un pod es la unidad más pequeña para poder trabajar en Kubernetes. Se le notifica a la API que hable con Schedule y Controller y busquen un nodo donde crear ese pod con ciertas especifiaciones. Lo que corre dentro es el contenedor, el POD no corre.   </p>
</li>
<li>
<p>Creamos uno pod mal aposta para ver el error:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel curso_kubernetes]$ kubectl run pod-test2 --image=nginx:miguelllll
pod/pod-test2 created
[isx46410800@miguel curso_kubernetes]$ kubectl get pods
NAME        READY   STATUS         RESTARTS   AGE
pod-test    1/1     Running        0          5m19s
pod-test2   0/1     ErrImagePull   0          14s
</code></pre>

<ul>
<li>Para ver los ´logs´ usamos <code>kubectl describe pod nombrePod</code>:<br />
<code>kubectl describe pod pod-test</code>  <blockquote>
<p>En el apartado <code>events</code> nos describe los logs paso a paso.  </p>
</blockquote>
</li>
</ul>
<h3 id="api-resources">API-RESOURCES</h3>
<ul>
<li>Para ver todos los recursos que hay y los shortnames de comandos se usa:<br />
<code>kubectl api-resources</code>  </li>
</ul>
<h3 id="eliminar-pods">ELIMINAR PODS</h3>
<ul>
<li>
<p>Para eliminar pods usamos <code>kubectl delete pod podName ...</code>:<br />
<code>kubectl delete pod pod-test2</code>  </p>
</li>
<li>
<p>Todos:<br />
<code>kubectl delete pod --all</code>  </p>
</li>
</ul>
<h3 id="obtener-yaml-pod">OBTENER YAML POD</h3>
<ul>
<li>
<p>Podemos obtener info solo del pod concreto:<br />
<code>kubectl get pod pod-test</code></p>
</li>
<li>
<p>Para más info para obtener el contenido YAML, lo que comunica al API de kubernetes en los request:<br />
<code>kubectl get pod pod-test -o yaml</code>  </p>
</li>
<li>
<p>Es mejor enviar las cosas por manifiestos en YAML ya que si quieres crear 50 pods, a través de un fichero será mejor que no ir poniendo una orden 50 veces.  </p>
</li>
</ul>
<h3 id="ip-pod">IP POD</h3>
<ul>
<li>Para poder ver la IP del POD podemos usar cualquiera de estos comandos:<br />
<code>kubectl describe pod pod-test</code><br />
<code>kubectl get pod pod-test -o yaml</code>  </li>
</ul>
<blockquote>
<p>En este caso es 172.18.0.3  </p>
</blockquote>
<ul>
<li>
<p>Para verlo ingresamos directamente al navegador la ip.  </p>
</li>
<li>
<p>Si no funciona tenemos que mapear el puerto:<br />
<code>kubectl port-forward pod-test 7000:80</code>  </p>
</li>
</ul>
<p><img alt="" src="../images/kubernetes4.png" />  </p>
<ul>
<li>Comprobamos la respuesta:<br />
<code>curl 172.18.0.3:80</code>  </li>
</ul>
<h3 id="entrar-al-pod">ENTRAR AL POD</h3>
<ul>
<li>
<p>Para ingresar a la consola del POD:<br />
<code>kubectl exec -it pod-test -- sh</code>  </p>
<blockquote>
<p>Cuando solo hay un contenedor, no se especifica el nombre del pod.  </p>
</blockquote>
</li>
<li>
<p>Cuando hay más contenedores <code>c, --container=''</code>:<br />
<code>kubectl exec -it pod-test -c containerName -- sh</code>  </p>
</li>
</ul>
<h3 id="crear-pod-yaml">CREAR POD YAML</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>
<p>Las api versions las podemos ver en:<br />
<code>kubectl api-versions</code>  </p>
</li>
<li>
<p>Los kind los podemos ver en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Para crearlo a través del fichero YAML:<br />
<code>kubectl apply -f pod.yaml</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl apply -f pod.yaml
pod/pod-test2 created
[isx46410800@miguel pods]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test    1/1     Running   0          6h54m
pod-test2   1/1     Running   0          7s
</code></pre>

<ul>
<li>
<p>Para borrarlo:<br />
<code>kubectl delete -f pod.yaml</code>  </p>
</li>
<li>
<p>Para crear dos o más PODS, se pone <code>---</code> de separación:  </p>
</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: nginx:alpine
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
spec:
  containers:
    - name: container2
      image: nginx:alpine
</code></pre>

<h3 id="2-container-por-pod">2+ CONTAINER POR POD</h3>
<ul>
<li>Para crear dos o  más containers en un POD se añade en la subsección containers:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: container1
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
    - name: container2
      image: python:3.6-alpine
      command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
</code></pre>

<blockquote>
<p>Nos dará error porque estamos levantando dos container en el mismo puerto. El problema es que en un POD todos los containers comparten la misma red , por lo que si se levanta uno en el puerto 8082, el otro tiene que ser diferente.  </p>
</blockquote>
<ul>
<li>Vemos los <code>logs</code> en <code>kubectl logs podName -c container</code>:  </li>
</ul>
<pre><code>263dab[isx46410800@miguel pods]$ kubectl logs pod-test2 -c container2
Traceback (most recent call last):
...
  File &quot;/usr/local/lib/python3.6/socketserver.py&quot;, line 470, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address in use
</code></pre>

<ul>
<li>Arreglamos el fallo del puerto y comprobamos cada container del POD:  </li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container1 -- sh
/ # cat index.html 
cont1
/ # exit
[isx46410800@miguel pods]$ kubectl exec -it pod-test2 -c container2 -- sh
/ # cat index.html 
cont2
</code></pre>

<h3 id="labels">LABELS</h3>
<ul>
<li>Los labels son etiquetas que se ponen debajo de los <code>metadata</code>:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
  labels:
    app: front-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
---   
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>Los labels nos sirve para poder filtrar PODs con <code>kubectl get pods -l nombre=valor</code>:</li>
</ul>
<pre><code>[isx46410800@miguel pods]$ kubectl get pods -l app=back-end
NAME        READY   STATUS    RESTARTS   AGE
pod-test3   1/1     Running   0          62s
[isx46410800@miguel pods]$ kubectl get pods -l env=dev
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   1/1     Running   0          78s
pod-test3   1/1     Running   0          78s
</code></pre>

<blockquote>
<p>Los LABELS más usado es el de APP. Muy importantes para administrar replicas.  </p>
</blockquote>
<h3 id="problemas-pods">PROBLEMAS PODs</h3>
<ul>
<li>
<p>Los PODS no se regeneran solos si lo eliminamos manualmente, aunque se diga que haya dos replicas siempre. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs por si solo en un archivo YAML no puede indicarse que haya 50 replicas por ejemplo. Tiene que ser un objeto mayor quien lo haga.   </p>
</li>
<li>
<p>Los PODs no se pueden actualizar directamente, tiene que hacerlo alguien externo. Como mucho solo se puede modificar la seccion <code>image</code> y al hacer el apply puede ser que te deje actualizar.  </p>
</li>
</ul>
<h2 id="replicasets">REPLICASETS</h2>
<ul>
<li>
<p>Es un objeto separado del POD a un nivel más alto(el replicaset crea PODs y es su dueño).  </p>
</li>
<li>
<p>Si se le indica que haya dos, si se muere uno y solo queda un POD, levanta uno nuevo. Para ello es muy importante los LABELS para ver que PODs tiene que manejar.  </p>
</li>
<li>
<p>En la metadata del POD mete el <code>OWNER REFERENCE</code> para indicar quien el propietario de los PODs y los suyos no los maneje otro ReplicaSet.  </p>
</li>
</ul>
<h3 id="crear-replicaset">CREAR REPLICASET</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los replicasets en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-test
  labels:
    app: rs-test
spec:
  # modify replicas according to your case
  replicas: 5
  selector:
    matchLabels:
      app: pod-label
  # pertenece a los PODs que vas a crear
  template:
    metadata:
      labels:
        app: pod-label
    spec:
      containers:
        - name: container1
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont1&quot; &gt; index.html &amp;&amp; python -m http.server 8082']
        - name: container2
          image: python:3.6-alpine
          command: ['sh', '-c', 'echo &quot;cont2&quot; &gt; index.html &amp;&amp; python -m http.server 8083']
</code></pre>

<ul>
<li>
<p>Lo creamos:<br />
<code>kubectl apply -f replica-set.yaml</code>  </p>
<blockquote>
<p>Lo que creamos son 5 PODs con label(pod-label, sino está lo crea) y dentro de cada POD creamos dos containers con label(pod-label)  </p>
</blockquote>
</li>
<li>
<p>Comprobamos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          105s
rs-test-9jpjg   2/2     Running   0          105s
rs-test-fbwjb   2/2     Running   0          105s
rs-test-hz2kx   2/2     Running   0          105s
rs-test-s6cxx   2/2     Running   0          105s
[isx46410800@miguel replicaset]$ kubectl get pods -l app=pod-label
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          119s
rs-test-9jpjg   2/2     Running   0          119s
rs-test-fbwjb   2/2     Running   0          119s
rs-test-hz2kx   2/2     Running   0          119s
rs-test-s6cxx   2/2     Running   0          119s
</code></pre>

<ul>
<li>Ver los <code>replicasets</code> con <code>kubectl get rs</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl get rs
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m53s
[isx46410800@miguel replicaset]$ kubectl get replicaset
NAME      DESIRED   CURRENT   READY   AGE
rs-test   5         5         5       3m56s
</code></pre>

<h3 id="eliminarmodificar">ELIMINAR/MODIFICAR</h3>
<ul>
<li>En un replicaset creado, si borramos un pod, vemos como actualiza directamente para mantener los 5 pods indicados:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl delete pod rs-test-s6cxx
pod &quot;rs-test-s6cxx&quot; deleted
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          5m43s
rs-test-9jpjg   2/2     Running   0          5m43s
rs-test-b9lf4   2/2     Running   0          43s
rs-test-fbwjb   2/2     Running   0          5m43s
rs-test-hz2kx   2/2     Running   0          5m43s
</code></pre>

<ul>
<li>Si modifico el replicaset a 2 copias, veremos como se eliminan 3, se quedan dos:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ vim replica-set.yaml 
[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml 
replicaset.apps/rs-test configured
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS        RESTARTS   AGE
rs-test-5tsfh   2/2     Running       0          8m29s
rs-test-9jpjg   2/2     Terminating   0          8m29s
rs-test-b9lf4   2/2     Terminating   0          3m29s
rs-test-fbwjb   2/2     Running       0          8m29s
rs-test-hz2kx   2/2     Terminating   0          8m29s
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
rs-test-5tsfh   2/2     Running   0          9m26s
rs-test-fbwjb   2/2     Running   0          9m26s
</code></pre>

<h3 id="logs">LOGS</h3>
<ul>
<li>
<p>Por describe:<br />
<code>kubectl get rs rs-test -o yaml</code>  </p>
</li>
<li>
<p>Por manifiesto YAML:<br />
<code>kubectl describe rs rs-test</code>  </p>
</li>
</ul>
<h3 id="owner-refernce">OWNER REFERNCE</h3>
<ul>
<li>Lo vemos en la metadata de un pod creado por ReplicaSet <code>kubectl get pod podName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get pod rs-test-5tsfh -o yaml
name: rs-test-5tsfh
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: rs-test
    uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<ul>
<li>Comprobamos que el <code>UID</code> anterior coincide con el replicaset creado <code>kubectl get rs rsName -o yaml</code>:  </li>
</ul>
<pre><code>kubectl get rs rs-test -o yaml
name: rs-test
  namespace: default
  resourceVersion: &quot;22732&quot;
  selfLink: /apis/apps/v1/namespaces/default/replicasets/rs-test
  uid: 646a4a62-6acc-41a7-b3d1-7fe095c441d0
</code></pre>

<h3 id="adopcion-de-pods-planos">ADOPCIÓN DE PODS PLANOS</h3>
<ul>
<li>Vamos a crear primero dos PODs manualmente:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl run pod-test --image=nginx:alpine
pod/pod-test created
[isx46410800@miguel replicaset]$ kubectl run pod-test2 --image=nginx:alpine
pod/pod-test2 created
</code></pre>

<ul>
<li>Les creamos un LABEL a cada uno con <code>kubectl label pods podName label=valor</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl label pods pod-test app=pod-label
pod/pod-test labeled
[isx46410800@miguel replicaset]$ kubectl label pods pod-test2 app=pod-label
pod/pod-test2 labeled
</code></pre>

<blockquote>
<p>Tendran el nuevo label pero no tendrán ningun OWNER REFERENCE porque no han sido creados por ningun REPLICASET.  </p>
</blockquote>
<ul>
<li>Ahora mediante replicaset cremos 3 replicas con mismo label:  </li>
</ul>
<pre><code>[isx46410800@miguel replicaset]$ kubectl apply -f replica-set.yaml
replicaset.apps/rs-test created
[isx46410800@miguel replicaset]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
pod-test        1/1     Running   0          3m49s
pod-test2       1/1     Running   0          3m45s
rs-test-8mk72   2/2     Running   0          10s
</code></pre>

<blockquote>
<p>Tenemos un GRAN PROBLEMA ya que adopta dos pods huerfanos sin owner y los coge pero los adopta y ellos solo tienen un container y no dos con la estructura del replicaset. Si borramos un pod de uno, el replicaset regenera uno nuevo con la estructura buena.  </p>
</blockquote>
<h3 id="problemas">PROBLEMAS</h3>
<ul>
<li>
<p>Si modificamos cosas del container como la imagen, nombre container, etc. Si hacemos un apply no se actualiza nada, ya que como sigue con los mismos LABELS, el replicaset solo se guia por pods y labels y no de actualizar nada de lo que contiene.  </p>
</li>
<li>
<p>NO se auto-actualizan solos.  </p>
</li>
<li>
<p>Si modificamos por ejemplo la imagen de un container de un pod de python de 3.6 a 3.7, se actualiza el ReplicSet pero no los PODs. Si se borra un POD, entonces el nuevo POD si que tiene las nuevas actualizaciones.  </p>
</li>
</ul>
<h2 id="deployments">DEPLOYMENTS</h2>
<ul>
<li>
<p>Es un objeto de nivel mayor que los replicaset. Es el dueño del replicaset que a su vez es de sus PODs.  </p>
</li>
<li>
<p>Al deployment se le da una imagen o una nueva versión: genera un replicaset con sus pods. Si se actualiza algo, se crea un segundo replicaset con un primer POD, y luego va matando del primer POD el pod viejo de v1 y crea otro POD en el nuevo replicaset con V2, y así succesivamente.  </p>
</li>
<li>
<p>Esto se logra porque los deployments tienen dos valores: Uno de máximo extra y otra de un máximo de inutilizado. Normalmente este valor por defecto es un 25%. Por lo que en el ejemplo anterior podemos hacer 1 pod más y solo dejar 1 pod inutilizado.    </p>
</li>
<li>
<p>Los deployments pueden mantener un máximo de 10 replicasets  </p>
</li>
</ul>
<h3 id="crear-deployment">CREAR DEPLOYMENT</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los deployments en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>Lo creamos con <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test created
</code></pre>

<ul>
<li>Vemos el deployment creado <code>kubectl get deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           8s
</code></pre>

<ul>
<li>Vemos los labels del deployment <code>kubectl get deployment --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get deployment --show-labels
NAME              READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
deployment-test   3/3     3            3           21s   app=front
</code></pre>

<ul>
<li>Vemos el estado del deployment <code>kubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>Vemos que se ha creado un replicaset y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       4m28s
[isx46410800@miguel deployments]$ kubectl get replicaset --show-labels
NAME                         DESIRED   CURRENT   READY   AGE    LABELS
deployment-test-659b64d66c   3         3         3       5m8s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos que se ha creado 3 replicas del pod y tiene los mismo labels:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running   0          4m32s
deployment-test-659b64d66c-pzdct   1/1     Running   0          4m32s
deployment-test-659b64d66c-thknz   1/1     Running   0          4m32s
[isx46410800@miguel deployments]$ kubectl get pods --show-labels
NAME                               READY   STATUS    RESTARTS   AGE     LABELS
deployment-test-659b64d66c-n5qgr   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-pzdct   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
deployment-test-659b64d66c-thknz   1/1     Running   0          5m14s   app=front,pod-template-hash=659b64d66c
</code></pre>

<ul>
<li>Vemos la jerarquía de lo creado para saber quien es el <code>owner reference</code> de cada cosa con <code>kubectl get rs/pod/deployment NAME -o yaml</code>:  </li>
<li>Deployment no tiene dueño</li>
<li>Replicaset su dueño es deployment</li>
<li>Pod su dueño es replicaset</li>
</ul>
<h3 id="rolling-update">ROLLING UPDATE</h3>
<ul>
<li>Actualizamos por ejemplo la imagen de un container del POD en vez de <code>nginx:alpine</code> ponemos <code>nginx</code> y hacemos de nuevo el <code>kubectl apply -f deployment.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml 
deployment.apps/deployment-test configured
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   1/1     Running             0          13m
deployment-test-659b64d66c-pzdct   1/1     Running             0          13m
deployment-test-659b64d66c-thknz   1/1     Running             0          13m
deployment-test-69b674677d-2cq4l   0/1     ContainerCreating   0          5s
[isx46410800@miguel deployments]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     1            3           14m
[isx46410800@miguel deployments]$ kubectl get replicaset
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-659b64d66c   3         3         3       14m
deployment-test-69b674677d   1         1         0       18s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
deployment-test-659b64d66c-n5qgr   0/1     Terminating         0          14m
deployment-test-659b64d66c-pzdct   1/1     Running             0          14m
deployment-test-659b64d66c-thknz   1/1     Terminating         0          14m
deployment-test-69b674677d-2cq4l   1/1     Running             0          25s
deployment-test-69b674677d-dwdlr   0/1     ContainerCreating   0          1s
deployment-test-69b674677d-dwspw   1/1     Running             0          6s
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-69b674677d-2cq4l   1/1     Running   0          43s
deployment-test-69b674677d-dwdlr   1/1     Running   0          19s
deployment-test-69b674677d-dwspw   1/1     Running   0          24s
</code></pre>

<ul>
<li>Vemos el estado en directo de lo que hace con <code>ubectl rollout status deployment deploymentName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
Waiting for deployment &quot;deployment-test&quot; rollout to finish: 1 old replicas are pending termination...
deployment &quot;deployment-test&quot; successfully rolled out
</code></pre>

<ul>
<li>También podemos ver el resultado en <code>kubectl describe deployment deploymentName</code>:  </li>
</ul>
<pre><code>Events:
  Type    Reason             Age                    From                   Message
  ----    ------             ----                   ----                   -------
  Normal  ScalingReplicaSet  19m                    deployment-controller  Scaled up replica set deployment-test-659b64d66c to 3
  Normal  ScalingReplicaSet  5m18s                  deployment-controller  Scaled up replica set deployment-test-69b674677d to 1
  Normal  ScalingReplicaSet  4m59s                  deployment-controller  Scaled down replica set deploy
</code></pre>

<ul>
<li>Aquí vemos también la estrategía de los valores que comentamos en la introducción:<br />
<code>RollingUpdateStrategy:  25% max unavailable, 25% max surge</code>  </li>
</ul>
<h3 id="historial-de-deployments">HISTORIAL DE DEPLOYMENTS</h3>
<ul>
<li>Podemos ver las actualizaciones o revisiones en el historial de deployments en <code>kubectl rollout history deployment deployment</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
2         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre>

<ul>
<li>Podemos con esto volver a cualquier versión anterior. Por defecto es 10 replicasets que guarda pero podemos cambiarlo añadiento en la parte de replicaset del manifiesto YAML <code>revisionHistoryLimit: 5</code>:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  revisionHistoryLimit: 5
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
</code></pre>

<ul>
<li>
<p>Para poner un motivo en el <code>change-cause</code> cuando hacemos una versión de deployments indicamos dos maneras:  </p>
</li>
<li>
<p>Con la linea de desplegar <code>kubectl apply -f deployment.yaml --record</code>:<br />
<code>[isx46410800@miguel deployments]$ kubectl apply -f deployment.yaml --record
  deployment.apps/deployment-test configured
  [isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true</code>  </p>
</li>
<li>
<p>Con una subsección en el manifiesto deployment.yaml <code>annotations-&gt; kubernetes.io/change-cause: "message"</code>:<br />
<code>esto es del deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-test
    annotations:
      kubernetes.io/change-cause: "changes port to 110"
    labels:
      app: front</code><br />
<code>kubectl rollout history deployment deployment-test
  deployment.apps/deployment-test 
  REVISION  CHANGE-CAUSE
  1         &lt;none&gt;
  2         &lt;none&gt;
  3         kubectl apply --filename=deployment.yaml --record=true
  4         changes port to 110</code>  </p>
</li>
<li>
<p>Para luego ver una revisión en concreta usamos <code>kubectl rollout history deployment deployment-test --revision=3</code>:  </p>
</li>
</ul>
<pre><code>deployment.apps/deployment-test with revision #3
Pod Template:
  Labels:   app=front
    pod-template-hash=fd8445c88
  Annotations:  kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true
  Containers:
   nginx:
    Image:  nginx:alpine
    Port:   90/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
</code></pre>

<h3 id="roll-backs">ROLL BACKS</h3>
<ul>
<li>Se utiliza para volver a un estado bien porque por ejemplo ha ido mal una actualización de la imagen:  </li>
</ul>
<pre><code>containers:
      - name: nginx
        image: nginx:fake
        ports:
        - containerPort: 110
</code></pre>

<ul>
<li>Vemos el nuevo historial y su fallo:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout history deployment deployment-test
deployment.apps/deployment-test 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
3         kubectl apply --filename=deployment.yaml --record=true
4         changes port to 110
5         new version nginx
#
[isx46410800@miguel deployments]$ kubectl get pods
NAME                               READY   STATUS         RESTARTS   AGE
deployment-test-5c6896bcd5-h5qts   0/1     ErrImagePull   0          32s
deployment-test-74fb9c6d9f-7dwnr   1/1     Running        0          6m50s
deployment-test-74fb9c6d9f-f5qs8   1/1     Running        0          6m45s
deployment-test-74fb9c6d9f-lsmzj   1/1     Running        0          6m54s
</code></pre>

<ul>
<li>Volvemos haciendo un <code>rollback</code> a una versión anterior con <code>kubectl rollout undo deployment deployment-test --to-revision=4</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel deployments]$ kubectl rollout undo deployment deployment-test --to-revision=4
deployment.apps/deployment-test rolled back
#
[isx46410800@miguel deployments]$ kubectl rollout status deployment deployment-test
deployment &quot;deployment-test&quot; successfully rolled out
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test
Name:                   deployment-test
Namespace:              default
CreationTimestamp:      Sun, 11 Oct 2020 19:21:04 +0200
Labels:                 app=front
Annotations:            deployment.kubernetes.io/revision: 6
                        kubernetes.io/change-cause: changes port to 110
Selector:               app=front
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=front
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         110/TCP
    Host Port:    0/TCP
#
[isx46410800@miguel deployments]$ kubectl describe deployment deployment-test 
Normal  ScalingReplicaSet  117s (x12 over 15m)  deployment-controller  (combined from similar events): Scaled down replica set deployment-test-5c6896bcd5 to 0
</code></pre>

<h2 id="servicios">SERVICIOS</h2>
<ul>
<li>
<p>Los servicios es una cosa aparte en el que el usuario se comunica a su IP del servicio y éste observar todos los pods que tienen un label con ese servicio y actua como balanzador de carga para ver a donde llamar para que le de la data. Siempre va a tener la misma data con cualquier que se comunique.  </p>
</li>
<li>
<p>Los PODs tienen una IP unica cada uno, que si se muere, se regenera con otra IP. El servicio también tiene su IP unica en el que kubernetes se encarga de que siempre tenga la misma con el tiempo.  </p>
</li>
<li>
<p>Los <code>endpoints</code> se crean automaticamente cuando se crea un servicio. Todas las IPs de los PODs se van guardando en el endpoint y así el servicio sabe a que IPs se puede comunicar para los PODs con su mismo label de servicio.  </p>
</li>
</ul>
<h3 id="crear-servicio">CREAR SERVICIO</h3>
<ul>
<li>
<p>Vemos a donde pertenece la api-version y el kind de los servicios en:<br />
<code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---       
# añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<blockquote>
<p>El puerto del servicio MY-SERVICE es el 8888 y se comunica a la IP de cada POD por el 80.  </p>
</blockquote>
<h3 id="info-servicio">INFO SERVICIO</h3>
<ul>
<li>Vemos lo creado con <code>kubectl get services/svc</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    41h
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   63s
[isx46410800@miguel services]$ kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   3/3     3            3           79s
</code></pre>

<ul>
<li>Vemos por el label que le indicamos en el YAML:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=front
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
my-service   ClusterIP   10.97.182.119   &lt;none&gt;        8888/TCP   3m35s
</code></pre>

<blockquote>
<p>El cluster-ip se lo da kubernetes si no se lo asignamos directamente  </p>
</blockquote>
<ul>
<li>Profundizamos el servicio con <code>kubectl describe svc my-service</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl describe svc my-service
Name:              my-service
Namespace:         default
Labels:            app=front
Annotations:       &lt;none&gt;
Selector:          app=front
Type:              ClusterIP
IP:                10.97.182.119
Port:              &lt;unset&gt;  8888/TCP
TargetPort:        80/TCP
Endpoints:         172.18.0.2:80,172.18.0.4:80,172.18.0.5:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos nuestra IP como antes y la lista de IPs de los pods que nos podemos comunicar en el endpoint.  </p>
</blockquote>
<h3 id="endpoints">ENDPOINTS</h3>
<ul>
<li>
<p>Lista de IPs de los pods que tienen el label de mi servicio creado.  </p>
</li>
<li>
<p>Vemos la lista de endpoints con <code>kubectl get endpoints</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.4:80,172.18.0.5:80   10m
</code></pre>

<ul>
<li>Comprobamos que son las mismas de los PODS:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-cgds6   1/1     Running   0          10m   172.18.0.4   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          10m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          10m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<ul>
<li>Si eliminamos un Pod, se crea uno nuevo con otra ip y se actualiza el endpoint:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl delete pod deployment-test-b7c99d94b-cgds6
pod &quot;deployment-test-b7c99d94b-cgds6&quot; deleted
[isx46410800@miguel services]$ kubectl get endpoints
NAME         ENDPOINTS                                   AGE
kubernetes   172.17.0.2:8443                             41h
my-service   172.18.0.2:80,172.18.0.5:80,172.18.0.6:80   13m
[isx46410800@miguel services]$ kubectl get pods -l app=front -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
deployment-test-b7c99d94b-fmpdc   1/1     Running   0          14m   172.18.0.2   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-kcdnx   1/1     Running   0          39s   172.18.0.6   minikube   &lt;none&gt;           &lt;none&gt;
deployment-test-b7c99d94b-t8bdz   1/1     Running   0          14m   172.18.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>

<h3 id="dns">DNS</h3>
<ul>
<li>
<p>Creamos un POD nuevo:<br />
<code>[isx46410800@miguel services]$ kubectl run --rm -it podtest2 --image=nginx:alpine -- sh</code>  </p>
</li>
<li>
<p>Funciona que escucha al servicio:  </p>
</li>
</ul>
<pre><code># curl 10.97.182.119:8888
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<blockquote>
<p>Con esto hacemos que desde un pod llamamos al servicio web de nginx que lo coge de sus pods de label web.  </p>
</blockquote>
<ul>
<li>Se crea como un tipo de DNS ya que por el nombre del servicio también se comunica y obtiene respuesta:<br />
<code># curl my-service:8888</code>  </li>
</ul>
<h3 id="servicio-cluster-ip">SERVICIO CLUSTER-IP</h3>
<ul>
<li>
<p>IP permanente que le da kubernetes y es solo para ambito privado, no desde el exterior.  </p>
</li>
<li>
<p>Le podemos poner un tipo de servicio a los servicios que creamos:  </p>
</li>
</ul>
<pre><code># añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  type: ClusterIP
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<h3 id="servicio-node-port">SERVICIO NODE-PORT</h3>
<ul>
<li>
<p>IP que nos permite conectar la IP desde el exterior exponendo su puerto abriendose desde el nodo. A la vez se crea también un ClusterIP.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test2
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---
# añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service2
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel services]$ kubectl get services -l app=backend
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
my-service2   NodePort   10.101.52.193   &lt;none&gt;        8888:30072/TCP   21s
[isx46410800@miguel services]$ kubectl get pods -l app=backend
NAME                                READY   STATUS    RESTARTS   AGE
deployment-test2-77448c6d65-gj6l7   1/1     Running   0          36s
deployment-test2-77448c6d65-n8td7   1/1     Running   0          36s
deployment-test2-77448c6d65-sd6zq   1/1     Running   0          36s
</code></pre>

<ul>
<li>
<p>Si pusiera mi IP y el puerto que redirige el nodo, veriamos el servicio:<br />
<code>http://192.168.1.104:30072</code>  </p>
</li>
<li>
<p>Si no hace en minikube podemos hacer lo siguiente y lo veremos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services]$ minikube service my-service2
|-----------|-------------|-------------|-------------------------|
| NAMESPACE |    NAME     | TARGET PORT |           URL           |
|-----------|-------------|-------------|-------------------------|
| default   | my-service2 |        8888 | http://172.17.0.2:30072 |
|-----------|-------------|-------------|-------------------------|
</code></pre>

<blockquote>
<p>Esa url nos dará el servicio web a través del node port.  </p>
</blockquote>
<h3 id="servicio-load-balancer">SERVICIO LOAD BALANCER</h3>
<ul>
<li>
<p>Hace referencia a un servicio de balanzador de carga.  </p>
</li>
<li>
<p>Se crea un load balancer, que a su vez crea un node port para poder conectarnos desde la nube por ejemplo y a su vez crea un cluster IP.  </p>
</li>
</ul>
<h2 id="golang">GOLANG</h2>
<ul>
<li>Queremos crear dos deployments, uno de backend y otro de front, uno con servicio clusterip y otro nodeip. Queremos que el usuario de fuera haga una petición al front y este le devuelva el servicio que está en backend.  </li>
</ul>
<h3 id="crear-api-rest-go">CREAR API REST GO</h3>
<ul>
<li>
<p><a href="https://dev.to/moficodes/build-your-first-rest-api-with-go-2gcj">DOCUMENTACIÓN</a>  </p>
</li>
<li>
<p>Creamos un fichero simple de API REST en Goland:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel src]$ cat main.go 
package main
import (
    &quot;log&quot;
    &quot;net/http&quot;
)
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write([]byte(`{&quot;message&quot;: &quot;hello world&quot;}`))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil))
}
</code></pre>

<blockquote>
<p>Esto crea una funcion principal de que corra el servicio http cuando encuentre la /, nos contestará a la petición el hello wolld como respuesta.  </p>
</blockquote>
<ul>
<li>Descargamos la imagen Goland y construimos un docker con el volumen del main.go creado para probarlo:<br />
<code>[isx46410800@miguel k8s-hands-on]$ docker pull golang</code>  </li>
</ul>
<p><code>[isx46410800@miguel src]$ docker run --rm --name goland -v $PWD/:/go --net host -dti golang /bin/bash</code>  </p>
<ul>
<li>Iniciamos el fichero y comprobamos el resultado:  </li>
</ul>
<pre><code>[isx46410800@miguel src]$ docker exec -it goland /bin/bash
root@miguel:/go# go run main.go 
</code></pre>

<p>![./images/kubernetes4.png]  </p>
<h3 id="cambios-mensaje-respuesta">CAMBIOS MENSAJE RESPUESTA</h3>
<h3 id="mensaje-1">MENSAJE 1</h3>
<ul>
<li>Añadimos unas variables para cambiar el mensaje de respuesta a la petición de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;fmt&quot;
    &quot;os&quot;
    &quot;time&quot;
)
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    resp := fmt.Sprintf(&quot;La hora es %v y el hostname es %v&quot;, time.Now(), os.Getenv(&quot;HOSTNAME&quot;))
    w.Write([]byte(resp))
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<p>![./images/kubernetes5.png]  </p>
<h4 id="mensaje-2">MENSAJE 2</h4>
<ul>
<li>Añadimos unas variables para cambiar el mensaje de respuesta a la petición de request:  </li>
</ul>
<pre><code>package main
import (
    &quot;net/http&quot;
    &quot;os&quot;
    &quot;time&quot;
    &quot;encoding/json&quot;
)
type HandsOn struct {
   Time     time.Time   `json:&quot;time&quot;`
   Hostname string      `json:&quot;hostname&quot;`
}
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
    if r.URL.Path != &quot;/&quot; {
        http.NotFound(w, r)
        return
    }
    resp := HandsOn{
        Time:       time.Now(),
        Hostname:   os.Getenv(&quot;HOSTNAME&quot;),
    }
    jsonResp, err := json.Marshal(&amp;resp)
    if err != nil {
        w.Write([]byte(&quot;Error&quot;))
        return
    }
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(http.StatusOK)
    w.Write(jsonResp)
}
func main() {
    http.HandleFunc(&quot;/&quot;, ServerHTTP)
    http.ListenAndServe(&quot;:9090&quot;, nil)
}
</code></pre>

<blockquote>
<p>Hemos creado una estructura con type y luego hemos creado un objetivo y convertido a string para cambiar el mensaje y también darle un mensaje de error si no acaba en /.  </p>
</blockquote>
<p>![./images/kubernetes6.png]  </p>
<p>![./images/kubernetes7.png]  </p>
<h3 id="dockerfile-golang">DOCKERFILE GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM golang:1.13 as builder
# DIRECTORIO A TRABAJAR
WORKDIR /app
# COPIAMOS FICHERO MAIN
COPY main.go .
RUN CGO_ENABLED=0 GOOS=linux GOPROXY=https://proxy.golang.org go build -o app ./main.go
# DESDE IMAGEN ALPINE
FROM alpine:latest
# mailcap adds mime detection and ca-certificates help with TLS (basic stuff)
WORKDIR /app
COPY --from=builder /app/app .
# PARA EJECUTARLO
ENTRYPOINT [&quot;./app&quot;]
</code></pre>

<ul>
<li>
<p>Construimos imagen:<br />
<code>[isx46410800@miguel src]$ docker build -t isx46410800/k8s-hands-on .</code>  </p>
</li>
<li>
<p>Encendemos:<br />
<code>[isx46410800@miguel src]$ docker run --rm --name k8s-hands-on -p 9091:9090 -d isx46410800/k8s-hands-on</code>  </p>
</li>
</ul>
<p>![./images/kubernetes8.png]  </p>
<blockquote>
<p>Ahora nuestra aplicación de golang ya puede correr en un contenedor y entramos a localhost:9091 y lo vemos.  </p>
</blockquote>
<h3 id="deployment-golang">DEPLOYMENT GOLANG</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  labels:
    app: backend
spec:
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<blockquote>
<p>Ponemos lo de <code>imagePullPolicy: IfNotPresent</code> para que primero busque si la imagen está constuida localmente antes de mirar en los repos de internet de dockerhub.  </p>
</blockquote>
<ul>
<li>Comprobaciones:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-5d548949c7-dgw9l   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-fg8wr   1/1     Running   0          15m
backend-k8s-hands-on-5d548949c7-q9s6g   1/1     Running   0          15m
[isx46410800@miguel backend]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-5d548949c7   3         3         3       15m
[isx46410800@miguel backend]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           15m
[isx46410800@miguel backend]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
backend-k8s-hands-on   ClusterIP   10.101.44.56   &lt;none&gt;        80/TCP    16m
kubernetes             ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   3d19h
</code></pre>

<ul>
<li>Visual cambiando a nodeport, nos contestará unos de los PODs la respuesta a la request del usuario:  </li>
</ul>
<p>![./images/kubernetes9.png]  </p>
<h3 id="consumo-del-servicio">CONSUMO DEL SERVICIO</h3>
<ul>
<li>Si creamos un POD aparte como si fuera de FRONT, comprobamos que nos podemos conectar tanto por la IP como por el nombre como si tuviera un DNS y nos darán las respuestas los PODs del servicio:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run --rm -ti pod-test2 --image=nginx:alpine -- sh
/ # apk add -U curl
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:57:49.446174694Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
/ # curl 10.111.54.241:80
{&quot;time&quot;:&quot;2020-10-13T19:58:10.218346403Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-9qp82&quot;}/ # 
{&quot;time&quot;:&quot;2020-10-13T19:58:25.365295183Z&quot;,&quot;hostname&quot;:&quot;backend-k8s-hands-on-5d548949c7-66dgx&quot;}/ # 
</code></pre>

<h3 id="fronted">FRONTED</h3>
<ul>
<li>Creamos ahora un index.html de respuesta en un fronted/src/index.html:  </li>
</ul>
<pre><code>[isx46410800@miguel backend]$ kubectl run pod-test2 --image=nginx:alpine
vi /usr/share/nginx/html/index.html
&lt;div id=&quot;id01&quot;&gt;&lt;/div&gt;

&lt;script&gt;
var xmlhttp = new XMLHttpRequest();
var url = &quot;http://backend-k8s-hands-on&quot;;

xmlhttp.onreadystatechange = function() {
    if (this.readyState == 4 &amp;&amp; this.status == 200) {
        var resp = JSON.parse(this.responseText);
        document.getElementById(&quot;id01&quot;).innerHTML = &quot;&lt;h2&gt;La hora es &quot; + resp.time + &quot;y el hostname es&quot; + resp.hostname &quot;&lt;/h2&quot;&gt;;
    }
};
xmlhttp.open(&quot;GET&quot;, url, true);
xmlhttp.send();
&lt;/script&gt;
</code></pre>

<ul>
<li>Dockerfile:  </li>
</ul>
<pre><code># IMAGEN DE GOLAND
FROM nginx:alpine
# COPIAMOS FICHERO MAIN
COPY ./src/index.html /usr/share/nginx/html/index.html
</code></pre>

<h3 id="manifiesto-fronted">MANIFIESTO FRONTED</h3>
<ul>
<li>Despliegue del fronted:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fronted
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: fronted
    spec:
      containers:
      - name: fronted
        image: isx46410800/k8s-hands-on:fronted
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: fronted-k8s-hands-on
  labels:
    app: fronted
spec:
  type: NodePort
  selector:
    app: fronted
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 80
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get svc
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort    10.111.54.241   &lt;none&gt;        80:30740/TCP   78m
fronted-k8s-hands-on   NodePort    10.105.156.14   &lt;none&gt;        80:30159/TCP   9m22s
kubernetes             ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        3d20h
[isx46410800@miguel k8s-hands-on]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-lzrr4   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-mdjh7   1/1     Running   0          51m
backend-k8s-hands-on-7d5b6dc559-qxzdv   1/1     Running   0          51m
fronted-k8s-hands-on-78f59c5f77-dpvck   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-q7h9r   1/1     Running   0          9m27s
fronted-k8s-hands-on-78f59c5f77-r7fnm   1/1     Running   0          9m27s
[isx46410800@miguel k8s-hands-on]$ kubectl cluster-info
Kubernetes master is running at https://172.17.0.2:8443
KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<h2 id="namespaces">NAMESPACES</h2>
<ul>
<li>
<p>Son como ambientes separados dentro del cluster de kubernetes.  </p>
</li>
<li>
<p>Cada uno de estos ambientes tienen su deployment, replicaset, pods...  </p>
</li>
<li>
<p>Pueden haber namespaces de dev, test, de finanzas...son identicos y se pueden usar para hacer pruebas sin tener que crear otro cluster.  </p>
</li>
<li>
<p>En cada namespace se puede limitar los pods, la memoria, usuarios...</p>
</li>
<li>
<p>Ordenes básicas:<br />
<code>kubectl get namespaces</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   3d23h
kube-node-lease   Active   3d23h
kube-public       Active   3d23h
kube-system       Active   3d23h
</code></pre>

<ul>
<li>
<p>Especifica por namespace:<br />
<code>kubectl get pods --namespace default</code>  </p>
</li>
<li>
<p>El default van todos los recursos, lo creado donde no se asignan ningun namespace.  </p>
</li>
<li>Todos los usuarios pueden ver este namespace kube-public.  </li>
<li>
<p>El kube-system tiene todos los objetos del kubernetes.  </p>
</li>
<li>
<p>Si cuando queremos mirar pods, rs, deploys no ponemos nada, seran los defaults y no saldrán los asignados. Habrá que poner -n/--namespace namespaceName</p>
</li>
</ul>
<h3 id="crear-namespace">CREAR NAMESPACE</h3>
<ul>
<li>Por comando <code>kubectl create namespace nameNamespace</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl create namespace test-ns
namespace/test-ns created
</code></pre>

<ul>
<li>Para verlo <code>kubectl get namespaces</code> y <code>kubectl describe namespaces test-ns</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel k8s-hands-on]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
test-ns           Active   4s
[isx46410800@miguel k8s-hands-on]$ kubectl describe namespaces test-ns
Name:         test-ns
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Status:       Active
No resource quota.
No LimitRange resource.
</code></pre>

<ul>
<li>Por manifiesto YAML:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
</code></pre>

<ul>
<li>Comprobamos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f namespace.yaml 
namespace/development created
[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE     LABELS
default           Active   4d      &lt;none&gt;
development       Active   19s     name=development
kube-node-lease   Active   4d      &lt;none&gt;
kube-public       Active   4d      &lt;none&gt;
kube-system       Active   4d      &lt;none&gt;
test-ns           Active   6m33s   &lt;none&gt;
</code></pre>

<h3 id="asignar-namespaces">ASIGNAR NAMESPACES</h3>
<ul>
<li>Creamos un pod y lo asignamos:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl run podtest2 --image=nginx:alpine --namespace test-ns
pod/podtest2 created
[isx46410800@miguel namespaces]$ kubectl get pods -n test-ns
NAME       READY   STATUS    RESTARTS   AGE
podtest2   1/1     Running   0          22s
</code></pre>

<h3 id="borrar-namespaces">BORRAR NAMESPACES</h3>
<ul>
<li>
<p>Borramos POD asignado a namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete pods podtest2 -n test-ns</code>  </p>
</li>
<li>
<p>Borrar manifiesto:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete -f namespace.yaml</code>  </p>
</li>
<li>
<p>Borrar namespace:<br />
<code>[isx46410800@miguel namespaces]$ kubectl delete namespaces test-ns</code>  </p>
</li>
</ul>
<h3 id="deploy-namespaces">DEPLOY NAMESPACES</h3>
<ul>
<li>Creamos dos namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
--- 
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
</code></pre>

<ul>
<li>Lo vemos <code>kubectl get namespaces --show-labels</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl get namespaces --show-labels
NAME              STATUS   AGE   LABELS
default           Active   4d    &lt;none&gt;
dev               Active   6s    name=dev
kube-node-lease   Active   4d    &lt;none&gt;
kube-public       Active   4d    &lt;none&gt;
kube-system       Active   4d    &lt;none&gt;
prod              Active   6s    name=prod
</code></pre>

<ul>
<li>Creamos un deployment con los namespaces:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    name: prod
--- 
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-dev
  namespace: dev
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
---     
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-prod
  namespace: prod
  labels:
    app: back
# aqui viene el replicaset
spec:
  replicas: 5
  selector:
    matchLabels:
      app: back
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: back
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f deploy-namespace.yaml 
namespace/dev unchanged
namespace/prod unchanged
deployment.apps/deploy-dev created
deployment.apps/deploy-prod created
[isx46410800@miguel namespaces]$ kubectl get deploy -n dev
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
deploy-dev   1/1     1            1           26s
[isx46410800@miguel namespaces]$ kubectl get deploy -n prod
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
deploy-prod   5/5     5            5           29s
[isx46410800@miguel namespaces]$ kubectl get rs -n dev
NAME                   DESIRED   CURRENT   READY   AGE
deploy-dev-b7c99d94b   1         1         1       36s
[isx46410800@miguel namespaces]$ kubectl get rs -n prod
NAME                     DESIRED   CURRENT   READY   AGE
deploy-prod-7bfb7875fd   5         5         5       38s
[isx46410800@miguel namespaces]$ kubectl get pods -n dev
NAME                         READY   STATUS    RESTARTS   AGE
deploy-dev-b7c99d94b-xc696   1/1     Running   0          50s
[isx46410800@miguel namespaces]$ kubectl get pods -n prod
NAME                           READY   STATUS    RESTARTS   AGE
deploy-prod-7bfb7875fd-49kzd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-9m7x8   1/1     Running   0          54s
deploy-prod-7bfb7875fd-nbhfd   1/1     Running   0          54s
deploy-prod-7bfb7875fd-tl5gs   1/1     Running   0          54s
deploy-prod-7bfb7875fd-wxrwc   1/1     Running   0          54s
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   4d
dev               Active   10m
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
prod              Active   10m
</code></pre>

<h3 id="dns-namespaces">DNS NAMESPACES</h3>
<ul>
<li>Creamos un namespace y un deploy asignados:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: ci
  labels:
    name: ci
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: isx46410800/k8s-hands-on:v2
        imagePullPolicy: IfNotPresent
---
# añadimos el servicio
apiVersion: v1
kind: Service
metadata:
  name: backend-k8s-hands-on
  namespace: ci
  labels:
    app: backend
spec:
  type: NodePort
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 80 # servicio por donde escucha
      targetPort: 9090
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl apply -f dns-namespace.yaml 
namespace/ci created
deployment.apps/backend-k8s-hands-on created
service/backend-k8s-hands-on created
[isx46410800@miguel namespaces]$ kubectl get namespaces
NAME              STATUS   AGE
ci                Active   15s
default           Active   4d
kube-node-lease   Active   4d
kube-public       Active   4d
kube-system       Active   4d
[isx46410800@miguel namespaces]$ kubectl get deploy -n ci
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           32s
[isx46410800@miguel namespaces]$ kubectl get svc -n ci
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   38s
[isx46410800@miguel namespaces]$ kubectl get rs -n ci
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       46s
[isx46410800@miguel namespaces]$ kubectl get pods -n ci
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          49s
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          49s
</code></pre>

<ul>
<li>
<p>Ahora creamos un POD con el namespace default:<br />
<code>[isx46410800@miguel namespaces]$ kubectl run --rm -it podtest2 --image=nginx:alpine --namespace default -- sh</code>  </p>
</li>
<li>
<p>Por defecto, cuando los dns que se crean en un namespace siguen esta regla:<br />
<code>serviceName + namespaceName + service.cluster.local</code>  </p>
</li>
<li>
<p>Así desde un pod fuera del namespace se comunicaria al namespaces del deploy backend siguiendo lo anterior:<br />
<code>/ # curl backend-k8s-hands-on.ci.svc.cluster.local</code><br />
<code>{"time":"2020-10-14T01:09:56.22990857Z","hostname":"backend-k8s-hands-on-7d5b6dc559-7xv59"}/</code>  </p>
</li>
<li>
<p>Si no daría error:  </p>
</li>
</ul>
<pre><code>/ # curl backend-k8s-hands-on
curl: (6) Could not resolve host: backend-k8s-hands-on
</code></pre>

<h3 id="contextos-namespaces">CONTEXTOS NAMESPACES</h3>
<ul>
<li>
<p>Esto significa que cuando creamos algo lo creamos en default y todas las ordenes se refieren aqui, si creamos cosas en otros namespaces, podemos cambiarnos y asi no tenemos que poner el --namespace nsName todo el rato.  </p>
</li>
<li>
<p>Para ver en que contexto estamos usamos:<br />
<code>kubectl config current-context</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config current-context
minikube
</code></pre>

<ul>
<li>Vemos el archivo de configuración <code>./kube/config</code> que es de donde lee el current-context:  </li>
</ul>
<pre><code>[root@miguel ~]# cat /home/isx46410800/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Mejor con este comando <code>kubectl config view</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Crear un nuevo contexto <code>ubectl config set-context Namecontext --namespace=nsName --cluster=clusterName --user=userName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config set-context ci-context --namespace=ci --cluster=minikube --user=minikube
Context &quot;ci-context&quot; created.
</code></pre>

<ul>
<li>Ahora vemos que tenemos dos contextos y uno apuntando al namespace creado de ci:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    namespace: ci
    user: minikube
  name: ci-context
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/isx46410800/.minikube/profiles/minikube/client.crt
    client-key: /home/isx46410800/.minikube/profiles/minikube/client.key
</code></pre>

<ul>
<li>Para cambiar de contexto usamos <code>kubectl config use-context Namecontext</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel namespaces]$ kubectl config use-context ci-context
Switched to context &quot;ci-context&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
backend-k8s-hands-on-7d5b6dc559-7xv59   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-b4sqq   1/1     Running   0          19m
backend-k8s-hands-on-7d5b6dc559-bdktk   1/1     Running   0          19m
[isx46410800@miguel namespaces]$ kubectl get deploy
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
backend-k8s-hands-on   3/3     3            3           19m
[isx46410800@miguel namespaces]$ kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
backend-k8s-hands-on-7d5b6dc559   3         3         3       20m
[isx46410800@miguel namespaces]$ kubectl get services
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
backend-k8s-hands-on   NodePort   10.105.98.188   &lt;none&gt;        80:31030/TCP   20m
[isx46410800@miguel namespaces]$ kubectl config use-context minikube
Switched to context &quot;minikube&quot;.
[isx46410800@miguel namespaces]$ kubectl get pods
No resources found in default namespace.
[isx46410800@miguel namespaces]$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   4d
</code></pre>

<h2 id="limitar-ramcpu">LIMITAR RAM/CPU</h2>
<ul>
<li>
<p>La RAM se puede limitar en B, MB y G.  </p>
</li>
<li>
<p>La CPU: 1 cpu es 1000 milicores/milicpus.  </p>
</li>
</ul>
<h3 id="limitsrequest">LIMITS/REQUEST</h3>
<ul>
<li>
<p>Los <strong>LIMITS</strong> es la cifra marcada de cantidad que tiene de limite. Puede tener 30M y un pod con 20M podría tener más si tiene ese espacio libre. Si lo sobrepasa el pod ese limite, kubernetes eliminará o reiniciará el pod y lo pondrá en otro sitio que le garantice esa cantidad de recursos indicada.  </p>
</li>
<li>
<p>Los <strong>REQUESTS</strong> es la cantidad de recursos que el pod siempre va a poder disponer. Estará garantizado la cantidad que se le indique.  </p>
</li>
</ul>
<h4 id="ram">RAM</h4>
<ul>
<li>Creamos un ejemplo de limite de RAM:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: &quot;200Mi&quot;
      requests:
        memory: &quot;100Mi&quot;
    command: [&quot;stress&quot;]
    # se indica que le va a dar 150Megas
    args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;]
</code></pre>

<blockquote>
<p>Garantizamos que va a tener 100Megas seguras y un limite maximo de 200. Se le asignar a crear un pod de 150M.  </p>
</blockquote>
<ul>
<li>Comprobamos que lo ha creado <code>kubectl apply -f limit-request.yaml</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   1/1     Running   0          38s
</code></pre>

<ul>
<li>Si ponemos el ejemplo anterior con 250M vemos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limit-request2.yaml 
pod/memory-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
memory-demo   0/1     ContainerCreating   0          4s
[isx46410800@miguel limits-requests]$ kubectl get pods --watch
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   1          12s
memory-demo   0/1     OOMKilled          2          25s
memory-demo   0/1     CrashLoopBackOff   2          26s
^C[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS             RESTARTS   AGE
memory-demo   0/1     CrashLoopBackOff   2          48s
</code></pre>

<ul>
<li>Si ponemos un limit y request de 1000G de Ram (algo imposible), veremos los errores:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
memory-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod memory-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient memory.
</code></pre>

<h4 id="cpu">CPU</h4>
<ul>
<li>Ejemplo de limitar CPU:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
spec:
  containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;0.5&quot;
    args:
    - -cpus
    - &quot;2&quot;
    # se le pide 2 cpus y hay limite de 1
</code></pre>

<blockquote>
<p>Aunque se le pida 2, no se eliminará como la RAM sino que soolo tendrá de máximo el LIMIT indicado(1).  </p>
</blockquote>
<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl apply -f limitar-cpu.yaml 
pod/cpu-demo created
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS              RESTARTS   AGE
cpu-demo   0/1     ContainerCreating   0          7s
[isx46410800@miguel limits-requests]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
cpu-demo   1/1     Running   0          11s
</code></pre>

<ul>
<li>Si vemos la capacidad total de mi cluster <code>kubectl describe node minikube</code>: </li>
</ul>
<pre><code>Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1150m (28%)  1 (25%)
  memory             70Mi (0%)    170Mi (2%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
</code></pre>

<blockquote>
<p>Siempre hay un poco más para que no sobrepase el limite y me vaya todo lento.  </p>
</blockquote>
<ul>
<li>Nuestra cantidad de CPU <code>kubectl describe node minikube</code>:  </li>
</ul>
<pre><code>kubectl describe node minikube
Capacity:
  cpu:                4
</code></pre>

<ul>
<li>Si le ponemos limite y request 100 cpu veremos que nos sale los mismos errores que la RAM:  </li>
</ul>
<pre><code>[isx46410800@miguel limits-requests]$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
cpu-demo   0/1     Pending   0          5s
[isx46410800@miguel limits-requests]$ kubectl describe pod cpu-demo
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  25s (x2 over 25s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.
</code></pre>

<h3 id="qosquality-of-service">QOS(Quality of Service)</h3>
<ul>
<li>
<p>Es una propiedad que se le asigna a los pods.  </p>
</li>
<li>
<p>Podemos ver el estado de QoS con:<br />
<code>kubectl get pod podName -o yaml | grep -i qos</code>  </p>
</li>
<li>
<p>Hay diferentes tipos de clases de estado en el que entra el POD:  </p>
<ul>
<li><strong>BestEffort</strong>: No se definen los limites y request. Los asignará el schedule pero puede ser que este consuma y consuma recursos sin parar.</li>
<li><strong>Guaranteed</strong>: Tiene los mismos limites que de request</li>
<li><strong>Burstable</strong>: cuando pueda aumentar el request. El limite es mayor que el request.  </li>
</ul>
</li>
</ul>
<h2 id="limitrange">LIMITRANGE</h2>
<ul>
<li>
<p>Es un objeto de kubernetes que nos permite controlar limites a nivel de objetos, a nivel de namespaces.  </p>
</li>
<li>
<p>Puedo indicar limites por defectos de los pods en el namespaces si no tiene asignado ninguno, podemos definir minimos y maxinos de recursos de los pods</p>
</li>
</ul>
<h3 id="valores-por-defecto">VALORES POR DEFECTO</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
</code></pre>

<blockquote>
<p>El objeto LIMITRANGE se crea en el namespace indicado, sino, se crea en el default.  </p>
</blockquote>
<ul>
<li>Comprobamos con <code>kubectl get limitrange -n namespaceName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl apply -f default-cpu-ram.yaml 
namespace/dev created
limitrange/mem-limit-range created
#
[isx46410800@miguel limitRange]$ kubectl get limitrange -n dev
NAME              CREATED AT
mem-limit-range   2020-10-14T18:10:15Z
</code></pre>

<ul>
<li>Comprobamos con <code>kubectl describe limitrange LRName -n NSName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl describe limitrange mem-limit-range -n dev
Name:       mem-limit-range
Namespace:  dev
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    500m             1              -
Container   memory    -    -    256Mi            512Mi          -
</code></pre>

<h3 id="valores-pod">VALORES POD</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
---
# pod
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  namespace: dev
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
</code></pre>

<ul>
<li>Comprobamos los resultados del pod y sus limites creados al asignarlo a este namespaces con el objeto de limitRange:  </li>
</ul>
<pre><code>[isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i limits -C3
      Started:      Wed, 14 Oct 2020 20:21:43 +0200
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
[isx46410800@miguel limitRange]$ kubectl describe pods pod-test3 -n dev | grep -i requests -C3
    Limits:
      cpu:     1
      memory:  512Mi
    Requests:
      cpu:        500m
      memory:     256Mi
    Environment:  &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos que se han asignado la cpu de 0.5 y Ram 256M.  </p>
</blockquote>
<h3 id="limites">LIMITES</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># namespace
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# limit range para el namespace dev
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: dev
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 1
    defaultRequest:
      memory: 256Mi
      cpu: 0.5
    type: Container
---
# pod
apiVersion: v1
kind: Pod
metadata:
  name: pod-test3
  namespace: dev
  labels:
    app: back-end
    env: dev
spec:
  containers:
    - name: container1
      image: nginx:alpine
    resources:
      limits:
        memory: 500M
        cpu: 0.5
      requests:
        memory: 400M
        cpu: 0.3
</code></pre>

<blockquote>
<p>Si se superan los limites en los PODs te dará error, ya que sobrepasa los limites de memoria y ram  </p>
</blockquote>
<h2 id="resource-quota">RESOURCE QUOTA</h2>
<ul>
<li>
<p>Actua a nivel de namespace. Limita la sumatoria de todos los objetos individuales de lo que tiene dentro.  </p>
</li>
<li>
<p>Si el RQ tiene limite 3cpu, la suma de sus pods dentro del namespaces de no puede sobrepasar el uso de 3 cpus.  </p>
</li>
<li>
<p>El limitrange opera por objeto, por pod.  </p>
</li>
</ul>
<h3 id="crear-rq">CREAR RQ</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: uat
  labels:
    name: uat
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
  namespace: uat
spec:
  hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
</code></pre>

<ul>
<li>Comprobamos con <code>kubectl describe resourcequota -n nsName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota.yaml 
namespace/uat created
resourcequota/mem-cpu-demo created
[isx46410800@miguel resource-quota]$ kubectl describe resourcequota -n uat mem-cpu-demo
Name:            mem-cpu-demo
Namespace:       uat
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     2
limits.memory    0     2Gi
requests.cpu     0     1
requests.memory  0     1Gi
</code></pre>

<ul>
<li>Resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe ns uat 
Name:         uat
Labels:       name=uat
Annotations:  &lt;none&gt;
Status:       Active
Resource Quotas
 Name:            mem-cpu-demo
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       0     2
 limits.memory    0     2Gi
 requests.cpu     0     1
 requests.memory  0     1Gi
No LimitRange resource.
</code></pre>

<h3 id="deploy-rq">DEPLOY RQ</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: uat
  labels:
    name: uat
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
  namespace: uat
spec:
  hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
--- 
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  namespace: uat
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 2
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            memory: 500M
            cpu: 0.5
          limits:
            memory: 500M
            cpu: 0.5
</code></pre>

<ul>
<li>Comprobamos lo creado:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl get pods -n uat
NAME                               READY   STATUS    RESTARTS   AGE
deployment-test-5f869977fb-84nqs   1/1     Running   0          2m40s
deployment-test-5f869977fb-vg5cj   1/1     Running   0          2m45s
[isx46410800@miguel resource-quota]$ kubectl get rs -n uat
NAME                         DESIRED   CURRENT   READY   AGE
deployment-test-5f869977fb   2         2         2       2m54s
deployment-test-df54c6d6d    0         0         0       5m41s
[isx46410800@miguel resource-quota]$ kubectl get deploy -n uat
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
deployment-test   2/2     2            2           5m47s
[isx46410800@miguel resource-quota]$ kubectl get resourcequota -n uat
NAME           AGE     REQUEST                                      LIMIT
mem-cpu-demo   5m57s   requests.cpu: 1/1, requests.memory: 1G/1Gi   limits.cpu: 1/2, limits.memory: 1G/2Gi
</code></pre>

<ul>
<li>Con lo creado ahora podemos ver que hemos llegado a los limites <code>kubectl describe ns nsName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe ns uat
Name:         uat
Labels:       name=uat
Annotations:  &lt;none&gt;
Status:       Active
Resource Quotas
 Name:            mem-cpu-demo
 Resource         Used  Hard
 --------         ---   ---
 limits.cpu       1     2
 limits.memory    1G    2Gi
 requests.cpu     1     1
 requests.memory  1G    1Gi
No LimitRange resource.
</code></pre>

<ul>
<li>Si ahora modificamos el fichero y creamos 3 replicas, superará el limite indicado. Por lo que solo creará dos y no tres, ya que el 3 superará los limites asignados en el RESOURCE QUOTA.  </li>
</ul>
<h3 id="limitar-no-pods-en-ns">LIMITAR Nº PODS EN NS</h3>
<ul>
<li>Vemos un ejemplo de como limitar el número de pods que se pueden crear en un namespace a través del ResourceQuota:  </li>
</ul>
<pre><code>---
# creamos namespaces
apiVersion: v1
kind: Namespace
metadata:
  name: qa
  labels:
    name: qa
---
# creamos resoucequota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
  namespace: qa
spec:
  hard:
    pods: &quot;3&quot;
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-qa
  namespace: qa
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Comprobamos lo creado:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl apply -f resource_quota_limitarPods.yaml 
namespace/qa created
resourcequota/pod-demo created
deployment.apps/deployment-qa created
#
[isx46410800@miguel resource-quota]$ kubectl get pods -n qa
NAME                            READY   STATUS    RESTARTS   AGE
deployment-qa-b7c99d94b-h5bxr   1/1     Running   0          10s
deployment-qa-b7c99d94b-tttpn   1/1     Running   0          10s
deployment-qa-b7c99d94b-xdl45   1/1     Running   0          10s
[isx46410800@miguel resource-quota]$ kubectl get rs -n qa
NAME                      DESIRED   CURRENT   READY   AGE
deployment-qa-b7c99d94b   3         3         3       14s
#
[isx46410800@miguel resource-quota]$ kubectl get ns -n qa
NAME              STATUS   AGE
ci                Active   18h
default           Active   4d19h
kube-node-lease   Active   4d19h
kube-public       Active   4d19h
kube-system       Active   4d19h
qa                Active   18s
#
[isx46410800@miguel resource-quota]$ kubectl get resourcequota -n qa
NAME       AGE   REQUEST     LIMIT
pod-demo   99s   pods: 3/3   
</code></pre>

<ul>
<li>Más info <code>kubectl describe resourcequota pod-demo -n qa</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel resource-quota]$ kubectl describe resourcequota pod-demo -n qa
Name:       pod-demo
Namespace:  qa
Resource    Used  Hard
--------    ----  ----
pods        3     3
</code></pre>

<ul>
<li>Si ponemos 4 replicas, solo se habrán creado 3 y el 4 veremos en errores de NS que no se pudo crear un 4 pod porque supera los limites asignados al Resource Quota.  </li>
</ul>
<h2 id="probes">PROBES</h2>
<ul>
<li>
<p>Es una prueba diagnostico que se ejecuta en un POD para saber el estado de un container.  </p>
</li>
<li>
<p>Cada cierto tiempo va ir preguntando al POD para ver como se encuentra y si tiene algun fallo sino contesta.  </p>
</li>
<li>
<p>Puede ser este PROBE por:  </p>
</li>
<li>Comando</li>
<li>TCP</li>
<li>HTTP</li>
</ul>
<h3 id="tipos-probes">TIPOS PROBES</h3>
<ul>
<li>
<p>Liveness: es una prueba que se ejecuta en el contenedor cada N tiempo. Esperamos una respuesta de este contenedor. Asegurarnos que esté funcionando la aplicación del contenedor.  </p>
</li>
<li>
<p>Readiness:  nos ayuda a garantizar el servicio del pod está listo para el request.  </p>
</li>
<li>
<p>Startup: es una prueba que se sube para ver que esté todo configurado y este listo la aplicación para ejecutarse.  </p>
</li>
</ul>
<h3 id="crear-liveness-probe">CREAR LIVENESS PROBE</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># probe liveness
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
</code></pre>

<blockquote>
<p>Cada 5 segundos crea un fichero, y al 35 se elimina. Va haciendo una prueba de que sigue vivo.  </p>
</blockquote>
<ul>
<li>
<p>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </p>
</li>
<li>
<p>Pruebas:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel probes]$ kubectl apply -f liveness.yaml 
pod/liveness-exec created
[isx46410800@miguel probes]$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
liveness-exec   1/1     Running   0          9s
#
[isx46410800@miguel probes]$ kubectl describe pod liveness-exec
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  95s                default-scheduler  Successfully assigned default/liveness-exec to minikube
  Normal   Pulled     90s                kubelet            Successfully pulled image &quot;k8s.gcr.io/busybox&quot; in 3.165552593s
  Warning  Unhealthy  46s (x3 over 56s)  kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    46s                kubelet            Container liveness failed liveness probe, will be restarted
  Normal   Pulling    15s (x2 over 93s)  kubelet            Pulling image &quot;k8s.gcr.io/busybox&quot;
  Normal   Pulled     15s                kubelet            Successfully pulled image &quot;k8s.gcr.io/busybox&quot; in 751.39074ms
  Normal   Created    14s (x2 over 89s)  kubelet            Created container liveness
  Normal   Started    14s (x2 over 88s)  kubelet            Started container liveness
</code></pre>

<h3 id="liveness-tcp">LIVENESS TCP</h3>
<ul>
<li>Una probe con liveness TCP:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
</code></pre>

<ul>
<li>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </li>
</ul>
<h3 id="liveness-http">LIVENESS HTTP</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 5
      periodSeconds: 3
</code></pre>

<ul>
<li>Vemos resultados de lo que pasa en <code>kubectl describe pod podName</code>  </li>
</ul>
<h3 id="readiness-probe">READINESS PROBE</h3>
<ul>
<li>Una probe con readiness TCP:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
</code></pre>

<blockquote>
<p>La diferencia es que el readiness no reinicia el contenedor, sino que desenregistra el puerto para que no entren más peticiones de request y por lo tanto no se le de más carga a este contenedor/pod.  </p>
</blockquote>
<h2 id="variables-y-configmap">VARIABLES Y CONFIGMAP</h2>
<h3 id="crear-variables">CREAR VARIABLES</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: envar-demo
spec:
  containers:
  - name: envar-demo-container
    image: nginx:alpine
    env:
    - name: VAR1
      value: &quot;valor de prueba 1&quot;
    - name: VAR2
      value: &quot;valor de prubea 2&quot;
    - name: VAR3
      value: &quot;valor de prubea 3&quot;
</code></pre>

<ul>
<li>Prueba:  </li>
</ul>
<pre><code>[isx46410800@miguel env_variables]$ kubectl apply -f env.yaml 
pod/envar-demo created
#
[isx46410800@miguel env_variables]$ kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
envar-demo   1/1     Running   0          12s
#
[isx46410800@miguel env_variables]$ kubectl exec -it envar-demo -- sh
/ # env
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT=tcp://10.96.0.1:443
HOSTNAME=envar-demo
SHLVL=1
HOME=/root
VAR1=valor de prueba 1
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
VAR2=valor de prubea 2
VAR3=valor de prubea 3
PWD=/
#
/ # echo $VAR1
valor de prueba 1
</code></pre>

<h3 id="variables-referenciadas">VARIABLES REFERENCIADAS</h3>
<ul>
<li>Se crearian a partir de conseguir la info del pod a partir del <code>[isx46410800@miguel env_variables]$ kubectl get pods envar-demo -o yaml</code>:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-fieldref
spec:
  containers:
    - name: test-container
      image: ngix:alpine
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
  restartPolicy: Never
</code></pre>

<blockquote>
<p>Dentro del POD tendremos estas variables con el valor obtenido de su spec, metadata,etc.  </p>
</blockquote>
<h3 id="configmap">CONFIGMAP</h3>
<ul>
<li>
<p>Es un objeto de kubernetes distinto a un POD en el cual tienes configuraciones que un POD puede consumir de el para su creación.  </p>
</li>
<li>
<p>Se forma con la estructura <code>clave: valor</code>. Desde el POD se indica que llave quiere consumir del configmap.  </p>
</li>
<li>
<p>Se puede crear mediante un file.conf o en un objeto configmap.  </p>
</li>
<li>
<p>Copiamos en un subdirectorio el fichero de conf de nginx y creamos un confimap a partir de este fichero.</p>
</li>
<li>
<p>Lo creamos con <code>kubectl create configmap nginx-config --from-file=examples/nginx.conf</code> y lo vemos con <code>kubectl get cm</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl create configmap nginx-config --from-file=examples/nginx.conf
configmap/nginx-config created
#
[isx46410800@miguel configmap]$ kubectl get cm
NAME           DATA   AGE
nginx-config   1      14s
#
[isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config
Name:         nginx-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Data
====
nginx.conf:
----
server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
Events:  &lt;none&gt;
</code></pre>

<blockquote>
<p>Vemos que se ha creado en formato llave(nginx.conf) y valor la configuración.  </p>
</blockquote>
<ul>
<li>Ejemplo con todos los archivos del subdirectorio y vemos que se crean más llaves-valor:  </li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl create configmap nginx-config2 --from-file=examples
configmap/nginx-config2 created
#
[isx46410800@miguel configmap]$ kubectl get cm
NAME            DATA   AGE
nginx-config    1      4m27s
nginx-config2   2      4s
#
[isx46410800@miguel configmap]$ kubectl describe configmaps nginx-config2
Name:         nginx-config2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Data
====
index.html:
----
hola nginx
nginx.conf:
----
server {
    listen       80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
Events:  &lt;none&gt;
</code></pre>

<h3 id="montando-volumen-configmap">MONTANDO VOLUMEN CONFIGMAP</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  labels:
    app: front
  data:
    test: hola
    nginx: |
      server {
        listen       80;
        server_name  localhost;

        location / {
                root   /usr/share/nginx/html;
                index  index.html index.htm;
         }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
                root   /usr/share/nginx/html;
         }
      }
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  annotations:
    kubernetes.io/change-cause: &quot;new version nginx&quot;
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
        apiVersion: v1
        kind: Pod
        metadata:
          name: dapi-test-pod
        spec:
          containers:
            - name: nginx
              image: nginx:alpine
              volumeMounts:
              - name: nginx-volume
                mountPath: /etc/nginx/conf.d/ ## la ruta que va a tener, solo carpetas
          volumes:
            - name: nginx-volume
              configMap:
                name: nginx-config
                items:
                - key: nginx
                  path: default.conf
</code></pre>

<blockquote>
<p>En la data son las llaves-valor del configmap. Volumemount el volumen a crear y a que carpeta ira sin coger la ruta de los archivos. Volumes el que se crea a raiz del nombre de configmap y items son que llave coge y path el nombre que le pondremos al valor de la llave. Si no se pone items, creara varios archivos con los nombres de las keys y su contenido como archivo.  </p>
</blockquote>
<h3 id="volumen-env-configmap">VOLUMEN-ENV CONFIGMAP</h3>
<ul>
<li>Ejemplo de montar un volumen y variables de entorno referenciando otro configmap con las variables y creando otro volumen para montar una llave que es un script:  </li>
</ul>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  labels:
    app: front
data:
  nginx: |
    server {
        listen       9090;
        server_name  localhost;
        location / {
            root   /usr/share/nginx/html;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   /usr/share/nginx/html;
        }
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vars
  labels:
    app: front
data:
  db_host: dev.host.local
  db_user: dev_user
  script: |
    echo DB host es $DB_HOST y DB user es $DB_USER &gt; /usr/share/nginx/html/test.html
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
  labels:
    app: front
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          env:
            - name: DB_HOST
              valueFrom:
                configMapKeyRef:
                  name: vars
                  key: db_host
            - name: DB_USER
              valueFrom:
                configMapKeyRef:
                  name: vars
                  key: db_user
          volumeMounts:
          - name: nginx-vol
            mountPath: /etc/nginx/conf.d
          - name: script-vol
            mountPath: /opt
      volumes:
        - name: nginx-vol
          configMap:
            name: nginx-config
            items:
            - key: nginx
              path: default.conf
        - name: script-vol
          configMap:
            name: vars
            items:
            - key: script
              path: script.sh
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel configmap]$ kubectl exec -it deployment-test-56457d48c5-7sg8z -- sh
/ # ls /opt
script.sh
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
KUBERNETES_PORT_443_TCP_PORT=443
NJS_VERSION=0.4.4
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=10.96.0.1
PWD=/
DB_HOST=dev.host.local
DB_USER=dev_user
/ # echo $DB_HOST
dev.host.local
/ # apk add python
/ # sh /opt/script.sh 
/ # cat /usr/share/nginx/html/test.html
DB host es dev.host.local y DB user es dev_user
</code></pre>

<h2 id="secrets">SECRETS</h2>
<ul>
<li>
<p>Un secreto es un objeto que nos ayuda a guardar data sensible, aquella que no debería de verse. Funciona al estilo configmap.  </p>
</li>
<li>
<p>Lo podemos montar como una variable de entorno o como un volumen.  </p>
</li>
</ul>
<h3 id="crear">CREAR</h3>
<ul>
<li>Ejemplo de como crearlo:<br />
<code>kubectl create secret generic mysecret --from-file=secret-files/text.txt</code><br />
<code>kubectl get secrets</code>  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ cat secret-files/text.txt 
secret1=hola
#
[isx46410800@miguel secrets]$ kubectl create secret generic mysecret --from-file=secret-files/text.txt
secret/mysecret created
#
[isx46410800@miguel secrets]$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-xbv2l   kubernetes.io/service-account-token   3      7d
mysecret              Opaque                                1      7s
#
[isx46410800@miguel secrets]$ kubectl describe secrets mysecret
Name:         mysecret
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Type:  Opaque
Data
====
text.txt:  26 bytes
#
secret2=adios[isx46410800@miguel secrets]$ kubectl get secrets mysecret -o yaml
apiVersion: v1
data:
  text.txt: c2VjcmV0MT1ob2xhCnNlY3JldDI9YWRpb3M=
kind: Secret
metadata:
  creationTimestamp: &quot;2020-10-17T00:55:07Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:text.txt: {}
      f:type: {}
    manager: kubectl-create
    operation: Update
    time: &quot;2020-10-17T00:55:07Z&quot;
  name: mysecret
  namespace: default
  resourceVersion: &quot;72991&quot;
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 46d433c6-2c0f-4646-aa9d-b165c6abfee2
type: Opaque
</code></pre>

<blockquote>
<p>Vemos que el contenido de los secretos no se ven, están cifrados en BASE64, que se puede descrifrar poniendo <code>| base65 -decode</code>  </p>
</blockquote>
<h3 id="manifiestos">MANIFIESTOS</h3>
<ul>
<li>Creando SECRETS con manifiesto:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
</code></pre>

<blockquote>
<p>Para descrifrarlo hay que pasarlo de base64.  </p>
</blockquote>
<ul>
<li>Con Datastring para que lo codifique en base64:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: opaque
stringData:
  username: usertest
  password: test
</code></pre>

<h3 id="envsubts">ENVSUBTS</h3>
<ul>
<li>Herramienta para poder reemplazar contenido de variables por el contenido:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: mysecret2
type: opaque
data:
  username: $VAR1
  password: $VAR2
</code></pre>

<pre><code>[isx46410800@miguel secrets]$ export VAR1=miguel
[isx46410800@miguel secrets]$ export VAR2=amoros
[isx46410800@miguel secrets]$ envsubst &lt; secret-secure.yaml &gt; tmp.yaml
[isx46410800@miguel secrets]$ cat tmp.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret2
type: opaque
data:
  username: miguel
  password: amoros
[isx46410800@miguel secrets]$ kubectl apply -f tmp.yaml
</code></pre>

<blockquote>
<p>Luego podemos decode con base64 y obtenemos el resultado.  </p>
</blockquote>
<h3 id="volume-secrets">VOLUME SECRETS</h3>
<ul>
<li>Un ejemplo de crear un secreto y montarlo como volumen:  </li>
</ul>
<pre><code># creamos el secreto
apiVersion: v1
kind: Secret
metadata:
  name: secret1
type: opaque
stringData:
  username: admin
  password: &quot;123456&quot;
---
# montamos el secreto
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: nginx:alpine
    volumeMounts:
    - name: test
      #donde montamos el secreto
      mountPath: &quot;/opt&quot;
      readOnly: true
  volumes:
  - name: test
    secret:
      secretName: secret1
</code></pre>

<blockquote>
<p>En lo ultimo tambien podemos crearlo poniendo items e indicarle el path. ahora nos creara dos files al no ponerlo.  </p>
</blockquote>
<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ kubectl apply -f pod-vol-secret.yaml 
secret/secret1 created
pod/mypod created
#
[isx46410800@miguel secrets]$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-xbv2l   kubernetes.io/service-account-token   3      7d
secret1               opaque                                2      6s
#
[isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh
/ # ls /opt/
password  username
/ # cat /opt/password 
123456/ # 
/ # cat /opt/username 
admin/ # 
</code></pre>

<h3 id="env-secrets">ENV SECRETS</h3>
<ul>
<li>Un ejemplo de crear un secreto y montarlo como varibale de entorno:  </li>
</ul>
<pre><code># creamos el secreto
apiVersion: v1
kind: Secret
metadata:
  name: secret1
type: opaque
stringData:
  username: admin
  password: &quot;123456&quot;
---
# montamos el secreto
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: nginx:alpine
    env:
      - name: USERTEST
        valueFrom:
          secretKeyRef:
            name: secret1
            key: username
      - name: PASSWORDTEST
        valueFrom:
          secretKeyRef:
            name: secret1
            key: password
    volumeMounts:
    - name: test
      #donde montamos el secreto
      mountPath: &quot;/opt&quot;
      readOnly: true
  volumes:
  - name: test
    secret:
      secretName: secret1
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel secrets]$ kubectl apply -f pod-vol-env-secret.yaml 
secret/secret1 created
pod/mypod created
[isx46410800@miguel secrets]$ kubectl exec -it mypod -- sh
/ # ls /opt/
password  username
/ # echo $USERTEST $PASSWORDTEST
admin 123456
</code></pre>

<h2 id="volumes">VOLUMES</h2>
<ul>
<li>
<p>Sirven para persistir data de los container y no se pierdan cuando se borran.  </p>
</li>
<li>
<p>Tipos de volumenes:  </p>
</li>
<li><strong>EMPTYDIR</strong>: es un directorio vacio que se crea cuando se crea el pod. Si se elimina el container se pierde la xixa, pero esta xixa se queda como en un directorio de pod y cuando se crea de nuevo el container, el container puede recuperar esta xixa montandola.  </li>
<li><strong>HOSTPATH</strong>: nos ayuda a crear un volumen en el nodo donde corre el pod. Si se elimina el pod no se pierde todo como en el anterior, sino que solo se pierde si se elimina el nodo.  </li>
<li><strong>CLOUDVOLS</strong>: en amazon son discos que se llaman EBS y en GCP se llaman PD. Busca el contenido en la nube. Así si se elimina el POD puede construirse de nuevo y la info sigue apuntando en el volumen de la nube.  </li>
<li><strong>PV y PVC</strong>: es la reclamación de un PV. El PV contiene un mount y un volume de origen. A través del PVC accedemos al PV, reclamando los recursos que necesita, y éste accede al cloud.  </li>
<li><strong>RECLAIM</strong>: un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).  </li>
</ul>
<h3 id="emptydir">EMPTYDIR</h3>
<ul>
<li>
<p>Si creamos un pod solo y lo reiniciamos, el contenido creado dentro se pierde.  </p>
</li>
<li>
<p>Si creamos un pod con un volume emptydir, cuando se reinicia el contenedor, seguimos manteniendo la xixa dentro, ya que <em>emptydir</em> te crea un directorio a la altura del pod con la xixa del contenedor.  </p>
</li>
<li>
<p>Solo si se elimina el pod es cuando perdemos este directorio y por tanto la xixa.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-test2
spec:
  containers:
    - name: cont-emptydir
      image: nginx:alpine
      volumeMounts:
        - name: vol-emptydir
          mountPath: var/log/nginx
  volumes:
  - name: vol-emptydir
    emptyDir: {}
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl apply -f emptydir.yaml 
pod/pod-test2 created
#
[isx46410800@miguel volumes]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   1/1     Running   0          5s
#
[isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh
#
/ # touch /var/log/nginx/empytdir.txt
#
/ # ps aix
PID   USER     TIME  COMMAND
    1 root      0:00 nginx: master process nginx -g daemon off;
   29 nginx     0:00 nginx: worker process
   30 nginx     0:00 nginx: worker process
   31 nginx     0:00 nginx: worker process
   32 nginx     0:00 nginx: worker process
   33 root      0:00 sh
   39 root      0:00 ps aix
/ # pkill nginx
/ # command terminated with exit code 137
#
[isx46410800@miguel volumes]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   1/1     Running   1          47s
#
[isx46410800@miguel volumes]$ kubectl exec -it pod-test2 -- sh
/ # ls /var/log/nginx/
access.log    empytdir.txt  error.log
/ # 
</code></pre>

<h3 id="hostpath-pv">HOSTPATH-PV</h3>
<ul>
<li>En el hostpath la carpeta con el contenido se guarda en altura de nodo.  </li>
<li>
<p>El pv es el trozo de hardware que se crea con recursos indicados. Es el disco de recursos y se guarda la xixa en este caso en hostpath que es una carpeta.   </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/test&quot; # donde esta el storage real d mi pv
</code></pre>

<ul>
<li>Comprobar con <code>kubectl get pv</code> y <code>kubectl describe pv pvName</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl get pv --show-labels
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE   LABELS
task-pv-volume   10Gi       RWO            Retain           Available           manual                  18s   type=local
#
[isx46410800@miguel volumes]$ kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Available
Claim:           
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        10Gi
Node Affinity:   &lt;none&gt;
Message:         
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /test
    HostPathType:  
Events:            &lt;none&gt;
</code></pre>

<h3 id="pvc">PVC</h3>
<ul>
<li>
<p>El PVC sirve para reclamar el espacio necesario para nuestro PV que queremos crear.  </p>
</li>
<li>
<p>Cuando no se especifica el PV a unirse, el PVC reclama un PV que reuna las caracteristicas que se indican.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/test&quot; # donde esta el storage real d mi pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl get pvc
NAME            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume   10Gi       RWO            manual         5s
#
[isx46410800@miguel volumes]$ kubectl get pv
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Bound    default/task-pv-claim   manual                  5m14s
</code></pre>

<blockquote>
<p>El estado ahora del PV es <code>bound</code> que significa que se ha unido a un PVC.  </p>
</blockquote>
<h3 id="pvc-pv">PVC-PV</h3>
<ul>
<li>
<p>Para unir un PVC a un PV concreto, se hace con selectors.  </p>
</li>
<li>
<p>Ejemplo:  </p>
</li>
</ul>
<pre><code># PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/test&quot; # donde esta el storage real d mi pv
---
# PV con selector para un PVC concreto
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume2
  labels:
    mysql: ready
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/mysql&quot; # donde esta el storage real d mi pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
        mysql: ready
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl apply -f pv-pvc.yaml 
persistentvolume/task-pv-volume created
persistentvolume/task-pv-volume2 created
persistentvolumeclaim/task-pv-claim created
#
[isx46410800@miguel volumes]$ kubectl get pvc
NAME            STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume2   10Gi       RWO            manual         3s
#
[isx46410800@miguel volumes]$ kubectl get pv --show-labels
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                   STORAGECLASS   REASON   AGE   LABELS
task-pv-volume    10Gi       RWO            Retain           Available                           manual                  19s   type=local
task-pv-volume2   10Gi       RWO            Retain           Bound       default/task-pv-claim   manual                  19s   mysql=ready
</code></pre>

<blockquote>
<p>Vemos que se ha unido el PV2 con el PVC como indicamos en los selector.  </p>
</blockquote>
<h3 id="pvc-pods">PVC-PODS</h3>
<ul>
<li>De esta manera sin indicar en el POD los volumenes, no persiste la información. Por ejemplo si creamos una base de datos y eliminamos el POD, el nuevo pod no tendrá esa base de datos:  </li>
</ul>
<pre><code># PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
  labels:
    mysql: ready
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/mysql&quot; # donde esta el storage real d mi pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
        mysql: ready
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  annotations:
    kubernetes.io/change-cause: &quot;new version nginx&quot;
  labels:
    app: mysql
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        env:
          - name: MYSQL_ROOT_PASSWORD
            value: &quot;12345678&quot;
</code></pre>

<ul>
<li>Ahora lo creamos con volumenes para que persista la data:  </li>
</ul>
<pre><code># PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
  labels:
    mysql: ready
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/mysql&quot; # donde esta el storage real d mi pv
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
        mysql: ready
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  annotations:
    kubernetes.io/change-cause: &quot;new version nginx&quot;
  labels:
    app: mysql
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        env:
          - name: MYSQL_ROOT_PASSWORD
            value: &quot;12345678&quot;
        volumeMounts: # montamos dentro del contenedor, lo que queremos guardar
          - mountPath: &quot;/var/lib/mysql&quot;
            name: vol-mysql
      volumes:
        - name: vol-mysql
          persistentVolumeClaim:
            claimName: test-pvc
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl apply -f pod-pvc-volumen.yaml 
persistentvolume/test-pv created
persistentvolumeclaim/test-pvc created
deployment.apps/mysql created
#
[isx46410800@miguel volumes]$ kubectl get pvc
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-pvc   Bound    test-pv   10Gi       RWO            manual         7s
#
[isx46410800@miguel volumes]$ kubectl get pv
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
test-pv   10Gi       RWO            Retain           Bound    default/test-pvc   manual                  9s
#
[isx46410800@miguel volumes]$ kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
mysql-555cf6cd95   1         1         1       16s
[isx46410800@miguel volumes]$ kubectl get deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mysql   1/1     1            1           19s
#
[isx46410800@miguel volumes]$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
mysql-555cf6cd95-nj8xd   1/1     Running   0          22s
#
[isx46410800@miguel volumes]$ kubectl describe pv test-pv
Name:            test-pv
Labels:          mysql=ready
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    manual
Status:          Bound
Claim:           default/test-pvc
Reclaim Policy:  Retain
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        10Gi
Node Affinity:   &lt;none&gt;
Message:         
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /mysql
    HostPathType:  
Events:            &lt;none&gt;
</code></pre>

<ul>
<li>Vemos que persiste la data creada en el pod original y al eliminarlo y crear otro está la bbdd creada de antes:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
mysql-555cf6cd95-nj8xd   1/1     Running   0          56m
#
[isx46410800@miguel volumes]$ kubectl delete pod mysql-555cf6cd95-nj8xd
pod &quot;mysql-555cf6cd95-nj8xd&quot; deleted
#
[isx46410800@miguel volumes]$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
mysql-555cf6cd95-6ns2n   1/1     Running   0          12s
#
[isx46410800@miguel volumes]$ kubectl exec -it mysql-555cf6cd95-6ns2n -- sh
# mysql -u root -p12345678
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| test               |
+--------------------+
5 rows in set (0.00 sec)
mysql&gt; 
</code></pre>

<h3 id="cloud-volumes">CLOUD VOLUMES</h3>
<ul>
<li>
<p>Son los storages que estan en la nube.  </p>
</li>
<li>
<p>Son de provisionamiento dinámico, no hace falta crear manualmente el PV para unirlo al PVC.  </p>
</li>
<li>
<p>Para verlos se usa <code>kubectl get sc|storageclass</code>, por defecto en minikube es el <em>standard</em>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl get sc
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  7d18h
</code></pre>

<ul>
<li>Creamos un PVC con cloud:  </li>
</ul>
<pre><code># PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sc-pvc
spec:
  #storageClassName: standard(por defecto)  
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel volumes]$ kubectl get sc
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  7d18h
#
[isx46410800@miguel volumes]$ kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sc-pvc     Bound    pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8   20Gi       RWO            standard       11s
test-pvc   Bound    test-pv                                    10Gi       RWO            manual         67m
#
[isx46410800@miguel volumes]$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
pvc-61def8c2-64a3-4f88-a7f1-e1572b1477f8   20Gi       RWO            Delete           Bound    default/sc-pvc     standard                14s
test-pv                                    10Gi       RWO            Retain           Bound    default/test-pvc   manual                  67m
</code></pre>

<blockquote>
<p>Crea dinamicamente un PV al PVC.  </p>
</blockquote>
<h3 id="reclaim-policy">RECLAIM POLICY</h3>
<ul>
<li>
<p>Por defecto, si creamos un PVC manualmente es <code>retain</code> y si lo creamos dinamicamente es <code>delete</code>.  </p>
</li>
<li>
<p>Si es retain y eliminamos el PVC, el PV se mantiene vivo con la xixa dentro.  </p>
</li>
<li>
<p>Para cambiar el estado del reclaim policy se usa <code>kubectl edit pv pvName</code> y lo cambiamos a recycle.  </p>
</li>
<li>
<p>El <code>kubectl edit cualquiercosa</code> se pueda usar para editar la gran mayoria de cosas.  </p>
</li>
<li>
<p><strong>RECLAIM</strong>: un PV se puede hacer un retain(se mantiene la data en el cloud y se ha de crear otro PV vacio para reclamarlo); Recycle(se elimina el contenido del cloud) y Delete(que elimina el pV y la data).  </p>
</li>
</ul>
<h2 id="usersgroups-rbac">USERS/GROUPS RBAC</h2>
<ul>
<li>
<p>RBAC(Role Base Access Control) control basado en roles.  </p>
</li>
<li>
<p>Nos permite dar/crear ciertos permisos para usuarios mediante roles.  </p>
</li>
<li>
<p>En un role definimos reglas que se enlazarán a usuarios para lo que puedan hacer en el cluster.  </p>
</li>
</ul>
<h3 id="roles-vs-clusterroles">ROLES vs CLUSTERROLES</h3>
<ul>
<li>
<p>En un role se definen Resources(objetos) y Verbs(acciones) especificando el namespace.  </p>
</li>
<li>
<p>El clusterRole es lo mismo pero sin definir ningun namespace, por lo tanto, se podrá conectar a todo.  </p>
</li>
</ul>
<h3 id="rolebinding-vs-clusterrolebinding">ROLEBINDING vs CLUSTERROLEBINDING</h3>
<ul>
<li>Son otro documento YAML en el que se espeficia el ROLE y el subject, es decir, usuarios,grupos o service account que enlazarán este role con el sujeto que lo utilizará.  </li>
</ul>
<h3 id="crear-users-groups">CREAR USERS &amp; GROUPS</h3>
<ul>
<li>
<p>Se basa en la autenticación de certificados para la C.A(Certification Authority) de kubernetes.  </p>
</li>
<li>
<p>Se necesita:  </p>
</li>
<li>Creamos el certificado</li>
<li>Creamos el file de petición de firma CSR. El CommonName y Organization serán el user y el group.</li>
<li>La firma</li>
<li>Kubectl</li>
</ul>
<p><code>PASOS: CREAMOS CERTIFICADOS DE UN USER/GROUP</code><br />
+ Creamos las keys:<br />
<code>openssl genrsa -out miguel.key 2048</code>  </p>
<ul>
<li>
<p>Creamos el certificado pasando la key e indicando el nombre de user CN y el grupo O:<br />
<code>openssl req -new -key miguel.key -out miguel.csr -subj "/CN=miguel/O=dev"</code>  </p>
</li>
<li>
<p>Vemos nuestro CA con <code>kubectl config view</code> para poder firmar nuestro certificado:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/isx46410800/.minikube/ca.crt
    server: https://172.17.0.2:8443
</code></pre>

<ul>
<li>Lo firmamos:<br />
<code>sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500</code>  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ sudo openssl x509 -req -in miguel.csr -CA /home/isx46410800/.minikube/ca.crt -CAkey /home/isx46410800/.minikube/ca.key -CAcreateserial -out miguel.crt -days 500
[sudo] password for isx46410800: 
Signature ok
subject=CN = miguel, O = dev
Getting CA Private Key
</code></pre>

<ul>
<li>Comprobamos el certificado:<br />
<code>openssl x509 -in  miguel.crt  -noout -text</code>  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ openssl x509 -in  miguel.crt  -noout -text
Certificate:
    Data:
        Version: 1 (0x0)
        Serial Number:
            a5:c7:06:8f:8f:4c:ec:4e
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = minikubeCA
        Validity
            Not Before: Oct 19 17:28:14 2020 GMT
            Not After : Mar  3 17:28:14 2022 GMT
        Subject: CN = miguel, O = dev
</code></pre>

<p><code>PASOS: CREAMOS UN CONTAINER DE PRUEBA</code><br />
+ Configuramos kubectl en modo de prueba en un container, creando un contexto nuevo a través de mis credenciales y mis llaves/certificados:  </p>
<pre><code>kubectl config view  | grep server
docker run --rm -ti -v $PWD:/test -w /test  -v /home/isx46410800/.minikube/ca.crt:/ca.crt -v /usr/bin/kubectl:/usr/bin/kubectl alpine sh
</code></pre>

<ul>
<li>
<p>Configuramos el kubectl con el usuario CN indicado(miguel):<br />
<code>kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt</code><br />
<code>kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key</code><br />
<code>kubectl config set-context miguel --cluster=minikube --user=miguel</code><br />
<code>kubectl config use-context miguel</code>  </p>
</li>
<li>
<p>Comprobamos lo creado con <code>kubectl config view</code>:  </p>
</li>
</ul>
<pre><code>/test # kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /ca.crt
    server: https://172.17.0.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: miguel
  name: miguel
current-context: miguel
kind: Config
preferences: {}
users:
- name: miguel
  user:
    client-certificate: /test/miguel.crt
    client-key: /test/miguel.key
#
/test # kubectl config current-context
miguel
</code></pre>

<ul>
<li>Vemos que como usuario nuevo y sin tener ningun RBAC asignado, que no tenemos permisos para ver pods ni nada de objetos:  </li>
</ul>
<pre><code>/test # kubectl get pods
Error from server (Forbidden): pods is forbidden: User &quot;miguel&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>

<h3 id="habilitar-rbac">HABILITAR RBAC</h3>
<ul>
<li>Vemos si está:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl cluster-info dump | grep autho
&quot;--authorization-mode=Node,RBAC&quot;,
</code></pre>

<ul>
<li>Sino, lo habitamos así:<br />
<code>minikube start --vm-driver=none  --extra-config=apiserver.authorization-mode=RBAC</code>  </li>
</ul>
<h3 id="simplificamos-contexto">SIMPLIFICAMOS CONTEXTO</h3>
<ul>
<li>Ahora lo hacemos en real y así simplificamos trabajo y ordenes en nuestro contexto creado:  </li>
</ul>
<pre><code>kubectl config set-cluster minikube --server=https://172.17.0.2:8443 --certificate-authority=/ca.crt
kubectl config set-credentials miguel --client-certificate=miguel.crt --client-key=miguel.key
kubectl config set-context miguel --cluster=minikube --user=miguel
kubectl config use-context miguel
</code></pre>

<h3 id="crear-roles">CREAR ROLES</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
</code></pre>

<ul>
<li>Comprobamos <code>kubectl get roles</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml
role.rbac.authorization.k8s.io/pod-reader created
#
[isx46410800@miguel rbac]$ kubectl get roles -n default
NAME         CREATED AT
pod-reader   2020-10-19T18:01:37Z
#
[isx46410800@miguel rbac]$ kubectl describe role pod-reader -n default
Name:         pod-reader
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [get watch list]
</code></pre>

<blockquote>
<p>No podemos hacer con el usuario miguel kubectl get pods porque todavia no está enlazado el role con el user.</p>
</blockquote>
<h3 id="enlazar-role-user">ENLAZAR ROLE &amp; USER</h3>
<ul>
<li>
<p>Para ver el tipo de <code>api groups</code> recordamos que es mirando <code>kubectl api-resources</code>  </p>
</li>
<li>
<p>Verbs o acciones que se pueden hacer:   </p>
</li>
<li>GET</li>
<li>LIST</li>
<li>WATCH</li>
<li>DELETE</li>
<li>UPDATE</li>
<li>
<p>PATCH</p>
</li>
<li>
<p>Hacemos el RoleBinding de enlazar el role con el user creado:  </p>
</li>
</ul>
<pre><code># CREAR ROLE
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
---
# ROLEBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: miguel # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos lo creado con <code>kubectl get rolebinding</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f miguel-pods.yaml 
role.rbac.authorization.k8s.io/pod-reader unchanged
rolebinding.rbac.authorization.k8s.io/read-pods created
#
[isx46410800@miguel rbac]$ kubectl get roles
NAME         CREATED AT
pod-reader   2020-10-19T18:01:37Z
#
[isx46410800@miguel rbac]$ kubectl get rolebinding
NAME        ROLE              AGE
read-pods   Role/pod-reader   21s
#
[isx46410800@miguel rbac]$ kubectl describe rolebinding read-pods
Name:         read-pods
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  Role
  Name:  pod-reader
Subjects:
  Kind  Name    Namespace
  ----  ----    ---------
  User  miguel  
</code></pre>

<ul>
<li>Comprobamos ahora con el usuario <code>miguel</code> sí puedo hacer esas acciones que antes no me dejaban(este caso con pods). Eso sí, unicamente en el namespace por default que fue el que indicamos:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get pods
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get pods -n ci
Error from server (Forbidden): pods is forbidden: User &quot;miguel&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;ci&quot;
#
[isx46410800@miguel rbac]$ kubectl get rs
Error from server (Forbidden): replicasets.apps is forbidden: User &quot;miguel&quot; cannot list resource &quot;replicasets&quot; in API group &quot;apps&quot; in the namespace &quot;default&quot;
#
[isx46410800@miguel rbac]$ kubectl get svc
Error from server (Forbidden): services is forbidden: User &quot;miguel&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>

<ul>
<li>Ahora creamos otro role con que también podamos ver deploys. Para ver el tipo de <code>api groups</code> recordamos que es mirando <code>kubectl api-resources</code>:  </li>
</ul>
<pre><code># CREAR ROLE
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-deploy-reader #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
- apiGroups: [&quot;apps&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;deployments&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
---
# ROLEBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-deploy-pods
  namespace: default
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: miguel # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: pod-deploy-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f miguel-deploy-pods.yaml 
role.rbac.authorization.k8s.io/pod-deploy-reader created
rolebinding.rbac.authorization.k8s.io/read-deploy-pods created
#
[isx46410800@miguel rbac]$ kubectl get roles
NAME                CREATED AT
pod-deploy-reader   2020-10-19T18:20:23Z
pod-reader          2020-10-19T18:01:37Z
#
[isx46410800@miguel rbac]$ kubectl get rolebinding
NAME               ROLE                     AGE
read-deploy-pods   Role/pod-deploy-reader   14s
read-pods          Role/pod-reader          10m
#
[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get pods
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get deploy
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get svc
Error from server (Forbidden): services is forbidden: User &quot;miguel&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
#             
[isx46410800@miguel rbac]$ kubectl apply -f ../pods/pod-2containers.yaml 
Error from server (Forbidden): error when creating &quot;../pods/pod-2containers.yaml&quot;: pods is forbidden: User &quot;miguel&quot; cannot create resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>

<h3 id="config-maps">CONFIG MAPS</h3>
<ul>
<li>Un ejemplo de crear un namespace y un configmaps y que el usuario pueda moverse en estos objetos:  </li>
</ul>
<pre><code># CREAR NAMESPACE
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    name: dev
---
# CREAR ROLE
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: cm-role #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;configmaps&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
---
# ROLEBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cm-role
  namespace: dev
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: miguel # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: cm-role # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
---
# CREAR CONFIGMAP
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: dev
  name: vars
  labels:
    app: front
data:
  db_host: dev.host.local
  db_user: dev_user
</code></pre>

<ul>
<li>Comprobamos resultados:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f configmap-role.yaml 
namespace/dev created
role.rbac.authorization.k8s.io/cm-role created
rolebinding.rbac.authorization.k8s.io/cm-role created
configmap/vars created
#
[isx46410800@miguel rbac]$ kubectl get roles -n dev
NAME      CREATED AT
cm-role   2020-10-19T18:35:07Z
#
[isx46410800@miguel rbac]$ kubectl get rolebinding -n dev
NAME      ROLE           AGE
cm-role   Role/cm-role   27s
#
[isx46410800@miguel rbac]$ kubectl describe role cm-role -n dev
Name:         cm-role
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 []              [get watch list]
#
[isx46410800@miguel rbac]$ kubectl describe rolebinding cm-role -n dev
Name:         cm-role
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  Role
  Name:  cm-role
Subjects:
  Kind  Name    Namespace
  ----  ----    ---------
  User  miguel  
#
[isx46410800@miguel rbac]$ kubectl get cm -n dev
NAME   DATA   AGE
vars   2      43s
</code></pre>

<ul>
<li>Como usuario miguel:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get cm
Error from server (Forbidden): configmaps is forbidden: User &quot;miguel&quot; cannot list resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
#
[isx46410800@miguel rbac]$ kubectl get cm -n dev
NAME   DATA   AGE
vars   2      2m50s
#
[isx46410800@miguel rbac]$ kubectl edit cm vars
Error from server (Forbidden): configmaps &quot;vars&quot; is forbidden: User &quot;miguel&quot; cannot get resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
#
[isx46410800@miguel rbac]$ kubectl edit cm vars -n dev
error: configmaps &quot;vars&quot; could not be patched: configmaps &quot;vars&quot; is forbidden: User &quot;miguel&quot; cannot patch resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;dev&quot;
You can run `kubectl replace -f /tmp/kubectl-edit-jum69.yaml` to try this update again.
</code></pre>

<h3 id="crear-clusterole">CREAR CLUSTEROLE</h3>
<ul>
<li>Creamos un clusterRole teniendo en cuenta que aquí no se ponen namespaces:  </li>
</ul>
<pre><code># CREAR ROLE
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-pod-reader #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
---
# CLUSTERBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-pod-reader
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: miguel # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: ClusterRole #this must be Role or ClusterRole
  name: cluster-pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos con el usuario miguel:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f clusterrole-miguel.yaml 
role.rbac.authorization.k8s.io/cluster-pod-reader created
rolebinding.rbac.authorization.k8s.io/cluster-pod-reader created
#
[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get pods -n dev
No resources found in dev namespace.
#
[isx46410800@miguel rbac]$ kubectl get pods -n ci
No resources found in ci namespace.
#
[isx46410800@miguel rbac]$ kubectl get pods
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-lgrd4            1/1     Running   0          49m
etcd-minikube                      1/1     Running   0          49m
kube-apiserver-minikube            1/1     Running   0          49m
kube-controller-manager-minikube   1/1     Running   0          49m
kube-proxy-22t6g                   1/1     Running   0          49m
kube-scheduler-minikube            1/1     Running   0          49m
storage-provisioner                1/1     Running   0          50m
</code></pre>

<h3 id="crear-user-admin">CREAR USER ADMIN</h3>
<ul>
<li>Miramos los clusteroles que hay con <code>kubectl get clusterroles</code> y vemos el de <code>cluster-admin</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl get clusterroles
NAME                                                                   CREATED AT
admin                                                                  2020-10-19T18:00:44Z
cluster-admin                                                          2020-10-19T18:00:44Z
cluster-pod-reader                                                     2020-10-19T18:50:22Z
edit                                                                   2020-10-19T18:00:44Z
kubeadm:get-nodes                                                      2020-10-19T18:00:48Z
system:aggregate-to-admin                                              2020-10-19T18:00:44Z
system:aggregate-to-edit                                               2020-10-19T18:00:44Z
system:aggregate-to-view                                               2020-10-19T18:00:44Z
</code></pre>

<ul>
<li>Creamos un cluster-admin enlazando solo al usuario miguel al grupo:  </li>
</ul>
<pre><code># CLUSTERBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-miguel
subjects:
# You can specify more than one &quot;subject&quot;
- kind: User
  name: miguel # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: ClusterRole #this must be Role or ClusterRole
  name: cluster-admin # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos ahora que como usuario miguel podemos hacer de todo y ver de todo:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl apply -f cluster-admin.yaml 
clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-miguel created
#
[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get pods
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get cm
No resources found in default namespace.
#
[isx46410800@miguel rbac]$ kubectl get cm -n dev
NAME   DATA   AGE
vars   2      24m
#
[isx46410800@miguel rbac]$ kubectl get roles
NAME                CREATED AT
pod-deploy-reader   2020-10-19T18:20:23Z
pod-reader          2020-10-19T18:01:37Z
#
[isx46410800@miguel rbac]$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   59m
</code></pre>

<h3 id="roles-a-grupos">ROLES A GRUPOS</h3>
<ul>
<li>
<p>Veremos como crear un grupo y como asignar roles a grupos.  </p>
</li>
<li>
<p>Creamos un nuevo usuario como miguel pero ahora como juan:</p>
</li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl config get-contexts
CURRENT   NAME         CLUSTER    AUTHINFO   NAMESPACE
          ci-context   minikube   minikube   ci
          juan         minikube   juan       
          miguel       minikube   miguel     
*         minikube     minikube   minikube 
</code></pre>

<ul>
<li>Creamos un clusterrole para el grupo dev y que pueda hacer todo en servicios:  </li>
</ul>
<pre><code># CREAR CLUSTERROLE
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: svc-clusterrole #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;services&quot;] #objetos
  verbs: [&quot;*&quot;] # acciones
---
# CLUSTERBINDING-ENLAZAR ROLE-USER
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-svc
subjects:
# You can specify more than one &quot;subject&quot;
- kind: Group
  name: dev # &quot;name&quot; is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: ClusterRole #this must be Role or ClusterRole
  name: svc-clusterrole # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos como juan y miguel podemos ver todo de services:  </li>
</ul>
<pre><code>[isx46410800@miguel rbac]$ kubectl get clusterroles
NAME                                                                   CREATED AT
admin                                                                  2020-10-19T18:00:44Z
cluster-admin                                                          2020-10-19T18:00:44Z
cluster-pod-reader                                                     2020-10-19T18:50:22Z
edit                                                                   2020-10-19T18:00:44Z
kubeadm:get-nodes                                                      2020-10-19T18:00:48Z
svc-clusterrole                                                        2020-10-19T19:09:44Z
#
[isx46410800@miguel rbac]$ kubectl config use-context juan
Switched to context &quot;juan&quot;.
#
[isx46410800@miguel rbac]$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   70m
#
[isx46410800@miguel rbac]$ kubectl get svc -n dev
No resources found in dev namespace.
#
[isx46410800@miguel rbac]$ kubectl config use-context miguel
Switched to context &quot;miguel&quot;.
#
[isx46410800@miguel rbac]$ kubectl get svc -n dev
No resources found in dev namespace.
#
[isx46410800@miguel rbac]$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   70m
</code></pre>

<h2 id="services-account">SERVICES ACCOUNT</h2>
<ul>
<li>
<p>Tiene un token que lo crea kubernetes. Cada pod tiene asociado un service account.  </p>
</li>
<li>
<p>El pod quiere preguntar el estado de otros pods; pregunta a la API y este se conecta con el token del Service Acount en el cual se ha dado un role y un rolebinding para poder acceder a esta petición.  </p>
</li>
<li>
<p>Todos los namespaces tienen un service account por defecto.  </p>
</li>
<li>
<p>Lo podemos ver con <code>kubectl get serviceaccount</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl get sa
NAME      SECRETS   AGE
default   1         23h
[isx46410800@miguel services_account]$ kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         23h
[isx46410800@miguel services_account]$ kubectl get serviceaccount -n default
NAME      SECRETS   AGE
default   1         23h
</code></pre>

<ul>
<li>Lo exploramos y vemos que tienen el token que se crea del SA por cada namespace:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl describe sa default
Name:                default
Namespace:           default
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  &lt;none&gt;
Mountable secrets:   default-token-6ccpr
Tokens:              default-token-6ccpr
Events:              &lt;none&gt;
</code></pre>

<pre><code>[isx46410800@miguel services_account]$ kubectl get sa default -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: &quot;2020-10-19T18:00:54Z&quot;
  name: default
  namespace: default
  resourceVersion: &quot;346&quot;
  selfLink: /api/v1/namespaces/default/serviceaccounts/default
  uid: 562a0b3d-1696-4b4e-b6cc-42b895f3a19b
secrets:
- name: default-token-6ccpr
</code></pre>

<h3 id="secret-sa">SECRET SA</h3>
<ul>
<li>Vemos que el token de un SA es un secreto y lo podemos investigar <code>kubectl get secret TOKEN</code>:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-6ccpr   kubernetes.io/service-account-token   3      23h
[isx46410800@miguel services_account]$ kubectl get secret default-token-6ccpr -o yaml
...
</code></pre>

<blockquote>
<p>El token contiene el certificado de kubernetes, la llave publica y el contenido del namespace, seervica account,etc</p>
</blockquote>
<h3 id="crear-sa">CREAR SA</h3>
<ul>
<li>Ejemplo de crear un service account:  </li>
</ul>
<pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl apply -f sa.yaml 
serviceaccount/my-sa created
[isx46410800@miguel services_account]$ kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         23h
my-sa     1         6s
[isx46410800@miguel services_account]$ kubectl describe sa my-sa
Name:                my-sa
Namespace:           default
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  &lt;none&gt;
Mountable secrets:   my-sa-token-5lv4s
Tokens:              my-sa-token-5lv4s
Events:              &lt;none&gt;
[isx46410800@miguel services_account]$ kubectl get sa my-sa -o yaml
apiVersion: v1
kind: ServiceAccount
secrets:
- name: my-sa-token-5lv4s
</code></pre>

<h3 id="relacion-pod-sa">RELACION POD-SA</h3>
<ul>
<li>Cuando creamos un pod sin especificar un SA, se asigna al por defecto:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-2containers.yaml 
pod/pod-test2 created
[isx46410800@miguel services_account]$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
pod-test2   2/2     Running   0          29s
[isx46410800@miguel services_account]$ kubectl get pods pod-test2 -o yaml
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-6ccpr
    secret:
      defaultMode: 420
      secretName: default-token-6ccpr
</code></pre>

<ul>
<li>Dentro del pod podemos encontrar la info del SA y su token en:<br />
<code>/var/run/secrets/kubernetes.io/serviceaccount/</code>  </li>
</ul>
<h3 id="requests">REQUESTS</h3>
<ul>
<li>A través del servicio de kubernetes podemos llamar a objetos a través de la api de kubernetes sin pasar por el comando kubectl:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   23h
</code></pre>

<ul>
<li>Podemos hacer request a la api con esta <a href="https://v1-15.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#-strong-read-operations-pod-v1-core-strong-">DOCS</a> como por ejemplo querer listar los pods del namespace por defecto:<br />
<code>/api/v1/namespaces/{namespace}/pods/{name}</code>  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl apply -f ../pods/pod-labels.yaml 
pod/pod-test2 created
pod/pod-test3 created
[isx46410800@miguel services_account]$ kubectl exec -it pod-test2 -- sh
/ # apk add curl
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
OK: 25 MiB in 42 packages
/ # curl /api/v1/namespaces/default/pods
curl: (3) URL using bad/illegal format or missing URL
/ # curl https://10.96.0.1/api/v1/namespaces/default/pods --insecure
{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {

  },
  &quot;status&quot;: &quot;Failure&quot;,
  &quot;message&quot;: &quot;pods is forbidden: User \&quot;system:anonymous\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;default\&quot;&quot;,
  &quot;reason&quot;: &quot;Forbidden&quot;,
  &quot;details&quot;: {
    &quot;kind&quot;: &quot;pods&quot;
  },
  &quot;code&quot;: 403
}/ # 
</code></pre>

<blockquote>
<p>nos sale error como de permisos ya que es como si fuesemos un usuario que no tiene la autenticación para poder hacer estas acciones.  </p>
</blockquote>
<h3 id="request-jwt">REQUEST JWT</h3>
<ul>
<li>
<p>Peticiones Jason Web Token autenticadas con el token/secret del service account.  </p>
</li>
<li>
<p>Dentro del pod podemos encontrar la info del SA y su token en:<br />
<code>/var/run/secrets/kubernetes.io/serviceaccount/</code> </p>
</li>
<li>
<p>Guardamos el token del POD en una variable:<br />
<code># TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)</code>  </p>
</li>
<li>
<p><a href="https://medium.com/@nieldw/using-curl-to-authenticate-with-jwt-bearer-tokens-55b7fac506bd">DOCS</a>:<br />
<code>/ # curl -H "Authorization: Bearer ${TOKEN}" https://10.96.0.1/api/v1 --insecure</code>  </p>
<blockquote>
<p>Ahora nos da una respuesta de todos los recursos que hay en v1, pero este token no tienen tantos permisos para llegar a mas adelante.  </p>
</blockquote>
</li>
</ul>
<h3 id="sa-deployment">SA DEPLOYMENT</h3>
<ul>
<li>Ejemplo de crear un deploy asignando un service account creado:  </li>
</ul>
<pre><code># CREAMOS SERVICE ACCOUNT
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      serviceAccountName: my-sa
      containers:
      - name: nginx
        image: nginx:alpine
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
test-7bb9d96578-v6x5m   1/1     Running   0          15s
[isx46410800@miguel services_account]$ kubectl get pods test-7bb9d96578-v6x5m -o yaml
spec:
  containers:
  - image: nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: my-sa-token-5lv4s
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: my-sa
  serviceAccountName: my-sa
</code></pre>

<h3 id="role-sa">ROLE SA</h3>
<ul>
<li>Creamos un rol y un rolebinding para que un serviceaccount sea capaz de leer pods del namespace. Asignamos este role al SA del deployment y pods creados:  </li>
</ul>
<pre><code># CREAMOS SERVICE ACCOUNT
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sa
---
# esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      serviceAccountName: my-sa
      containers:
      - name: nginx
        image: nginx:alpine
---
# CREAR ROLE SA
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: sa-reader #nombre role
rules:
- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group
  resources: [&quot;pods&quot;] #objetos
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # acciones
---
# ROLEBINDING-ENLAZAR ROLE-SA
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sa-pods
  namespace: default
subjects:
# You can specify more than one &quot;subject&quot;
- kind: ServiceAccount
  name: my-sa # &quot;name&quot; is case sensitive
  apiGroup: 
roleRef:
  # &quot;roleRef&quot; specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: sa-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel services_account]$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
test-7bb9d96578-v6x5m   1/1     Running   0          12m
[isx46410800@miguel services_account]$ kubectl get roles
NAME                CREATED AT
pod-deploy-reader   2020-10-19T18:20:23Z
pod-reader          2020-10-19T18:01:37Z
sa-reader           2020-10-20T18:05:58Z
[isx46410800@miguel services_account]$ kubectl get rolebinding
NAME                 ROLE                             AGE
cluster-pod-reader   ClusterRole/cluster-pod-reader   23h
read-deploy-pods     Role/pod-deploy-reader           23h
read-pods            Role/pod-reader                  23h
sa-pods              Role/sa-reader                   3m39s
[isx46410800@miguel services_account]$ kubectl get sa
NAME      SECRETS   AGE
default   1         24h
my-sa     1         44m
</code></pre>

<ul>
<li>Comprobamos que ahora entramos al POD y podemos comunicarnos a través de la api con JWT para listar los pods del namespace:<br />
<code>/ # curl -H "Authorization: Bearer ${TOKEN}" https://10.96.0.1/api/v1/namespaces/default/pods --insecure</code>  </li>
</ul>
<pre><code>&quot;hostIP&quot;: &quot;172.17.0.2&quot;,
        &quot;podIP&quot;: &quot;172.18.0.3&quot;,
        &quot;podIPs&quot;: [
          {
            &quot;ip&quot;: &quot;172.18.0.3&quot;
          }
        ],
        &quot;startTime&quot;: &quot;2020-10-20T17:56:32Z&quot;,
</code></pre>

<ul>
<li>Si añadimos el permiso de ver tambien deployments despues hariamos:<br />
<code># curl -H "Authorization: Bearer ${TOKEN}" https://10.96.0.1/apis/apps/v1/namespaces/default/deployments --insecure</code>  </li>
</ul>
<pre><code>&quot;restartPolicy&quot;: &quot;Always&quot;,
            &quot;terminationGracePeriodSeconds&quot;: 30,
            &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
            &quot;serviceAccountName&quot;: &quot;my-sa&quot;,
            &quot;serviceAccount&quot;: &quot;my-sa&quot;,
            &quot;securityContext&quot;: {
      &quot;status&quot;: {
        &quot;observedGeneration&quot;: 1,
        &quot;replicas&quot;: 1,
        &quot;updatedReplicas&quot;: 1,
        &quot;readyReplicas&quot;: 1,
        &quot;availableReplicas&quot;: 1,
</code></pre>

<h2 id="ingress">INGRESS</h2>
<ul>
<li>
<p>Es un componente de kubernetes que se coloca en la entrada de nuestro cluster que recibe las solicitudes de los usuarios.  </p>
</li>
<li>
<p>Crea unas reglas en esta entrada redireccionando cada petición por el servicio que le toca.  </p>
</li>
<li>
<p>Con esto evitamos usar diferentes nodes port o diferentes balanceos de carga a la hora de contestar las solicitudes de los usuarios.  </p>
</li>
<li>
<p>También se puede crear reglas de DNS, IPs, servicios...que se definen en un único punto de entrada.  </p>
</li>
</ul>
<h3 id="ingress-controller">INGRESS CONTROLLER</h3>
<ul>
<li>
<p>Ingress es unicamente el sitio donde se definen las reglas. El que aplica las reglas lo hace el Ingress Controller.  </p>
</li>
<li>
<p>Normalmente está en un deployment que apunta a este ingress para leer las reglas.  </p>
</li>
<li>
<p>Puede ser de dos tipos: nginx o cloud. </p>
</li>
<li>
<p>Nginx define un node port para las peticiones del usuario y después leer las reglas del ingress.  </p>
</li>
<li>
<p>Si es con balanzador de cloud, el ingress controler crea un balanzador de carga en la nube y la entrada la tiene en el cloud. Cuando se comunica el usuario, el ingress controller se comunica con el ingress y despues le envia la respuesta al balanzador de carga con la API de cloud.  </p>
</li>
</ul>
<h3 id="crear-ingress-controller">CREAR INGRESS CONTROLLER</h3>
<ul>
<li>
<p><a href="https://kubernetes.github.io/ingress-nginx/deploy/">Documentacion</a>  </p>
</li>
<li>
<p>Creamos un ingress-controller de nginx y comprobamos que lo tenemos funcionando:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-54b86f8f7b-s7vzl   1/1     Running   0          81s
</code></pre>

<ul>
<li>Creamos el servicio de ingress-controller nginx de tipo node-port:  </li>
</ul>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
</code></pre>

<ul>
<li>Comprobamos que funciona:  </li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.109.126.218   &lt;none&gt;        80:30540/TCP,443:32602/TCP   29s
</code></pre>

<h3 id="ip-ingress-controller">IP INGRESS CONTROLLER</h3>
<ul>
<li>Ip del cluster:  </li>
</ul>
<pre><code>Kubernetes master is running at https://172.17.0.2:8443
KubeDNS is running at https://172.17.0.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<ul>
<li>Ip del servicio node-port del IController Nginx:  </li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.109.126.218   &lt;none&gt;        80:30540/TCP,443:32602/TCP   4m43s
</code></pre>

<ul>
<li>Obtenemos la url con la ip para conectarnos:  </li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ minikube service ingress-nginx --url -n ingress-nginx
http://172.17.0.2:30540
http://172.17.0.2:32602
</code></pre>

<p><img alt="" src="../images/kubernetes10.png" />  </p>
<h3 id="app-ingress-controller">APP INGRESS-CONTROLLER</h3>
<ul>
<li>Creamos un servicio con nuestra app de cambiar el index al nginx y hacemos un deployment con 3 replicas:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-deploy
  labels:
    app: front
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: front
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: front
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        command: [&quot;sh&quot;,&quot;-c&quot;, &quot;echo VERSION 1.0 desde $HOSTNAME &gt; /usr/share/nginx/html/index.html &amp;&amp; nginx -g 'daemon off;'&quot;]
---
# añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: app-v1-svc
  labels:
    app: front
spec:
  type: ClusterIP
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8080 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code># añadimos el servicio que observará los FRONT
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: front
spec:
  selector:
    app: front
  ports:
    - protocol: TCP
      port: 8888 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Entramos a un pod y comprobamos que cuando llamamos al servicio, nos contesta algunos de los pods con nuestra app que es la ejecución del index.html:  </li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ kubectl exec -it ingress-deploy-7cd6549d66-26cwb -- sh
/ # apk add curl
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz
OK: 25 MiB in 42 packages
/ # curl app-v1-svc:8080
VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv
/ # curl 10.96.97.25:8080
VERSION 1.0 desde ingress-deploy-7cd6549d66-ncjpv
</code></pre>

<h3 id="exponer-el-puerto-al-exterior">EXPONER EL PUERTO AL EXTERIOR</h3>
<ul>
<li>Ahora queremos exponer el puerto externamente. Para ello creamos unas reglas para el controller:  </li>
</ul>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /appv1
        backend:
          serviceName: app-v1-svc # nombre del servicio de la app
          servicePort: 8080
</code></pre>

<ul>
<li>Comprobamos que ahora con la url(ip/appv1) vemos también la respuesta al servicio:  </li>
</ul>
<p><img alt="" src="../images/kubernetes11.png" />  </p>
<ul>
<li>Podemos añadirlo un dominio tambien en el apartado hosts. Probamos haciendo un dominio en /etc/hosts <code>172.17.0.2 app1.mydomain.com</code>:  </li>
</ul>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: app1.mydomain.com
    http:
      paths:
      - path: /appv1
        backend:
          serviceName: app-v1-svc # nombre del servicio de la app
          servicePort: 8080
  - http:
      paths:
      - path: /appv1
        backend:
          serviceName: app-v1-svc # nombre del servicio de la app
          servicePort: 8080
</code></pre>

<p><img alt="" src="../images/kubernetes12.png" />  </p>
<h3 id="2-apps-en-ic">2 APPS EN IC</h3>
<ul>
<li>Ejemplo:  </li>
</ul>
<pre><code># esto es del deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-deploy2
  labels:
    app: backend
# aqui viene el replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  # aqui viene el pod
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        command: [&quot;sh&quot;,&quot;-c&quot;, &quot;echo Soy app2 desde $HOSTNAME &gt; /usr/share/nginx/html/index.html &amp;&amp; nginx -g 'daemon off;'&quot;]
---
# añadimos el servicio que observará los backend
apiVersion: v1
kind: Service
metadata:
  name: app2-v1-svc
  labels:
    app: backend
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
    - protocol: TCP
      port: 9090 # servicio por donde escucha
      targetPort: 80 # a que puerto dentro del pod vamos a mandar la peticion(nginx 80)
</code></pre>

<ul>
<li>Comprobamos que funcionan:  </li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ kubectl get svc
NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
app-v1-svc    ClusterIP   10.96.97.25     &lt;none&gt;        8080/TCP   34m
app2-v1-svc   ClusterIP   10.106.106.71   &lt;none&gt;        9090/TCP   10s
kubernetes    ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    30h
[isx46410800@miguel ingress]$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
ingress-deploy-7cd6549d66-26cwb    1/1     Running   0          34m
ingress-deploy-7cd6549d66-9b9d4    1/1     Running   0          34m
ingress-deploy-7cd6549d66-ncjpv    1/1     Running   0          34m
ingress-deploy2-69fcf646dd-m8zn4   1/1     Running   0          13s
ingress-deploy2-69fcf646dd-nnn89   1/1     Running   0          13s
ingress-deploy2-69fcf646dd-xq977   1/1     Running   0          13s
</code></pre>

<ul>
<li>Agregamos nueva regla para la app2:  </li>
</ul>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: app1.mydomain.com
    http:
      paths:
      - path: /appv1
        backend:
          serviceName: app-v1-svc # nombre del servicio de la app
          servicePort: 8080
  - host: app2.mydomain.com
    http:
      paths:
      - path: /appv2
        backend:
          serviceName: app2-v1-svc # nombre del servicio de la app
          servicePort: 9090
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<p><img alt="" src="../images/kubernetes13.png" /><br />
<img alt="" src="../images/kubernetes14.png" />  </p>
<ul>
<li>Ahora cambiando varios paths:  </li>
</ul>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: app1.mydomain.com
    http:
      paths:
      - path: /myservice1
        backend:
          serviceName: app-v1-svc # nombre del servicio de la app
          servicePort: 8080
      paths:
      - path: /myservice2
        backend:
          serviceName: app2-v1-svc # nombre del servicio de la app
          servicePort: 9090
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<p><img alt="" src="../images/kubernetes15.png" />  </p>
<h2 id="aws-kubernetes">AWS KUBERNETES</h2>
<ul>
<li>
<p>Tenemos que crear cuenta en AWS.  </p>
</li>
<li>
<p>Instalar pip3 de python.  </p>
</li>
<li>
<p>Tenemos que instalar la herramienta AWS CLI:<br />
<code>pip3 install -U awscli</code>  </p>
</li>
<li>
<p>Comprobamos la version:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ aws --version
aws-cli/1.18.160 Python/3.6.6 Linux/4.18.19-100.fc27.x86_64 botocore/1.19.0
</code></pre>

<ul>
<li>
<p>Creamos un usuario administrador en IAM de AWS.  </p>
</li>
<li>
<p>COnfiguramos en nuestra máquina real el AWS con el usuario creado:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ingress]$ aws configure
AWS Access Key ID [None]: AKIA5RIFOUI3OMSWWHNM
AWS Secret Access Key [None]: xxxx
Default region name [None]: eu-west-2
Default output format [None]: 
</code></pre>

<ul>
<li>Nos crea un home de AWS en nuestro home:  </li>
</ul>
<pre><code>[isx46410800@miguel .aws]$ pwd
/home/isx46410800/.aws
</code></pre>

<ul>
<li>Testeamos con una petición para saber quien es el que hace el request:  </li>
</ul>
<pre><code>[isx46410800@miguel .aws]$ aws sts get-caller-identity
{
    &quot;UserId&quot;: &quot;AIDA5RIFOUI3IP6OESXCW&quot;,
    &quot;Account&quot;: &quot;930408735286&quot;,
    &quot;Arn&quot;: &quot;arn:aws:iam::930408735286:user/miguel&quot;
}
</code></pre>

<ul>
<li>Instalamos la herramienta <code>eksctl</code> que es para gestionar los cluster de kubernetes en AWS:  </li>
</ul>
<pre><code>[isx46410800@miguel .aws]$ curl --silent --location &quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz&quot; | tar xz -C /tmp
[isx46410800@miguel .aws]$ sudo mv /tmp/eksctl /usr/local/bin
[isx46410800@miguel .aws]$ sudo chmod +x /usr/local/bin/eksctl 
[isx46410800@miguel .aws]$ eksctl version
0.30.0
</code></pre>

<h3 id="crear-cluster-aws-eksctl">CREAR CLUSTER AWS EKSCTL</h3>
<ul>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html">docs install</a>  </p>
</li>
<li>
<p>Creamos cluster master sin nodos:<br />
<code>eksctl create cluster --name test-cluster --without-nodegroup --region eu-west-2 --zones eu-west-2a,eu-west-2b</code>  </p>
</li>
<li>
<p>Vemos lo creado en el apartado <a href="https://eu-west-2.console.aws.amazon.com/eks/home?region=eu-west-2#/clusters">EKS</a> y <a href="https://eu-west-2.console.aws.amazon.com/cloudformation/home?region=eu-west-2#/stacks?filteringText=&amp;filteringStatus=active&amp;viewNested=true&amp;hideStacks=false">CloudFormation</a>:  </p>
</li>
</ul>
<p><img alt="" src="../images/kubernetes16.png" /><br />
<img alt="" src="../images/kubernetes17.png" />  </p>
<ul>
<li>Eksctl lee de estos archivos para comunicarse:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ cat .aws/credentials 
[default]
aws_access_key_id = AKIA5RIFOUI3OMSWWHNM
aws_secret_access_key = xxxxx
[isx46410800@miguel ~]$ cat .aws/config 
[default]
region = eu-west-2
</code></pre>

<ul>
<li>
<p>Al crear el cluster nos crea un directorio <code>~/.kube/config</code>  </p>
</li>
<li>
<p>Si eliminamos este directorio, como si no lo tuvieramos y nos queremos conectar a este cluster usamos la orden:<br />
<code>aws eks --region eu-west-2 update-kubeconfig --name test-cluster</code>  </p>
</li>
<li>
<p>Ahora si hacemos <code>kubectl get svc</code> y <code>kubectl cluster-info</code> vemos que estamos conectados y referenciados al cluster de AWS:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.100.0.1   &lt;none&gt;        443/TCP   7m11s
[isx46410800@miguel ~]$ kubectl cluster-info
Kubernetes master is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com
CoreDNS is running at https://5CE8052655A3A5961205F0A612B79D00.gr7.eu-west-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<ul>
<li>Ahora intentamos crear un POD pero vemos que no se acaba de crear porque no tenemos ningun nodo unido a nuestro CLUSTER:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl run pod-test --image=nginx:alpine
pod/pod-test created
#
[isx46410800@miguel ~]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
pod-test   0/1     Pending   0          10s
#
[isx46410800@miguel ~]$ kubectl describe pod pod-test
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  15s   default-scheduler  no nodes available to schedule pods
</code></pre>

<ul>
<li>Ahora creamos nodos con eksctl con ami version kubernetes auto y asg access para ser escalable:<br />
<code>eksctl create nodegroup --cluster test-cluster --region eu-west-2 --name test-workers --node-type t3.medium --node-ami auto --nodes 1 --nodes-min 1 --nodes-max 3 --asg-access</code>  </li>
</ul>
<p><img alt="" src="../images/kubernetes18.png" /><br />
<img alt="" src="../images/kubernetes19.png" />  </p>
<ul>
<li>Comprobamos que el pod de prueba está ahora running y asignado al nodo creado:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE   VERSION
ip-192-168-38-128.eu-west-2.compute.internal   Ready    &lt;none&gt;   68s   v1.17.11-eks-cfdc40
#
[isx46410800@miguel ~]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
pod-test   1/1     Running   0          6m38s
#
[isx46410800@miguel ~]$ kubectl describe pod pod-test
Name:         pod-test
Namespace:    default
Priority:     0
Node:         ip-192-168-38-128.eu-west-2.compute.internal/192.168.38.128
</code></pre>

<h3 id="ingress-aws-eks">INGRESS AWS EKS</h3>
<ul>
<li>
<p>Para exponerlo, crearemos un balanzador de carga, un ingress y un ingress controller.  </p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">DOCS</a> para crear el ingress controller nos dice que nuestro servicio(VPC) tiene que seguir una estructura de tag. Vemos los servicios VPC que se crearon automaticamente al crear el cluster y los nodos.  </p>
</li>
</ul>
<p><img alt="" src="../images/kubernetes20.png" />  </p>
<ul>
<li>Las subnets tambien tienen que seguir una estructura de tags. No obstante todos estos pasos al crearlos con EKSCTL ya vienen por defecto.  </li>
</ul>
<p><img alt="" src="../images/kubernetes21.png" />  </p>
<ul>
<li>IAM OIDC </li>
</ul>
<p><code>eksctl utils associate-iam-oidc-provider --region eu-west-2 --cluster test-cluster --approve</code>  </p>
<ul>
<li>Politica para crear recursos de balanceador de carga:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ aws iam create-policy \
&gt;     --policy-name ALBIngressControllerIAMPolicy \
&gt;     --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json
{
    &quot;Policy&quot;: {
        &quot;PolicyName&quot;: &quot;ALBIngressControllerIAMPolicy&quot;,
        &quot;PolicyId&quot;: &quot;ANPA5RIFOUI3IFJHOR5SB&quot;,
        &quot;Arn&quot;: &quot;arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy&quot;,
        &quot;Path&quot;: &quot;/&quot;,
        &quot;DefaultVersionId&quot;: &quot;v1&quot;,
        &quot;AttachmentCount&quot;: 0,
        &quot;PermissionsBoundaryUsageCount&quot;: 0,
        &quot;IsAttachable&quot;: true,
        &quot;CreateDate&quot;: &quot;2020-10-21T17:06:33Z&quot;,
        &quot;UpdateDate&quot;: &quot;2020-10-21T17:06:33Z&quot;
    }
}
</code></pre>

<ul>
<li>
<p>Creamos un service account para ingress con un clusterrole y un clusterrolebinding de ingress controller para balanceador de carga:<br />
<code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml</code>  </p>
</li>
<li>
<p>Creamos un service account para que nuestro ingress controller sea capaz de crear recursos en AWS:  </p>
</li>
</ul>
<pre><code>eksctl create iamserviceaccount \
    --region eu-west-2 \
    --name alb-ingress-controller \
    --namespace kube-system \
    --cluster test-cluster \
    --attach-policy-arn arn:aws:iam::930408735286:policy/ALBIngressControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --approve
</code></pre>

<blockquote>
<p>La policy la vemos en IAM-POLICIES</p>
</blockquote>
<ul>
<li>Resumen: creamos un service account que tiene un clusterrolebinding para ver los permisos de ingress y de balanzador de carga, por esto, de este ultimo, creamos una politica para que pueda crear recursos en AWS y en balanceador de carga.  </li>
</ul>
<h3 id="deploy-ingress-controller-aws">DEPLOY INGRESS CONTROLLER AWS</h3>
<ul>
<li>
<p>Creamos un deployment que crea un pod de ingress controller con una imagen de aws ingress controller que lo que hará es que si ve cambios, los modifica en el balanceador de carga:<br />
<code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml</code>  </p>
</li>
<li>
<p>Cambiamos unas lineas del deploy:<br />
<code>kubectl edit deployment.apps/alb-ingress-controller -n kube-system</code>  </p>
</li>
</ul>
<pre><code>spec:
      containers:
      - args:
        - --ingress-class=alb
        - --cluster-name=test-cluster
</code></pre>

<ul>
<li>Comprobamos que esto funciona:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
alb-ingress-controller-868ddb9874-gzsvx   1/1     Running   0          41s
aws-node-gcd69                            1/1     Running   0          35m
coredns-6ddcfb5bcf-h7qrx                  1/1     Running   0          48m
coredns-6ddcfb5bcf-t7wnz                  1/1     Running   0          48m
kube-proxy-jdnj5                          1/1     Running   0          35m
</code></pre>

<h3 id="deploy-app">DEPLOY APP</h3>
<ul>
<li>Creamos el ejemplo de aplicación que es un juego, creamos un servicio, un deploy y namespaces:  </li>
</ul>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml
</code></pre>

<ul>
<li>Comprobamos:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get all -n 2048-game
NAME                                  READY   STATUS    RESTARTS   AGE
pod/2048-deployment-dd74cc68d-88w46   1/1     Running   0          29s
pod/2048-deployment-dd74cc68d-gc9pp   1/1     Running   0          29s
pod/2048-deployment-dd74cc68d-lw72w   1/1     Running   0          29s
pod/2048-deployment-dd74cc68d-wk8tp   1/1     Running   0          29s
pod/2048-deployment-dd74cc68d-zlshx   1/1     Running   0          29s
#
NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/service-2048   NodePort   10.100.179.203   &lt;none&gt;        80:30798/TCP   20s
#
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/2048-deployment   5/5     5            5           30s
#
NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/2048-deployment-dd74cc68d   5         5         5       30s
</code></pre>

<ul>
<li>Para comprobar que funciona la app internamente usamos:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl port-forward pod/2048-deployment-dd74cc68d-88w46 -n 2048-game 7000:80
Forwarding from 127.0.0.1:7000 -&gt; 80
Forwarding from [::1]:7000 -&gt; 80
</code></pre>

<p><img alt="" src="../images/kubernetes22.png" />  </p>
<h3 id="exponer-la-app-externamente">EXPONER LA APP EXTERNAMENTE</h3>
<ul>
<li>Enrutamos con el ingress la app:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get ingress -n 2048-game
NAME           HOSTS   ADDRESS                                                                 PORTS   AGE
2048-ingress   *       d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com   80      14s
</code></pre>

<ul>
<li>Si vamos a nuestro EC2 de amazon. a nuestro balanceador de carga veremos que nos sale la url en la que podemos ir a la aplicación ya que la regla estaba asignada.  </li>
</ul>
<p><img alt="" src="../images/kubernetes23.png" />  </p>
<h3 id="modificando-reglas-ingress">MODIFICANDO REGLAS INGRESS</h3>
<ul>
<li>Vemos que IPs apuntan al balanceador de carga que nos da la url del juego:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ nslookup d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com
Server:     192.168.1.1
Address:    192.168.1.1#53
Non-authoritative answer:
Name:   d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com
Address: 18.134.190.250
Name:   d7f12bb1-2048game-2048ingr-6fa0-882565039.eu-west-2.elb.amazonaws.com
Address: 18.133.107.232
</code></pre>

<ul>
<li>Las añadimos a nuestro /etc/hosts:  </li>
</ul>
<pre><code>18.134.190.250 app.aws.game.test
18.133.107.232 app.aws.game.test
</code></pre>

<ul>
<li>Cambiamos reglas para que utilicen el nombre y no la ip ni dns:  </li>
</ul>
<pre><code>kubectl edit ingress 2048-ingress -n 2048-game
spec:
  rules:
  - host: app.aws.game.test
    http:
      paths:
      - path: /*
        backend:
          serviceName: service-2048
          servicePort: 80
</code></pre>

<blockquote>
<p>Ahora entraremos solo por nombre  </p>
</blockquote>
<p><img alt="" src="../images/kubernetes24.png" />  </p>
<h3 id="borrar-todo">BORRAR TODO</h3>
<ul>
<li>Borramos todo y vemos que no hay el balanceador de carga:  </li>
</ul>
<pre><code>kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml
</code></pre>

<p><img alt="" src="../images/kubernetes25.png" />  </p>
<h3 id="aws-hpa-install">AWS HPA INSTALL</h3>
<ul>
<li>
<p>HPA(Horizontal Pod Autoescaler) consulta unas metricas y se asocia a un deployment.  </p>
</li>
<li>
<p>Basado a unas metricas dice cuanta cantidad de pods creas, segun la carga que se pueda ir soportando. Solo escala por CPU.  </p>
</li>
<li>
<p>Se ha de instalar el <code>Metrics Server</code>:<br />
<code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml</code>  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get deployment metrics-server -n kube-system
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   1/1     1            1           6s
</code></pre>

<h3 id="crear-un-hpa">CREAR UN HPA</h3>
<ul>
<li>Ejemplo de una app:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl apply -f https://k8s.io/examples/application/php-apache.yaml
deployment.apps/php-apache created
service/php-apache created
[isx46410800@miguel ~]$ kubectl get deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   0/1     1            0           15s
[isx46410800@miguel ~]$ kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.100.0.1       &lt;none&gt;        443/TCP   105m
php-apache   ClusterIP   10.100.137.253   &lt;none&gt;        80/TCP    19s
[isx46410800@miguel ~]$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
php-apache-79544c9bd9-h2xhh   1/1     Running   0          50s
</code></pre>

<ul>
<li>
<p>Ahora Escalamos. Esto quiere decir que si la carga pasa del 50% creee pods hasta un maximo de 10 pods:<br />
<code>kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</code>  </p>
</li>
<li>
<p>Comprobamos con <code>kubectl get hpa</code>:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get hpa
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        1          74s
[isx46410800@miguel ~]$ kubectl get hpa -o yaml
apiVersion: v1
items:
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
</code></pre>

<h3 id="autoescalar-hpa">AUTOESCALAR HPA</h3>
<ul>
<li>
<p>Nuestra maquina de AWS es un t3.medium y tiene 2 cpus y 4 de ram.  </p>
</li>
<li>
<p>Creamos un container y dentro de el le hacemos muchas peticiones, veremos como se va cargando y se van creando pods para balancear esta carga:  </p>
</li>
</ul>
<pre><code>kubectl run -it --rm load-generator --image=busybox /bin/sh --generator=run-pod/v1
# while true; do wget -q -O- http://php-apache; done
</code></pre>

<ul>
<li>Vemos los pods y los hpa:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get pods -w
NAME                          READY   STATUS    RESTARTS   AGE
apache-bench                  1/1     Running   1          21m
httpd                         1/1     Running   0          2m10s
load-generator                1/1     Running   0          20s
php-apache-79544c9bd9-cnq6h   1/1     Running   0          43s
php-apache-79544c9bd9-xs4tl   0/1     Pending   0          0s
php-apache-79544c9bd9-xs4tl   0/1     Pending   0          0s
php-apache-79544c9bd9-ckcgb   0/1     Pending   0          0s
php-apache-79544c9bd9-m29bz   0/1     Pending   0          0s
php-apache-79544c9bd9-ckcgb   0/1     Pending   0          0s
php-apache-79544c9bd9-m29bz   0/1     Pending   0          0s
php-apache-79544c9bd9-xs4tl   0/1     ContainerCreating   0          0s
php-apache-79544c9bd9-ckcgb   0/1     ContainerCreating   0          0s
php-apache-79544c9bd9-m29bz   0/1     ContainerCreating   0          0s
#
[isx46410800@miguel ~]$ kubectl get hpa -w
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        1          35m
php-apache   Deployment/php-apache   54%/50%   1         10        1          35m

php-apache   Deployment/php-apache   250%/50%   1         10        1          36m
php-apache   Deployment/php-apache   250%/50%   1         10        4          36m
php-apache   Deployment/php-apache   250%/50%   1         10        5          36m
php-apache   Deployment/php-apache   74%/50%    1         10        5          37m
php-apache   Deployment/php-apache   74%/50%    1         10        8          37m
php-apache   Deployment/php-apache   68%/50%    1         10        8          38m
php-apache   Deployment/php-apache   68%/50%    1         10        8          39m
php-apache   Deployment/php-apache   0%/50%     1         10        8          40m
</code></pre>

<h3 id="cluster-autoscaler">CLUSTER AUTOSCALER</h3>
<ul>
<li>
<p>Se dispara cuando el HPA dispara pods y no hay nodos donde colocarlos. Entonces se autoescala en nodos para ponerlos.  </p>
</li>
<li>
<p>Se dispara cuando desde fuera se hace un deploy y se llena el nodo. Si se dispara otro deploy, como no hay espacio, el cluster autoescaler crea otro nodo para poner los pods que falten por poner.  </p>
</li>
<li>
<p>La politica que se tiene que agregar al cluster de cluster autoscale se crea de por sí cuando creamos el cluster con la herramienta eksctl con la opcion --asg-access.  </p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">DOCS autoscaler</a>  </p>
</li>
<li>
<p>Trabaja como otro pod corriendo en mi cluster.  </p>
</li>
<li>
<p>Lo desplegamos:  </p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
serviceaccount/cluster-autoscaler created
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
deployment.apps/cluster-autoscaler created
#
[isx46410800@miguel ~]$ kubectl get deploy -n kube-system
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
alb-ingress-controller   1/1     1            1           5h18m
cluster-autoscaler       1/1     1            1           13s
coredns                  2/2     2            2           6h2m
metrics-server           1/1     1            1           4h25m
</code></pre>

<ul>
<li>Editamos el deploy:  </li>
</ul>
<pre><code>kubectl -n kube-system edit deploy cluster-autoscaler
- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/test-cluster
- --balance-similar-node-groups
- --skip-nodes-with-system-pods=false
</code></pre>

<ul>
<li>
<p>Borramos el HPA para que no haya conflictos.</p>
</li>
<li>
<p>Editamos el deploy y ponemos 3 replicas:</p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   1/1     1            1           3h55m
[isx46410800@miguel ~]$ kubectl edit deploy php-apache
deployment.apps/php-apache edited
#
[isx46410800@miguel ~]$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
php-apache-79544c9bd9-6zqcc   1/1     Running   0          5s
php-apache-79544c9bd9-cnq6h   1/1     Running   0          3h56m
php-apache-79544c9bd9-pfsrq   1/1     Running   0          5s
</code></pre>

<ul>
<li>
<p>Si editamos el deploy y añadimos mas replicas, veremos que se nos crean varias maquinas, varios nodes.<br />
<code>kubectl edit deploy php-apache</code>  </p>
</li>
<li>
<p>Comprobamos:</p>
</li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
php-apache-79544c9bd9-5slhp   1/1     Running   0          114s
php-apache-79544c9bd9-6zqcc   1/1     Running   0          6m59s
php-apache-79544c9bd9-cnq6h   1/1     Running   0          4h3m
php-apache-79544c9bd9-dlmrz   1/1     Running   0          114s
php-apache-79544c9bd9-dq8f2   1/1     Running   0          3m29s
php-apache-79544c9bd9-hbxnr   1/1     Running   0          3m29s
php-apache-79544c9bd9-n594l   1/1     Running   0          114s
php-apache-79544c9bd9-pfsrq   1/1     Running   0          6m59s
php-apache-79544c9bd9-pv5cl   1/1     Running   0          114s
php-apache-79544c9bd9-pzz4w   1/1     Running   0          114s
php-apache-79544c9bd9-x4czh   1/1     Running   0          4m19s
php-apache-79544c9bd9-zm7fj   1/1     Running   0          114s
#
[isx46410800@miguel ~]$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-192-168-22-127.eu-west-2.compute.internal   Ready    &lt;none&gt;   41s     v1.17.11-eks-cfdc40
ip-192-168-38-128.eu-west-2.compute.internal   Ready    &lt;none&gt;   6h11m   v1.17.11-eks-cfdc40
</code></pre>

<p><img alt="" src="../images/kubernetes26.png" /><br />
<img alt="" src="../images/kubernetes27.png" />  </p>
<ul>
<li>Ahora comprobamos que cuando no usa un nodo, el autoscale lo elimine automaticamente y va pasando pods a un solo nodo y dejar el minimo de maquinas running:  </li>
</ul>
<pre><code>[isx46410800@miguel ~]$ kubectl edit deploy php-apache
deployment.apps/php-apache edited
#
[isx46410800@miguel ~]$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
php-apache-79544c9bd9-5slhp   1/1     Running   0          5m47s
php-apache-79544c9bd9-dlmrz   1/1     Running   0          5m47s
php-apache-79544c9bd9-n594l   1/1     Running   0          5m47s
php-apache-79544c9bd9-pv5cl   1/1     Running   0          5m47s
php-apache-79544c9bd9-pzz4w   1/1     Running   0          5m47s
#
[isx46410800@miguel ~]$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-192-168-38-128.eu-west-2.compute.internal   Ready    &lt;none&gt;   6h14m   v1.17.11-eks-cfdc40
</code></pre>

<h3 id="eliminamos-todo-de-la-nube">ELIMINAMOS TODO DE LA NUBE</h3>
<ul>
<li>Vamos a AWS - CLOUD FORMATION y eliminamos todo.  </li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../openshift/" class="btn btn-neutral float-right" title="Openshift">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../docker/" class="btn btn-neutral" title="Docker"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../docker/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../openshift/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
